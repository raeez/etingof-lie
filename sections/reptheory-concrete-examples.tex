\documentclass[etingof-lie.tex]{subfiles}
\begin{document}
\section{Representation theory: concrete examples}

\subsection{Some lemmata about exponentials and commutators}

This section is devoted to some elementary lemmata about power series and
iterated commutators over noncommutative rings. These lemmata are well-known
in geometrical contexts (in these contexts they tend to appear in Lie groups
textbooks), but here we will formulate and prove them purely algebraically. We
will not use these lemmata until Theorem \ref{thm.euler}, but I prefer to put
them here in order not to interrupt the flow of representation-theoretical
arguments later.

We start with easy things:

\begin{lemma}
\label{lem.powerseries1}Let $K$ be a commutative ring. If $\alpha$ and $\beta$
are two elements of a topological $K$-algebra $R$ such that $\left[
\alpha,\beta\right]  $ commutes with $\beta$, then $\left[  \alpha,P\left(
\beta\right)  \right]  =\left[  \alpha,\beta\right]  \cdot P^{\prime}\left(
\beta\right)  $ for every power series $P\in K\left[  \left[  X\right]
\right]  $ for which the series $P\left(  \beta\right)  $ and $P^{\prime
}\left(  \beta\right)  $ converge.
\end{lemma}

\textit{Proof of Lemma \ref{lem.powerseries1}.} Let $\gamma=\left[
\alpha,\beta\right]  $. Then, $\gamma$ commutes with $\beta$ (since we know
that $\left[  \alpha,\beta\right]  $ commutes with $\beta$), so that
$\gamma\beta=\beta\gamma$.

Write $P$ in the form $P=\sum\limits_{i=0}^{\infty}u_{i}X^{i}$ for some
$\left(  u_{0},u_{1},u_{2},...\right)  \in K^{\mathbb{N}}$. Then, $P^{\prime
}=\sum\limits_{i=1}^{\infty}iu_{i}X^{i-1}$, so that $P^{\prime}\left(
\beta\right)  =\sum\limits_{i=1}^{\infty}iu_{i}\beta^{i-1}$. On the other
hand, $P=\sum\limits_{i=0}^{\infty}u_{i}X^{i}$ shows that $P\left(
\beta\right)  =\sum\limits_{i=0}^{\infty}u_{i}\beta^{i}$ and thus
\[
\left[  \alpha,P\left(  \beta\right)  \right]  =\left[  \alpha,\sum
\limits_{i=0}^{\infty}u_{i}\beta^{i}\right]  =\sum\limits_{i=0}^{\infty}%
u_{i}\left[  \alpha,\beta^{i}\right]  =u_{0}\underbrace{\left[  \alpha
,\beta^{0}\right]  }_{\substack{=0\\\text{(since }\beta^{0}=1\in Z\left(
R\right)  \text{)}}}+\sum\limits_{i=1}^{\infty}u_{i}\left[  \alpha,\beta
^{i}\right]  =\sum\limits_{i=1}^{\infty}u_{i}\left[  \alpha,\beta^{i}\right]
.
\]


Now, it is easy to prove that every positive $i\in\mathbb{N}$ satisfies
$\left[  \alpha,\beta^{i}\right]  =i\gamma\beta^{i-1}$%
\ \ \ \ \footnote{\textit{Proof.} We will prove this by induction over $i$:
\par
\textit{Induction base:} For $i=1$, we have $\left[  \alpha,\beta^{i}\right]
=\left[  \alpha,\beta^{1}\right]  =\left[  \alpha,\beta\right]  =\gamma$ and
$\underbrace{i}_{=1}\gamma\underbrace{\beta^{i-1}}_{=\beta^{1-1}=1}=\gamma$,
so that $\left[  \alpha,\beta^{i}\right]  =\gamma=i\gamma\beta^{i-1}$. This
proves $\left[  \alpha,\beta^{i}\right]  =i\gamma\beta^{i-1}$ for $i=1$, and
thus the induction base is complete.
\par
\textit{Induction step:} Let $j\in\mathbb{N}$ be positive. Assume that
$\left[  \alpha,\beta^{i}\right]  =i\gamma\beta^{i-1}$ is proven for $i=j$. We
must then prove $\left[  \alpha,\beta^{i}\right]  =i\gamma\beta^{i-1}$ for
$i=j+1$.
\par
Since $\left[  \alpha,\beta^{i}\right]  =i\gamma\beta^{i-1}$ is proven for
$i=j$, we have $\left[  \alpha,\beta^{j}\right]  =j\gamma\beta^{j-1}$.
\par
Now,
\begin{align*}
\left[  \alpha,\underbrace{\beta^{j+1}}_{=\beta\beta^{j}}\right]   &  =\left[
\alpha,\beta\beta^{j}\right]  =\alpha\beta\beta^{j}-\beta\beta^{j}%
\alpha=\underbrace{\left(  \alpha\beta\beta^{j}-\beta\alpha\beta^{j}\right)
}_{=\left(  \alpha\beta-\beta\alpha\right)  \beta^{j}}+\underbrace{\left(
\beta\alpha\beta^{j}-\beta\beta^{j}\alpha\right)  }_{=\beta\left(  \alpha
\beta^{j}-\beta^{j}\alpha\right)  }\\
&  =\underbrace{\left(  \alpha\beta-\beta\alpha\right)  }_{=\left[
\alpha,\beta\right]  =\gamma}\beta^{j}+\beta\underbrace{\left(  \alpha
\beta^{j}-\beta^{j}\alpha\right)  }_{=\left[  \alpha,\beta^{j}\right]
=j\gamma\beta^{j-1}}=\gamma\beta^{j}+\beta j\gamma\beta^{j-1}=\gamma\beta
^{j}+j\underbrace{\beta\gamma}_{=\gamma\beta}\beta^{j-1}\\
&  =\gamma\beta^{j}+j\gamma\underbrace{\beta\beta^{j-1}}_{=\beta^{j}}%
=\gamma\beta^{j}+j\gamma\beta^{j}=\left(  j+1\right)  \gamma\beta^{j}=\left(
j+1\right)  \gamma\beta^{\left(  j+1\right)  -1}.
\end{align*}
In other words, $\left[  \alpha,\beta^{i}\right]  =i\gamma\beta^{i-1}$ holds
for $i=j+1$. This completes the induction step, and thus by induction we have
proven that $\left[  \alpha,\beta^{i}\right]  =i\gamma\beta^{i-1}$ for every
positive $i\in\mathbb{N}$.}. Hence,%
\[
\left[  \alpha,P\left(  \beta\right)  \right]  =\sum\limits_{i=1}^{\infty
}u_{i}\underbrace{\left[  \alpha,\beta^{i}\right]  }_{=i\gamma\beta^{i-1}%
}=\sum\limits_{i=1}^{\infty}u_{i}i\gamma\beta^{i-1}=\underbrace{\gamma
}_{=\left[  \alpha,\beta\right]  }\underbrace{\sum\limits_{i=1}^{\infty}%
iu_{i}\beta^{i-1}}_{=P^{\prime}\left(  \beta\right)  }=\left[  \alpha
,\beta\right]  \cdot P^{\prime}\left(  \beta\right)  .
\]
Lemma \ref{lem.powerseries1} is proven.

\begin{corollary}
\label{cor.powerseries2}If $\alpha$ and $\beta$ are two elements of a
topological $\mathbb{Q}$-algebra $R$ such that $\left[  \alpha,\beta\right]  $
commutes with $\beta$, then $\left[  \alpha,\exp\beta\right]  =\left[
\alpha,\beta\right]  \cdot\exp\beta$ whenever the power series $\exp\beta$ converges.
\end{corollary}

\textit{Proof of Corollary \ref{cor.powerseries2}.} Applying Lemma
\ref{lem.powerseries1} to $P=\exp X$ and $K=\mathbb{Q}$, and recalling that
$\exp^{\prime}=\exp$, we obtain $\left[  \alpha,\exp\beta\right]  =\left[
\alpha,\beta\right]  \cdot\exp\beta$. This proves Corollary
\ref{cor.powerseries2}.

In Lemma \ref{lem.powerseries1} and Corollary \ref{cor.powerseries2}, we had
to require convergence of certain power series in order for the results to
make sense. In the following, we will prove some results for which such
requirements are not sufficient anymore\footnote{At least they are not
sufficient for my proofs...}; instead we need more global conditions. A
standard condition to require in such cases is that all the elements to which
we apply power series lie in some ideal $I$ of $R$ such that $R$ is complete
and Hausdorff with respect to the $I$-adic topology. Under this condition,
things work nicely, due to the following fact (which is one part of the
universal property of the power series ring $K\left[  \left[  X\right]
\right]  $):

\begin{proposition}
\label{prop.K[[X]].univ}Let $K$ be a commutative ring. Let $R$ be a
$K$-algebra, and $I$ be an ideal of $R$ such that $R$ is complete and
Hausdorff with respect to the $I$-adic topology. Then, for every power series
$P\in K\left[  \left[  X\right]  \right]  $ and every $\alpha\in I$, there is
a well-defined element $P\left(  \alpha\right)  \in R$ (which is defined as
the limit $\lim\limits_{n\rightarrow\infty}\sum\limits_{i=0}^{n}u_{i}%
\alpha^{i}$ (with respect to the $I$-adic topology), where the power series
$P$ is written in the form $P=\sum\limits_{i=0}^{\infty}u_{i}X^{i}$ for some
$\left(  u_{0},u_{1},u_{2},...\right)  \in K^{\mathbb{N}}$). For every
$\alpha\in I$, the map $K\left[  \left[  X\right]  \right]  \rightarrow R$
which sends every $P\in K\left[  \left[  X\right]  \right]  $ to $P\left(
\alpha\right)  $ is a continuous $K$-algebra homomorphism (where the topology
on $K\left[  \left[  X\right]  \right]  $ is the standard one, and the
topology on $R$ is the $I$-adic one).
\end{proposition}

\begin{theorem}
\label{thm.exp(u+v)}Let $R$ be a $\mathbb{Q}$-algebra, and let $I$ be an ideal
of $R$ such that $R$ is complete and Hausdorff with respect to the $I$-adic
topology. Let $\alpha\in I$ and $\beta\in I$ be such that $\alpha\beta
=\beta\alpha$. Then, $\exp\alpha$, $\exp\beta$ and $\exp\left(  \alpha
+\beta\right)  $ are well-defined (by Proposition \ref{prop.K[[X]].univ}) and
satisfy $\exp\left(  \alpha+\beta\right)  =\left(  \exp\alpha\right)
\cdot\left(  \exp\beta\right)  $.
\end{theorem}

\textit{Proof of Theorem \ref{thm.exp(u+v)}.} We know that $\alpha\beta
=\beta\alpha$. That is, $\alpha$ and $\beta$ commute, so that we can apply the
binomial formula to $\alpha$ and $\beta$.

Comparing%
\[
\exp\left(  \alpha+\beta\right)  =\sum\limits_{n=0}^{\infty}\dfrac{\left(
\alpha+\beta\right)  ^{n}}{n!}=\sum\limits_{n=0}^{\infty}\dfrac{1}%
{n!}\underbrace{\left(  \alpha+\beta\right)  ^{n}}_{\substack{=\sum
\limits_{i=0}^{n}\dbinom{n}{i}\alpha^{i}\beta^{n-i}\\\text{(by the binomial
formula,}\\\text{since }\alpha\text{ and }\beta\text{ commute)}}%
}=\sum\limits_{n=0}^{\infty}\dfrac{1}{n!}\sum\limits_{i=0}^{n}\dbinom{n}%
{i}\alpha^{i}\beta^{n-i}%
\]
with%
\begin{align*}
\underbrace{\left(  \exp\alpha\right)  }_{=\sum\limits_{i=0}^{\infty}%
\dfrac{\alpha^{i}}{i!}}\cdot\underbrace{\left(  \exp\beta\right)  }%
_{=\sum\limits_{j=0}^{\infty}\dfrac{\beta^{j}}{j!}}  &  =\left(
\sum\limits_{i=0}^{\infty}\dfrac{\alpha^{i}}{i!}\right)  \cdot\left(
\sum\limits_{j=0}^{\infty}\dfrac{\beta^{j}}{j!}\right)  =\sum\limits_{i=0}%
^{\infty}\sum\limits_{j=0}^{\infty}\dfrac{\alpha^{i}\beta^{j}}{i!j!}%
=\sum\limits_{i=0}^{\infty}\sum\limits_{j=0}^{\infty}\dfrac{1}{i!j!}\alpha
^{i}\beta^{j}\\
&  =\underbrace{\sum\limits_{i=0}^{\infty}\sum\limits_{n=i}^{\infty}}%
_{=\sum\limits_{n=0}^{\infty}\sum\limits_{i=0}^{n}}\underbrace{\dfrac
{1}{i!\left(  n-i\right)  !}}_{\substack{=\dfrac{1}{n!}\dbinom{n}%
{i}\\\text{(since }\dbinom{n}{i}=\dfrac{n!}{i!\left(  n-i\right)  !}\text{)}%
}}\alpha^{i}\beta^{n-i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }n\text{ for
}i+j\text{ in the second sum}\right) \\
&  =\sum\limits_{n=0}^{\infty}\sum\limits_{i=0}^{n}\dfrac{1}{n!}\dbinom{n}%
{i}\alpha^{i}\beta^{n-i}=\sum\limits_{n=0}^{\infty}\dfrac{1}{n!}%
\sum\limits_{i=0}^{n}\dbinom{n}{i}\alpha^{i}\beta^{n-i},
\end{align*}
we obtain $\exp\left(  \alpha+\beta\right)  =\left(  \exp\alpha\right)
\cdot\left(  \exp\beta\right)  $. This proves Theorem \ref{thm.exp(u+v)}.

\begin{corollary}
\label{cor.exp(-w)}Let $R$ be a $\mathbb{Q}$-algebra, and let $I$ be an ideal
of $R$ such that $R$ is complete and Hausdorff with respect to the $I$-adic
topology. Let $\gamma\in I$. Then, $\exp\gamma$ and $\exp\left(
-\gamma\right)  $ are well-defined (by Proposition \ref{prop.K[[X]].univ}) and
satisfy $\left(  \exp\gamma\right)  \cdot\left(  \exp\left(  -\gamma\right)
\right)  =1$.
\end{corollary}

\textit{Proof of Corollary \ref{cor.exp(-w)}.} By Theorem \ref{thm.exp(u+v)}
(applied to $\alpha=\gamma$ and $\beta=-\gamma$), we have $\exp\left(
\gamma+\left(  -\gamma\right)  \right)  =\left(  \exp\gamma\right)
\cdot\left(  \exp\left(  -\gamma\right)  \right)  $, thus%
\[
\left(  \exp\gamma\right)  \cdot\left(  \exp\left(  -\gamma\right)  \right)
=\exp\underbrace{\left(  \gamma+\left(  -\gamma\right)  \right)  }_{=0}%
=\exp0=1.
\]
This proves Corollary \ref{cor.exp(-w)}.

\begin{theorem}
\label{thm.exp(a)bexp(-a)}Let $R$ be a $\mathbb{Q}$-algebra, and let $I$ be an
ideal of $R$ such that $R$ is complete and Hausdorff with respect to the
$I$-adic topology. Let $\alpha\in I$. Denote by $\operatorname*{ad}\alpha$ the
map $R\rightarrow R,\ x\mapsto\left[  \alpha,x\right]  $ (where $\left[
\alpha,x\right]  $ denotes the commutator $\alpha x-x\alpha$).

\textbf{(a)} Then, the infinite series $\sum\limits_{n=0}^{\infty}%
\dfrac{\left(  \operatorname*{ad}\alpha\right)  ^{n}}{n!}$ converges pointwise
(i. e., for every $x\in R$, the infinite series $\sum\limits_{n=0}^{\infty
}\dfrac{\left(  \operatorname*{ad}\alpha\right)  ^{n}}{n!}\left(  x\right)  $
converges). Denote the value of this series by $\exp\left(  \operatorname*{ad}%
\alpha\right)  $.

\textbf{(b)} We have $\left(  \exp\alpha\right)  \cdot\beta\cdot\left(
\exp\left(  -\alpha\right)  \right)  =\left(  \exp\left(  \operatorname*{ad}%
\alpha\right)  \right)  \left(  \beta\right)  $ for every $\beta\in R$.
\end{theorem}

To prove this, we will use a lemma:

\begin{lemma}
\label{lem.exp(a)bexp(-a)}Let $R$ be a ring. Let $\alpha$ and $\beta$ be
elements of $R$. Denote by $\operatorname*{ad}\alpha$ the map $R\rightarrow
R,\ x\mapsto\left[  \alpha,x\right]  $ (where $\left[  \alpha,x\right]  $
denotes the commutator $\alpha x-x\alpha$). Let $n\in\mathbb{N}$. Then,
\[
\left(  \operatorname*{ad}\alpha\right)  ^{n}\left(  \beta\right)
=\sum\limits_{i=0}^{n}\dbinom{n}{i}\alpha^{i}\beta\left(  -\alpha\right)
^{n-i}.
\]

\end{lemma}

\textit{Proof of Lemma \ref{lem.exp(a)bexp(-a)}.} Let $L_{\alpha}$ denote the
map $R\rightarrow R,$ $x\mapsto\alpha x$. Let $R_{\alpha}$ denote the map
$R\rightarrow R,\ x\mapsto x\alpha$. Then, every $x\in R$ satisfies%
\[
\left(  L_{\alpha}-R_{\alpha}\right)  \left(  x\right)  =\underbrace{L_{\alpha
}\left(  x\right)  }_{\substack{=\alpha x\\\text{(by the definition of
}L_{\alpha}\text{)}}}-\underbrace{R_{\alpha}\left(  x\right)  }%
_{\substack{=x\alpha\\\text{(by the definition of }R_{\alpha}\text{)}}}=\alpha
x-x\alpha=\left[  \alpha,x\right]  =\left(  \operatorname*{ad}\alpha\right)
\left(  x\right)  .
\]
Hence, $L_{\alpha}-R_{\alpha}=\operatorname*{ad}\alpha$.

Also, every $x\in R$ satisfies%
\[
\left(  L_{\alpha}\circ R_{\alpha}\right)  \left(  x\right)  =L_{\alpha
}\underbrace{\left(  R_{\alpha}\left(  x\right)  \right)  }%
_{\substack{=x\alpha\\\text{(by the definition of }R_{\alpha}\text{)}%
}}=L_{\alpha}\left(  x\alpha\right)  =\alpha x\alpha
\]
(by the definition of $L_{\alpha}$) and%
\[
\left(  R_{\alpha}\circ L_{\alpha}\right)  \left(  x\right)  =R_{\alpha
}\underbrace{\left(  L_{\alpha}\left(  x\right)  \right)  }_{\substack{=\alpha
x\\\text{(by the definition of }L_{\alpha}\text{)}}}=R_{\alpha}\left(  \alpha
x\right)  =\alpha x\alpha
\]
(by the definition of $R_{\alpha}$), so that $\left(  L_{\alpha}\circ
R_{\alpha}\right)  \left(  x\right)  =\left(  R_{\alpha}\circ L_{\alpha
}\right)  \left(  x\right)  $. Hence, $L_{\alpha}\circ R_{\alpha}=R_{\alpha
}\circ L_{\alpha}$. In other words, the maps $L_{\alpha}$ and $R_{\alpha}$
commute. Thus, we can apply the binomial formula to $L_{\alpha}$ and
$R_{\alpha}$, and conclude that $\left(  L_{\alpha}-R_{\alpha}\right)
^{n}=\sum\limits_{i=0}^{n}\left(  -1\right)  ^{n-i}\dbinom{n}{i}L_{\alpha}%
^{i}\circ R_{\alpha}^{n-i}$. Since $L_{\alpha}-R_{\alpha}=\operatorname*{ad}%
\alpha$, this rewrites as $\left(  \operatorname*{ad}\alpha\right)  ^{n}%
=\sum\limits_{i=0}^{n}\left(  -1\right)  ^{n-i}\dbinom{n}{i}L_{\alpha}%
^{i}\circ R_{\alpha}^{n-i}$.

Now, it is easy to see (by induction over $j$) that
\begin{equation}
L_{\alpha}^{j}y=\alpha^{j}y\ \ \ \ \ \ \ \ \ \ \text{for every }j\in
\mathbb{N}\text{ and }y\in R. \label{pf.exp(a)bexp(-a).1}%
\end{equation}
Also, it is easy to see (by induction over $j$) that
\begin{equation}
R_{\alpha}^{j}y=y\alpha^{j}\ \ \ \ \ \ \ \ \ \ \text{for every }j\in
\mathbb{N}\text{ and }y\in R. \label{pf.exp(a)bexp(-a).2}%
\end{equation}


Now, since $\left(  \operatorname*{ad}\alpha\right)  ^{n}=\sum\limits_{i=0}%
^{n}\left(  -1\right)  ^{n-i}\dbinom{n}{i}L_{\alpha}^{i}\circ R_{\alpha}%
^{n-i}$, we have%
\begin{align*}
\left(  \operatorname*{ad}\alpha\right)  ^{n}\left(  \beta\right)   &
=\sum\limits_{i=0}^{n}\left(  -1\right)  ^{n-i}\dbinom{n}{i}%
\underbrace{\left(  L_{\alpha}^{i}\circ R_{\alpha}^{n-i}\right)  \left(
\beta\right)  }_{\substack{=L_{\alpha}^{i}\left(  R_{\alpha}^{n-i}%
\beta\right)  =\alpha^{i}R_{\alpha}^{n-i}\beta\\\text{(by
(\ref{pf.exp(a)bexp(-a).1}), applied to }j=i\text{ and }y=R_{\alpha}%
^{n-i}\beta\text{)}}}\\
&  =\sum\limits_{i=0}^{n}\left(  -1\right)  ^{n-i}\dbinom{n}{i}\alpha
^{i}\underbrace{R_{\alpha}^{n-i}\beta}_{\substack{=\beta\alpha^{n-i}%
\\\text{(by (\ref{pf.exp(a)bexp(-a).2}), applied to }j=n-i\text{ and }%
y=\beta\text{)}}}\\
&  =\sum\limits_{i=0}^{n}\left(  -1\right)  ^{n-i}\dbinom{n}{i}\alpha^{i}%
\beta\alpha^{n-i}=\sum\limits_{i=0}^{n}\dbinom{n}{i}\alpha^{i}\beta\left(
-\alpha\right)  ^{n-i}.
\end{align*}
This proves Lemma \ref{lem.exp(a)bexp(-a)}.

\textit{Proof of Theorem \ref{thm.exp(a)bexp(-a)}.} \textbf{(a)} For every
$x\in R$ and every $n\in\mathbb{N}$, we have $\left(  \operatorname*{ad}%
\alpha\right)  ^{n}\left(  x\right)  \in I^{n}$ (this can be easily proven by
induction over $n$, using the fact that $I$ is an ideal) and thus
$\dfrac{\left(  \operatorname*{ad}\alpha\right)  ^{n}}{n!}\left(  x\right)
=\dfrac{1}{n!}\underbrace{\left(  \operatorname*{ad}\alpha\right)  ^{n}\left(
x\right)  }_{\in I^{n}}\in I^{n}$. Hence, for every $x\in R$, the infinite
series $\sum\limits_{n=0}^{\infty}\dfrac{\left(  \operatorname*{ad}%
\alpha\right)  ^{n}}{n!}\left(  x\right)  $ converges (because $R$ is complete
and Hausdorff with respect to the $I$-adic topology). In other words, the
infinite series $\sum\limits_{n=0}^{\infty}\dfrac{\left(  \operatorname*{ad}%
\alpha\right)  ^{n}}{n!}$ converges pointwise. Theorem
\ref{thm.exp(a)bexp(-a)} \textbf{(a)} is proven.

\textbf{(b)} Let $\beta\in R$. By the definition of of $\exp\left(
\operatorname*{ad}\alpha\right)  $, we have%
\begin{align*}
\left(  \exp\left(  \operatorname*{ad}\alpha\right)  \right)  \left(
\beta\right)   &  =\sum\limits_{n=0}^{\infty}\dfrac{\left(  \operatorname*{ad}%
\alpha\right)  ^{n}}{n!}\left(  \beta\right)  =\sum\limits_{n=0}^{\infty
}\dfrac{1}{n!}\underbrace{\left(  \operatorname*{ad}\alpha\right)  ^{n}\left(
\beta\right)  }_{\substack{=\sum\limits_{i=0}^{n}\dbinom{n}{i}\alpha^{i}%
\beta\left(  -\alpha\right)  ^{n-i}\\\text{(by Lemma \ref{lem.exp(a)bexp(-a)}%
)}}}\\
&  =\sum\limits_{n=0}^{\infty}\dfrac{1}{n!}\sum\limits_{i=0}^{n}\dbinom{n}%
{i}\alpha^{i}\beta\left(  -\alpha\right)  ^{n-i}.
\end{align*}
Compared with%
\begin{align*}
\underbrace{\left(  \exp\alpha\right)  }_{=\sum\limits_{i=0}^{\infty}%
\dfrac{\alpha^{i}}{i!}}\cdot\beta\cdot\underbrace{\left(  \exp\left(
-\alpha\right)  \right)  }_{=\sum\limits_{j=0}^{\infty}\dfrac{\left(
-\alpha\right)  ^{j}}{j!}}  &  =\left(  \sum\limits_{i=0}^{\infty}%
\dfrac{\alpha^{i}}{i!}\right)  \cdot\beta\cdot\left(  \sum\limits_{j=0}%
^{\infty}\dfrac{\left(  -\alpha\right)  ^{j}}{j!}\right) \\
&  =\sum\limits_{i=0}^{\infty}\sum\limits_{j=0}^{\infty}\dfrac{\alpha^{i}%
\beta\left(  -\alpha\right)  ^{j}}{i!j!}=\sum\limits_{i=0}^{\infty}%
\sum\limits_{j=0}^{\infty}\dfrac{1}{i!j!}\alpha^{i}\beta\left(  -\alpha
\right)  ^{j}\\
&  =\underbrace{\sum\limits_{i=0}^{\infty}\sum\limits_{n=i}^{\infty}}%
_{=\sum\limits_{n=0}^{\infty}\sum\limits_{i=0}^{n}}\underbrace{\dfrac
{1}{i!\left(  n-i\right)  !}}_{\substack{=\dfrac{1}{n!}\dbinom{n}%
{i}\\\text{(since }\dbinom{n}{i}=\dfrac{n!}{i!\left(  n-i\right)  !}\text{)}%
}}\alpha^{i}\beta\left(  -\alpha\right)  ^{n-i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }n\text{ for
}i+j\text{ in the second sum}\right) \\
&  =\sum\limits_{n=0}^{\infty}\sum\limits_{i=0}^{n}\dfrac{1}{n!}\dbinom{n}%
{i}\alpha^{i}\beta\left(  -\alpha\right)  ^{n-i}=\sum\limits_{n=0}^{\infty
}\dfrac{1}{n!}\sum\limits_{i=0}^{n}\dbinom{n}{i}\alpha^{i}\beta\left(
-\alpha\right)  ^{n-i},
\end{align*}
this yields $\left(  \exp\alpha\right)  \cdot\beta\cdot\left(  \exp\left(
-\alpha\right)  \right)  =\left(  \exp\left(  \operatorname*{ad}\alpha\right)
\right)  \left(  \beta\right)  $. This proves Theorem \ref{thm.exp(a)bexp(-a)}
\textbf{(b)}.

\begin{corollary}
\label{cor.exp(a)exp(b)exp(-a)}Let $R$ be a $\mathbb{Q}$-algebra, and let $I$
be an ideal of $R$ such that $R$ is complete and Hausdorff with respect to the
$I$-adic topology. Let $\alpha\in I$. Denote by $\operatorname*{ad}\alpha$ the
map $R\rightarrow R,\ x\mapsto\left[  \alpha,x\right]  $ (where $\left[
\alpha,x\right]  $ denotes the commutator $\alpha x-x\alpha$).

As we know from Theorem \ref{thm.exp(a)bexp(-a)} \textbf{(a)}, the infinite
series $\sum\limits_{n=0}^{\infty}\dfrac{\left(  \operatorname*{ad}%
\alpha\right)  ^{n}}{n!}$ converges pointwise. Denote the value of this series
by $\exp\left(  \operatorname*{ad}\alpha\right)  $.

We have $\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)
\cdot\left(  \exp\left(  -\alpha\right)  \right)  =\exp\left(  \left(
\exp\left(  \operatorname*{ad}\alpha\right)  \right)  \left(  \beta\right)
\right)  $ for every $\beta\in I$.
\end{corollary}

\textit{Proof of Corollary \ref{cor.exp(a)exp(b)exp(-a)}.} Corollary
\ref{cor.exp(-w)} (applied to $\gamma=-\alpha$) yields $\left(  \exp\left(
-\alpha\right)  \right)  \cdot\left(  \exp\left(  -\left(  -\alpha\right)
\right)  \right)  =1$. Since $-\left(  -\alpha\right)  =\alpha$, this rewrites
as $\left(  \exp\left(  -\alpha\right)  \right)  \cdot\left(  \exp
\alpha\right)  =1$.

Let $\beta\in I$. Let $T$ denote the map $R\rightarrow R,\ x\mapsto\left(
\exp\alpha\right)  \cdot x\cdot\left(  \exp\left(  -\alpha\right)  \right)  $.
Clearly, this map $T$ is $\mathbb{Q}$-linear. It also satisfies%
\begin{align*}
T\left(  1\right)   &  =\left(  \exp\alpha\right)  \cdot1\cdot\left(
\exp\left(  -\alpha\right)  \right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the
definition of }T\right) \\
&  =\left(  \exp\alpha\right)  \cdot\left(  \exp\left(  -\alpha\right)
\right)  =1,
\end{align*}
and any $x\in R$ and $y\in R$ satisfy%
\begin{align*}
\underbrace{T\left(  x\right)  }_{\substack{=\left(  \exp\alpha\right)  \cdot
x\cdot\left(  \exp\left(  -\alpha\right)  \right)  \\\left(  \text{by the
definition of }T\right)  }}\cdot\underbrace{T\left(  y\right)  }%
_{\substack{=\left(  \exp\alpha\right)  \cdot y\cdot\left(  \exp\left(
-\alpha\right)  \right)  \\\left(  \text{by the definition of }T\right)  }}
&  =\left(  \exp\alpha\right)  \cdot x\cdot\underbrace{\left(  \exp\left(
-\alpha\right)  \right)  \cdot\left(  \exp\alpha\right)  }_{=1}\cdot
y\cdot\left(  \exp\left(  -\alpha\right)  \right) \\
&  =\left(  \exp\alpha\right)  \cdot xy\cdot\left(  \exp\left(  -\alpha
\right)  \right)  =T\left(  xy\right)
\end{align*}
(since $T\left(  xy\right)  =\left(  \exp\alpha\right)  \cdot xy\cdot\left(
\exp\left(  -\alpha\right)  \right)  $ by the definition of $T$). Hence, $T$
is a $\mathbb{Q}$-algebra homomorphism. Also, $T$ is continuous (with respect
to the $I$-adic topology). Thus, $T$ is a continuous $\mathbb{Q}$-algebra
homomorphism, and hence commutes with the application of power series. Thus,
$T\left(  \exp\beta\right)  =\exp\left(  T\left(  \beta\right)  \right)  $.
But since $T\left(  \exp\beta\right)  =\left(  \exp\alpha\right)  \cdot\left(
\exp\beta\right)  \cdot\left(  \exp\left(  -\alpha\right)  \right)  $ (by the
definition of $T$) and%
\begin{align*}
T\left(  \beta\right)   &  =\left(  \exp\alpha\right)  \cdot\beta\cdot\left(
\exp\left(  -\alpha\right)  \right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the
definition of }T\right) \\
&  =\left(  \exp\left(  \operatorname*{ad}\alpha\right)  \right)  \left(
\beta\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem
\ref{thm.exp(a)bexp(-a)} \textbf{(b)}}\right)  ,
\end{align*}
this rewrites as $\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)
\cdot\left(  \exp\left(  -\alpha\right)  \right)  =\exp\left(  \left(
\exp\left(  \operatorname*{ad}\alpha\right)  \right)  \left(  \beta\right)
\right)  $. This proves Corollary \ref{cor.exp(a)exp(b)exp(-a)}.

\begin{lemma}
\label{lem.powerseries3}Let $R$ be a $\mathbb{Q}$-algebra, and let $I$ be an
ideal of $R$ such that $R$ is complete and Hausdorff with respect to the
$I$-adic topology. Let $\alpha\in I$ and $\beta\in I$. Assume that $\left[
\alpha,\beta\right]  $ commutes with each of $\alpha$ and $\beta$. Then,
$\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  =\left(  \exp
\beta\right)  \cdot\left(  \exp\alpha\right)  \cdot\left(  \exp\left[
\alpha,\beta\right]  \right)  $.
\end{lemma}

First we give two short proofs of this lemma.

\textit{First proof of Lemma \ref{lem.powerseries3}.} Define the map
$\operatorname*{ad}\alpha$ as in Corollary \ref{cor.exp(a)exp(b)exp(-a)}.
Then, $\left(  \operatorname*{ad}\alpha\right)  ^{2}\left(  \beta\right)
=\left[  \alpha,\left[  \alpha,\beta\right]  \right]  =0$ (since $\left[
\alpha,\beta\right]  $ commutes with $\alpha$). Hence, $\left(
\operatorname*{ad}\alpha\right)  ^{n}\left(  \beta\right)  =0$ for every
integer $n\geq2$. Now, by the definition of $\exp\left(  \operatorname*{ad}%
\alpha\right)  $, we have%
\begin{align*}
\left(  \exp\left(  \operatorname*{ad}\alpha\right)  \right)  \left(
\beta\right)   &  =\sum\limits_{n=0}^{\infty}\dfrac{\left(  \operatorname*{ad}%
\alpha\right)  ^{n}}{n!}\left(  \beta\right)  =\sum\limits_{n=0}^{\infty
}\dfrac{1}{n!}\left(  \operatorname*{ad}\alpha\right)  ^{n}\left(
\beta\right) \\
&  =\underbrace{\dfrac{1}{0!}}_{=1}\underbrace{\left(  \operatorname*{ad}%
\alpha\right)  ^{0}}_{=\operatorname*{id}}\left(  \beta\right)
+\underbrace{\dfrac{1}{1!}}_{=1}\underbrace{\left(  \operatorname*{ad}%
\alpha\right)  ^{1}}_{=\operatorname*{ad}\alpha}\left(  \beta\right)
+\sum\limits_{n=2}^{\infty}\dfrac{1}{n!}\underbrace{\left(  \operatorname*{ad}%
\alpha\right)  ^{n}\left(  \beta\right)  }_{\substack{=0\\\text{(since }%
n\geq2\text{)}}}\\
&  =\underbrace{\operatorname*{id}\left(  \beta\right)  }_{=\beta
}+\underbrace{\left(  \operatorname*{ad}\alpha\right)  \left(  \beta\right)
}_{=\left[  \alpha,\beta\right]  }+\underbrace{\sum\limits_{n=2}^{\infty
}\dfrac{1}{n!}0}_{=0}=\beta+\left[  \alpha,\beta\right]  .
\end{align*}
By Corollary \ref{cor.exp(a)exp(b)exp(-a)}, we now have%
\[
\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  \cdot\left(
\exp\left(  -\alpha\right)  \right)  =\exp\underbrace{\left(  \left(
\exp\left(  \operatorname*{ad}\alpha\right)  \right)  \left(  \beta\right)
\right)  }_{=\beta+\left[  \alpha,\beta\right]  }=\exp\left(  \beta+\left[
\alpha,\beta\right]  \right)  .
\]
But $\beta$ and $\left[  \alpha,\beta\right]  $ commute, so that $\beta\left[
\alpha,\beta\right]  =\left[  \alpha,\beta\right]  \beta$. Hence, Theorem
\ref{thm.exp(u+v)} (applied to $\beta$ and $\left[  \alpha,\beta\right]  $
instead of $\alpha$ and $\beta$) yields $\exp\left(  \beta+\left[
\alpha,\beta\right]  \right)  =\left(  \exp\beta\right)  \cdot\left(
\exp\left[  \alpha,\beta\right]  \right)  $.

On the other hand,%
\begin{align*}
\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  \cdot\left(
\exp\left(  -\alpha\right)  \right)  \cdot\left(  \exp\underbrace{\alpha
}_{=-\left(  -\alpha\right)  }\right)   &  =\left(  \exp\alpha\right)
\cdot\left(  \exp\beta\right)  \cdot\underbrace{\left(  \exp\left(
-\alpha\right)  \right)  \cdot\left(  \exp\left(  -\left(  -\alpha\right)
\right)  \right)  }_{\substack{=1\\\text{(by Corollary \ref{cor.exp(-w)},
applied to }\gamma=-\alpha\text{)}}}\\
&  =\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  .
\end{align*}
Compared with%
\[
\underbrace{\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)
\cdot\left(  \exp\left(  -\alpha\right)  \right)  }_{=\exp\left(
\beta+\left[  \alpha,\beta\right]  \right)  =\left(  \exp\beta\right)
\cdot\left(  \exp\left[  \alpha,\beta\right]  \right)  }\cdot\left(
\exp\alpha\right)  =\left(  \exp\beta\right)  \cdot\left(  \exp\left[
\alpha,\beta\right]  \right)  \cdot\left(  \exp\alpha\right)  ,
\]
this yields%
\begin{equation}
\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  =\left(  \exp
\beta\right)  \cdot\left(  \exp\left[  \alpha,\beta\right]  \right)
\cdot\left(  \exp\alpha\right)  . \label{pf.powerseries3.4}%
\end{equation}


Besides, $\alpha$ and $\left[  \alpha,\beta\right]  $ commute, so that
$\alpha\left[  \alpha,\beta\right]  =\left[  \alpha,\beta\right]  \alpha$.
Hence, Theorem \ref{thm.exp(u+v)} (applied to $\left[  \alpha,\beta\right]  $
instead of $\beta$) yields $\exp\left(  \alpha+\left[  \alpha,\beta\right]
\right)  =\left(  \exp\alpha\right)  \cdot\left(  \exp\left[  \alpha
,\beta\right]  \right)  $.

On the other hand, $\alpha$ and $\left[  \alpha,\beta\right]  $ commute, so
that $\left[  \alpha,\beta\right]  \alpha=\alpha\left[  \alpha,\beta\right]
$. Hence, Theorem \ref{thm.exp(u+v)} (applied to $\left[  \alpha,\beta\right]
$ and $\alpha$ instead of $\alpha$ and $\beta$) yields $\exp\left(  \left[
\alpha,\beta\right]  +\alpha\right)  =\left(  \exp\left[  \alpha,\beta\right]
\right)  \cdot\left(  \exp\alpha\right)  $.

Thus, $\left(  \exp\left[  \alpha,\beta\right]  \right)  \cdot\left(
\exp\alpha\right)  =\exp\underbrace{\left(  \left[  \alpha,\beta\right]
+\alpha\right)  }_{=\alpha+\left[  \alpha,\beta\right]  }=\exp\left(
\alpha+\left[  \alpha,\beta\right]  \right)  =\left(  \exp\alpha\right)
\cdot\left(  \exp\left[  \alpha,\beta\right]  \right)  $. Now,
(\ref{pf.powerseries3.4}) becomes%
\[
\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  =\left(  \exp
\beta\right)  \cdot\underbrace{\left(  \exp\left[  \alpha,\beta\right]
\right)  \cdot\left(  \exp\alpha\right)  }_{=\left(  \exp\alpha\right)
\cdot\left(  \exp\left[  \alpha,\beta\right]  \right)  }=\left(  \exp
\beta\right)  \cdot\left(  \exp\alpha\right)  \cdot\left(  \exp\left[
\alpha,\beta\right]  \right)  .
\]
This proves Lemma \ref{lem.powerseries3}.

\textit{Second proof of Lemma \ref{lem.powerseries3}.} Clearly, $\left[
\beta,\alpha\right]  =-\left[  \alpha,\beta\right]  $ commutes with each of
$\alpha$ and $\beta$ (since $\left[  \alpha,\beta\right]  $ commutes with each
of $\alpha$ and $\beta$).

The Baker-Campbell-Hausdorff formula has the form%
\[
\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  =\exp\left(
\alpha+\beta+\dfrac{1}{2}\left[  \alpha,\beta\right]  +\left(  \text{higher
terms}\right)  \right)  ,
\]
where the ``higher terms'' on the right hand side mean $\mathbb{Q}$-linear
combinations of nested Lie brackets of three or more $\alpha$'s and $\beta$'s.
Since $\left[  \alpha,\beta\right]  $ commutes with each of $\alpha$ and
$\beta$, all of these higher terms are zero, and thus the
Baker-Campbell-Hausdorff formula simplifies to%
\begin{equation}
\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  =\exp\left(
\alpha+\beta+\dfrac{1}{2}\left[  \alpha,\beta\right]  \right)  .
\label{pf.powerseries3.6}%
\end{equation}
Applying this to $\beta$ and $\alpha$ instead of $\alpha$ and $\beta$, we
obtain%
\[
\left(  \exp\beta\right)  \cdot\left(  \exp\alpha\right)  =\exp\left(
\beta+\alpha+\dfrac{1}{2}\left[  \beta,\alpha\right]  \right)  .
\]
Since $\left[  \beta,\alpha\right]  =-\left[  \alpha,\beta\right]  $, this
becomes%
\begin{equation}
\left(  \exp\beta\right)  \cdot\left(  \exp\alpha\right)  =\exp\left(
\beta+\alpha+\dfrac{1}{2}\underbrace{\left[  \beta,\alpha\right]  }_{=-\left[
\alpha,\beta\right]  }\right)  =\exp\left(  \beta+\alpha-\dfrac{1}{2}\left[
\alpha,\beta\right]  \right)  . \label{pf.powerseries3.8}%
\end{equation}


Now, $\left[  \alpha,\beta\right]  $ commutes with each of $\alpha$ and
$\beta$ (by the assumptions of the lemma) and also with $\left[  \alpha
,\beta\right]  $ itself (clearly). Hence, $\left[  \alpha,\beta\right]  $
commutes with $\beta+\alpha-\dfrac{1}{2}\left[  \alpha,\beta\right]  $. In
other words, $\left(  \beta+\alpha-\dfrac{1}{2}\left[  \alpha,\beta\right]
\right)  \left[  \alpha,\beta\right]  =\left[  \alpha,\beta\right]  \left(
\beta+\alpha-\dfrac{1}{2}\left[  \alpha,\beta\right]  \right)  $. Hence,
Theorem \ref{thm.exp(u+v)} (applied to $\beta+\alpha-\dfrac{1}{2}\left[
\alpha,\beta\right]  $ and $\left[  \alpha,\beta\right]  $ instead of $\alpha$
and $\beta$) yields $\exp\left(  \beta+\alpha-\dfrac{1}{2}\left[  \alpha
,\beta\right]  +\left[  \alpha,\beta\right]  \right)  =\left(  \exp\left(
\beta+\alpha-\dfrac{1}{2}\left[  \alpha,\beta\right]  \right)  \right)
\cdot\left(  \exp\left[  \alpha,\beta\right]  \right)  $. Now,%
\begin{align*}
\underbrace{\left(  \exp\beta\right)  \cdot\left(  \exp\alpha\right)
}_{\substack{=\exp\left(  \beta+\alpha-\dfrac{1}{2}\left[  \alpha
,\beta\right]  \right)  \\\text{(by (\ref{pf.powerseries3.8}))}}}\cdot\left(
\exp\left[  \alpha,\beta\right]  \right)   &  =\left(  \exp\left(
\beta+\alpha-\dfrac{1}{2}\left[  \alpha,\beta\right]  \right)  \right)
\cdot\left(  \exp\left[  \alpha,\beta\right]  \right) \\
&  =\exp\underbrace{\left(  \beta+\alpha-\dfrac{1}{2}\left[  \alpha
,\beta\right]  +\left[  \alpha,\beta\right]  \right)  }_{=\alpha+\beta
+\dfrac{1}{2}\left[  \alpha,\beta\right]  }\\
&  =\exp\left(  \alpha+\beta+\dfrac{1}{2}\left[  \alpha,\beta\right]  \right)
=\left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)
\end{align*}
(by (\ref{pf.powerseries3.6})). Lemma \ref{lem.powerseries3} is proven.

We are going to also present a third, very elementary (term-by-term) proof of
Lemma \ref{lem.powerseries3}. It relies on the following proposition, which
can also be applied in some other contexts (e. g., computing in universal
enveloping algebras):

\begin{proposition}
\label{prop.powerseries3.fin}Let $R$ be a ring. Let $\alpha\in R$ and
$\beta\in R$. Assume that $\left[  \alpha,\beta\right]  $ commutes with each
of $\alpha$ and $\beta$. Then, for every $i\in\mathbb{N}$ and $j\in\mathbb{N}%
$, we have%
\[
\alpha^{j}\beta^{i}=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq
j}}k!\dbinom{i}{k}\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\left[  \alpha
,\beta\right]  ^{k}.
\]

\end{proposition}

\textit{Proof of Proposition \ref{prop.powerseries3.fin}.} Let $\gamma$ denote
$\left[  \alpha,\beta\right]  $. Then, $\gamma$ commutes with each of $\alpha$
and $\beta$ (since $\left[  \alpha,\beta\right]  $ commutes with each of
$\alpha$ and $\beta$). In other words, $\gamma\alpha=\alpha\gamma$ and
$\gamma\beta=\beta\gamma$.

As we showed in the proof of Lemma \ref{lem.powerseries1}, every positive
$i\in\mathbb{N}$ satisfies $\left[  \alpha,\beta^{i}\right]  =i\gamma
\beta^{i-1}$. Since $\gamma=\left[  \alpha,\beta\right]  $, this rewrites as
follows:
\begin{equation}
\text{every positive }i\in\mathbb{N}\text{ satisfies }\left[  \alpha,\beta
^{i}\right]  =i\left[  \alpha,\beta\right]  \beta^{i-1}.
\label{pf.powerseries3.fin.1}%
\end{equation}


Since $\left[  \beta,\alpha\right]  =-\underbrace{\left[  \alpha,\beta\right]
}_{=\gamma}=-\gamma$, we see that $\underbrace{\left[  \beta,\alpha\right]
}_{=-\gamma}\alpha=-\underbrace{\gamma\alpha}_{=\alpha\gamma}=-\alpha
\gamma=\alpha\underbrace{\left(  -\gamma\right)  }_{=\left[  \beta
,\alpha\right]  }=\alpha\left[  \beta,\alpha\right]  $ and
$\underbrace{\left[  \beta,\alpha\right]  }_{=-\gamma}\beta
=-\underbrace{\gamma\beta}_{=\beta\gamma}=-\beta\gamma=\beta
\underbrace{\left(  -\gamma\right)  }_{=\left[  \beta,\alpha\right]  }%
=\beta\left[  \beta,\alpha\right]  $. In other words, $\left[  \beta
,\alpha\right]  $ commutes with each of $\alpha$ and $\beta$. Therefore, the
roles of $\alpha$ and $\beta$ are symmetric, and thus we can apply
(\ref{pf.powerseries3.fin.1}) to $\beta$ and $\alpha$ instead of $\alpha$ and
$\beta$, and conclude that%
\begin{equation}
\text{every positive }i\in\mathbb{N}\text{ satisfies }\left[  \beta,\alpha
^{i}\right]  =i\left[  \beta,\alpha\right]  \alpha^{i-1}.
\label{pf.powerseries3.fin.1a}%
\end{equation}
Thus, every positive $i\in\mathbb{N}$ satisfies $\beta\alpha^{i}-\alpha
^{i}\beta=\left[  \beta,\alpha^{i}\right]  =i\underbrace{\left[  \beta
,\alpha\right]  }_{=-\gamma}\alpha^{i-1}=-i\gamma\alpha^{i-1}$, so that
$\beta\alpha^{i}=\alpha^{i}\beta-i\gamma\alpha^{i-1}$ and thus $\alpha
^{i}\beta=\beta\alpha^{i}+i\gamma\alpha^{i-1}$. We have thus proven that%
\begin{equation}
\text{every positive }i\in\mathbb{N}\text{ satisfies }\alpha^{i}\beta
=\beta\alpha^{i}+i\gamma\alpha^{i-1}. \label{pf.powerseries3.fin.1b}%
\end{equation}


Now, we are going to prove that every $i\in\mathbb{N}$ and $j\in\mathbb{N}$
satisfy%
\begin{equation}
\alpha^{j}\beta^{i}=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq
j}}k!\dbinom{i}{k}\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\gamma^{k}.
\label{pf.powerseries3.fin.2}%
\end{equation}


\textit{Proof of (\ref{pf.powerseries3.fin.2}):} We will prove
(\ref{pf.powerseries3.fin.2}) by induction over $i$:

\textit{Induction base:} Let $j\in\mathbb{N}$ be arbitrary. For $i=0$, we have
$\alpha^{j}\beta^{i}=\alpha^{j}\underbrace{\beta^{0}}_{=1}=\alpha^{j}$ and
\begin{align*}
\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq j}}k!\dbinom{i}%
{k}\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\gamma^{k}  &  =\underbrace{\sum
\limits_{\substack{k\in\mathbb{N};\\k\leq0;\ k\leq j}}}_{=\sum\limits_{k\in
\left\{  0\right\}  }}k!\dbinom{0}{k}\dbinom{j}{k}\beta^{0-k}\alpha
^{j-k}\gamma^{k}=\sum\limits_{k\in\left\{  0\right\}  }k!\dbinom{0}{k}%
\dbinom{j}{k}\beta^{0-k}\alpha^{j-k}\gamma^{k}\\
&  =\underbrace{0!}_{=1}\underbrace{\dbinom{0}{0}}_{=1}\underbrace{\dbinom
{j}{0}}_{=1}\underbrace{\beta^{0-0}}_{=1}\underbrace{\alpha^{j-0}}%
_{=\alpha^{j}}\underbrace{\gamma^{0}}_{=1}=\alpha^{j}.
\end{align*}
Hence, for $i=0$, we have $\alpha^{j}\beta^{i}=\alpha^{j}=\sum
\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq j}}k!\dbinom{i}{k}%
\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\gamma^{k}$. Thus,
(\ref{pf.powerseries3.fin.2}) holds for $i=0$, so that the induction base is complete.

\textit{Induction step:} Let $u\in\mathbb{N}$. Assume that
(\ref{pf.powerseries3.fin.2}) holds for $i=u$. We must now prove that
(\ref{pf.powerseries3.fin.2}) holds for $i=u+1$.

Since (\ref{pf.powerseries3.fin.2}) holds for $i=u$, we have%
\begin{equation}
\alpha^{j}\beta^{u}=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq
j}}k!\dbinom{u}{k}\dbinom{j}{k}\beta^{u-k}\alpha^{j-k}\gamma^{k}%
\ \ \ \ \ \ \ \ \ \ \text{for every }j\in\mathbb{N}.
\label{pf.powerseries3.fin.3}%
\end{equation}


Now, let $j\in\mathbb{N}$ be positive. Then, $j-1\in\mathbb{N}$. Now,%
\begin{align}
&  \alpha^{j}\underbrace{\beta^{u+1}}_{=\beta\beta^{u}}\nonumber\\
&  =\underbrace{\alpha^{j}\beta}_{\substack{=\beta\alpha^{j}+j\gamma
\alpha^{j-1}\\\text{(by (\ref{pf.powerseries3.fin.1b}),}\\\text{applied to
}j\text{ instead of }i\text{)}}}\beta^{u}=\left(  \beta\alpha^{j}%
+j\gamma\alpha^{j-1}\right)  \beta^{u}=\beta\alpha^{j}\beta^{u}%
+j\underbrace{\gamma\alpha^{j-1}\beta^{u}}_{\substack{=\alpha^{j-1}\beta
^{u}\gamma\\\text{(since }\gamma\text{ commutes with}\\\text{each of }%
\beta\text{ and }\alpha\text{)}}}\nonumber\\
&  =\beta\underbrace{\alpha^{j}\beta^{u}}_{\substack{=\sum
\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j}}k!\dbinom{u}{k}%
\dbinom{j}{k}\beta^{u-k}\alpha^{j-k}\gamma^{k}\\\text{(by
(\ref{pf.powerseries3.fin.3}))}}}+j\underbrace{\alpha^{j-1}\beta^{u}%
}_{\substack{=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq
j-1}}k!\dbinom{u}{k}\dbinom{j-1}{k}\beta^{u-k}\alpha^{j-1-k}\gamma
^{k}\\\text{(by (\ref{pf.powerseries3.fin.3}), applied to }j-1\\\text{instead
of }j\text{ (since }j-1\in\mathbb{N}\text{))}}}\gamma\nonumber\\
&  =\beta\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j}%
}k!\dbinom{u}{k}\dbinom{j}{k}\beta^{u-k}\alpha^{j-k}\gamma^{k}+j\left(
\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j-1}}k!\dbinom{u}%
{k}\dbinom{j-1}{k}\beta^{u-k}\alpha^{j-1-k}\gamma^{k}\right)  \gamma
\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j}}k!\dbinom{u}%
{k}\dbinom{j}{k}\underbrace{\beta\beta^{u-k}}_{=\beta^{u+1-k}}\alpha
^{j-k}\gamma^{k}+j\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq
j-1}}k!\dbinom{u}{k}\dbinom{j-1}{k}\beta^{u-k}\alpha^{j-1-k}\underbrace{\gamma
^{k}\gamma}_{=\gamma^{k+1}}\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j}}k!\dbinom{u}%
{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}+j\sum
\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j-1}}k!\dbinom{u}%
{k}\dbinom{j-1}{k}\beta^{u-k}\alpha^{j-1-k}\gamma^{k+1}.
\label{pf.powerseries3.fin.twosums}%
\end{align}
Let us separately simplify the two addends on the right hand side of this equation.

First of all, every $k\in\mathbb{N}$ which satisfies $k\leq u+1$ and $k\leq j$
but does \textbf{not} satisfy $k\leq u$ must satisfy \newline$k!\dbinom{u}%
{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}=0$ (because this $k$ does
not satisfy $k\leq u$, so that we have $k>u$, and thus $\dbinom{u}{k}=0$).
Thus, $\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ \left(  \text{not
}k\leq u\right)  ;\ k\leq j}}k!\dbinom{u}{k}\dbinom{j}{k}\beta^{u+1-k}%
\alpha^{j-k}\gamma^{j}=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq
u+1;\ \left(  \text{not }k\leq u\right)  ;\ k\leq j}}0=0$. Hence,%
\begin{align}
&  \sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ k\leq j}}k!\dbinom
{u}{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}\nonumber\\
&  =\underbrace{\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ k\leq
u;\ k\leq j}}}_{=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j}%
}}k!\dbinom{u}{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}%
+\underbrace{\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ \left(
\text{not }k\leq u\right)  ;\ k\leq j}}k!\dbinom{u}{k}\dbinom{j}{k}%
\beta^{u+1-k}\alpha^{j-k}\gamma^{j}}_{=0}\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j}}k!\dbinom{u}%
{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}.
\label{pf.powerseries3.fin.5}%
\end{align}


On the other hand,
\begin{align}
&  \sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j-1}}k!\dbinom
{u}{k}\dbinom{j-1}{k}\beta^{u-k}\alpha^{j-1-k}\gamma^{k+1}\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq
j}}\left(  k-1\right)  !\dbinom{u}{k-1}\dbinom{j-1}{k-1}\underbrace{\beta
^{u-\left(  k-1\right)  }}_{=\beta^{u+1-k}}\underbrace{\alpha^{j-1-\left(
k-1\right)  }}_{=\alpha^{j-k}}\underbrace{\gamma^{\left(  k-1\right)  +1}%
}_{=\gamma^{k}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }k-1\text{ for
}k\text{ in the sum}\right) \nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq
j}}\left(  k-1\right)  !\dbinom{u}{k-1}\dbinom{j-1}{k-1}\beta^{u+1-k}%
\alpha^{j-k}\gamma^{j}. \label{pf.powerseries3.fin.6a}%
\end{align}
But every $k\in\mathbb{N}$ satisfying $k\geq1$ and $k\leq j$ satisfies%
\[
\dbinom{j-1}{k-1}=\dfrac{\left(  j-1\right)  !}{\left(  k-1\right)  !\left(
\left(  j-1\right)  -\left(  k-1\right)  \right)  !}=\dfrac{\left(
j-1\right)  !}{\left(  k-1\right)  !\left(  j-k\right)  !}.
\]
Hence, every $k\in\mathbb{N}$ satisfying $k\geq1$ and $k\leq j$ satisfies%
\begin{align}
\left(  k-1\right)  !\dbinom{u}{k-1}\underbrace{\dbinom{j-1}{k-1}}%
_{=\dfrac{\left(  j-1\right)  !}{\left(  k-1\right)  !\left(  j-k\right)  !}}
&  =\left(  k-1\right)  !\dbinom{u}{k-1}\dfrac{\left(  j-1\right)  !}{\left(
k-1\right)  !\left(  j-k\right)  !}\nonumber\\
&  =\dbinom{u}{k-1}\dfrac{\left(  j-1\right)  !}{\left(  j-k\right)  !}.
\label{pf.powerseries3.fin.6b}%
\end{align}
But multiplying both sides of (\ref{pf.powerseries3.fin.6a}) with $j$, we
obtain%
\begin{align}
&  j\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j-1}}k!\dbinom
{u}{k}\dbinom{j-1}{k}\beta^{u-k}\alpha^{j-1-k}\gamma^{k+1}\nonumber\\
&  =j\sum\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq
j}}\underbrace{\left(  k-1\right)  !\dbinom{u}{k-1}\dbinom{j-1}{k-1}%
}_{\substack{=\dbinom{u}{k-1}\dfrac{\left(  j-1\right)  !}{\left(  j-k\right)
!}\\\text{(by (\ref{pf.powerseries3.fin.6b}))}}}\beta^{u+1-k}\alpha
^{j-k}\gamma^{j}\nonumber\\
&  =j\sum\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq
j}}\dbinom{u}{k-1}\dfrac{\left(  j-1\right)  !}{\left(  j-k\right)  !}%
\beta^{u+1-k}\alpha^{j-k}\gamma^{j}=\sum\limits_{\substack{k\in\mathbb{N}%
;\ k\geq1;\\k\leq u+1;\ k\leq j}}\dbinom{u}{k-1}j\dfrac{\left(  j-1\right)
!}{\left(  j-k\right)  !}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}.
\label{pf.powerseries3.fin.6c}%
\end{align}
But every $k\in\mathbb{N}$ satisfying $k\geq1$ and $k\leq j$ satisfies%
\begin{align*}
\dbinom{j}{k}  &  =\dfrac{j!}{k!\left(  j-k\right)  !}=\dfrac{j\left(
j-1\right)  !}{k!\left(  j-k\right)  !}\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}j!=j\left(  j-1\right)  !\right) \\
&  =\dfrac{1}{k!}\cdot j\dfrac{\left(  j-1\right)  !}{\left(  j-k\right)  !}.
\end{align*}
Hence, every $k\in\mathbb{N}$ satisfying $k\geq1$ and $k\leq j$ satisfies%
\begin{equation}
k!\dbinom{j}{k}=j\dfrac{\left(  j-1\right)  !}{\left(  j-k\right)  !}.
\label{pf.powerseries3.fin.6d}%
\end{equation}
Thus, (\ref{pf.powerseries3.fin.6c}) becomes%
\begin{align}
&  j\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j-1}}k!\dbinom
{u}{k}\dbinom{j-1}{k}\beta^{u-k}\alpha^{j-1-k}\gamma^{k+1}\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq
j}}\dbinom{u}{k-1}\underbrace{j\dfrac{\left(  j-1\right)  !}{\left(
j-k\right)  !}}_{\substack{=k!\dbinom{j}{k}\\\text{(by
(\ref{pf.powerseries3.fin.6d}))}}}\beta^{u+1-k}\alpha^{j-k}\gamma
^{j}\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq
j}}\dbinom{u}{k-1}k!\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}%
=\sum\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq
j}}k!\dbinom{u}{k-1}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}.
\label{pf.powerseries3.fin.6e}%
\end{align}


But every $k\in\mathbb{N}$ which satisfies $k\leq u+1$ and $k\leq j$ but does
\textbf{not} satisfy $k\geq1$ must satisfy \newline$k!\dbinom{u}{k-1}%
\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}=0$ (because this $k$ does not
satisfy $k\geq1$, so that we have $k<1$, and thus $\dbinom{u}{k-1}=0$). Thus,
\newline$\sum\limits_{\substack{k\in\mathbb{N};\ \left(  \text{not }%
k\geq1\right)  ;\\k\leq u+1;\ k\leq j}}k!\dbinom{u}{k-1}\dbinom{j}{k}%
\beta^{u+1-k}\alpha^{j-k}\gamma^{j}=\sum\limits_{\substack{k\in\mathbb{N}%
;\ \left(  \text{not }k\geq1\right)  ;\\k\leq u+1;\ k\leq j}}0=0$. Hence,%
\begin{align}
&  \sum\limits_{\substack{k\in\mathbb{N}\\k\leq u+1;\ k\leq j}}k!\dbinom
{u}{k-1}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq
j}}k!\dbinom{u}{k-1}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma
^{j}+\underbrace{\sum\limits_{\substack{k\in\mathbb{N};\ \left(  \text{not
}k\geq1\right)  ;\\k\leq u+1;\ k\leq j}}k!\dbinom{u}{k-1}\dbinom{j}{k}%
\beta^{u+1-k}\alpha^{j-k}\gamma^{j}}_{=0}\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq
j}}k!\dbinom{u}{k-1}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}.
\label{pf.powerseries3.fin.6f}%
\end{align}
Thus, (\ref{pf.powerseries3.fin.6e}) becomes%
\begin{align}
&  j\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq j-1}}k!\dbinom
{u}{k}\dbinom{j-1}{k}\beta^{u-k}\alpha^{j-1-k}\gamma^{k+1}\nonumber\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\ k\geq1;\\k\leq u+1;\ k\leq
j}}k!\dbinom{u}{k-1}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}%
=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ k\leq j}}k!\dbinom
{u}{k-1}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}
\label{pf.powerseries3.fin.6g}%
\end{align}
(by (\ref{pf.powerseries3.fin.6f})).

Also, notice that every $k\in\mathbb{N}$ satisfies%
\begin{equation}
k!\dbinom{u}{k}+k!\dbinom{u}{k-1}=k!\underbrace{\left(  \dbinom{u}{k}%
+\dbinom{u}{k-1}\right)  }_{\substack{=\dbinom{u+1}{k}\\\text{(by the
recurrence equation}\\\text{of the binomial coefficients)}}}=k!\dbinom{u+1}%
{k}. \label{pf.powerseries3.fin.binom}%
\end{equation}


Now, (\ref{pf.powerseries3.fin.twosums}) becomes%
\begin{align*}
\alpha^{j}\beta^{u+1}  &  =\underbrace{\sum\limits_{\substack{k\in
\mathbb{N};\\k\leq u;\ k\leq j}}k!\dbinom{u}{k}\dbinom{j}{k}\beta
^{u+1-k}\alpha^{j-k}\gamma^{j}}_{\substack{=\sum\limits_{\substack{k\in
\mathbb{N};\\k\leq u+1;\ k\leq j}}k!\dbinom{u}{k}\dbinom{j}{k}\beta
^{u+1-k}\alpha^{j-k}\gamma^{j}\\\text{(by (\ref{pf.powerseries3.fin.5}))}%
}}+\underbrace{j\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u;\ k\leq
j-1}}k!\dbinom{u}{k}\dbinom{j-1}{k}\beta^{u-k}\alpha^{j-1-k}\gamma^{k+1}%
}_{\substack{=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ k\leq
j}}k!\dbinom{u}{k-1}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma
^{j}\\\text{(by (\ref{pf.powerseries3.fin.6g}))}}}\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ k\leq j}}k!\dbinom
{u}{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}+\sum
\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ k\leq j}}k!\dbinom{u}%
{k-1}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ k\leq j}%
}\underbrace{\left(  k!\dbinom{u}{k}+k!\dbinom{u}{k-1}\right)  }%
_{\substack{=k!\dbinom{u+1}{k}\\\text{(by (\ref{pf.powerseries3.fin.binom}))}%
}}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}\\
&  =\sum\limits_{\substack{k\in\mathbb{N};\\k\leq u+1;\ k\leq j}%
}k!\dbinom{u+1}{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}\gamma^{j}.
\end{align*}


Now, forget that we fixed $j$. We thus have shown that%
\begin{equation}
\alpha^{j}\beta^{u+1}=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq
u+1;\ k\leq j}}k!\dbinom{u+1}{k}\dbinom{j}{k}\beta^{u+1-k}\alpha^{j-k}%
\gamma^{j} \label{pf.powerseries3.fin.9}%
\end{equation}
holds for every positive $j\in\mathbb{N}$. Since it is easy to see that
(\ref{pf.powerseries3.fin.9}) also holds for $j=0$ (the proof is similar to
our induction base above), this yields that (\ref{pf.powerseries3.fin.9})
holds for every $j\in\mathbb{N}$. In other words, (\ref{pf.powerseries3.fin.2}%
) holds for $i=u+1$. Thus, the induction step is complete. Hence, we have
proven (\ref{pf.powerseries3.fin.2}) by induction over $i$.

Since $\gamma=\left[  \alpha,\beta\right]  $, the (now proven) identity
(\ref{pf.powerseries3.fin.2}) rewrites as%
\[
\alpha^{j}\beta^{i}=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq
j}}k!\dbinom{i}{k}\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\left.
\underbrace{\gamma}_{=\left[  \alpha,\beta\right]  }\right.  ^{k}%
=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq j}}k!\dbinom{i}%
{k}\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\left[  \alpha,\beta\right]  ^{k}.
\]
Proposition \ref{prop.powerseries3.fin} is thus proven.

\textit{Third proof of Lemma \ref{lem.powerseries3}.} By the definition of the
exponential, we have $\exp\left[  \alpha,\beta\right]  =\sum\limits_{k\in
\mathbb{N}}\dfrac{\left[  \alpha,\beta\right]  ^{k}}{k!}$, $\exp\alpha
=\sum\limits_{j\in\mathbb{N}}\dfrac{\alpha^{j}}{j!}$ and $\exp\beta
=\sum\limits_{i\in\mathbb{N}}\dfrac{\beta^{i}}{i!}$. Multiplying the last two
of these three equalities, we obtain%
\begin{align*}
&  \left(  \exp\alpha\right)  \cdot\left(  \exp\beta\right)  \\
&  =\left(  \sum\limits_{j\in\mathbb{N}}\dfrac{\alpha^{j}}{j!}\right)
\cdot\left(  \sum\limits_{i\in\mathbb{N}}\dfrac{\beta^{i}}{i!}\right)
=\sum\limits_{i\in\mathbb{N}}\sum\limits_{j\in\mathbb{N}}\dfrac{\alpha^{j}%
}{j!}\cdot\dfrac{\beta^{i}}{i!}=\sum\limits_{i\in\mathbb{N}}\sum
\limits_{j\in\mathbb{N}}\dfrac{1}{i!j!}\underbrace{\alpha^{j}\beta^{i}%
}_{\substack{=\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq
j}}k!\dbinom{i}{k}\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\left[  \alpha
,\beta\right]  ^{k}\\\text{(by Proposition \ref{prop.powerseries3.fin})}}}\\
&  =\sum\limits_{i\in\mathbb{N}}\sum\limits_{j\in\mathbb{N}}\dfrac{1}%
{i!j!}\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq j}}k!\dbinom
{i}{k}\dbinom{j}{k}\beta^{i-k}\alpha^{j-k}\left[  \alpha,\beta\right]  ^{k}\\
&  =\underbrace{\sum\limits_{i\in\mathbb{N}}\sum\limits_{j\in\mathbb{N}}%
\sum\limits_{\substack{k\in\mathbb{N};\\k\leq i;\ k\leq j}}}_{=\sum
\limits_{k\in\mathbb{N}}\sum\limits_{\substack{i\in\mathbb{N};\\k\leq i}%
}\sum\limits_{\substack{j\in\mathbb{N};\\k\leq j}}}\underbrace{\dfrac{1}%
{i!j!}k!\dbinom{i}{k}\dbinom{j}{k}}_{\substack{=\dfrac{1}{\left(  i-k\right)
!\left(  j-k\right)  !k!}\\\text{(by easy computations)}}}\beta^{i-k}%
\alpha^{j-k}\left[  \alpha,\beta\right]  ^{k}\\
&  =\sum\limits_{k\in\mathbb{N}}\sum\limits_{\substack{i\in\mathbb{N};\\k\leq
i}}\sum\limits_{\substack{j\in\mathbb{N};\\k\leq j}}\dfrac{1}{\left(
i-k\right)  !\left(  j-k\right)  !k!}\beta^{i-k}\alpha^{j-k}\left[
\alpha,\beta\right]  ^{k}=\sum\limits_{k\in\mathbb{N}}\sum\limits_{i\in
\mathbb{N}}\sum\limits_{j\in\mathbb{N}}\dfrac{1}{i!j!k!}\beta^{i}\alpha
^{j}\left[  \alpha,\beta\right]  ^{k}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we substituted }i\text{ for }i-k\text{ in the second sum,}\\
\text{and we substituted }j\text{ for }j-k\text{ in the third sum}%
\end{array}
\right)  \\
&  =\sum\limits_{i\in\mathbb{N}}\sum\limits_{j\in\mathbb{N}}\sum
\limits_{k\in\mathbb{N}}\dfrac{\beta^{i}}{i!}\cdot\dfrac{\alpha^{j}}{j!}%
\cdot\dfrac{\left[  \alpha,\beta\right]  ^{k}}{k!}=\underbrace{\left(
\sum\limits_{i\in\mathbb{N}}\dfrac{\beta^{i}}{i!}\right)  }_{=\exp\beta}%
\cdot\underbrace{\left(  \sum\limits_{j\in\mathbb{N}}\dfrac{\alpha^{j}}%
{j!}\right)  }_{=\exp\alpha}\cdot\underbrace{\left(  \sum\limits_{k\in
\mathbb{N}}\dfrac{\left[  \alpha,\beta\right]  ^{k}}{k!}\right)  }%
_{=\exp\left[  \alpha,\beta\right]  }\\
&  =\left(  \exp\beta\right)  \cdot\left(  \exp\alpha\right)  \cdot\left(
\exp\left[  \alpha,\beta\right]  \right)  .
\end{align*}
This proves Lemma \ref{lem.powerseries3} once again.

\subsection{\label{subsect.fockvir}Representations of
\texorpdfstring{$\operatorname*{Vir}$}{Vir} on
\texorpdfstring{$F_{\mu}$}{the Fock module}}

\subsubsection{The Lie-algebraic semidirect product: the general case}

Let us define the ``full-fledged'' version of the Lie-algebraic semidirect
product, although it will not be central to what we will later do:

\begin{definition}
\label{def.semidir.lielie}Let $\mathfrak{g}$ be a Lie algebra. Let
$\mathfrak{h}$ be a vector space equipped with both a Lie algebra structure
and a $\mathfrak{g}$-module structure.

\textbf{(a)} Let $\rho:\mathfrak{g}\rightarrow\operatorname*{End}\mathfrak{h}$
be the map representing the action of $\mathfrak{g}$ on $\mathfrak{h}$. We say
that $\mathfrak{g}$ \textit{acts on }$\mathfrak{h}$\textit{ by derivations} if
$\rho\left(  \mathfrak{g}\right)  \subseteq\operatorname*{Der}\mathfrak{h}$,
or, equivalently, if the map%
\[
\mathfrak{h}\rightarrow\mathfrak{h},\ \ \ \ \ \ \ \ \ \ x\mapsto
a\rightharpoonup x
\]
is a derivation for every $a\in\mathfrak{g}$. (Here and in the following, the
symbol $\rightharpoonup$ means action; i. e., a term like $c\rightharpoonup h$
(with $c\in\mathfrak{g}$ and $h\in\mathfrak{h}$) means the action of $c$ on
$h$.)

\textbf{(b)} Assume that $\mathfrak{g}$ acts on $\mathfrak{h}$ by derivations.
Then, we define the \textit{semidirect product} $\mathfrak{g}\ltimes
\mathfrak{h}$ to be the Lie algebra which, as a vector space, is
$\mathfrak{g}\oplus\mathfrak{h}$, but whose Lie bracket is defined by%
\begin{align*}
\left[  \left(  a,\alpha\right)  ,\left(  b,\beta\right)  \right]   &
=\left(  \left[  a,b\right]  ,\left[  \alpha,\beta\right]  +a\rightharpoonup
\beta-b\rightharpoonup\alpha\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }a\in\mathfrak{g}\text{, }%
\alpha\in\mathfrak{h}\text{, }b\in\mathfrak{g}\text{ and }\beta\in
\mathfrak{h}\right.  .
\end{align*}


Thus, the canonical injection $\mathfrak{g}\rightarrow\mathfrak{g}%
\ltimes\mathfrak{h},$ $a\mapsto\left(  a,0\right)  $ is a Lie algebra
homomorphism, and so is the canonical projection $\mathfrak{g}\ltimes
\mathfrak{h}\rightarrow\mathfrak{g},$ $\left(  a,\alpha\right)  \mapsto a$.
Also, the canonical injection $\mathfrak{h}\rightarrow\mathfrak{g}%
\ltimes\mathfrak{h},$ $\alpha\mapsto\left(  0,\alpha\right)  $ is a Lie
algebra homomorphism.
\end{definition}

All statements made in Definition \ref{def.semidir.lielie} (including the
tacit statement that the Lie bracket on $\mathfrak{g}\ltimes\mathfrak{h}$
defined in Definition \ref{def.semidir.lielie} satisfies antisymmetry and the
Jacobi identity) are easy to verify by computation.

\begin{remark}
If $\mathfrak{g}$ is a Lie algebra, and $\mathfrak{h}$ is an \textbf{abelian}
Lie algebra with any $\mathfrak{g}$-module structure, then $\mathfrak{g}$
automatically acts on $\mathfrak{h}$ by derivations (because any endomorphism
of the vector space $\mathfrak{h}$ is a derivation), and thus Definition
\ref{def.semidir.lielie} \textbf{(b)} defines a semidirect product
$\mathfrak{g}\ltimes\mathfrak{h}$. In this case, this semidirect product
$\mathfrak{g}\ltimes\mathfrak{h}$ coincides with the semidirect product
$\mathfrak{g}\ltimes\mathfrak{h}$ defined in Definition \ref{def.semidir}
(applied to $M=\mathfrak{h}$). However, when $\mathfrak{h}$ is not abelian,
the semidirect product $\mathfrak{g}\ltimes\mathfrak{h}$ defined in Definition
\ref{def.semidir.lielie} (in general) differs from that defined in Definition
\ref{def.semidir} (since the former depends on the Lie algebra structure on
$\mathfrak{h}$, while the latter does not). Care must therefore be taken when
speaking of semidirect products.
\end{remark}

An example for the semidirect product construction given in Definition
\ref{def.semidir.lielie} \textbf{(b)} is given by the following proposition:

\begin{proposition}
\label{prop.VirtoDerA}Consider the Witt algebra $W$, the Virasoro algebra
$\operatorname*{Vir}$ and the Heisenberg algebra $\mathcal{A}$.

\textbf{(a)} In Lemma \ref{lem.WtoDerA}, we constructed a homomorphism
$\eta:W\rightarrow\operatorname*{Der}\mathcal{A}$ of Lie algebras. This
homomorphism $\eta$ makes $\mathcal{A}$ into a $W$-module, and $W$ acts on
$\mathcal{A}$ by derivations. Therefore, a Lie algebra $W\ltimes\mathcal{A}$
is defined (according to Definition \ref{def.semidir.lielie} \textbf{(b)}).

\textbf{(b)} There is a natural homomorphism $\widetilde{\eta}%
:\operatorname*{Vir}\rightarrow\operatorname*{Der}\mathcal{A}$ of Lie algebras
given by
\[
\left(  \widetilde{\eta}\left(  f\partial+\lambda K\right)  \right)  \left(
g,\alpha\right)  =\left(  fg^{\prime},0\right)  \ \ \ \ \ \ \ \ \ \ \text{for
all }f\in\mathbb{C}\left[  t,t^{-1}\right]  \text{, }g\in\mathbb{C}\left[
t,t^{-1}\right]  \text{, }\lambda\in\mathbb{C}\text{ and }\alpha\in
\mathbb{C}.
\]
This homomorphism $\widetilde{\eta}$ is simply the extension of the
homomorphism $\eta:W\rightarrow\operatorname*{Der}\mathcal{A}$ (defined in
Lemma \ref{lem.WtoDerA}) to $\operatorname*{Vir}$ by means of requiring that
$\widetilde{\eta}\left(  K\right)  =0$.

This homomorphism $\widetilde{\eta}$ makes $\mathcal{A}$ a
$\operatorname*{Vir}$-module, and $\operatorname*{Vir}$ acts on $\mathcal{A}$
by derivations. Therefore, a Lie algebra $\operatorname*{Vir}\ltimes
\mathcal{A}$ is defined (according to Definition \ref{def.semidir.lielie}
\textbf{(b)}).
\end{proposition}

The proof of Proposition \ref{prop.VirtoDerA} is straightforward and left to
the reader.

\subsubsection{The action of \texorpdfstring{$\operatorname*{Vir}$}{Vir} on
\texorpdfstring{$F_{\mu}$}{the Fock module}}

Let us now return to considering the Witt and Heisenberg algebras.

According to Proposition \ref{prop.VirtoDerA} \textbf{(a)}, we have a Lie
algebra $W\ltimes\mathcal{A}$, of which $\mathcal{A}$ is a Lie subalgebra.
Now, recall (from Definition \ref{def.fock}) that, for every $\mu\in
\mathbb{C}$, we have a representation $F_{\mu}$ of the Lie algebra
$\mathcal{A}$ on the Fock space $F$.

Can we extend this representation $F_{\mu}$ of $\mathcal{A}$ to a
representation of the semidirect product $W\ltimes\mathcal{A}$ ?

This question splits into two questions:

\textbf{Question 1:} Can we find linear operators $L_{n}:F_{\mu}\rightarrow
F_{\mu}$ for all $n\in\mathbb{Z}$ such that $\left[  L_{n},a_{m}\right]
=-ma_{n+m}$ ? (Note that there are several abuses of notation in this
question. First, we denote the sought operators $L_{n}:F_{\mu}\rightarrow
F_{\mu}$ by the same letters as the elements $L_{n}$ of $W$ because our
intuition for the $L_{n}$ is as if they would form a representation of $W$,
although we do not actually require them to form a representation of $W$ in
Question 1. Second, in the equation $\left[  L_{n},a_{m}\right]  =-ma_{n+m}$,
we use $a_{m}$ and $a_{n+m}$ as abbreviations for $a_{m}\mid_{F_{\mu}}$ and
$a_{n+m}\mid_{F_{\mu}}$, respectively (so that this equation actually means
$\left[  L_{n},a_{m}\mid_{F_{\mu}}\right]  =-ma_{n+m}\mid_{F_{\mu}}$).)

\textbf{Question 2:} Do the operators $L_{n}:F_{\mu}\rightarrow F_{\mu}$ that
answer Question 1 also satisfy $\left[  L_{n},L_{m}\right]  =\left(
n-m\right)  L_{n+m}$? (In other words, do they really form a representation of
$W$ ?)

The answers to these questions are the following:

\textbf{Answer to Question 1:} Yes, and moreover, these operators are unique
up to adding a constant (a new constant for each operator). (The uniqueness is
rather easy to prove: If we have two families $\left(  L_{n}^{\prime}\right)
_{n\in\mathbb{Z}}$ and $\left(  L_{n}^{\prime\prime}\right)  _{n\in\mathbb{Z}%
}$ of linear maps $F_{\mu}\rightarrow F_{\mu}$ satisfying $\left[
L_{n}^{\prime},a_{m}\right]  =-ma_{n+m}$ and $\left[  L_{n}^{\prime\prime
},a_{m}\right]  =-ma_{n+m}$, then every $L_{n}^{\prime}-L_{n}^{\prime\prime}$
commutes with all $a_{m}$, and thus is constant by Dixmier's lemma.)

\textbf{Answer to Question 2:} No, but almost. Our operators $L_{n}$ satisfy
$\left[  L_{n},L_{m}\right]  =\left(  n-m\right)  L_{n+m}$ whenever $n+m\neq
0$, but the $n+m=0$ case requires a correction term. This correction term (as
a function of $\left(  L_{n},L_{m}\right)  $) happens to be the $2$-cocycle
$\omega$ of Theorem \ref{thm.H^2(W)}. So the $\mathcal{A}$-module $F_{\mu}$
does not extend to a $W\ltimes\mathcal{A}$-module, but extends to a
$\operatorname*{Vir}\ltimes\mathcal{A}$-module, where $\operatorname*{Vir}%
\ltimes\mathcal{A}$ is defined as in Proposition \ref{prop.VirtoDerA}
\textbf{(b)}.

Now we are going to prove the answers to Questions 1 and 2 formulated above.
First, we must define our operators $L_{n}$. ``Formally'' (in the sense of
``not caring about divergence of sums''), one could try to define $L_{n}$ by
\begin{equation}
L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}a_{-m}a_{n+m}%
\ \ \ \ \ \ \ \ \ \ \text{for all }n\in\mathbb{Z} \label{def.fockvir.wrong}%
\end{equation}
(where $a_{\ell}$ is shorthand notation for $a_{\ell}\mid_{F_{\mu}}$ for every
$\ell\in\mathbb{Z}$), and this would ``formally'' make $F_{\mu}$ into a
$W\ltimes\mathcal{A}$-module (in the sense that if the sums were not
divergent, one could manipulate them to ``prove'' that $\left[  L_{n}%
,a_{m}\right]  =-ma_{n+m}$ and $\left[  L_{n},L_{m}\right]  =\left(
n-m\right)  L_{n+m}$ for all $n$ and $m$). But the problem with this
``formal'' approach is that the sum $\sum\limits_{m\in\mathbb{Z}}a_{-m}%
a_{n+m}$ does not make sense for $n=0$: it is an infinite sum, and infinitely
many of its terms yield nonzero values when applied to a given
vector.\footnote{In fact, assume that this sum would make sense for $n=0$.
Thus we would have $L_{0}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}a_{-m}a_{m}%
$. Applied to the vector $1\in F_{0}$, this would give $L_{0}1=\dfrac{1}%
{2}\sum\limits_{m\in\mathbb{Z}}a_{-m}a_{m}1$. The terms for $m>0$ will get
killed (since $a_{m}1=0$ for $m>0$), but the terms for $m\leq0$ will survive.
The sum would become
\begin{align*}
L_{0}1  &  =\dfrac{1}{2}\left(  a_{0}a_{-0}1+a_{1}a_{-1}1+a_{2}a_{-2}%
1+a_{3}a_{-3}1+...\right) \\
&  =\dfrac{1}{2}\left(  \mu^{2}1+1\dfrac{\partial}{\partial x_{1}}%
x_{1}+2\dfrac{\partial}{\partial x_{2}}x_{2}+3\dfrac{\partial}{\partial x_{3}%
}x_{3}+...\right)  =\dfrac{1}{2}\left(  \mu^{2}+1+2+3+...\right)  .
\end{align*}
Unless we interpret $1+2+3+...$ as $-\dfrac{1}{12}$ (which we are going to do
in some sense: the modified formulae further below include $-\dfrac{1}{12}$
factors), this makes no sense.} So we are not allowed to make the definition
(\ref{def.fockvir.wrong}), and we cannot rescue it just by defining a more
liberal notion of convergence. Instead, we must modify this ``definition''.

In order to modify it, we define the so-called \textit{normal ordering}:

\begin{definition}
\label{def.fockvir.normal}For any two integers $m$ and $n$, define the
\textit{normal ordered product }$\left.  :a_{m}a_{n}:\right.  $ in the
universal enveloping algebra $U\left(  \mathcal{A}\right)  $ by
\[
\left.  :a_{m}a_{n}:\right.  \ =\ \left\{
\begin{array}
[c]{c}%
a_{m}a_{n},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq n;\\
a_{n}a_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>n
\end{array}
\right.  .
\]


More generally, for any integers $n_{1}$, $n_{2}$, $...$, $n_{k}$, define the
\textit{normal ordered product }$\left.  :a_{n_{1}}a_{n_{2}}...a_{n_{k}%
}:\right.  $ in the universal enveloping algebra $U\left(  \mathcal{A}\right)
$ by%
\[
\left.  :a_{n_{1}}a_{n_{2}}...a_{n_{k}}:\right.  \ =\left(
\begin{array}
[c]{c}%
\text{the product of the elements }a_{n_{1}}\text{, }a_{n_{2}}\text{,
}...\text{, }a_{n_{k}}\text{ of }U\left(  \mathcal{A}\right)  \text{,}\\
\text{rearranged in such a way that the subscripts are in increasing order}%
\end{array}
\right)  .
\]
(More formally, this normal ordered product $\left.  :a_{n_{1}}a_{n_{2}%
}...a_{n_{k}}:\right.  $ is defined as the product $a_{m_{1}}a_{m_{2}%
}...a_{m_{k}}$, where $\left(  m_{1},m_{2},...,m_{k}\right)  $ is the
permutation of the list $\left(  n_{1},n_{2},...,n_{k}\right)  $ satisfying
$m_{1}\leq m_{2}\leq...\leq m_{k}$.)
\end{definition}

Note that we have thus defined only normal ordered products of elements of the
form $a_{n}$ for $n\in\mathbb{Z}$. Normal ordered products of basis elements
of other Lie algebras are not always defined by the same formulas (although
sometimes they are).

\begin{remark}
\label{rmk.fockvir.normal.mn}If $m$ and $n$ are integers such that $m\neq-n$,
then $\left.  :a_{m}a_{n}:\right.  =a_{m}a_{n}$. (This is because $\left[
a_{m},a_{n}\right]  =0$ in $\mathcal{A}$ when $m\neq-n$.)
\end{remark}

Normal ordered products have the property of being commutative:

\begin{remark}
\label{rmk.fockvir.normal.comm}\textbf{(a)} Any $m\in\mathbb{Z}$ and
$n\in\mathbb{Z}$ satisfy $\left.  :a_{m}a_{n}:\right.  =\left.  :a_{n}%
a_{m}:\right.  $.

\textbf{(b)} Any integers $n_{1}$, $n_{2}$, $...$, $n_{k}$ and any permutation
$\pi\in S_{k}$ satisfy $\left.  :a_{n_{1}}a_{n_{2}}...a_{n_{k}}:\right.
=\left.  :a_{n_{\pi\left(  1\right)  }}a_{n_{\pi\left(  2\right)  }%
}...a_{n_{\pi\left(  k\right)  }}:\right.  $.
\end{remark}

The proof of this is trivial.

By Remark \ref{rmk.fockvir.normal.mn} (and by the rather straightforward
generalization of this fact to many integers), normal ordered products are
rarely different from the usual products. But even when they are different,
they don't differ much:

\begin{remark}
\label{rmk.fockvir.normal.K}Let $m$ and $n$ be integers.

\textbf{(a)} Then, $\left.  :a_{m}a_{n}:\right.  =a_{m}a_{n}+n\left[
m>0\right]  \delta_{m,-n}K$. Here, when $\mathfrak{A}$ is an assertion, we
denote by $\left[  \mathfrak{A}\right]  $ the truth value of $\mathfrak{A}$
(that is, the number $\left\{
\begin{array}
[c]{c}%
1\text{, if }\mathfrak{A}\text{ is true;}\\
0\text{, if }\mathfrak{A}\text{ is false }%
\end{array}
\right.  $).

\textbf{(b)} For any $x\in U\left(  \mathcal{A}\right)  $, we have $\left[
x,\left.  :a_{m}a_{n}:\right.  \right]  =\left[  x,a_{m}a_{n}\right]  $ (where
$\left[  \cdot,\cdot\right]  $ denotes the commutator in $U\left(
\mathcal{A}\right)  $).
\end{remark}

Note that when we denote by $\left[  \cdot,\cdot\right]  $ the commutator in
$U\left(  \mathcal{A}\right)  $, we are seemingly risking a confusion with the
notation $\left[  \cdot,\cdot\right]  $ for the Lie bracket of $\mathcal{A}$
(because we embed $\mathcal{A}$ in $U\left(  \mathcal{A}\right)  $). However,
this confusion is harmless, because the very definition of $U\left(
\mathcal{A}\right)  $ ensures that the commutator of two elements of
$\mathcal{A}$, taken in $U\left(  \mathcal{A}\right)  $, equals to their Lie
bracket in $\mathcal{A}$.

\textit{Proof of Remark \ref{rmk.fockvir.normal.K}.} \textbf{(a)} We
distinguish between three cases:

\textit{Case 1:} We have $m\neq-n$.

\textit{Case 2:} We have $m=-n$ and $m>0$.

\textit{Case 3:} We have $m=-n$ and $m\leq0$.

In Case 1, we have $m\neq-n$, so that $\delta_{m,-n}=0$ and thus%
\[
a_{m}a_{n}+n\left[  m>0\right]  \underbrace{\delta_{m,-n}}_{=0}K=a_{m}%
a_{n}=\left.  :a_{m}a_{n}:\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Remark
\ref{rmk.fockvir.normal.mn}}\right)  .
\]
Hence, Remark \ref{rmk.fockvir.normal.K} \textbf{(a)} is proven in Case 1.

In Case 2, we have $m=-n$ and $m>0$, so that $m>n$, and thus%
\begin{align*}
\left.  :a_{m}a_{n}:\right.  \  &  =\ \left\{
\begin{array}
[c]{c}%
a_{m}a_{n},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq n;\\
a_{n}a_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>n
\end{array}
\right.  =a_{n}a_{m}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }m>n\right) \\
&  =a_{m}a_{n}+\underbrace{\left[  a_{n},a_{m}\right]  }_{=n\delta
_{n,-m}K=n1\delta_{m,-n}K}=a_{m}a_{n}+n\underbrace{1}_{\substack{=\left[
m>0\right]  \\\text{(since }m>0\text{)}}}\delta_{m,-n}K=a_{m}a_{n}+n\left[
m>0\right]  \delta_{m,-n}K.
\end{align*}
Hence, Remark \ref{rmk.fockvir.normal.K} \textbf{(a)} is proven in Case 2.

In Case 3, we have $m=-n$ and $m\leq0$, so that $m\leq n$, and thus%
\begin{align*}
\left.  :a_{m}a_{n}:\right.  \  &  =\ \left\{
\begin{array}
[c]{c}%
a_{m}a_{n},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq n;\\
a_{n}a_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>n
\end{array}
\right.  =a_{m}a_{n}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }m\leq n\right) \\
&  =a_{m}a_{n}+\underbrace{0}_{\substack{=n\left[  m>0\right]  \delta
_{m,-n}K\\\text{(since }m\leq0\text{, so that }\left(  \text{not }m>0\right)
\text{, thus}\\\left[  m>0\right]  =0\text{ and hence }n\left[  m>0\right]
\delta_{m,-n}K=0\text{)}}}=a_{m}a_{n}+n\left[  m>0\right]  \delta_{m,-n}K.
\end{align*}
Hence, Remark \ref{rmk.fockvir.normal.K} \textbf{(a)} is proven in Case 3.

Thus, we have proven Remark \ref{rmk.fockvir.normal.K} \textbf{(a)} in all
three possible cases. This completes the proof of Remark
\ref{rmk.fockvir.normal.K} \textbf{(a)}.

\textbf{(b)} We have $K\in Z\left(  \mathcal{A}\right)  \subseteq Z\left(
U\left(  \mathcal{A}\right)  \right)  $ (since the center of a Lie algebra is
contained in the center of its universal enveloping algebra). Hence, $\left[
x,K\right]  =0$ for any $x\in U\left(  \mathcal{A}\right)  $.

Since $\left.  :a_{m}a_{n}:\right.  =a_{m}a_{n}+n\left[  m>0\right]
\delta_{m,-n}K$, we have%
\begin{align*}
\left[  x,\left.  :a_{m}a_{n}:\right.  \right]   &  =\left[  x,a_{m}%
a_{n}+n\left[  m>0\right]  \delta_{m,-n}K\right] \\
&  =\left[  x,a_{m}a_{n}\right]  +n\left[  m>0\right]  \delta_{m,-n}%
\underbrace{\left[  x,K\right]  }_{=0}=\left[  x,a_{m}a_{n}\right]
\end{align*}
for every $x\in U\left(  \mathcal{A}\right)  $. This proves Remark
\ref{rmk.fockvir.normal.K} \textbf{(b)}.

Now, the true definition of our maps $L_{n}:F_{\mu}\rightarrow F_{\mu}$ will
be the following:

\begin{definition}
\label{def.fockvir}For every $n\in\mathbb{Z}$ and $\mu\in\mathbb{C}$, define a
linear map $L_{n}:F_{\mu}\rightarrow F_{\mu}$ by%
\begin{equation}
L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.
\label{def.fockvir.def}%
\end{equation}
(where $a_{\ell}$ is shorthand notation for $a_{\ell}\mid_{F_{\mu}}$ for every
$\ell\in\mathbb{Z}$). This sum $\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{n+m}:\right.  $ is an infinite sum, but it is well-defined in the
following sense: For any vector $v\in F_{\mu}$, applying $\sum\limits_{m\in
\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  $ to the vector $v$ gives the sum
$\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  v$, which has
only finitely many nonzero addends (because of Lemma \ref{lem.fockvir.welldef}
\textbf{(c)} below) and thus has a well-defined value.
\end{definition}

Note that we have not defined the meaning of the sum $\sum\limits_{m\in
\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  $ in the universal enveloping
algebra $U\left(  \mathcal{A}\right)  $ itself, but only its meaning as an
endomorphism of $F_{\mu}$. However, if we wanted, we could also define the sum
$\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  $ as an element
of a suitable completion of the universal enveloping algebra $U\left(
\mathcal{A}\right)  $ (although not in $U\left(  \mathcal{A}\right)  $
itself). We don't really have a reason to do so here, however.

\begin{Convention}
\label{conv.fockvir.L}During the rest of Section \ref{subsect.fockvir}, we are
going to use the labels $L_{n}$ for the maps $L_{n}:F_{\mu}\rightarrow F_{\mu
}$ introduced in Definition \ref{def.fockvir}, and \textbf{not} for the
eponymous elements of the Virasoro algebra $\operatorname*{Vir}$ or of the
Witt algebra $W$, unless we explicitly refer to ``the element $L_{n}$ of
$\operatorname*{Vir}$'' or ``the element $L_{n}$ of $W$'' or something
similarly unambiguous.

(While it is correct that the maps $L_{n}:F_{\mu}\rightarrow F_{\mu}$ satisfy
the same relations as the eponymous elements $L_{n}$ of $\operatorname*{Vir}$
(but not the eponymous elements $L_{n}$ of $W$), this is a nontrivial fact
that needs to be proven, and until it is proven we must avoid any confusion
between these different meanings of $L_{n}$.)
\end{Convention}

Let us first show that Definition \ref{def.fockvir} makes sense:

\begin{lemma}
\label{lem.fockvir.welldef}Let $n\in\mathbb{Z}$ and $\mu\in\mathbb{C}$. Let
$v\in F_{\mu}$. Then:

\textbf{(a)} If $m\in\mathbb{Z}$ is sufficiently high, then $\left.
:a_{-m}a_{n+m}:\right.  v=0$.

\textbf{(b)} If $m\in\mathbb{Z}$ is sufficiently low, then $\left.
:a_{-m}a_{n+m}:\right.  v=0$.

\textbf{(c)} All but finitely many $m\in\mathbb{Z}$ satisfy $\left.
:a_{-m}a_{n+m}:\right.  v=0$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.fockvir.welldef}.} \textbf{(a)} Since $v\in
F_{\mu}\in\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $, the vector $v$ is
a polynomial in infinitely many variables. Since every polynomial contains
only finitely many variables, there exists an integer $N\in\mathbb{N}$ such
that no variable $x_{r}$ with $r>N$ occurs in $v$. Consider this $N$. Then,
\begin{equation}
\dfrac{\partial}{\partial x_{r}}v=0\ \ \ \ \ \ \ \ \ \ \text{for every integer
}r>N. \label{pf.fockvir.welldef.1}%
\end{equation}


Now, let $m\geq\max\left\{  -n+N+1,-\dfrac{1}{2}n\right\}  $. Then,
$m\geq-n+N+1$ and $m\geq-\dfrac{1}{2}n$.

Since $m\geq-\dfrac{1}{2}n$, we have $2m\geq-n$, so that $-m\leq n+m$.

From $m\geq-n+N+1$, we get $n+m\geq N+1$, so that $n+m>0$. Hence, $a_{n+m}%
\mid_{F_{\mu}}=\left(  n+m\right)  \dfrac{\partial}{\partial x_{n+m}}$, so
that $a_{n+m}v=\left(  n+m\right)  \dfrac{\partial}{\partial x_{n+m}}v$. Since
$\dfrac{\partial}{\partial x_{n+m}}v=0$ (by (\ref{pf.fockvir.welldef.1}),
applied to $r=n+m$ (since $n+m\geq N+1>N$)), we thus have $a_{n+m}v=0$.

By Definition \ref{def.fockvir.normal}, we have
\[
\left.  :a_{-m}a_{n+m}:\right.  \ =\ \left\{
\begin{array}
[c]{c}%
a_{-m}a_{n+m},\ \ \ \ \ \ \ \ \ \ \text{if }-m\leq n+m;\\
a_{n+m}a_{-m},\ \ \ \ \ \ \ \ \ \ \text{if }-m>n+m
\end{array}
\right.  .
\]
Since $-m\leq n+m$, this rewrites as $\left.  :a_{-m}a_{n+m}:\right.
=a_{-m}a_{n+m}$. Thus, $\left.  :a_{-m}a_{n+m}:\right.  v=a_{-m}%
\underbrace{a_{n+m}v}_{=0}=0$, and Lemma \ref{lem.fockvir.welldef}
\textbf{(a)} is proven.

\textbf{(b)} Applying Lemma \ref{lem.fockvir.welldef} \textbf{(a)} to $-n-m$
instead of $m$, we see that, if $m\in\mathbb{Z}$ is sufficiently low, then
$\left.  :a_{-\left(  -n-m\right)  }a_{n+\left(  -n-m\right)  }:\right.  v=0$.
Since%
\[
\left.  :a_{-\left(  -n-m\right)  }a_{n+\left(  -n-m\right)  }:\right.
=\left.  :a_{n+m}a_{-m}:\right.  =\left.  :a_{-m}a_{n+m}:\right.
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Remark \ref{rmk.fockvir.normal.comm}
\textbf{(a)}}\right)  ,
\]
this rewrites as follows: If $m\in\mathbb{Z}$ is sufficiently low, then
$\left.  :a_{-m}a_{n+m}:\right.  v=0$. This proves Lemma
\ref{lem.fockvir.welldef} \textbf{(b)}.

\textbf{(c)} Lemma \ref{lem.fockvir.welldef} \textbf{(c)} follows immediately
by combining Lemma \ref{lem.fockvir.welldef} \textbf{(a)} and Lemma
\ref{lem.fockvir.welldef} \textbf{(b)}.

\begin{remark}
\label{rmk.fockvir.explicit}\textbf{(a)} If $n\neq0$, then the operator
$L_{n}$ defined in Definition \ref{def.fockvir} can be rewritten as%
\[
L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}a_{-m}a_{n+m}.
\]
In other words, for $n\neq0$, our old definition (\ref{def.fockvir.wrong}) of
$L_{n}$ makes sense and is equivalent to the new definition (Definition
\ref{def.fockvir}).

\textbf{(b)} But when $n=0$, the formula (\ref{def.fockvir.wrong}) is devoid
of sense, whereas Definition \ref{def.fockvir} is legit. However, we can
rewrite the definition of $L_{0}$ without using normal ordered products:
Namely, we have%
\[
L_{0}=\sum\limits_{m>0}a_{-m}a_{m}+\dfrac{a_{0}^{2}}{2}=\sum\limits_{m>0}%
a_{-m}a_{m}+\dfrac{\mu^{2}}{2}.
\]


\textbf{(c)} Let us grade the space $F_{\mu}$ as in Definition
\ref{def.fock.grad}. (Recall that this is the grading which gives every
variable $x_{i}$ the degree $-i$ and makes $F_{\mu}=\mathbb{C}\left[
x_{1},x_{2},x_{3},...\right]  $ into a graded $\mathbb{C}$-algebra. This is
\textbf{not} the modified grading that we gave to the space $F_{\mu}$ in
Remark \ref{rmk.fockgrad}.) Let $d\in\mathbb{N}$. Then, every homogeneous
polynomial $f\in F_{\mu}$ of degree $d$ (with respect to this grading)
satisfies $L_{0}f=\left(  \dfrac{\mu^{2}}{2}-d\right)  f$.

\textbf{(d)} Consider the grading on $F_{\mu}$ defined in part \textbf{(c)}.
For every $n\in\mathbb{Z}$, the map $L_{n}:F_{\mu}\rightarrow F_{\mu}$ is
homogeneous of degree $n$. (The notion ``homogeneous of degree $n$'' we are
using here is that defined in Definition \ref{def.hg} \textbf{(a)}, not the
one defined in Definition \ref{def.det.US.poly.hom} \textbf{(a)}.)
\end{remark}

\textit{Proof of Remark \ref{rmk.fockvir.explicit}.} \textbf{(a)} Let $n\neq
0$. Then, every $m\in\mathbb{Z}$ satisfies $-m\neq-\left(  n+m\right)  $ and
thus $\left.  :a_{-m}a_{n+m}:\right.  =a_{-m}a_{n+m}$ (by Remark
\ref{rmk.fockvir.normal.mn}, applied to $-m$ and $n+m$ instead of $m$ and
$n$). Hence, the formula $L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}%
}\left.  :a_{-m}a_{n+m}:\right.  $ (which is how we defined $L_{n}$) rewrites
as $L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}a_{-m}a_{n+m}$. This proves
Remark \ref{rmk.fockvir.explicit} \textbf{(a)}.

\textbf{(b)} By the definition of $L_{0}$ (in Definition \ref{def.fockvir}),
we have%
\begin{align*}
L_{0}  &  =\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}%
a_{0+m}:\right.  =\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}%
a_{m}:\right. \\
&  =\dfrac{1}{2}\left(  \sum\limits_{m<0}\underbrace{\left.  :a_{-m}%
a_{m}:\right.  }_{\substack{=a_{m}a_{-m}\\\text{(by the definition of }\left.
:a_{-m}a_{m}:\right.  \\\text{(since }m<0\text{ and thus }-m>m\text{))}%
}}+\underbrace{\left.  :a_{-0}a_{0}:\right.  }_{\substack{=\left.  :a_{0}%
a_{0}:\right.  =a_{0}a_{0}\\\text{(by the definition of }\left.  :a_{0}%
a_{0}:\right.  \\\text{(since }0\leq0\text{))}}}+\sum\limits_{m>0}%
\underbrace{\left.  :a_{-m}a_{m}:\right.  }_{\substack{=a_{-m}a_{m}\\\text{(by
the definition of }\left.  :a_{-m}a_{m}:\right.  \\\text{(since }m>0\text{ and
thus }-m\leq m\text{))}}}\right) \\
&  =\dfrac{1}{2}\left(  \underbrace{\sum\limits_{m<0}a_{m}a_{-m}%
}_{\substack{=\sum\limits_{m>0}a_{-m}a_{m}\\\text{(here, we substituted
}m\text{ for }-m\text{ in the sum)}}}+\underbrace{a_{0}a_{0}}_{=a_{0}^{2}%
}+\sum\limits_{m>0}a_{-m}a_{m}\right) \\
&  =\dfrac{1}{2}\left(  \sum\limits_{m>0}a_{-m}a_{m}+a_{0}^{2}+\sum
\limits_{m>0}a_{-m}a_{m}\right)  =\dfrac{1}{2}\left(  2\sum\limits_{m>0}%
a_{-m}a_{m}+a_{0}^{2}\right)  =\sum\limits_{m>0}a_{-m}a_{m}+\dfrac{a_{0}^{2}%
}{2}\\
&  =\sum\limits_{m>0}a_{-m}a_{m}+\dfrac{\mu^{2}}{2}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }a_{0}\text{ acts as multiplication with }\mu\text{ on }F_{\mu
}\right)
\end{align*}
on $F_{\mu}$. This proves Remark \ref{rmk.fockvir.explicit} \textbf{(b)}.

\textbf{(c)} We must prove the equation $L_{0}f=\left(  \dfrac{\mu^{2}}%
{2}-d\right)  f$ for every homogeneous polynomial $f\in F_{\mu}$ of degree
$d$. Since this equation is linear in $f$, it is clearly enough to prove this
for the case of $f$ being a monomial\footnote{Here, ``monomial'' means
``monomial without coefficient''.} of degree $d$. So let $f$ be a monomial of
degree $d$. Then, $f$ can be written in the form $f=x_{1}^{\alpha_{1}}%
x_{2}^{\alpha_{2}}x_{3}^{\alpha_{3}}...$ for a sequence $\left(  \alpha
_{1},\alpha_{2},\alpha_{3},...\right)  $ of nonnegative integers such that
$\sum\limits_{m>0}\left(  -m\right)  \alpha_{m}=d$ (the $-m$ coefficient comes
from $\deg\left(  x_{m}\right)  =-m$) and such that all but finitely many
$i\in\left\{  1,2,3,...\right\}  $ satisfy $\alpha_{i}=0$. Consider this
sequence. Clearly, $\sum\limits_{m>0}\left(  -m\right)  \alpha_{m}=d$ yields
$\sum\limits_{m>0}m\alpha_{m}=-d$.

By Remark \ref{rmk.fockvir.explicit} \textbf{(b)}, we have $L_{0}%
=\sum\limits_{m>0}a_{-m}a_{m}+\dfrac{\mu^{2}}{2}$. Since $a_{m}=m\dfrac
{\partial}{\partial x_{m}}$ and $a_{-m}=x_{m}$ for every integer $m>0$ (by the
definition of the action of $a_{m}$ on $F_{\mu}$), this rewrites as
$L_{0}=\sum\limits_{m>0}x_{m}m\dfrac{\partial}{\partial x_{m}}+\dfrac{\mu^{2}%
}{2}$. Now, since $f=x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}x_{3}^{\alpha_{3}%
}...$, every $m>0$ satisfies%
\begin{align*}
x_{m}m\dfrac{\partial}{\partial x_{m}}f  &  =x_{m}m\underbrace{\dfrac
{\partial}{\partial x_{m}}\left(  x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}%
x_{3}^{\alpha_{3}}...\right)  }_{\substack{=\alpha_{m}x_{1}^{\alpha_{1}}%
x_{2}^{\alpha_{2}}...x_{m-1}^{\alpha_{m-1}}x_{m}^{\alpha_{m}-1}x_{m+1}%
^{\alpha_{m+1}}x_{m+2}^{\alpha_{m+2}}...\\\text{(this term should be
understood as }0\text{ if }\alpha_{m}=0\text{)}}}\\
&  =x_{m}m\alpha_{m}x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}...x_{m-1}%
^{\alpha_{m-1}}x_{m}^{\alpha_{m}-1}x_{m+1}^{\alpha_{m+1}}x_{m+2}^{\alpha
_{m+2}}...\\
&  =m\alpha_{m}\cdot\underbrace{x_{m}\cdot x_{1}^{\alpha_{1}}x_{2}^{\alpha
_{2}}...x_{m-1}^{\alpha_{m-1}}x_{m}^{\alpha_{m}-1}x_{m+1}^{\alpha_{m+1}%
}x_{m+2}^{\alpha_{m+2}}...}_{=x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}%
...x_{m-1}^{\alpha_{m-1}}x_{m}^{\alpha_{m}}x_{m+1}^{\alpha_{m+1}}%
x_{m+2}^{\alpha_{m+2}}...=x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}x_{3}%
^{\alpha_{3}}...=f}=m\alpha_{m}f.
\end{align*}
Hence,
\begin{align*}
L_{0}f  &  =\sum\limits_{m>0}\underbrace{x_{m}m\dfrac{\partial}{\partial
x_{m}}f}_{=m\alpha_{m}f}+\dfrac{\mu^{2}}{2}f\ \ \ \ \ \ \ \ \ \ \left(
\text{since }L_{0}=\sum\limits_{m>0}x_{m}m\dfrac{\partial}{\partial x_{m}%
}+\dfrac{\mu^{2}}{2}\right) \\
&  =\underbrace{\sum\limits_{m>0}m\alpha_{m}}_{=-d}f+\dfrac{\mu^{2}}%
{2}f=-df+\dfrac{\mu^{2}}{2}f=\left(  \dfrac{\mu^{2}}{2}-d\right)  f.
\end{align*}
We thus have proven the equation $L_{0}f=\left(  \dfrac{\mu^{2}}{2}-d\right)
f$ for every monomial $f$ of degree $d$. As we said above, this completes the
proof of Remark \ref{rmk.fockvir.explicit} \textbf{(c)}.

\textbf{(d)} For every $m\in\mathbb{Z}$,%
\begin{equation}
\text{the map }a_{m}:F_{\mu}\rightarrow F_{\mu}\text{ is homogeneous of degree
}m\text{.} \label{pf.fockvir.explicit.5}%
\end{equation}
(In fact, this is easily seen from the definition of how $a_{m}$ acts on
$F_{\mu}$.)

Thus, for every $u\in\mathbb{Z}$ and $v\in\mathbb{Z}$, the map $\left.
:a_{u}a_{v}:\right.  $ is homogeneous of degree $u+v$%
\ \ \ \ \footnote{\textit{Proof.} Let $u\in\mathbb{Z}$ and $v\in\mathbb{Z}$.
By (\ref{pf.fockvir.explicit.5}) (applied to $m=u$), the map $a_{u}$ is
homogeneous of degree $u$. Similarly, the map $a_{v}$ is homogeneous of degree
$v$. Thus, the map $a_{u}a_{v}$ is homogeneous of degree $u+v$. Similarly, the
map $a_{v}a_{u}$ is homogeneous of degree $v+u=u+v$.
\par
Since $\left.  :a_{u}a_{v}:\right.  \ =\ \left\{
\begin{array}
[c]{c}%
a_{u}a_{v},\ \ \ \ \ \ \ \ \ \ \text{if }u\leq v;\\
a_{v}a_{u},\ \ \ \ \ \ \ \ \ \ \text{if }u>v
\end{array}
\right.  $ (by the definition of normal ordered products), the map $\left.
:a_{u}a_{v}:\right.  $ equals one of the maps $a_{u}a_{v}$ and $a_{v}a_{u}$.
Since both of these maps $a_{u}a_{v}$ and $a_{v}a_{u}$ are homogeneous of
degree $u+v$, this yields that $\left.  :a_{u}a_{v}:\right.  $ is homogeneous
of degree $u+v$, qed.}. Applied to $u=-m$ and $v=n+m$, this yields: For every
$n\in\mathbb{Z}$ and $m\in\mathbb{Z}$, the map $\left.  :a_{-m}a_{n+m}%
:\right.  $ is homogeneous of degree $\left(  -m\right)  +\left(  n+m\right)
=n$. Now, the map%
\[
L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\underbrace{\left.
:a_{-m}a_{n+m}:\right.  }_{\text{this map is homogeneous of degree }n}%
\]
must be homogeneous of degree $n$. This proves Remark
\ref{rmk.fockvir.explicit} \textbf{(d)}.

Now it turns out that the operators $L_{n}$ that we have defined give a
positive answer to question \textbf{1)}:

\begin{proposition}
\label{prop.fockvir.answer1}Let $n\in\mathbb{Z}$, $m\in\mathbb{Z}$ and $\mu
\in\mathbb{C}$. Then, $\left[  L_{n},a_{m}\right]  =-ma_{n+m}$ (where $L_{n}$
is defined as in Definition \ref{def.fockvir}, and $a_{\ell}$ is shorthand
notation for $a_{\ell}\mid_{F_{\mu}}$).
\end{proposition}

\textit{Proof of Proposition \ref{prop.fockvir.answer1}.} Since%
\[
L_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.
=\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}}\left.  :a_{-j}a_{n+j}:\right.  ,
\]
we have%
\begin{align}
\left[  L_{n},a_{m}\right]   &  =\left[  \dfrac{1}{2}\sum\limits_{j\in
\mathbb{Z}}\left.  :a_{-j}a_{n+j}:\right.  ,a_{m}\right]  =\dfrac{1}{2}%
\sum\limits_{j\in\mathbb{Z}}\underbrace{\left[  \left.  :a_{-j}a_{n+j}%
:\right.  ,a_{m}\right]  }_{=-\left[  a_{m},\left.  :a_{-j}a_{n+j}:\right.
\right]  }\nonumber\\
&  =-\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}}\underbrace{\left[  a_{m},\left.
:a_{-j}a_{n+j}:\right.  \right]  }_{\substack{=\left[  a_{m},a_{-j}%
a_{n+j}\right]  \\\text{(by Remark \ref{rmk.fockvir.normal.K} \textbf{(b)},
applied}\\\text{to }a_{m}\text{, }-j\text{ and }n+j\text{ instead of }x\text{,
}m\text{ and }n\text{)}}}=-\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}%
}\underbrace{\left[  a_{m},a_{-j}a_{n+j}\right]  }_{=\left[  a_{m}%
,a_{-j}\right]  a_{n+j}+a_{-j}\left[  a_{m},a_{n+j}\right]  }\nonumber\\
&  =-\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}}\left(  \underbrace{\left[
a_{m},a_{-j}\right]  }_{=m\delta_{m,-\left(  -j\right)  }K}a_{n+j}%
+a_{-j}\underbrace{\left[  a_{m},a_{n+j}\right]  }_{=m\delta_{m,-\left(
n+j\right)  }K}\right) \nonumber\\
&  =-\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}}\left(  m\underbrace{\delta
_{m,-\left(  -j\right)  }}_{=\delta_{m,j}}Ka_{n+j}+a_{-j}m\underbrace{\delta
_{m,-\left(  n+j\right)  }}_{=\delta_{-m,n+j}=\delta_{-m-n,j}}K\right)
\nonumber\\
&  =-\dfrac{1}{2}\sum\limits_{j\in\mathbb{Z}}\left(  m\delta_{m,j}%
Ka_{n+j}+a_{-j}m\delta_{-m-n,j}K\right)  . \label{pf.fockvir.answer1.2}%
\end{align}


But each of the two sums $\sum\limits_{j\in\mathbb{Z}}m\delta_{m,j}Ka_{n+j}$
and $\sum\limits_{j\in\mathbb{Z}}a_{-j}m\delta_{-m-n,j}K$ is
convergent\footnote{In fact, due to the factors $\delta_{m,j}$ and
$\delta_{-m-n,j}$ in the addends, it is clear that in each of these two sums,
only at most one addend can be nonzero. Concretely:%
\[
\sum\limits_{j\in\mathbb{Z}}m\delta_{m,j}Ka_{n+j}=mKa_{n+m}%
\ \ \ \ \ \ \ \ \ \ \text{and}\ \ \ \ \ \ \ \ \ \ \sum\limits_{j\in\mathbb{Z}%
}a_{-j}m\delta_{-m-n,j}K=a_{-\left(  -m-n\right)  }mK.
\]
}. Hence, we can split the sum $\sum\limits_{j\in\mathbb{Z}}\left(
m\delta_{m,j}Ka_{n+j}+a_{-j}m\delta_{-m-n,j}K\right)  $ into $\sum
\limits_{j\in\mathbb{Z}}m\delta_{m,j}Ka_{n+j}+\sum\limits_{j\in\mathbb{Z}%
}a_{-j}m\delta_{-m-n,j}K$. Thus, (\ref{pf.fockvir.answer1.2}) becomes%
\begin{align*}
\left[  L_{n},a_{m}\right]   &  =-\dfrac{1}{2}\left(  \underbrace{\sum
\limits_{j\in\mathbb{Z}}m\delta_{m,j}Ka_{n+j}}_{=mKa_{n+m}}+\underbrace{\sum
\limits_{j\in\mathbb{Z}}a_{-j}m\delta_{-m-n,j}K}_{=a_{-\left(  -m-n\right)
}mK}\right)  =-\dfrac{1}{2}\left(  mKa_{n+m}+a_{-\left(  -m-n\right)
}mK\right) \\
&  =-\dfrac{1}{2}\left(  ma_{n+m}+a_{-\left(  -m-n\right)  }m\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }K\text{ acts as }\operatorname*{id}%
\text{ on }F_{\mu}\right) \\
&  =-\dfrac{1}{2}m\left(  a_{n+m}+\underbrace{a_{-\left(  -m-n\right)  }%
}_{=a_{m+n}=a_{n+m}}\right)  =-\dfrac{1}{2}m\left(  a_{n+m}+a_{n+m}\right)
=-ma_{n+m}.
\end{align*}
This proves Proposition \ref{prop.fockvir.answer1}.

Now let us check whether our operators $L_{n}$ answer Question \textbf{2)}, or
at least try to do so. We are going to make some ``dirty'' arguments; cleaner
ones can be found in the proof of Proposition \ref{prop.fockvir.answer2} that
we give below.

First, it is easy to see that any $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$
satisfy%
\[
\left[  \left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m},a_{k}\right]
=0\ \ \ \ \ \ \ \ \ \ \text{for any }k\in\mathbb{Z}%
\]
\footnote{\textit{Proof.} Let $n\in\mathbb{Z}$, $m\in\mathbb{Z}$ and
$k\in\mathbb{Z}$. Then,%
\begin{align*}
&  \left[  \left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}%
,a_{k}\right] \\
&  =\underbrace{\left[  \left[  L_{n},L_{m}\right]  ,a_{k}\right]
}_{\substack{=\left[  \left[  L_{n},a_{k}\right]  ,L_{m}\right]  +\left[
L_{n},\left[  L_{m},a_{k}\right]  \right]  \\\text{(by the Leibniz identity
for commutators)}}}-\left(  n-m\right)  \left[  L_{n+m},a_{k}\right] \\
&  =\left[  \underbrace{\left[  L_{n},a_{k}\right]  }_{\substack{=-ka_{n+k}%
\\\text{(by Proposition \ref{prop.fockvir.answer1},}\\\text{applied to
}k\text{ instead of }m\text{)}}},L_{m}\right]  +\left[  L_{n}%
,\underbrace{\left[  L_{m},a_{k}\right]  }_{\substack{=-ka_{m+k}\\\text{(by
Proposition \ref{prop.fockvir.answer1},}\\\text{applied to }m\text{ and
}k\\\text{instead of }n\text{ and }m\text{)}}}\right]  -\left(  n-m\right)
\underbrace{\left[  L_{n+m},a_{k}\right]  }_{\substack{=-ka_{n+m+k}\\\text{(by
Proposition \ref{prop.fockvir.answer1},}\\\text{applied to }n+m\text{ and
}k\\\text{instead of }n\text{ and }m\text{)}}}\\
&  =-k\underbrace{\left[  a_{n+k},L_{m}\right]  }_{=-\left[  L_{m}%
,a_{n+k}\right]  }-k\left[  L_{n},a_{m+k}\right]  +\left(  n-m\right)
ka_{n+m+k}\\
&  =k\underbrace{\left[  L_{m},a_{n+k}\right]  }_{\substack{=-\left(
n+k\right)  a_{m+n+k}\\\text{(by Proposition \ref{prop.fockvir.answer1}%
,}\\\text{applied to }m\text{ and }n+k\text{ instead of }n\text{ and
}m\text{)}}}-k\underbrace{\left[  L_{n},a_{m+k}\right]  }_{\substack{=-\left(
m+k\right)  a_{n+m+k}\\\text{(by Proposition \ref{prop.fockvir.answer1}%
,}\\\text{applied to }m+k\text{ instead of }m\text{)}}}+\left(  n-m\right)
ka_{n+m+k}\\
&  =-k\left(  n+k\right)  \underbrace{a_{m+n+k}}_{=a_{n+m+k}}+k\left(
m+k\right)  a_{n+m+k}+\left(  n-m\right)  ka_{n+m+k}\\
&  =-k\left(  n+k\right)  a_{n+m+k}+k\left(  m+k\right)  a_{n+m+k}+\left(
n-m\right)  ka_{n+m+k}\\
&  =\underbrace{\left(  -k\left(  n+k\right)  +k\left(  m+k\right)  +\left(
n-m\right)  k\right)  }_{=0}a_{n+m+k}=0.
\end{align*}
Qed.}. Hence, for any $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$, the endomorphism
$\left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}$ of $F_{\mu}$ is an
$\mathcal{A}$-module homomorphism (since $\left[  \left[  L_{n},L_{m}\right]
-\left(  n-m\right)  L_{n+m},K\right]  =0$ also holds, for obvious reasons).
Since $F_{\mu}$ is an irreducible $\mathcal{A}$-module of countable dimension,
this yields (by Lemma \ref{lem.dix}) that, for any $n\in\mathbb{Z}$ and
$m\in\mathbb{Z}$, the map $\left[  L_{n},L_{m}\right]  -\left(  n-m\right)
L_{n+m}:F_{\mu}\rightarrow F_{\mu}$ is a scalar multiple of the identity. But
since this map $\left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}$ must
also be homogeneous of degree $n+m$ (by an application of Remark
\ref{rmk.fockvir.explicit} \textbf{(d)}), this yields that $\left[
L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}=0$ whenever $n+m\neq0$
(because any homogeneous map of degree $\neq0$ which is, at the same time, a
scalar multiple of the identity, must be the $0$ map). Thus, for every
$n\in\mathbb{Z}$ and $m\in\mathbb{Z}$, we can write%
\begin{equation}
\left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}=\gamma_{n}%
\delta_{n,-m}\operatorname*{id}\ \ \ \ \ \ \ \ \ \ \text{for some }\gamma
_{n}\in\mathbb{C}\text{ depending on }n\text{.} \label{pf.fockvir.answer2.1}%
\end{equation}
We can get some more information about these $\gamma_{n}$ if we consider the
Lie algebra with basis $\left(  L_{n}\right)  _{n\in\mathbb{Z}}\cup\left(
\operatorname*{id}\right)  $\ \ \ \ \footnote{This is well-defined because (as
the reader can easily check) the family $\left(  L_{n}\right)  _{n\in
\mathbb{Z}}\cup\left(  \operatorname*{id}\right)  $ of operators on $F_{\mu}$
is linearly independent.}. (Note that, according to Convention
\ref{conv.fockvir.L}, these $L_{n}$ still denote maps from $F_{\mu}$ to
$F_{\mu}$, rather than elements of $\operatorname*{Vir}$ or $W$. Of course,
this Lie algebra with basis $\left(  L_{n}\right)  _{n\in\mathbb{Z}}%
\cup\left(  \operatorname*{id}\right)  $ \textbf{will} turn out to be
isomorphic to $\operatorname*{Vir}$, but we have not yet proven this.) This
Lie algebra, due to the formula (\ref{pf.fockvir.answer2.1}) and to the fact
that $\operatorname*{id}$ commutes with everything, must be a $1$-dimensional
central extension of the Witt algebra. Hence, the map
\[
W\times W\rightarrow\mathbb{C},\ \ \ \ \ \ \ \ \ \ \left(  L_{n},L_{m}\right)
\mapsto\gamma_{n}\delta_{n,-m}%
\]
(where $L_{n}$ and $L_{m}$ really mean the elements $L_{n}$ and $L_{m}$ of $W$
this time) must be a $2$-cocycle on $W$. But since we know (from Theorem
\ref{thm.H^2(W)}) that every $2$-cocycle on $W$ is a scalar multiple of the
$2$-cocycle $\omega$ defined in Theorem \ref{thm.H^2(W)} modulo the
$2$-coboundaries, this yields that this $2$-cocycle is a scalar multiple of
$\omega$ modulo the $2$-coboundaries. In other words, there exist
$c\in\mathbb{C}$ and $\xi\in W^{\ast}$ such that%
\[
\gamma_{n}\delta_{n,-m}=c\omega\left(  L_{n},L_{m}\right)  +\xi\left(  \left[
L_{n},L_{m}\right]  \right)  \ \ \ \ \ \ \ \ \ \ \text{for all }n\in
\mathbb{Z}\text{ and }m\in\mathbb{Z}.
\]
Since $\omega\left(  L_{n},L_{m}\right)  =\dfrac{n^{3}-n}{6}\delta_{n,-m}$,
this rewrites as%
\[
\gamma_{n}\delta_{n,-m}=c\dfrac{n^{3}-n}{6}\delta_{n,-m}+\xi\left(  \left[
L_{n},L_{m}\right]  \right)  \ \ \ \ \ \ \ \ \ \ \text{for all }n\in
\mathbb{Z}\text{ and }m\in\mathbb{Z}.
\]
Applied to $m=-n$, this yields%
\begin{equation}
\gamma_{n}=c\dfrac{n^{3}-n}{6}+\xi\left(  \underbrace{\left[  L_{n}%
,L_{-n}\right]  }_{=2nL_{0}}\right)  =c\dfrac{n^{3}-n}{6}+2n\xi\left(
L_{0}\right)  . \label{pf.fockvir.answer2.2}%
\end{equation}


All that remains now, in order to get the values of $\left[  L_{n}%
,L_{m}\right]  -\left(  n-m\right)  L_{n+m}$, is to compute the scalars $c$
and $\xi\left(  L_{0}\right)  $. For this, we only need to compute $\gamma
_{1}$ and $\gamma_{2}$ (because this will give $2$ linear equations for $c$
and $L_{0}$). In order to do this, we will evaluate the endomorphisms $\left[
L_{1},L_{-1}\right]  -2L_{0}$ and $\left[  L_{2},L_{-2}\right]  -4L_{0}$ at
the element $1$ of $F_{\mu}$.

By Remark \ref{rmk.fockvir.explicit} \textbf{(c)} (applied to $d=0$ and
$f=1$), we get $L_{0}1=\left(  \dfrac{\mu^{2}}{2}-0\right)  1=\dfrac{\mu^{2}%
}{2}$.

Since $L_{1}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}%
a_{1+m}:\right.  $, we have $L_{1}1=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}%
}\left.  :a_{-m}a_{1+m}:\right.  1=0$ (because, as it is easily seen, $\left.
:a_{-m}a_{1+m}:\right.  1=0$ for every $m\in\mathbb{Z}$). Similarly,
$L_{2}1=0$.

Since $L_{-1}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}%
a_{-1+m}:\right.  $, we have $L_{-1}1=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}%
}\left.  :a_{-m}a_{-1+m}:\right.  1$. It is easy to see that the only
$m\in\mathbb{Z}$ for which $\left.  :a_{-m}a_{-1+m}:\right.  1$ is nonzero are
$m=0$ and $m=1$. Hence,
\[
\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{-1+m}:\right.
1=\underbrace{\left.  :a_{-0}a_{-1+0}:\right.  1}_{=\left.  :a_{0}%
a_{-1}:\right.  1=a_{-1}a_{0}1=x_{1}\cdot\mu1=\mu x_{1}}+\underbrace{\left.
:a_{-1}a_{-1+1}:\right.  1}_{=\left.  :a_{-1}a_{0}:\right.  1=a_{-1}%
a_{0}1=x_{1}\cdot\mu1=\mu x_{1}}=\mu x_{1}+\mu x_{1}=2\mu x_{1},
\]
so that $L_{-1}1=\dfrac{1}{2}\underbrace{\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{-1+m}:\right.  1}_{=2\mu x_{1}}=\mu x_{1}$. Thus,%
\begin{align*}
L_{1}L_{-1}1  &  =L_{1}\mu x_{1}=\mu\underbrace{L_{1}}_{=\dfrac{1}{2}%
\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{1+m}:\right.  }x_{1}=\mu
\cdot\dfrac{1}{2}\underbrace{\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{1+m}:\right.  x_{1}}_{\substack{=\left.  :a_{-\left(  -1\right)
}a_{1+\left(  -1\right)  }:\right.  x_{1}+\left.  :a_{-0}a_{1+0}:\right.
x_{1}\\\text{(in fact, it is easy to see that the only}\\m\in\mathbb{Z}\text{
for which }\left.  :a_{-m}a_{1+m}:\right.  x_{1}\neq0\text{ are }m=-1\text{
and }m=0\text{)}}}\\
&  =\mu\cdot\dfrac{1}{2}\left(  \underbrace{\left.  :a_{-\left(  -1\right)
}a_{1+\left(  -1\right)  }:\right.  x_{1}}_{=\left.  :a_{1}a_{0}:\right.
x_{1}=a_{0}a_{1}x_{1}=\mu\cdot1\dfrac{\partial}{\partial x_{1}}x_{1}=\mu
}+\underbrace{\left.  :a_{-0}a_{1+0}:\right.  x_{1}}_{=\left.  :a_{0}%
a_{1}:\right.  x_{1}=\mu\cdot1\dfrac{\partial}{\partial x_{1}}x_{1}=\mu
}\right) \\
&  =\mu\cdot\dfrac{1}{2}\left(  \mu+\mu\right)  =\mu^{2}.
\end{align*}


A similar (but messier) computation works for $L_{2}L_{-2}1$: Since
$L_{-2}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{-2+m}%
:\right.  $, we have $L_{-2}1=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{-2+m}:\right.  1$. It is easy to see that the only $m\in\mathbb{Z}$
for which $\left.  :a_{-m}a_{-2+m}:\right.  1$ is nonzero are $m=0$, $m=1$ and
$m=2$. This allows us to simplify $L_{-2}1=\dfrac{1}{2}\sum\limits_{m\in
\mathbb{Z}}\left.  :a_{-m}a_{-2+m}:\right.  1$ to $L_{-2}1=\mu x_{2}+\dfrac
{1}{2}x_{1}^{2}$ (the details are left to the reader). Thus,%
\[
L_{2}L_{-2}1=L_{2}\left(  \mu x_{2}+\dfrac{1}{2}x_{1}^{2}\right)  =\mu
L_{2}x_{2}+\dfrac{1}{2}L_{2}x_{1}^{2}.
\]
Straightforward computations, which I omit, show that $L_{2}x_{2}=2\mu$ and
$L_{2}x_{1}^{2}=1$. Hence,%
\[
L_{2}L_{-2}1=\mu\underbrace{L_{2}x_{2}}_{=2\mu}+\dfrac{1}{2}\underbrace{L_{2}%
x_{1}^{2}}_{=1}=2\mu^{2}+\dfrac{1}{2}.
\]


Now,%
\[
\left(  \left[  L_{1},L_{-1}\right]  -2L_{0}\right)  1=\underbrace{L_{1}%
L_{-1}1}_{=\mu^{2}}-L_{-1}\underbrace{L_{1}1}_{=0}-2\underbrace{L_{0}%
1}_{=\dfrac{\mu^{2}}{2}}=\mu^{2}-0-2\cdot\dfrac{\mu^{2}}{2}=0.
\]
Since%
\begin{align*}
\left[  L_{1},L_{-1}\right]  -2L_{0}  &  =\gamma_{1}\underbrace{\delta
_{1,-\left(  -1\right)  }}_{=1}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.fockvir.answer2.1}), applied to }n=1\text{ and }m=-1\right) \\
&  =\gamma_{1}=c\underbrace{\dfrac{1^{3}-1}{6}}_{=0}+2\cdot1\cdot\xi\left(
L_{0}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.fockvir.answer2.2}%
), applied to }n=1\right) \\
&  =0+2\cdot1\cdot\xi\left(  L_{0}\right)  =2\xi\left(  L_{0}\right)  ,
\end{align*}
this rewrites as $2\xi\left(  L_{0}\right)  \cdot1=0$, so that $\xi\left(
L_{0}\right)  =0$.

On the other hand,%
\[
\left(  \left[  L_{2},L_{-2}\right]  -4L_{0}\right)  1=\underbrace{L_{2}%
L_{-2}1}_{=2\mu^{2}+\dfrac{1}{2}}-L_{-2}\underbrace{L_{2}1}_{=0}%
-4\underbrace{L_{0}1}_{=\dfrac{\mu^{2}}{2}}=\left(  2\mu^{2}+\dfrac{1}%
{2}\right)  -0-4\cdot\dfrac{\mu^{2}}{2}=\dfrac{1}{2}.
\]
Since%
\begin{align*}
\left(  \left[  L_{2},L_{-2}\right]  -4L_{0}\right)  1  &  =\gamma
_{2}\underbrace{\delta_{2,-\left(  -2\right)  }}_{=1}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.fockvir.answer2.1}), applied to
}n=2\text{ and }m=-2\right) \\
&  =\gamma_{2}=c\underbrace{\dfrac{2^{3}-2}{6}}_{=1}+2\cdot2\cdot
\underbrace{\xi\left(  L_{0}\right)  }_{=0}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.fockvir.answer2.2}), applied to }n=2\right) \\
&  =c+0=c,
\end{align*}
this rewrites as $c=\dfrac{1}{2}$.

Due to $\xi\left(  L_{0}\right)  =0$ and $c=\dfrac{1}{2}$, we can rewrite
(\ref{pf.fockvir.answer2.2}) as
\[
\gamma_{n}=\dfrac{1}{2}\cdot\dfrac{n^{3}-n}{6}+2n0=\dfrac{n^{3}-n}{12}.
\]
Hence, (\ref{pf.fockvir.answer2.1}) becomes%
\[
\left[  L_{n},L_{m}\right]  -\left(  n-m\right)  L_{n+m}=\dfrac{n^{3}-n}%
{12}\delta_{n,-m}\operatorname*{id}.
\]
We have thus proven:

\begin{proposition}
\label{prop.fockvir.answer2}For any $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$, we
have%
\begin{equation}
\left[  L_{n},L_{m}\right]  =\left(  n-m\right)  L_{n+m}+\dfrac{n^{3}-n}%
{12}\delta_{n,-m}\operatorname*{id} \label{prop.fockvir.answer2.form}%
\end{equation}
(where $L_{n}$ and $L_{m}$ are maps $F_{\mu}\rightarrow F_{\mu}$ as explained
in Convention \ref{conv.fockvir.L}). Thus, we can make $F_{\mu}$ a
representation of $\operatorname*{Vir}$ by letting the element $L_{n}$ of
$\operatorname*{Vir}$ act as the map $L_{n}:F_{\mu}\rightarrow F_{\mu}$ for
every $n\in\mathbb{Z}$, and letting the element $C$ of $\operatorname*{Vir}$
act as $\operatorname*{id}$.
\end{proposition}

Due to Proposition \ref{prop.fockvir.answer1}, this $\operatorname*{Vir}%
$-action harmonizes with the $\mathcal{A}$-action on $F_{\mu}$:

\begin{proposition}
The $\mathcal{A}$-action on $F_{\mu}$ extends (essentially uniquely) to an
action of $\operatorname*{Vir}\ltimes\mathcal{A}$ on $F_{\mu}$ with $C$ acting
as $1$.
\end{proposition}

This is the reason why the construction of the Virasoro algebra involved the
$2$-cocycle $\dfrac{1}{2}\omega$ rather than $\omega$ (or, actually, rather
than simpler-looking $2$-cocycles like $\left(  L_{n},L_{m}\right)  \mapsto
n^{3}\delta_{n,-m}$).

Our proof of Proposition \ref{prop.fockvir.answer2} above was rather insidious
and nonconstructive: We used the Dixmier theorem to prove (what boils down to)
an algebraic identity, and later we used Theorem \ref{thm.H^2(W)} (which is
constructive but was applied in a rather unexpected way) to reduce our
computations to two concrete cases. We will now show a different, more direct
proof of Proposition \ref{prop.fockvir.answer2}:\footnote{The following proof
is a slight variation of the proof given in the Kac-Raina book (where our
Proposition \ref{prop.fockvir.answer2} is Proposition 2.3).}

\textit{Second proof of Proposition \ref{prop.fockvir.answer2}.} Let
$n\in\mathbb{Z}$ and $m\in\mathbb{Z}$. By (\ref{def.fockvir.def}) (with the
index $m$ renamed as $\ell$), we have $L_{n}=\dfrac{1}{2}\sum\limits_{\ell
\in\mathbb{Z}}\left.  :a_{-\ell}a_{n+\ell}:\right.  $. Hence,%
\begin{align}
\left[  L_{n},L_{m}\right]   &  =\left[  \dfrac{1}{2}\sum\limits_{\ell
\in\mathbb{Z}}\left.  :a_{-\ell}a_{n+\ell}:\right.  ,L_{m}\right]  =\dfrac
{1}{2}\sum\limits_{\ell\in\mathbb{Z}}\underbrace{\left[  \left.  :a_{-\ell
}a_{n+\ell}:\right.  ,L_{m}\right]  }_{=-\left[  L_{m},\left.  :a_{-\ell
}a_{n+\ell}:\right.  \right]  }\nonumber\\
&  =-\dfrac{1}{2}\sum\limits_{\ell\in\mathbb{Z}}\left[  L_{m},\left.
:a_{-\ell}a_{n+\ell}:\right.  \right]  . \label{pf.fockvir.answer2.pf0}%
\end{align}


Now, let $\ell\in\mathbb{Z}$. Then, we obtain $\left[  L_{m},\left.
:a_{-\ell}a_{n+\ell}:\right.  \right]  =\left[  L_{m},a_{-\ell}a_{n+\ell
}\right]  $ (more or less by applying Remark \ref{rmk.fockvir.normal.K}
\textbf{(b)} to $L_{m}$, $-\ell$ and $n+\ell$ instead of $x$, $m$ and
$n$\ \ \ \ \footnote{I am saying ``more or less'' because this is not
completely correct: We cannot apply Remark \ref{rmk.fockvir.normal.K}
\textbf{(b)} to $L_{m}$, $-\ell$ and $n+\ell$ instead of $x$, $m$ and $n$
(since $L_{m}$ does not lie in $U\left(  \mathcal{A}\right)  $). However,
there are two ways to get around this obstruction:
\par
One way is to generalize Remark \ref{rmk.fockvir.normal.K} \textbf{(b)} to a
suitable completion of $U\left(  \mathcal{A}\right)  $. We will not do this
here.
\par
Another way is to notice that we can replace $U\left(  \mathcal{A}\right)  $
by $\operatorname*{End}\left(  F_{\mu}\right)  $ throughout Remark
\ref{rmk.fockvir.normal.K}. (This, of course, means that $a_{n}$ and $a_{m}$
have to be reinterpreted as endomorphisms of $F_{\mu}$ rather than elements of
$\mathcal{A}$; but since the action of $\mathcal{A}$ on $F_{\mu}$ is a Lie
algebra representation, all equalities that hold in $U\left(  \mathcal{A}%
\right)  $ remain valid in $\operatorname*{End}\left(  F_{\mu}\right)  $.) The
proof of Remark \ref{rmk.fockvir.normal.K} still works after this replacement
(except that $\left[  x,K\right]  =0$ should no longer be proven using the
argument $K\in Z\left(  \mathcal{A}\right)  \subseteq Z\left(  U\left(
\mathcal{A}\right)  \right)  $, but simply follows from the fact that $K$ acts
as the identity on $F_{\mu}$). Now, after this replacement, we \textbf{can}
apply Remark \ref{rmk.fockvir.normal.K} \textbf{(b)} to $L_{m}$, $-\ell$ and
$n+\ell$ instead of $x$, $m$ and $n$, and we obtain $\left[  L_{m},\left.
:a_{-\ell}a_{n+\ell}:\right.  \right]  =\left[  L_{m},a_{-\ell}a_{n+\ell
}\right]  $.}), so that%
\begin{align*}
\left[  L_{m},\left.  :a_{-\ell}a_{n+\ell}:\right.  \right]   &  =\left[
L_{m},a_{-\ell}a_{n+\ell}\right] \\
&  =\underbrace{\left[  L_{m},a_{-\ell}\right]  }_{\substack{=-\left(
-\ell\right)  a_{m+\left(  -\ell\right)  }\\\text{(by Proposition
\ref{prop.fockvir.answer1}}\\\text{(applied to }m\text{ and }-\ell\text{
instead of }n\text{ and }m\text{))}}}a_{n+\ell}+a_{-\ell}\underbrace{\left[
L_{m},a_{n+\ell}\right]  }_{\substack{=-\left(  n+\ell\right)  a_{m+\left(
n+\ell\right)  }\\\text{(by Proposition \ref{prop.fockvir.answer1}%
}\\\text{(applied to }m\text{ and }n+\ell\text{ instead of }n\text{ and
}m\text{))}}}\\
&  =\underbrace{-\left(  -\ell\right)  }_{=\ell}\underbrace{a_{m+\left(
-\ell\right)  }}_{=a_{m-\ell}}a_{n+\ell}+\underbrace{a_{-\ell}\left(  -\left(
n+\ell\right)  a_{m+\left(  n+\ell\right)  }\right)  }_{=-\left(
n+\ell\right)  a_{-\ell}a_{m+n+\ell}}\\
&  =\ell a_{m-\ell}a_{n+\ell}-\left(  n+\ell\right)  a_{-\ell}a_{m+n+\ell}.
\end{align*}
Since $a_{m-\ell}a_{n+\ell}=\left.  :a_{m-\ell}a_{n+\ell}:\right.  -\left(
n+\ell\right)  \left[  \ell<m\right]  \delta_{m,-n}\operatorname*{id}%
$\ \ \ \ \footnote{because Remark \ref{rmk.fockvir.normal.K} \textbf{(a)}
(applied to $m-\ell$ and $n+\ell$ instead of $m$ and $n$) yields
\begin{align*}
\left.  :a_{m-\ell}a_{n+\ell}:\right.   &  =a_{m-\ell}a_{n+\ell}+\left(
n+\ell\right)  \underbrace{\left[  m-\ell>0\right]  }_{=\left[  \ell<m\right]
}\underbrace{\delta_{m-\ell,-\left(  n+\ell\right)  }}_{=\delta_{m-\ell
,-n-\ell}=\delta_{m,-n}}\underbrace{K}_{\substack{=\operatorname*{id}%
\\\text{(since }K\text{ acts as }\operatorname*{id}\text{ on }F_{\mu}\text{)}%
}}\\
&  =a_{m-\ell}a_{n+\ell}+\left(  n+\ell\right)  \left[  \ell<m\right]
\delta_{m,-n}\operatorname*{id}%
\end{align*}
} and $a_{-\ell}a_{m+n+\ell}=\left.  :a_{-\ell}a_{m+n+\ell}:\right.
-\ell\left[  \ell<0\right]  \delta_{m,-n}\operatorname*{id}$%
\ \ \ \ \footnote{because Remark \ref{rmk.fockvir.normal.K} \textbf{(a)}
(applied to $\ell$ and $n+m+\ell$ instead of $m$ and $n$) yields
\begin{align*}
\left.  :a_{-\ell}a_{m+n+\ell}:\right.   &  =a_{-\ell}a_{m+n+\ell}+\left(
m+n+\ell\right)  \underbrace{\left[  -\ell>0\right]  }_{=\left[
\ell<0\right]  }\underbrace{\delta_{-\ell,-\left(  m+n+\ell\right)  }%
}_{=\delta_{-\ell,-m-n-\ell}=\delta_{m,-n}}\underbrace{K}%
_{\substack{=\operatorname*{id}\\\text{(since }K\text{ acts as }%
\operatorname*{id}\text{ on }F_{\mu}\text{)}}}\\
&  =a_{-\ell}a_{m+n+\ell}+\underbrace{\left(  m+n+\ell\right)  \left[
\ell<0\right]  }_{=\left[  \ell<0\right]  \left(  m+n+\ell\right)  }%
\delta_{m,-n}\operatorname*{id}\\
&  =a_{-\ell}a_{m+n+\ell}+\left[  \ell<0\right]  \underbrace{\left(
m+n+\ell\right)  \delta_{m,-n}}_{\substack{=\ell\delta_{m,-n}\\\text{(this can
be easily proven by treating}\\\text{the cases of }m=-n\text{ and of }%
m\neq-n\text{ separately)}}}\operatorname*{id}\\
&  =a_{-\ell}a_{m+n+\ell}+\underbrace{\left[  \ell<0\right]  \ell}%
_{=\ell\left[  \ell<0\right]  }\delta_{m,-n}\operatorname*{id}=a_{-\ell
}a_{m+n+\ell}+\ell\left[  \ell<0\right]  \delta_{m,-n}\operatorname*{id}%
\end{align*}
}, this equation rewrites as%
\begin{align}
&  \left[  L_{m},\left.  :a_{-\ell}a_{n+\ell}:\right.  \right] \nonumber\\
&  =\ell\underbrace{a_{m-\ell}a_{n+\ell}}_{=\left.  :a_{m-\ell}a_{n+\ell
}:\right.  -\left(  n+\ell\right)  \left[  \ell<m\right]  \delta
_{m,-n}\operatorname*{id}}-\left(  n+\ell\right)  \underbrace{a_{-\ell
}a_{m+n+\ell}}_{=\left.  :a_{-\ell}a_{m+n+\ell}:\right.  -\ell\left[
\ell<0\right]  \delta_{m,-n}\operatorname*{id}}\nonumber\\
&  =\ell\left(  \left.  :a_{m-\ell}a_{n+\ell}:\right.  -\left(  n+\ell\right)
\left[  \ell<m\right]  \delta_{m,-n}\operatorname*{id}\right)  -\left(
n+\ell\right)  \left(  \left.  :a_{-\ell}a_{m+n+\ell}:\right.  -\ell\left[
\ell<0\right]  \delta_{m,-n}\operatorname*{id}\right) \nonumber\\
&  =\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.  -\ell\left(  n+\ell\right)
\left[  \ell<m\right]  \delta_{m,-n}\operatorname*{id}-\left(  n+\ell\right)
\left.  :a_{-\ell}a_{m+n+\ell}:\right.  +\left(  n+\ell\right)  \ell\left[
\ell<0\right]  \delta_{m,-n}\operatorname*{id}\nonumber\\
&  =\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.  -\underbrace{\left(
n+\ell\right)  }_{=\left(  n-m\right)  +\left(  m+\ell\right)  }\left.
:a_{-\ell}a_{m+n+\ell}:\right. \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\left(  n+\ell\right)  \ell\left[
\ell<0\right]  \delta_{m,-n}\operatorname*{id}-\ell\left(  n+\ell\right)
\left[  \ell<m\right]  \delta_{m,-n}\operatorname*{id}}_{=\ell\left(
n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\delta_{m,-n}\operatorname*{id}}\nonumber\\
&  =\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.  -\underbrace{\left(  \left(
n-m\right)  +\left(  m+\ell\right)  \right)  \left.  :a_{-\ell}a_{m+n+\ell
}:\right.  }_{=\left(  n-m\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.
+\left(  m+\ell\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.  }\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \delta_{m,-n}%
\operatorname*{id}\nonumber\\
&  =\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.  -\left(  n-m\right)  \left.
:a_{-\ell}a_{m+n+\ell}:\right.  -\left(  m+\ell\right)  \left.  :a_{-\ell
}a_{m+n+\ell}:\right. \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \delta_{m,-n}%
\operatorname*{id} \label{pf.fockvir.answer2.pf2}%
\end{align}


Now forget that we fixed $\ell$. We want to use the equality
(\ref{pf.fockvir.answer2.pf2}) in order to split the infinite sum
$\sum\limits_{\ell\in\mathbb{Z}}\left[  L_{m},\left.  :a_{-\ell}a_{n+\ell
}:\right.  \right]  $ on the right hand side of (\ref{pf.fockvir.answer2.pf0})
into
\begin{align*}
&  \sum\limits_{\ell\in\mathbb{Z}}\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.
-\left(  n-m\right)  \sum\limits_{\ell\in\mathbb{Z}}\left.  :a_{-\ell
}a_{m+n+\ell}:\right.  -\sum\limits_{\ell\in\mathbb{Z}}\left(  m+\ell\right)
\left.  :a_{-\ell}a_{m+n+\ell}:\right. \\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\ell\in\mathbb{Z}}\ell\left(
n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\delta_{m,-n}\operatorname*{id}.
\end{align*}
But before we can do this, we must check that this splitting is allowed (since
infinite sums cannot always be split: e. g., the sum $\sum\limits_{\ell
\in\mathbb{Z}}\left(  1-1\right)  $ is well-defined (and has value $0$), but
splitting it into $\sum\limits_{\ell\in\mathbb{Z}}1-\sum\limits_{\ell
\in\mathbb{Z}}1$ is not allowed). Clearly, in order to check this, it is
enough to check that the four infinite sums $\sum\limits_{\ell\in\mathbb{Z}%
}\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.  $, $\sum\limits_{\ell\in
\mathbb{Z}}\left.  :a_{-\ell}a_{m+n+\ell}:\right.  $, $\sum\limits_{\ell
\in\mathbb{Z}}\left(  m+\ell\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.  $
and $\sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \delta_{m,-n}%
\operatorname*{id}$ converge.

Before we do this, let us formalize what we mean by ``converge'': We consider
the product topology on the set $\left(  F_{\mu}\right)  ^{F_{\mu}}$ (the set
of all maps $F_{\mu}\rightarrow F_{\mu}$) by viewing this set as
$\prod\limits_{v\in F_{\mu}}F_{\mu}$, where each $F_{\mu}$ is endowed with the
discrete topology. With respect to this topology, a net $\left(  f_{i}\right)
_{i\in I}$ of maps $f_{i}:F_{\mu}\rightarrow F_{\mu}$ converges to a map
$f:F_{\mu}\rightarrow F_{\mu}$ if and only if%
\[
\left(
\begin{array}
[c]{c}%
\text{for every }v\in F_{\mu}\text{, the net of values }\left(  f_{i}\left(
v\right)  \right)  _{i\in I}\text{ converges to }f\left(  v\right)  \in
F_{\mu}\\
\text{with respect to the discrete topology on }F_{\mu}%
\end{array}
\right)  .
\]
Hence, with respect to this topology, an infinite sum $\sum\limits_{\ell
\in\mathbb{Z}}f_{\ell}$ of maps $f_{\ell}:F_{\mu}\rightarrow F_{\mu}$
converges if and only if%
\[
\left(  \text{for every }v\in F_{\mu}\text{, all but finitely many }\ell
\in\mathbb{Z}\text{ satisfy }f_{\ell}\left(  v\right)  =0\right)  .
\]
Hence, this is exactly the notion of convergence which we used in Definition
\ref{def.fockvir} to make sense of the infinite sum $\sum\limits_{m\in
\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  $.

Now, we are going to show that the infinite sums $\sum\limits_{\ell
\in\mathbb{Z}}\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.  $, $\sum
\limits_{\ell\in\mathbb{Z}}\left.  :a_{-\ell}a_{m+n+\ell}:\right.  $,
$\sum\limits_{\ell\in\mathbb{Z}}\left(  m+\ell\right)  \left.  :a_{-\ell
}a_{m+n+\ell}:\right.  $ and $\sum\limits_{\ell\in\mathbb{Z}}\ell\left(
n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\delta_{m,-n}\operatorname*{id}$ converge with respect to this topology.

\textit{Proof of the convergence of }$\sum\limits_{\ell\in\mathbb{Z}}\left.
:a_{-\ell}a_{m+n+\ell}:\right.  $\textit{:} For every $v\in F_{\mu}$, all but
finitely many $\ell\in\mathbb{Z}$ satisfy $\left.  :a_{-\ell}a_{m+n+\ell
}:\right.  v=0$ (by Lemma \ref{lem.fockvir.welldef} \textbf{(c)}, applied to
$m+n$ and $\ell$ instead of $n$ and $m$). Hence, the sum $\sum\limits_{\ell
\in\mathbb{Z}}\left.  :a_{-\ell}a_{m+n+\ell}:\right.  $ converges.

\textit{Proof of the convergence of }$\sum\limits_{\ell\in\mathbb{Z}}\left(
m+\ell\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.  $\textit{:} For every
$v\in F_{\mu}$, all but finitely many $\ell\in\mathbb{Z}$ satisfy $\left.
:a_{-\ell}a_{m+n+\ell}:\right.  v=0$ (by Lemma \ref{lem.fockvir.welldef}
\textbf{(c)}, applied to $m+n$ and $\ell$ instead of $n$ and $m$). Hence, for
every $v\in F_{\mu}$, all but finitely many $\ell\in\mathbb{Z}$ satisfy
$\left(  m+\ell\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.  =0$. Thus, the
sum $\sum\limits_{\ell\in\mathbb{Z}}\left(  m+\ell\right)  \left.  :a_{-\ell
}a_{m+n+\ell}:\right.  $ converges.

\textit{Proof of the convergence of }$\sum\limits_{\ell\in\mathbb{Z}}%
\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.  $\textit{:} We know that the sum
$\sum\limits_{\ell\in\mathbb{Z}}\left(  m+\ell\right)  \left.  :a_{-\ell
}a_{m+n+\ell}:\right.  $ converges. Thus, we have%
\begin{align}
\sum\limits_{\ell\in\mathbb{Z}}\left(  m+\ell\right)  \left.  :a_{-\ell
}a_{m+n+\ell}:\right.   &  =\sum\limits_{\ell\in\mathbb{Z}}\underbrace{\left(
m+\left(  \ell-m\right)  \right)  }_{=\ell}\left.  :\underbrace{a_{-\left(
\ell-m\right)  }}_{=a_{m-\ell}}\underbrace{a_{m+n+\left(  \ell-m\right)  }%
}_{=a_{n+\ell}}:\right. \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }\ell-m\text{ for
}\ell\text{ in the sum}\right) \nonumber\\
&  =\sum\limits_{\ell\in\mathbb{Z}}\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.
. \label{pf.fockvir.answer2.pf5}%
\end{align}
Hence, the sum $\sum\limits_{\ell\in\mathbb{Z}}\ell\left.  :a_{m-\ell
}a_{n+\ell}:\right.  $ converges.

\textit{Proof of the convergence of }$\sum\limits_{\ell\in\mathbb{Z}}%
\ell\left(  n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[
\ell<m\right]  \right)  \delta_{m,-n}\operatorname*{id}$\textit{:} It is easy
to see that:

\begin{itemize}
\item Every sufficiently small $\ell\in\mathbb{Z}$ satisfies $\ell\left(
n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\delta_{m,-n}\operatorname*{id}=0$.\ \ \ \ \footnote{\textit{Proof.} Every
sufficiently small $\ell\in\mathbb{Z}$ satisfies $\ell<0$ and $\ell<m$ and
thus%
\[
\ell\left(  n+\ell\right)  \left(  \underbrace{\left[  \ell<0\right]
}_{=1\text{ (since }\ell<0\text{)}}-\underbrace{\left[  \ell<m\right]
}_{=1\text{ (since }\ell<m\text{)}}\right)  \delta_{m,-n}\operatorname*{id}%
=\ell\left(  n+\ell\right)  \underbrace{\left(  1-1\right)  }_{=0}%
\delta_{m,-n}\operatorname*{id}=0.
\]
}

\item Every sufficiently high $\ell\in\mathbb{Z}$ satisfies $\ell\left(
n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\delta_{m,-n}\operatorname*{id}=0$.\ \ \ \ \footnote{\textit{Proof.} Every
sufficiently high $\ell\in\mathbb{Z}$ satisfies $\ell\geq0$ and $\ell\geq m$
and thus%
\[
\ell\left(  n+\ell\right)  \left(  \underbrace{\left[  \ell<0\right]
}_{=0\text{ (since }\ell\geq0\text{)}}-\underbrace{\left[  \ell<m\right]
}_{=0\text{ (since }\ell\geq m\text{)}}\right)  \delta_{m,-n}%
\operatorname*{id}=\ell\left(  n+\ell\right)  \underbrace{\left(  0-0\right)
}_{=0}\delta_{m,-n}\operatorname*{id}=0.
\]
}
\end{itemize}

Combining these two results, we conclude that all but finitely many $\ell
\in\mathbb{Z}$ satisfy $\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \delta_{m,-n}%
\operatorname*{id}=0$. The sum $\sum\limits_{\ell\in\mathbb{Z}}\ell\left(
n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\delta_{m,-n}\operatorname*{id}$ therefore converges.

We now know that all four sums that we care about converge, and that two of
them have the same value (by (\ref{pf.fockvir.answer2.pf5})). Let us compute
the other two of the sums:

First of all, by (\ref{def.fockvir.def}) (with the index $m$ renamed as $\ell
$), we have $L_{n}=\dfrac{1}{2}\sum\limits_{\ell\in\mathbb{Z}}\left.
:a_{-\ell}a_{n+\ell}:\right.  $. Applying this to $m+n$ instead of $n$, we get%
\begin{equation}
L_{m+n}=\dfrac{1}{2}\sum\limits_{\ell\in\mathbb{Z}}\left.  :a_{-\ell
}a_{m+n+\ell}:\right.  . \label{pf.fockvir.answer2.pf7}%
\end{equation}
This gives us the value of one of the sums we need.

Finally, let us notice that
\begin{equation}
\sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \delta_{m,-n}%
\operatorname*{id}=-\dfrac{n^{3}-n}{6}\delta_{m,-n}\operatorname*{id}.
\label{pf.fockvir.answer2.pf6}%
\end{equation}


\begin{vershort}
In fact, proving this is a completely elementary computation
exercise\footnote{Indeed, both sides of this equation are $0$ when $m\neq-n$,
so the only nontrivial case is the case when $m=-n$. This case splits further
into two subcases: $m\geq0$ and $m<0$. In the first of these two subcases, the
left hand side of (\ref{pf.fockvir.answer2.pf6}) simplifies as $-\sum
\limits_{\ell=0}^{m-1}\ell\left(  n+\ell\right)  \operatorname*{id}$; in the
second, it simplifies as $\sum\limits_{\ell=m}^{-1}\ell\left(  n+\ell\right)
\operatorname*{id}$. The rest is straightforward computation.}.
\end{vershort}

\begin{verlong}
Indeed, proving this is a completely straightforward
exercise\footnote{\textit{Proof of (\ref{pf.fockvir.answer2.pf6}).} We must be
in one of the following three cases:
\par
\textit{Case 1:} We have $m\neq-n$.
\par
\textit{Case 2:} We have $m=-n$ and $m\geq0$.
\par
\textit{Case 3:} We have $m=-n$ and $m<0$.
\par
First, let us consider Case 1. In this case, $m\neq-n$, so that $\delta
_{m,-n}=0$. Hence, $\sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)
\left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\underbrace{\delta_{m,-n}}_{=0}\operatorname*{id}=\sum\limits_{\ell
\in\mathbb{Z}}0=0$ and $-\dfrac{n^{3}-n}{6}\underbrace{\delta_{m,-n}}%
_{=0}\operatorname*{id}=0$. This shows that $\sum\limits_{\ell\in\mathbb{Z}%
}\ell\left(  n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[
\ell<m\right]  \right)  \delta_{m,-n}\operatorname*{id}=-\dfrac{n^{3}-n}%
{6}\delta_{m,-n}\operatorname*{id}$. Thus, (\ref{pf.fockvir.answer2.pf6}) is
proven in Case 1.
\par
Next, let us consider Case 2. In this case, $m=-n$ and $m\geq0$. Since $m=-n$,
we have $\delta_{m,-n}=1$. Now,%
\begin{align*}
&  \sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \underbrace{\delta_{m,-n}%
}_{=1}\operatorname*{id}\\
&  =\sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \operatorname*{id}\\
&  =\sum\limits_{\ell=-\infty}^{-1}\ell\left(  n+\ell\right)  \left(
\underbrace{\left[  \ell<0\right]  }_{=1\text{ (since }\ell<0\text{)}%
}-\underbrace{\left[  \ell<m\right]  }_{=1\text{ (since }\ell<0\leq m\text{)}%
}\right)  \operatorname*{id}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\ell=0}^{m-1}\ell\left(  n+\ell\right)
\left(  \underbrace{\left[  \ell<0\right]  }_{=0\text{ (since }\ell
\geq0\text{)}}-\underbrace{\left[  \ell<m\right]  }_{=1\text{ (since }%
\ell<m\text{)}}\right)  \operatorname*{id}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\ell=m}^{\infty}\ell\left(
n+\ell\right)  \left(  \underbrace{\left[  \ell<0\right]  }_{=0\text{ (since
}\ell\geq m\geq0\text{)}}-\underbrace{\left[  \ell<m\right]  }_{=0\text{
(since }\ell\geq m\text{)}}\right)  \operatorname*{id}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }m\geq0\right) \\
&  =\sum\limits_{\ell=-\infty}^{-1}\ell\left(  n+\ell\right)
\underbrace{\left(  1-1\right)  }_{=0}\operatorname*{id}+\sum\limits_{\ell
=0}^{m-1}\ell\left(  \underbrace{n}_{\substack{=-m\\\text{(since }%
m=-n\text{)}}}+\ell\right)  \underbrace{\left(  0-1\right)  }_{=-1}%
\operatorname*{id}+\sum\limits_{\ell=m}^{\infty}\ell\left(  n+\ell\right)
\underbrace{\left(  0-0\right)  }_{=0}\operatorname*{id}\\
&  =\underbrace{\sum\limits_{\ell=-\infty}^{-1}0}_{=0}+\sum\limits_{\ell
=0}^{m-1}\underbrace{\ell\left(  -m+\ell\right)  \left(  -1\right)  }%
_{=m\ell-\ell^{2}}\operatorname*{id}+\underbrace{\sum\limits_{\ell=m}^{\infty
}0}_{=0}=\underbrace{\sum\limits_{\ell=0}^{m-1}\left(  m\ell-\ell^{2}\right)
}_{=m\sum\limits_{\ell=0}^{m-1}\ell-\sum\limits_{\ell=0}^{m-1}\ell^{2}%
}\operatorname*{id}\\
&  =\left(  m\underbrace{\sum\limits_{\ell=0}^{m-1}\ell}_{\substack{=\dfrac
{\left(  m-1\right)  m}{2}\\\text{(by standard formulas)}}}-\underbrace{\sum
\limits_{\ell=0}^{m-1}\ell^{2}}_{\substack{=\dfrac{\left(  m-1\right)
m\left(  2m-1\right)  }{6}\\\text{(by standard formulas)}}}\right)
\operatorname*{id}=\underbrace{\left(  m\cdot\dfrac{\left(  m-1\right)  m}%
{2}-\dfrac{\left(  m-1\right)  m\left(  2m-1\right)  }{6}\right)
}_{\substack{=\dfrac{m^{3}-m}{6}=\dfrac{\left(  -n\right)  ^{3}-\left(
-n\right)  }{6}\\\text{(since }m=-n\text{)}}}\operatorname*{id}\\
&  =\underbrace{\dfrac{\left(  -n\right)  ^{3}-\left(  -n\right)  }{6}%
}_{=-\dfrac{n^{3}-n}{6}\cdot1}\operatorname*{id}=-\dfrac{n^{3}-n}{6}%
\cdot\underbrace{1}_{=\delta_{m,-n}}\operatorname*{id}=-\dfrac{n^{3}-n}%
{6}\delta_{m,-n}\operatorname*{id}.
\end{align*}
\par
Thus, (\ref{pf.fockvir.answer2.pf6}) is proven in Case 2.
\par
Finally, let us consider Case 3. In this case, $m=-n$ and $m<0$. Since $m=-n$,
we have $\delta_{m,-n}=1$ and $-m=n$. Now,%
\begin{align*}
&  \sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \underbrace{\delta_{m,-n}%
}_{=1}\operatorname*{id}\\
&  =\sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)  \left(  \left[
\ell<0\right]  -\left[  \ell<m\right]  \right)  \operatorname*{id}\\
&  =\sum\limits_{\ell=-\infty}^{m-1}\ell\left(  n+\ell\right)  \left(
\underbrace{\left[  \ell<0\right]  }_{=1\text{ (since }\ell<m<0\text{)}%
}-\underbrace{\left[  \ell<m\right]  }_{=1\text{ (since }\ell<m\text{)}%
}\right)  \operatorname*{id}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\ell=m}^{-1}\ell\left(  n+\ell\right)
\left(  \underbrace{\left[  \ell<0\right]  }_{=1\text{ (since }\ell<0\text{)}%
}-\underbrace{\left[  \ell<m\right]  }_{=0\text{ (since }\ell\geq m\text{)}%
}\right)  \operatorname*{id}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\ell=0}^{\infty}\ell\left(
n+\ell\right)  \left(  \underbrace{\left[  \ell<0\right]  }_{=0\text{ (since
}\ell\geq0\text{)}}-\underbrace{\left[  \ell<m\right]  }_{=0\text{ (since
}\ell\geq0>m\text{)}}\right)  \operatorname*{id}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }m<0\right) \\
&  =\sum\limits_{\ell=-\infty}^{m-1}\ell\left(  n+\ell\right)
\underbrace{\left(  1-1\right)  }_{=0}\operatorname*{id}+\sum\limits_{\ell
=m}^{-1}\ell\left(  n+\ell\right)  \underbrace{\left(  1-0\right)  }%
_{=1}\operatorname*{id}+\sum\limits_{\ell=0}^{\infty}\ell\left(
n+\ell\right)  \underbrace{\left(  0-0\right)  }_{=0}\operatorname*{id}\\
&  =\underbrace{\sum\limits_{\ell=-\infty}^{m-1}0}_{=0}+\sum\limits_{\ell
=m}^{-1}\ell\left(  n+\ell\right)  1\operatorname*{id}+\underbrace{\sum
\limits_{\ell=0}^{\infty}0}_{=0}=\sum\limits_{\ell=m}^{-1}\ell\left(
n+\ell\right)  1\operatorname*{id}=\sum\limits_{\ell=m}^{-1}\ell\left(
n+\ell\right)  \operatorname*{id}\\
&  =\underbrace{\sum\limits_{\ell=1}^{-m}}_{\substack{=\sum\limits_{\ell
=1}^{n}\\\text{(since }-m=n\text{)}}}\underbrace{\left(  -\ell\right)  \left(
n+\left(  -\ell\right)  \right)  }_{=\ell^{2}-n\ell}\operatorname*{id}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }\ell\text{ for }%
-\ell\text{ in the sum}\right) \\
&  =\underbrace{\sum\limits_{\ell=1}^{n}\left(  \ell^{2}-n\ell\right)
}_{=\sum\limits_{\ell=1}^{n}\ell^{2}-n\sum\limits_{\ell=1}^{n}\ell
}\operatorname*{id}=\left(  \underbrace{\sum\limits_{\ell=1}^{n}\ell^{2}%
}_{\substack{=\dfrac{n\left(  n+1\right)  \left(  2n+1\right)  }{6}\\\text{(by
standard formulas)}}}-n\underbrace{\sum\limits_{\ell=1}^{n}\ell}%
_{\substack{=\dfrac{n\left(  n+1\right)  }{2}\\\text{(by standard formulas)}%
}}\right)  \operatorname*{id}\\
&  =\underbrace{\left(  \dfrac{n\left(  n+1\right)  \left(  2n+1\right)  }%
{6}-n\cdot\dfrac{n\left(  n+1\right)  }{2}\right)  }_{=-\dfrac{n^{3}-n}{6}%
}\underbrace{\operatorname*{id}}_{=1\operatorname*{id}}=-\dfrac{n^{3}-n}%
{6}\underbrace{1}_{=\delta_{m,-n}}\operatorname*{id}=-\dfrac{n^{3}-n}{6}%
\delta_{m,-n}\operatorname*{id}.
\end{align*}
\par
Thus, (\ref{pf.fockvir.answer2.pf6}) is proven in Case 3.
\par
We have therefore proven (\ref{pf.fockvir.answer2.pf6}) in each of the three
cases 1, 2 and 3. Since these three cases cover all possibilities, this
completes the proof of (\ref{pf.fockvir.answer2.pf6}).}.
\end{verlong}

Now, since (\ref{pf.fockvir.answer2.pf2}) holds for every $\ell\in\mathbb{Z}$,
we have
\begin{align*}
&  \sum\limits_{\ell\in\mathbb{Z}}\left[  L_{m},\left.  :a_{-\ell}a_{n+\ell
}:\right.  \right] \\
&  =\sum\limits_{\ell\in\mathbb{Z}}\left(  \ell\left.  :a_{m-\ell}a_{n+\ell
}:\right.  -\left(  n-m\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.
-\left(  m+\ell\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.  \right. \\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left.  +\ell\left(  n+\ell\right)
\left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)  \delta
_{m,-n}\operatorname*{id}\right) \\
&  =\sum\limits_{\ell\in\mathbb{Z}}\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.
-\left(  n-m\right)  \underbrace{\sum\limits_{\ell\in\mathbb{Z}}\left.
:a_{-\ell}a_{m+n+\ell}:\right.  }_{\substack{=2L_{m+n}\\\text{(by
(\ref{pf.fockvir.answer2.pf7}))}}}-\underbrace{\sum\limits_{\ell\in\mathbb{Z}%
}\left(  m+\ell\right)  \left.  :a_{-\ell}a_{m+n+\ell}:\right.  }%
_{\substack{=\sum\limits_{\ell\in\mathbb{Z}}\ell\left.  :a_{m-\ell}a_{n+\ell
}:\right.  \\\text{(by (\ref{pf.fockvir.answer2.pf5}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum\limits_{\ell\in\mathbb{Z}}\ell\left(
n+\ell\right)  \left(  \left[  \ell<0\right]  -\left[  \ell<m\right]  \right)
\delta_{m,-n}\operatorname*{id}}_{\substack{=-\dfrac{n^{3}-n}{6}\delta
_{m,-n}\operatorname*{id}\\\text{(by (\ref{pf.fockvir.answer2.pf6}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split the sum; this was allowed, since the infinite
sums}\\
\sum\limits_{\ell\in\mathbb{Z}}\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.
\text{, }\sum\limits_{\ell\in\mathbb{Z}}\left.  :a_{-\ell}a_{m+n+\ell
}:\right.  \text{, }\sum\limits_{\ell\in\mathbb{Z}}\left(  m+\ell\right)
\left.  :a_{-\ell}a_{m+n+\ell}:\right. \\
\text{and }\sum\limits_{\ell\in\mathbb{Z}}\ell\left(  n+\ell\right)  \left(
\left[  \ell<0\right]  -\left[  \ell<m\right]  \right)  \delta_{m,-n}%
\operatorname*{id}\text{ converge}%
\end{array}
\right) \\
&  =\sum\limits_{\ell\in\mathbb{Z}}\ell\left.  :a_{m-\ell}a_{n+\ell}:\right.
-\left(  n-m\right)  \cdot2L_{m+n}-\sum\limits_{\ell\in\mathbb{Z}}\ell\left.
:a_{m-\ell}a_{n+\ell}:\right.  -\dfrac{n^{3}-n}{6}\delta_{m,-n}%
\operatorname*{id}\\
&  =-\left(  n-m\right)  \cdot2L_{m+n}-\dfrac{n^{3}-n}{6}\delta_{m,-n}%
\operatorname*{id}.
\end{align*}
Hence, (\ref{pf.fockvir.answer2.pf0}) becomes%
\begin{align*}
\left[  L_{n},L_{m}\right]   &  =-\dfrac{1}{2}\underbrace{\sum\limits_{\ell
\in\mathbb{Z}}\left[  L_{m},\left.  :a_{-\ell}a_{n+\ell}:\right.  \right]
}_{=-\left(  n-m\right)  \cdot2L_{m+n}-\dfrac{n^{3}-n}{6}\delta_{m,-n}%
\operatorname*{id}}\\
&  =-\dfrac{1}{2}\left(  -\left(  n-m\right)  \cdot2L_{m+n}-\dfrac{n^{3}-n}%
{6}\delta_{m,-n}\operatorname*{id}\right) \\
&  =\left(  n-m\right)  \underbrace{L_{m+n}}_{=L_{n+m}}-\dfrac{n^{3}-n}%
{12}\underbrace{\delta_{m,-n}}_{=\delta_{n,-m}}\operatorname*{id}=\left(
n-m\right)  L_{n+m}-\dfrac{n^{3}-n}{12}\delta_{n,-m}\operatorname*{id}.
\end{align*}


This proves Proposition \ref{prop.fockvir.answer2}.

We can generalize our family $\left(  L_{n}\right)  _{n\in\mathbb{Z}}$ of
operators on $F_{\mu}$ as follows (the so-called \textit{Fairlie construction}):

\begin{theorem}
\label{thm.fockvir.hw2ex1}Let $\mu\in\mathbb{C}$ and $\lambda\in\mathbb{C}$.
We can define a linear map $\widetilde{L}_{n}:F_{\mu}\rightarrow F_{\mu}$ for
every $n\in\mathbb{Z}$ as follows: For $n\neq0$, define the map $\widetilde{L}%
_{n}$ by%
\[
\widetilde{L}_{n}=\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{m+n}:\right.  +i\lambda na_{n}%
\]
(where $i$ stands for the complex number $\sqrt{-1}$). Define the map
$\widetilde{L}_{0}$ by%
\[
\widetilde{L}_{0}=\dfrac{\mu^{2}}{2}+\dfrac{\lambda^{2}}{2}+\sum
\limits_{j>0}a_{-j}a_{j}.
\]
Then, this defines an action of $\operatorname*{Vir}$ on $F_{\mu}$ with
$c=1+12\lambda^{2}$ (by letting $L_{n}\in\operatorname*{Vir}$ act as the
operator $\widetilde{L}_{n}$, and by letting $C\in\operatorname*{Vir}$ acting
as $\left(  1+12\lambda^{2}\right)  \operatorname*{id}$). Moreover, it
satisfies $\left[  \widetilde{L}_{n},a_{m}\right]  =-ma_{n+m}+i\lambda
n^{2}\delta_{n,-m}\operatorname*{id}$ for all $n\in\mathbb{Z}$ and
$m\in\mathbb{Z}$.
\end{theorem}

Proving this proposition was exercise 1 in homework problem set 2. It is
rather easy now that we have proven Propositions \ref{prop.fockvir.answer1}
and \ref{prop.fockvir.answer2} and thus left to the reader.

\subsubsection{\textbf{[unfinished]} Unitarity properties of the Fock module}

\begin{proposition}
\label{prop.fockvir.unitary}Let $\mu\in\mathbb{R}$. Consider the
representation $F_{\mu}$ of $\mathcal{A}$. Let $\left\langle \cdot
,\cdot\right\rangle :F_{\mu}\times F_{\mu}\rightarrow\mathbb{C}$ be the unique
Hermitian form satisfying $\left\langle 1,1\right\rangle =1$ and%
\begin{equation}
\left\langle av,w\right\rangle =\left\langle v,a^{\dag}w\right\rangle
\ \ \ \ \ \ \ \ \ \ \text{for all }a\in\mathcal{A}\text{, }v\in F_{\mu}\text{
and }w\in F_{\mu} \label{pf.fockvir.unitary.1}%
\end{equation}
(this is the usual Hermitian form on $F_{\mu}$). Then, equipped with this
form, $F_{\mu}$ is a unitary representation of $\mathcal{A}$.
\end{proposition}

\textit{Proof.} We must prove that the form $\left\langle \cdot,\cdot
\right\rangle $ is positive definite.

Let $\overrightarrow{n}=\left(  n_{1},n_{2},n_{3},...\right)  $ and
$\overrightarrow{m}=\left(  m_{1},m_{2},m_{3},...\right)  $ be two sequences
of nonnegative integers, each of them containing only finitely many nonzero
entries. We are going to compute the value $\left\langle x_{1}^{n_{1}}%
x_{2}^{n_{2}}x_{3}^{n_{3}}...,x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}%
}...\right\rangle $. This will give us the matrix that represents the
Hermitian form $\left\langle \cdot,\cdot\right\rangle $ with respect to the
monomial basis of $F_{\mu}$.

If $n_{1}+n_{2}+n_{3}+...\neq m_{1}+m_{2}+m_{3}+...$, then this value is
clearly zero, because the Hermitian form $\left\langle \cdot,\cdot
\right\rangle $ is of degree $0$ (as can be easily seen). Thus, we can WLOG
assume that $n_{1}+n_{2}+n_{3}+...=m_{1}+m_{2}+m_{3}+...$.

Let $k$ be a positive integer such that every $i>k$ satisfies $n_{i}=0$ and
$m_{i}=0$. (Such a $k$ clearly exists.) Then, $n_{1}+n_{2}+...+n_{k}%
=n_{1}+n_{2}+n_{3}+...$ and $m_{1}+m_{2}+...+m_{k}=m_{1}+m_{2}+m_{3}+...$.
Hence, the equality $n_{1}+n_{2}+n_{3}+...=m_{1}+m_{2}+m_{3}+...$ (which we
know to hold) rewrites as $n_{1}+n_{2}+...+n_{k}=m_{1}+m_{2}+...+m_{k}$. Now,
since every $i>k$ satisfies $n_{i}=0$ and $m_{i}=0$, we have%
\begin{align*}
&  \left\langle x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...,x_{1}^{m_{1}}%
x_{2}^{m_{2}}x_{3}^{m_{3}}...\right\rangle \\
&  =\left\langle x_{1}^{n_{1}}x_{2}^{n_{2}}...x_{k}^{n_{k}},\underbrace{x_{1}%
^{m_{1}}x_{2}^{m_{2}}...x_{k}^{m_{k}}}_{\substack{=a_{-1}^{m_{1}}a_{-2}%
^{m_{2}}...a_{-k}^{m_{k}}1\\=\left(  a_{1}^{\dag}\right)  ^{m_{1}}\left(
a_{2}^{\dag}\right)  ^{m_{2}}...\left(  a_{k}^{\dag}\right)  ^{m_{k}}%
1}}\right\rangle =\left\langle x_{1}^{n_{1}}x_{2}^{n_{2}}...x_{k}^{n_{k}%
},\left(  a_{1}^{\dag}\right)  ^{m_{1}}\left(  a_{2}^{\dag}\right)  ^{m_{2}%
}...\left(  a_{k}^{\dag}\right)  ^{m_{k}}1\right\rangle \\
&  =\left\langle a_{k}^{m_{k}}a_{k-1}^{m_{k-1}}...a_{1}^{m_{1}}x_{1}^{n_{1}%
}x_{2}^{n_{2}}...x_{k}^{n_{k}},1\right\rangle \ \ \ \ \ \ \ \ \ \ \left(
\text{due to (\ref{pf.fockvir.unitary.1}), applied several times}\right) \\
&  =\left\langle \underbrace{\left(  k\dfrac{\partial}{\partial x_{k}}\right)
^{m_{k}}\left(  \left(  k-1\right)  \dfrac{\partial}{\partial x_{k-1}}\right)
^{m_{k-1}}...\left(  1\dfrac{\partial}{\partial x_{1}}\right)  ^{m_{1}}%
x_{1}^{n_{1}}x_{2}^{n_{2}}...x_{k}^{n_{k}}}_{\substack{\text{this is a
constant polynomial,}\\\text{since }n_{1}+n_{2}+...+n_{k}=m_{1}+m_{2}%
+...+m_{k}}},1\right\rangle \\
&  =\left(  k\dfrac{\partial}{\partial x_{k}}\right)  ^{m_{k}}\left(  \left(
k-1\right)  \dfrac{\partial}{\partial x_{k-1}}\right)  ^{m_{k-1}}...\left(
1\dfrac{\partial}{\partial x_{1}}\right)  ^{m_{1}}x_{1}^{n_{1}}x_{2}^{n_{2}%
}...x_{k}^{n_{k}}\\
&  =\prod\limits_{j=1}^{k}j^{m_{j}}\cdot\underbrace{\left(  \dfrac{\partial
}{\partial x_{k}}\right)  ^{m_{k}}\left(  \dfrac{\partial}{\partial x_{k-1}%
}\right)  ^{m_{k-1}}...\left(  \dfrac{\partial}{\partial x_{1}}\right)
^{m_{1}}x_{1}^{n_{1}}x_{2}^{n_{2}}...x_{k}^{n_{k}}}_{\substack{=\delta
_{\overrightarrow{n},\overrightarrow{m}}\cdot\prod\limits_{j=1}^{k}%
m_{j}!\\\text{(since }n_{1}+n_{2}+...+n_{k}=m_{1}+m_{2}+...+m_{k}\text{)}%
}}=\delta_{\overrightarrow{n},\overrightarrow{m}}\cdot\prod\limits_{j=1}%
^{k}j^{m_{j}}\prod\limits_{j=1}^{k}m_{j}!.
\end{align*}
This term is $0$ when $\overrightarrow{n}\neq\overrightarrow{m}$, and a
positive integer when $\overrightarrow{n}=\overrightarrow{m}$. Thus, the
matrix which represents the form $\left\langle \cdot,\cdot\right\rangle $ with
respect to the monomial basis of $F_{\mu}$ is diagonal with positive diagonal
entries. This form is therefore positive definite. Proposition
\ref{prop.fockvir.unitary} is proven.

\begin{corollary}
If $\mu,\lambda\in\mathbb{R}$, then the $\operatorname*{Vir}$-representation
on $F_{\mu}$ given by $\widetilde{L}_{n}$ is unitary.
\end{corollary}

\textit{Proof.} For $n\neq0$, we have%
\begin{align*}
\widetilde{L}_{n}^{\dag}  &  =\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{n+m}:\right.  ^{\dag}+\left(  i\lambda na_{n}\right)  ^{\dag}\\
&  =\dfrac{1}{2}\sum\limits_{m\in\mathbb{Z}}\left.  :a_{m}a_{-n-m}:\right.
-i\lambda na_{-n}=\widetilde{L}_{-n}.
\end{align*}


\begin{corollary}
The $\operatorname*{Vir}$-representation $F_{\mu}$ is completely reducible for
$\mu\in\mathbb{R}$.
\end{corollary}

Now, $L_{0}1=\dfrac{\mu^{2}+\lambda^{2}}{2}1$ and $C1=\left(  1+12\lambda
^{2}\right)  1$. Thus, the Verma module $M_{h,c}:=M_{h,c}^{+}$ of the Virasoro
algebra $\operatorname*{Vir}$ for $h=\dfrac{\mu^{2}+\lambda^{2}}{2}$ and
$c=1+12\lambda^{2}$ maps to $F_{\mu}$ with $v_{h,c}\mapsto1$.

\begin{proposition}
For Weil generic $\mu$ and $\lambda$, this is an isomorphism.
\end{proposition}

\textit{Proof.} The dimension of the degree-$n$ part of both modules is
$p\left(  n\right)  $. The map has degree $0$. Hence, if it is injective, it
is surjective. But for Weil generic $\mu$ and $\lambda$, the
$\operatorname*{Vir}$-module $M_{h,c}$ is irreducible, so the map is injective.

\begin{corollary}
For Weil generic $\mu$ and $\lambda$ in $\mathbb{R}$, the representation
$M_{\dfrac{\mu^{2}+\lambda^{2}}{2},1+12\lambda^{2}}$ is unitary.

For any $\mu$ and $\lambda$ in $\mathbb{R}$, the representation $L_{\dfrac
{\mu^{2}+\lambda^{2}}{2},1+12\lambda^{2}}$ is unitary.

In other words, $L_{h,c}$ is unitary if $c\geq1$ and $h\geq\dfrac{c-1}{24}$.
\end{corollary}

\subsection{\label{subsect.quantumfields}Power series and quantum fields}

In this section, we are going to study different kinds of power series:
polynomials, formal power series, Laurent polynomials, Laurent series and,
finally, a notion of ``formal power series'' which can be infinite ``in both
directions''. Each of these kinds of power series will later be used in our
work; it is important to know the properties and the shortcomings of each of them.

\subsubsection{Definitions}

Parts of the following definition should sound familiar to the reader (indeed,
we have already been working with polynomials, formal power series and Laurent
polynomials), although maybe not in this generality.

\begin{definition}
\label{def.qf.powerseries}For every vector space $B$ and symbol $z$, we make
the following definitions:

\textbf{(a)} We denote by $B\left[  z\right]  $ the vector space of all
sequences $\left(  b_{n}\right)  _{n\in\mathbb{N}}\in B^{\mathbb{N}}$ such
that only finitely many $n\in\mathbb{N}$ satisfy $b_{n}\neq0$. Such a sequence
$\left(  b_{n}\right)  _{n\in\mathbb{N}}$ is denoted by $\sum\limits_{n\in
\mathbb{N}}b_{n}z^{n}$. The elements of $B\left[  z\right]  $ are called
\textit{polynomials in the indeterminate }$z$ \textit{over }$B$ (even when $B$
is not a ring).

\textbf{(b)} We denote by $B\left[  \left[  z\right]  \right]  $ the vector
space of all sequences $\left(  b_{n}\right)  _{n\in\mathbb{N}}\in
B^{\mathbb{N}}$. Such a sequence $\left(  b_{n}\right)  _{n\in\mathbb{N}}$ is
denoted by $\sum\limits_{n\in\mathbb{N}}b_{n}z^{n}$. The elements of $B\left[
\left[  z\right]  \right]  $ are called \textit{formal power series in the
indeterminate }$z$ \textit{over }$B$ (even when $B$ is not a ring).

\textbf{(c)} We denote by $B\left[  z,z^{-1}\right]  $ the vector space of all
two-sided sequences $\left(  b_{n}\right)  _{n\in\mathbb{Z}}\in B^{\mathbb{Z}%
}$ such that only finitely many $n\in\mathbb{Z}$ satisfy $b_{n}\neq0$. (A
\textit{two-sided sequence} means a sequence indexed by integers, not just
nonnegative integers.) Such a sequence $\left(  b_{n}\right)  _{n\in
\mathbb{Z}}$ is denoted by $\sum\limits_{n\in\mathbb{Z}}b_{n}z^{n}$. The
elements of $B\left[  z,z^{-1}\right]  $ are called\textit{ Laurent
polynomials in the indeterminate }$z$ \textit{over }$B$ (even when $B$ is not
a ring).

\textbf{(d)} We denote by $B\left(  \left(  z\right)  \right)  $ the vector
space of all two-sided sequences $\left(  b_{n}\right)  _{n\in\mathbb{Z}}\in
B^{\mathbb{Z}}$ such that only finitely many among the negative $n\in
\mathbb{Z}$ satisfy $b_{n}\neq0$. (A \textit{two-sided sequence} means a
sequence indexed by integers, not just nonnegative integers.) Such a sequence
$\left(  b_{n}\right)  _{n\in\mathbb{Z}}$ is denoted by $\sum\limits_{n\in
\mathbb{Z}}b_{n}z^{n}$. Sometimes, $B\left(  \left(  z\right)  \right)  $ is
also denoted by $B\left[  \left[  z,z^{-1}\right.  \right]  $. The elements of
$B\left(  \left(  z\right)  \right)  $ are called \textit{formal Laurent
series in the indeterminate }$z$ \textit{over }$B$ (even when $B$ is not a ring).

\textbf{(e)} We denote by $B\left[  \left[  z,z^{-1}\right]  \right]  $ the
vector space of all two-sided sequences $\left(  b_{n}\right)  _{n\in
\mathbb{Z}}\in B^{\mathbb{Z}}$. Such a sequence $\left(  b_{n}\right)
_{n\in\mathbb{Z}}$ is denoted by $\sum\limits_{n\in\mathbb{Z}}b_{n}z^{n}$.

All five of these spaces $B\left[  z\right]  $, $B\left[  \left[  z\right]
\right]  $, $B\left[  z,z^{-1}\right]  $, $B\left(  \left(  z\right)  \right)
$ and $B\left[  \left[  z,z^{-1}\right]  \right]  $ are $\mathbb{C}\left[
z\right]  $-modules. (Here, the $\mathbb{C}\left[  z\right]  $-module
structure on $B\left[  \left[  z,z^{-1}\right]  \right]  $ is given by%
\begin{equation}
\left(  \sum\limits_{n\in\mathbb{N}}c_{n}z^{n}\right)  \cdot\left(
\sum\limits_{n\in\mathbb{Z}}b_{n}z^{n}\right)  =\sum\limits_{n\in\mathbb{Z}%
}\left(  \sum\limits_{m\in\mathbb{N}}c_{m}\cdot b_{n-m}\right)  z^{n}
\label{def.qf.powerseries.prod.prototype}%
\end{equation}
for all $\sum\limits_{n\in\mathbb{Z}}b_{n}z^{n}\in B\left[  \left[
z,z^{-1}\right]  \right]  $ and $\sum\limits_{n\in\mathbb{N}}c_{n}z^{n}%
\in\mathbb{C}\left[  z\right]  $, and the $\mathbb{C}\left[  z\right]
$-module structures on the other four spaces are defined similarly.) Besides,
$B\left[  \left[  z\right]  \right]  $ and $B\left(  \left(  z\right)
\right)  $ are $\mathbb{C}\left[  \left[  z\right]  \right]  $-modules
(defined in a similar way to (\ref{def.qf.powerseries.prod.prototype})). Also,
$B\left(  \left(  z\right)  \right)  $ is a $\mathbb{C}\left(  \left(
z\right)  \right)  $-module (in a similar way). Besides, $B\left[
z,z^{-1}\right]  $, $B\left(  \left(  z\right)  \right)  $ and $B\left[
\left[  z,z^{-1}\right]  \right]  $ are $\mathbb{C}\left[  z,z^{-1}\right]
$-modules (defined analogously to (\ref{def.qf.powerseries.prod.prototype})).

Of course, if $B$ is a $\mathbb{C}$-algebra, then the above-defined spaces
$B\left[  z\right]  $, $B\left[  z,z^{-1}\right]  $, $B\left[  \left[
z\right]  \right]  $ and $B\left(  \left(  z\right)  \right)  $ are
$\mathbb{C}$-algebras themselves (with the multiplication defined similarly to
(\ref{def.qf.powerseries.prod.prototype})), and in fact $B\left[  z\right]  $
is the algebra of polynomials in the variable $z$ over $B$, and $B\left[
z,z^{-1}\right]  $ is the algebra of Laurent polynomials in the variable $z$
over $B$, and $B\left[  \left[  z\right]  \right]  $ is the algebra of formal
power series in the variable $z$ over $B$.

It should be noticed that $B\left[  z\right]  \cong B\otimes\mathbb{C}\left[
z\right]  $ and $B\left[  z,z^{-1}\right]  \cong B\otimes\mathbb{C}\left[
z,z^{-1}\right]  $ canonically, but such isomorphisms do \textbf{not} hold for
$B\left[  \left[  z\right]  \right]  $, $B\left(  \left(  z\right)  \right)  $
and $B\left[  \left[  z,z^{-1}\right]  \right]  $ unless $B$ is finite-dimensional.

We regard the obvious injections $B\left[  z\right]  \rightarrow B\left[
z,z^{-1}\right]  $, $B\left[  z^{-1}\right]  \rightarrow B\left[
z,z^{-1}\right]  $ (this is the map sending $z^{-1}\in B\left[  z^{-1}\right]
$ to $z^{-1}\in B\left[  z,z^{-1}\right]  $), $B\left[  z\right]  \rightarrow
B\left[  \left[  z\right]  \right]  $, $B\left[  z^{-1}\right]  \rightarrow
B\left[  \left[  z^{-1}\right]  \right]  $, $B\left[  \left[  z\right]
\right]  \rightarrow B\left(  \left(  z\right)  \right)  $, $B\left[  \left[
z^{-1}\right]  \right]  \rightarrow B\left(  \left(  z^{-1}\right)  \right)
$, $B\left[  z,z^{-1}\right]  \rightarrow B\left(  \left(  z\right)  \right)
$, $B\left[  z,z^{-1}\right]  \rightarrow B\left(  \left(  z^{-1}\right)
\right)  $, $B\left(  \left(  z\right)  \right)  \rightarrow B\left[  \left[
z,z^{-1}\right]  \right]  $ and $B\left(  \left(  z^{-1}\right)  \right)
\rightarrow B\left[  \left[  z,z^{-1}\right]  \right]  $ as inclusions.

Clearly, all five spaces $B\left[  z\right]  $, $B\left[  \left[  z\right]
\right]  $, $B\left[  z,z^{-1}\right]  $, $B\left(  \left(  z\right)  \right)
$ and $B\left[  \left[  z,z^{-1}\right]  \right]  $ depend functorially on $B$.
\end{definition}

Before we do anything further with these notions, let us give three warnings:

\textbf{1)} Given Definition \ref{def.qf.powerseries}, one might expect
$B\left[  \left[  z,z^{-1}\right]  \right]  $ to canonically become a
$\mathbb{C}\left[  \left[  z,z^{-1}\right]  \right]  $-algebra. But this is
not true even for $B=\mathbb{C}$ (because there is no reasonable way to define
a product of two elements of $\mathbb{C}\left[  \left[  z,z^{-1}\right]
\right]  $\ \ \ \ \footnote{If we would try the natural way, we would get
nonsense results. For instance, if we tried to compute the coefficient of
$\left(  \sum\limits_{n\in\mathbb{Z}}1z^{n}\right)  \cdot\left(
\sum\limits_{n\in\mathbb{Z}}1z^{n}\right)  $ before $z^{0}$, we would get
$\sum\limits_{\substack{\left(  n,m\right)  \in\mathbb{Z}^{2};\\n+m=0}%
}1\cdot1$, which is not a convergent series.}). This also answers why
$B\left[  \left[  z,z^{-1}\right]  \right]  $ does not become a ring when $B$
is a $\mathbb{C}$-algebra. Nor is $B\left[  \left[  z,z^{-1}\right]  \right]
$, in general, a $B\left[  \left[  z\right]  \right]  $-module.

\textbf{2)} The $\mathbb{C}\left[  z,z^{-1}\right]  $-module $B\left[  \left[
z,z^{-1}\right]  \right]  $ usually has torsion. For example, $\left(
1-z\right)  \cdot\sum\limits_{n\in\mathbb{Z}}z^{n}=0$ in $\mathbb{C}\left[
\left[  z,z^{-1}\right]  \right]  $ despite $\sum\limits_{n\in\mathbb{Z}}%
z^{n}\neq0$. As a consequence, working in $B\left[  \left[  z,z^{-1}\right]
\right]  $ requires extra care.

\textbf{3)} Despite the suggestive notation $B\left(  \left(  z\right)
\right)  $, it is of course not true that $B\left(  \left(  z\right)  \right)
$ is a field whenever $B$ is a commutative ring. However, $B\left(  \left(
z\right)  \right)  $ is a field whenever $B$ is a field.

\begin{Convention}
Let $B$ be a vector space, and $z$ a symbol. By analogy with the notations
$B\left[  z\right]  $, $B\left[  \left[  z\right]  \right]  $ and $B\left(
\left(  z\right)  \right)  $ introduced in Definition \ref{def.qf.powerseries}%
, we will occasionally also use the notations $B\left[  z^{-1}\right]  $,
$B\left[  \left[  z^{-1}\right]  \right]  $ and $B\left(  \left(
z^{-1}\right)  \right)  $. For example, $B\left[  z^{-1}\right]  $ will mean
the vector space of all ``reverse sequences'' $\left(  b_{n}\right)
_{n\in-\mathbb{N}}$ such that only finitely many $n\in-\mathbb{N}$ satisfy
$b_{n}\neq0$\ \ \ \ \footnotemark. Of course, $B\left[  z\right]  \cong
B\left[  z^{-1}\right]  $ as vector spaces, but $B\left[  z\right]  $ and
$B\left[  z^{-1}\right]  $ are two different subspaces of $B\left[
z,z^{-1}\right]  $, so it is useful to distinguish between $B\left[  z\right]
$ and $B\left[  z^{-1}\right]  $.
\end{Convention}

\footnotetext{Here, $-\mathbb{N}$ denotes the set $\left\{
0,-1,-2,-3,...\right\}  $, and a ``reverse sequence'' is a family indexed by
elements of $-\mathbb{N}$.}

Now, let us extend Definition \ref{def.qf.powerseries} to several variables.
The reader is advised to only skim through the following definition, as there
is nothing unexpected in it:

\begin{definition}
\label{def.qf.powerseries.mvars}Let $m\in\mathbb{N}$. Let $z_{1}%
,z_{2},...,z_{m}$ be $m$ symbols. For every vector space $B$, we make the
following definitions:

\textbf{(a)} We denote by $B\left[  z_{1},z_{2},...,z_{m}\right]  $ the vector
space of all families $\left(  b_{\left(  n_{1},n_{2},...,n_{m}\right)
}\right)  _{\left(  n_{1},n_{2},...,n_{m}\right)  \in\mathbb{N}^{m}}\in
B^{\mathbb{N}^{m}}$ such that only finitely many $\left(  n_{1},n_{2}%
,...,n_{m}\right)  \in\mathbb{N}^{m}$ satisfy $b_{\left(  n_{1},n_{2}%
,...,n_{m}\right)  }\neq0$. Such a family $\left(  b_{\left(  n_{1}%
,n_{2},...,n_{m}\right)  }\right)  _{\left(  n_{1},n_{2},...,n_{m}\right)
\in\mathbb{N}^{m}}$ is denoted by $\sum\limits_{\left(  n_{1},n_{2}%
,...,n_{m}\right)  \in\mathbb{N}^{m}}b_{\left(  n_{1},n_{2},...,n_{m}\right)
}z_{1}^{n_{1}}z_{2}^{n_{2}}...z_{m}^{n_{m}}$. The elements of $B\left[
z_{1},z_{2},...,z_{m}\right]  $ are called \textit{polynomials in the
indeterminates }$z_{1},z_{2},...,z_{m}$ \textit{over }$B$ (even when $B$ is
not a ring).

\textbf{(b)} We denote by $B\left[  \left[  z_{1},z_{2},...,z_{m}\right]
\right]  $ the vector space of all families $\left(  b_{\left(  n_{1}%
,n_{2},...,n_{m}\right)  }\right)  _{\left(  n_{1},n_{2},...,n_{m}\right)
\in\mathbb{N}^{m}}\in B^{\mathbb{N}^{m}}$. Such a family $\left(  b_{\left(
n_{1},n_{2},...,n_{m}\right)  }\right)  _{\left(  n_{1},n_{2},...,n_{m}%
\right)  \in\mathbb{N}^{m}}$ is denoted by $\sum\limits_{\left(  n_{1}%
,n_{2},...,n_{m}\right)  \in\mathbb{N}^{m}}b_{\left(  n_{1},n_{2}%
,...,n_{m}\right)  }z_{1}^{n_{1}}z_{2}^{n_{2}}...z_{m}^{n_{m}}$. The elements
of $B\left[  \left[  z_{1},z_{2},...,z_{m}\right]  \right]  $ are called
\textit{formal power series in the indeterminates }$z_{1},z_{2},...,z_{m}$
\textit{over }$B$ (even when $B$ is not a ring).

\textbf{(c)} We denote by $B\left[  z_{1},z_{1}^{-1},z_{2},z_{2}%
^{-1},...,z_{m},z_{m}^{-1}\right]  $ the vector space of all families $\left(
b_{\left(  n_{1},n_{2},...,n_{m}\right)  }\right)  _{\left(  n_{1}%
,n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}}\in B^{\mathbb{Z}^{m}}$ such that
only finitely many $\left(  n_{1},n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}$
satisfy $b_{\left(  n_{1},n_{2},...,n_{m}\right)  }\neq0$. Such a family
$\left(  b_{\left(  n_{1},n_{2},...,n_{m}\right)  }\right)  _{\left(
n_{1},n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}}$ is denoted by $\sum
\limits_{\left(  n_{1},n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}}b_{\left(
n_{1},n_{2},...,n_{m}\right)  }z_{1}^{n_{1}}z_{2}^{n_{2}}...z_{m}^{n_{m}}$.
The elements of $B\left[  z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m}%
,z_{m}^{-1}\right]  $ are called\textit{ Laurent polynomials in the
indeterminates }$z_{1},z_{2},...,z_{m}$ \textit{over }$B$ (even when $B$ is
not a ring).

\textbf{(d)} We denote by $B\left(  \left(  z_{1},z_{2},...,z_{m}\right)
\right)  $ the vector space of all families $\left(  b_{\left(  n_{1}%
,n_{2},...,n_{m}\right)  }\right)  _{\left(  n_{1},n_{2},...,n_{m}\right)
\in\mathbb{Z}^{m}}\in B^{\mathbb{Z}^{m}}$ for which there exists an
$N\in\mathbb{Z}$ such that every $\left(  n_{1},n_{2},...,n_{m}\right)
\in\mathbb{Z}^{m}\setminus\left\{  N,N+1,N+2,\ldots\right\}  ^{m}$ satisfies
$b_{\left(  n_{1},n_{2},...,n_{m}\right)  }=0$. Such a family $\left(
b_{\left(  n_{1},n_{2},...,n_{m}\right)  }\right)  _{\left(  n_{1}%
,n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}}$ is denoted by $\sum
\limits_{\left(  n_{1},n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}}b_{\left(
n_{1},n_{2},...,n_{m}\right)  }z_{1}^{n_{1}}z_{2}^{n_{2}}...z_{m}^{n_{m}}$.
The elements of $B\left(  \left(  z_{1},z_{2},...,z_{m}\right)  \right)  $ are
called\textit{ formal Laurent series in the indeterminates }$z_{1}%
,z_{2},...,z_{m}$ \textit{over }$B$ (even when $B$ is not a ring).

\textbf{(e)} We denote by $B\left[  \left[  z_{1},z_{1}^{-1},z_{2},z_{2}%
^{-1},...,z_{m},z_{m}^{-1}\right]  \right]  $ the vector space of all families
$\left(  b_{\left(  n_{1},n_{2},...,n_{m}\right)  }\right)  _{\left(
n_{1},n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}}\in B^{\mathbb{Z}^{m}}$. Such
a family $\left(  b_{\left(  n_{1},n_{2},...,n_{m}\right)  }\right)  _{\left(
n_{1},n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}}$ is denoted by $\sum
\limits_{\left(  n_{1},n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}}b_{\left(
n_{1},n_{2},...,n_{m}\right)  }z_{1}^{n_{1}}z_{2}^{n_{2}}...z_{m}^{n_{m}}$.

All five of these spaces $B\left[  z_{1},z_{2},...,z_{m}\right]  $, $B\left[
\left[  z_{1},z_{2},...,z_{m}\right]  \right]  $, $B\left[  z_{1},z_{1}%
^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  $, $B\left(  \left(
z_{1},z_{2},...,z_{m}\right)  \right)  $ and $B\left[  \left[  z_{1}%
,z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  \right]  $ are
$\mathbb{C}\left[  z_{1},z_{2},...,z_{m}\right]  $-modules. (Here, the
$\mathbb{C}\left[  z_{1},z_{2},...,z_{m}\right]  $-module structure on
$B\left[  \left[  z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}%
^{-1}\right]  \right]  $ is given by%
\begin{align}
&  \left(  \sum\limits_{\left(  n_{1},n_{2},...,n_{m}\right)  \in
\mathbb{N}^{m}}c_{\left(  n_{1},n_{2},...,n_{m}\right)  }z_{1}^{n_{1}}%
z_{2}^{n_{2}}...z_{m}^{n_{m}}\right)  \cdot\left(  \sum\limits_{\left(
n_{1},n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}}b_{\left(  n_{1}%
,n_{2},...,n_{m}\right)  }z_{1}^{n_{1}}z_{2}^{n_{2}}...z_{m}^{n_{m}}\right)
\nonumber\\
&  =\sum\limits_{\left(  n_{1},n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}%
}\left(  \sum\limits_{\left(  m_{1},m_{2},...,m_{m}\right)  \in\mathbb{N}^{m}%
}c_{\left(  m_{1},m_{2},...,m_{m}\right)  }\cdot b_{\left(  n_{1}-m_{1}%
,n_{2}-m_{2},...,n_{m}-m_{m}\right)  }\right)  z_{1}^{n_{1}}z_{2}^{n_{2}%
}...z_{m}^{n_{m}} \label{def.qf.powerseries.mvars.prod.prototype}%
\end{align}
for all $\sum\limits_{\left(  n_{1},n_{2},...,n_{m}\right)  \in\mathbb{Z}^{m}%
}b_{\left(  n_{1},n_{2},...,n_{m}\right)  }z_{1}^{n_{1}}z_{2}^{n_{2}}%
...z_{m}^{n_{m}}\in B\left[  \left[  z_{1},z_{1}^{-1},z_{2},z_{2}%
^{-1},...,z_{m},z_{m}^{-1}\right]  \right]  $ and $\sum\limits_{\left(
n_{1},n_{2},...,n_{m}\right)  \in\mathbb{N}^{m}}c_{\left(  n_{1}%
,n_{2},...,n_{m}\right)  }z_{1}^{n_{1}}z_{2}^{n_{2}}...z_{m}^{n_{m}}%
\in\mathbb{C}\left[  z_{1},z_{2},...,z_{m}\right]  $, and the $\mathbb{C}%
\left[  z_{1},z_{2},...,z_{m}\right]  $-module structures on the other four
spaces are defined similarly.) Besides, $B\left[  \left[  z_{1},z_{2}%
,...,z_{m}\right]  \right]  $ and $B\left(  \left(  z_{1},z_{2},...,z_{m}%
\right)  \right)  $ are $\mathbb{C}\left[  \left[  z_{1},z_{2},...,z_{m}%
\right]  \right]  $-modules (defined in a similar fashion to
(\ref{def.qf.powerseries.mvars.prod.prototype})). Also, $B\left(  \left(
z_{1},z_{2},...,z_{m}\right)  \right)  $ is a $\mathbb{C}\left(  \left(
z_{1},z_{2},...,z_{m}\right)  \right)  $-module (defined in analogy to
(\ref{def.qf.powerseries.mvars.prod.prototype})). Besides, $B\left[
z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  $, $B\left(
\left(  z_{1},z_{2},...,z_{m}\right)  \right)  $ and $B\left[  \left[
z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  \right]  $ are
$\mathbb{C}\left[  z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}%
^{-1}\right]  $-modules (in a similar way).

Of course, if $B$ is a $\mathbb{C}$-algebra, then the above-defined spaces
$B\left[  z_{1},z_{2},...,z_{m}\right]  $, $B\left[  z_{1},z_{1}^{-1}%
,z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  $, $B\left[  \left[
z_{1},z_{2},...,z_{m}\right]  \right]  $ and $B\left(  \left(  z_{1}%
,z_{2},...,z_{m}\right)  \right)  $ are $\mathbb{C}$-algebras themselves (with
multiplication defined by a formula analogous to
(\ref{def.qf.powerseries.mvars.prod.prototype}) again), and in fact $B\left[
z_{1},z_{2},...,z_{m}\right]  $ is the algebra of polynomials in the variables
$z_{1},z_{2},...,z_{m}$ over $B$, and $B\left[  z_{1},z_{1}^{-1},z_{2}%
,z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  $ is the algebra of Laurent
polynomials in the variables $z_{1},z_{2},...,z_{m}$ over $B$, and $B\left[
\left[  z_{1},z_{2},...,z_{m}\right]  \right]  $ is the algebra of formal
power series in the variables $z_{1},z_{2},...,z_{m}$ over $B$.

It should be noticed that $B\left[  z_{1},z_{2},...,z_{m}\right]  \cong
B\otimes\mathbb{C}\left[  z_{1},z_{2},...,z_{m}\right]  $ and $B\left[
z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  \cong
B\otimes\mathbb{C}\left[  z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m}%
,z_{m}^{-1}\right]  $ canonically, but such isomorphisms do \textbf{not} hold
for $B\left[  \left[  z_{1},z_{2},...,z_{m}\right]  \right]  $, $B\left(
\left(  z_{1},z_{2},...,z_{m}\right)  \right)  $ and $B\left[  \left[
z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  \right]  $
unless $B$ is finite-dimensional or $m=0$.

There are several obvious injections (analogous to the ones listed in
Definition \ref{def.qf.powerseries}) which we regard as inclusions. For
example, one of these is the injection $B\left[  z_{1},z_{2},...,z_{m}\right]
\rightarrow B\left[  \left[  z_{1},z_{2},...,z_{m}\right]  \right]  $; we
won't list the others here.

Clearly, all five spaces $B\left[  z_{1},z_{2},...,z_{m}\right]  $, $B\left[
\left[  z_{1},z_{2},...,z_{m}\right]  \right]  $, $B\left[  z_{1},z_{1}%
^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  $, $B\left(  \left(
z_{1},z_{2},...,z_{m}\right)  \right)  $ and $B\left[  \left[  z_{1}%
,z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  \right]  $ depend
functorially on $B$.
\end{definition}

Clearly, when $m=1$, Definition \ref{def.qf.powerseries.mvars} is equivalent
to Definition \ref{def.qf.powerseries}.

Definition \ref{def.qf.powerseries.mvars} can be extended to infinitely many
indeterminates; this is left to the reader.

Our definition of $B\left(  \left(  z_{1},z_{2},...,z_{m}\right)  \right)  $
is rather intricate. The reader might gain a better understanding from the
following equivalent definition: The set $B\left(  \left(  z_{1}%
,z_{2},...,z_{m}\right)  \right)  $ is the subset of $B\left[  \left[
z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  \right]  $
consisting of those $p\in B\left[  \left[  z_{1},z_{1}^{-1},z_{2},z_{2}%
^{-1},...,z_{m},z_{m}^{-1}\right]  \right]  $ for which there exists an
$\left(  a_{1},a_{2},\ldots,a_{m}\right)  \in\mathbb{Z}^{m}$ such that
$z_{1}^{a_{1}}z_{2}^{a_{2}}...z_{m}^{a_{m}}\cdot p\in B\left[  \left[
z_{1},z_{2},...,z_{m}\right]  \right]  $. It is easy to show that $B\left(
\left(  z_{1},z_{2},...,z_{m}\right)  \right)  $ is isomorphic to the
localization of the ring $B\left[  \left[  z_{1},z_{2},...,z_{m}\right]
\right]  $ at the multiplicatively closed subset consisting of all monomials.

The reader should be warned that if $B$ is a field, $m$ is an integer $>1$,
and $z_{1}$, $z_{2}$, $...$, $z_{m}$ are $m$ symbols, then the ring $B\left(
\left(  z_{1},z_{2},...,z_{m}\right)  \right)  $ is \textbf{not} a field
(unlike in the case $m=1$); for example, it does not contain an inverse to
$z_{1}-z_{2}$. This is potentially confusing and I would not be surprised if
some texts define $B\left(  \left(  z_{1},z_{2},...,z_{m}\right)  \right)  $
to mean a different ring which actually is a field.

When $B$ is a vector space and $z$ is a symbol, there is an operator we can
define on each of the five spaces $B\left[  z\right]  $, $B\left[  \left[
z\right]  \right]  $, $B\left[  z,z^{-1}\right]  $, $B\left(  \left(
z\right)  \right)  $ and $B\left[  \left[  z,z^{-1}\right]  \right]  $:
derivation with respect to $z$:

\begin{definition}
\label{def.qf.powerseries.d}For every vector space $B$ and symbol $z$, we make
the following definitions:

Define a linear map $\dfrac{d}{dz}:B\left[  z\right]  \rightarrow B\left[
z\right]  $ by the formula%
\begin{align}
\dfrac{d}{dz}\left(  \sum\limits_{n\in\mathbb{N}}b_{n}z^{n}\right)   &
=\sum\limits_{n\in\mathbb{N}}\left(  n+1\right)  b_{n+1}z^{n}%
\label{def.qf.powerseries.d.formula}\\
&  \ \ \ \ \ \ \ \ \ \ \text{for every }\sum\limits_{n\in\mathbb{N}}b_{n}%
z^{n}\in B\left[  z\right]  .\nonumber
\end{align}
Define a linear map $\dfrac{d}{dz}:B\left[  \left[  z\right]  \right]
\rightarrow B\left[  \left[  z\right]  \right]  $ by the very same formula,
and define linear maps $\dfrac{d}{dz}:B\left[  z,z^{-1}\right]  \rightarrow
B\left[  z,z^{-1}\right]  $, $\dfrac{d}{dz}:B\left(  \left(  z\right)
\right)  \rightarrow B\left(  \left(  z\right)  \right)  $ and $\dfrac{d}%
{dz}:B\left[  \left[  z,z^{-1}\right]  \right]  \rightarrow B\left[  \left[
z,z^{-1}\right]  \right]  $ by analogous formulas (more precisely, by formulas
which differ from (\ref{def.qf.powerseries.d.formula}) only in that the sums
range over $\mathbb{Z}$ instead of over $\mathbb{N}$).

For every $f\in B\left[  \left[  z,z^{-1}\right]  \right]  $, the image
$\dfrac{d}{dz}f$ of $f$ under the linear map $\dfrac{d}{dz}$ will be denoted
by $\dfrac{df}{dz}$ or by $f^{\prime}$ and called the $z$\textit{-derivative}
of $f$ (or, briefly, the \textit{derivative} of $f$). The operator $\dfrac
{d}{dz}$ itself (on any of the five vector spaces $B\left[  z\right]  $,
$B\left[  \left[  z\right]  \right]  $, $B\left[  z,z^{-1}\right]  $,
$B\left(  \left(  z\right)  \right)  $ and $B\left[  \left[  z,z^{-1}\right]
\right]  $) will be called the \textit{differentiation with respect to }$z$.
\end{definition}

An analogous definition can be made for several variables:

\begin{definition}
\label{def.qf.powerseries.mvars.d}Let $m\in\mathbb{N}$. Let $z_{1}%
,z_{2},...,z_{m}$ be $m$ symbols. Let $i\in\left\{  1,2,...,m\right\}  $. For
every vector space $B$, we make the following definitions:

Define a linear map $\dfrac{\partial}{\partial z_{i}}:B\left[  z_{1}%
,z_{2},...,z_{m}\right]  \rightarrow B\left[  z_{1},z_{2},...,z_{m}\right]  $
by the formula%
\begin{align}
&  \dfrac{\partial}{\partial z_{i}}\left(  \sum\limits_{\left(  n_{1}%
,n_{2},...,n_{m}\right)  \in\mathbb{N}^{m}}b_{\left(  n_{1},n_{2}%
,...,n_{m}\right)  }z_{1}^{n_{1}}z_{2}^{n_{2}}...z_{m}^{n_{m}}\right)
\nonumber\\
&  =\sum\limits_{\left(  n_{1},n_{2},...,n_{m}\right)  \in\mathbb{N}^{m}%
}\left(  n_{i}+1\right)  b_{\left(  n_{1},n_{2},...,n_{i-1},n_{i}%
+1,n_{i+1},n_{i+2},...,n_{m}\right)  }z_{1}^{n_{1}}z_{2}^{n_{2}}%
...z_{m}^{n_{m}}\label{def.qf.powerseries.mvars.d.formula}\\
&  \ \ \ \ \ \ \ \ \ \ \text{for every }\sum\limits_{\left(  n_{1}%
,n_{2},...,n_{m}\right)  \in\mathbb{N}^{m}}b_{\left(  n_{1},n_{2}%
,...,n_{m}\right)  }z_{1}^{n_{1}}z_{2}^{n_{2}}...z_{m}^{n_{m}}\in B\left[
z_{1},z_{2},...,z_{m}\right]  .\nonumber
\end{align}
Define a linear map $\dfrac{\partial}{\partial z_{i}}:B\left[  \left[
z_{1},z_{2},...,z_{m}\right]  \right]  \rightarrow B\left[  \left[
z_{1},z_{2},...,z_{m}\right]  \right]  $ by the very same formula, and define
linear maps $\dfrac{\partial}{\partial z_{i}}:B\left[  z_{1},z_{1}^{-1}%
,z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  \rightarrow B\left[
z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  $,
$\dfrac{\partial}{\partial z_{i}}:B\left(  \left(  z_{1},z_{2},...,z_{m}%
\right)  \right)  \rightarrow B\left(  \left(  z_{1},z_{2},...,z_{m}\right)
\right)  $ and $\dfrac{\partial}{\partial z_{i}}:B\left[  \left[  z_{1}%
,z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}^{-1}\right]  \right]  \rightarrow
B\left[  \left[  z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}%
^{-1}\right]  \right]  $ by analogous formulas (more precisely, by formulas
which differ from (\ref{def.qf.powerseries.mvars.d.formula}) only in that the
sums range over $\mathbb{Z}^{m}$ instead of over $\mathbb{N}^{m}$).

For every $f\in B\left[  \left[  z_{1},z_{1}^{-1},z_{2},z_{2}^{-1}%
,...,z_{m},z_{m}^{-1}\right]  \right]  $, the image $\dfrac{\partial}{\partial
z_{i}}f$ of $f$ under the linear map $\dfrac{\partial}{\partial z_{i}}$ will
be denoted by $\dfrac{\partial f}{\partial z_{i}}$ and called the $z_{i}%
$\textit{-derivative} of $f$ (or the \textit{partial derivative of }$f$
\textit{with respect to }$z_{i}$). The operator $\dfrac{\partial}{\partial
z_{i}}$ itself (on any of the five vector spaces $B\left[  z_{1}%
,z_{2},...,z_{m}\right]  $, $B\left[  \left[  z_{1},z_{2},...,z_{m}\right]
\right]  $, $B\left[  z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}%
^{-1}\right]  $, $B\left(  \left(  z_{1},z_{2},...,z_{m}\right)  \right)  $
and $B\left[  \left[  z_{1},z_{1}^{-1},z_{2},z_{2}^{-1},...,z_{m},z_{m}%
^{-1}\right]  \right]  $) will be called the \textit{differentiation with
respect to }$z_{i}$.
\end{definition}

Again, it is straightforward (and left to the reader) to extend this
definition to infinitely many indeterminates.

\subsubsection{Quantum fields}

Formal power series which are infinite ``in both directions'' might seem like
a perverse and artificial notion; their failure to form a ring certainly does
not suggest them to be useful. Nevertheless, they prove very suitable when
studying infinite-dimensional Lie algebras. Let us explain how.

For us, when we study Lie algebras, we are mainly concerned with their
elements, usually basis elements (e. g., the $a_{n}$ in $\mathcal{A}$). For
physicists, instead, certain generating functions built of these objects are
objects of primary concern, since they are closer to what they observe. They
are called \textit{quantum fields}.

Now, what are quantum fields?

For example, in $\mathcal{A}$, let us set $a\left(  z\right)  =\sum
\limits_{n\in\mathbb{Z}}a_{n}z^{-n-1}$, where $z$ is a formal variable. This
sum $\sum\limits_{n\in\mathbb{Z}}a_{n}z^{-n-1}$ is a formal sum which is
infinite in both directions, so it is not an element of any of the rings
$U\left(  \mathcal{A}\right)  \left[  \left[  z\right]  \right]  $ or
$U\left(  \mathcal{A}\right)  \left(  \left(  z \right)  \right) $, but only
an element of $U\left(  \mathcal{A}\right)  \left[  \left[  z,z^{-1}\right]
\right]  $.

As we said, the vector space $U\left(  \mathcal{A}\right)  \left[  \left[
z,z^{-1}\right]  \right]  $ is \textbf{not} a ring (even though $U\left(
\mathcal{A}\right)  $ is a $\mathbb{C}$-algebra), so we cannot multiply two
``sums'' like $a\left(  z\right)  $ in general. \textbf{However}, in the
following, we are going to learn about several things that we \textbf{can} do
with such ``sums''. One first thing that we notice about our concrete ``sum''
$a\left(  z\right)  =\sum\limits_{n\in\mathbb{Z}}a_{n}z^{-n-1}$ is that if we
apply $a\left(  z\right)  $ to some vector $v$ in $F_{\mu}$ (by evaluating the
term $\left(  a\left(  z\right)  \right)  v$ componentwise\footnote{By
``evaluating'' a term like $\left(  a\left(  z\right)  \right)  v$ at a vector
$v$ ``componentwise'', we mean evaluating $\sum\limits_{n\in\mathbb{Z}}\left(
a_{n}z^{-n-1}\right)  \left(  v\right)  $. Here, the variable $z$ is decreed
to commute with everything else, so that $\left(  a_{n}z^{-n-1}\right)
\left(  v\right)  $ means $z^{-n-1}a_{n}v$.}), then we get a sum
$\sum\limits_{n\in\mathbb{Z}}z^{-n-1}a_{n}v$ which evaluates to an element of
$F_{\mu}\left(  \left(  z \right)  \right)  $ (because every sufficiently
large $n\in\mathbb{Z}$ satisfies $z^{-n-1}\underbrace{a_{n}v}_{=0}=0$). As a
consequence, $a\left(  z\right)  $ ``acts'' on $F_{\mu}$. I am saying ``acts''
in quotation marks, since this ``action'' is not a map $F_{\mu}\rightarrow
F_{\mu}$ but a map $F_{\mu}\rightarrow F_{\mu}\left(  \left(   z \right)
\right)  $, and since $a\left(  z\right)  $ does not lie in a ring (as I said,
$U\left(  \mathcal{A}\right)  \left[  \left[  z,z^{-1}\right]  \right]  $ is
\textbf{not} a ring).

Physicists call $a\left(  z\right)  $ a \textit{quantum field} (more
precisely, a free bosonic field).

While we cannot take the square $\left(  a\left(  z\right)  \right)  ^{2}$ of
our ``sum'' $a\left(  z\right)  $ (since $U\left(  \mathcal{A}\right)  \left[
\left[  z,z^{-1}\right]  \right]  $ is not a ring), we can multiply two sums
``with different variables''; e. g., we can multiply $a\left(  z\right)  $ and
$a\left(  w\right)  $, where $z$ and $w$ are two distinct formal variables.
The product $a\left(  z\right)  a\left(  w\right)  $ is defined as the formal
sum $\sum\limits_{\left(  n,m\right)  \in\mathbb{Z}^{2}}a_{n}a_{m}%
z^{-n-1}w^{-m-1}\in U\left(  \mathcal{A}\right)  \left[  \left[
z,z^{-1}\right]  \right]  \left[  \left[  w,w^{-1}\right]  \right]  $. Note
that elements of $U\left(  \mathcal{A}\right)  \left[  \left[  z,z^{-1}%
\right]  \right]  \left[  \left[  w,w^{-1}\right]  \right]  $ are two-sided
sequences of two-sided sequences of elements of $U\left(  \mathcal{A}\right)
$; of course, we can interpret them as maps $\mathbb{Z}^{2}\rightarrow
U\left(  \mathcal{A}\right)  $.

It is easy to see that $\left[  a\left(  z\right)  ,a\left(  w\right)
\right]  =\sum\limits_{n\in\mathbb{Z}}nz^{-n-1}w^{n-1}$. This identity, in the
first place, holds on the level of formal sums (where $\sum\limits_{n\in
\mathbb{Z}}nz^{-n-1}w^{n-1}$ is a shorthand notation for a particular sequence
of sequences: namely, the one whose $j$-th element is the sequence whose
$i$-th element is $\delta_{i+j+2,0}\left(  j+1\right)  $), but if we evaluate
it on an element $v$ of $F_{\mu}$, then we get an identity $\left[  a\left(
z\right)  ,a\left(  w\right)  \right]  v=\sum\limits_{n\in\mathbb{Z}}%
nz^{-n-1}w^{n-1}v$ which holds in the space $F_{\mu}\left(  \left(  z\right)
\right)  \left(  \left(  w\right)  \right)  $.

We can obtain the ``series'' $\left[  a\left(  z\right)  ,a\left(  w\right)
\right]  =\sum\limits_{n\in\mathbb{Z}}nz^{-n-1}w^{n-1}$ by differentiating a
more basic ``series'':%
\[
\delta\left(  w-z\right)  :=\sum_{n\in\mathbb{Z}}z^{-n-1}w^{n}.
\]
This, again, is a formal series infinite in both directions. Why do we call it
$\delta\left(  w-z\right)  $ ? Because in analysis, the delta-``function''
(actually a distribution) satisfies the formula $\int\delta\left(  x-y\right)
f\left(  y\right)  dy=f\left(  x\right)  $ for every function $f$, whereas our
series $\delta\left(  w-z\right)  $ satisfies a remarkably similar
property\footnote{Namely, if we define the ``formal residue'' $\dfrac{1}{2\pi
i}\oint\limits_{\left\vert z\right\vert =1}q\left(  z\right)  dz$ of an
element $q\left(  z\right)  \in B\left(  \left(  z\right)  \right)  $ (for $B$
being some vector space) to be the coefficient of $q\left(  z\right)  $ before
$z^{-1}$, then every $f=\sum\limits_{n\in\mathbb{Z}}f_{n}z^{n}$ (with
$f_{n}\in B$) satisfies $\dfrac{1}{2\pi i}\oint\limits_{\left\vert
z\right\vert =1}z^{-n-1}f\left(  z\right)  dz=f_{n}$, and thus $\dfrac{1}{2\pi
i}\oint\limits_{\left\vert z\right\vert =1}\delta\left(  w-z\right)  f\left(
z\right)  dz=f\left(  w\right)  $.}. And now, $\left[  a\left(  z\right)
,a\left(  w\right)  \right]  =\sum\limits_{n\in\mathbb{Z}}nz^{-n-1}w^{n-1}$
becomes $\left[  a\left(  z\right)  ,a\left(  w\right)  \right]  =\partial
_{w}\delta\left(  w-z\right)  =:\delta^{\prime}\left(  w-z\right)  $.

Something more interesting comes out for the Witt algebra: Set $T\left(
z\right)  =\sum\limits_{n\in\mathbb{Z}}L_{n}z^{-n-2}$\textbf{ in the Witt
algebra}. Then, we have%
\begin{align*}
&  \left[  T\left(  z\right)  ,T\left(  w\right)  \right] \\
&  =\sum\limits_{\left(  n,m\right)  \in\mathbb{Z}^{2}}\left(  n-m\right)
L_{n+m}z^{-n-2}w^{-m-2}=\sum\limits_{\left(  k,m\right)  \in\mathbb{Z}^{2}%
}L_{k}\underbrace{\left(  k-2m\right)  }_{=\left(  k+2\right)  +2\left(
-m-1\right)  }z^{m-k-2}w^{-m-2}\\
&  =\underbrace{\left(  \sum\limits_{k\in\mathbb{Z}}L_{k}\left(  k+2\right)
z^{-k-3}\right)  }_{=-T^{\prime}\left(  z\right)  }\underbrace{\left(
\sum\limits_{m\in\mathbb{Z}}z^{m+1}w^{-m-2}\right)  }_{=\delta\left(
w-z\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ +2\underbrace{\left(  \sum\limits_{k\in\mathbb{Z}}%
L_{k}z^{-k-2}\right)  }_{=T\left(  z\right)  }\underbrace{\left(
\sum\limits_{m\in\mathbb{Z}}\left(  -m-1\right)  z^{m}w^{-m-2}\right)
}_{=\delta^{\prime}\left(  w-z\right)  }\\
&  =-T^{\prime}\left(  z\right)  \delta\left(  w-z\right)  +2T\left(
z\right)  \delta^{\prime}\left(  w-z\right)  .
\end{align*}
Note that this formula uniquely determines the Lie bracket of the Witt
algebra. This is how physicists would define the Witt algebra.

Now, let us set $T\left(  z\right)  =\sum\limits_{n\in\mathbb{Z}}L_{n}%
z^{-n-2}$\textbf{ in the Virasoro algebra}. (This power series $T$ looks
exactly like the one before, but note that the $L_{n}$ now mean elements of
the Virasoro algebra rather than the Witt algebra.) Then, our previous
computation of $\left[  T\left(  z\right)  ,T\left(  w\right)  \right]  $ must
be modified by adding a term of $\sum\limits_{n\in\mathbb{Z}}\dfrac{n^{3}%
-n}{12}Cz^{-n-2}w^{n-2}=\dfrac{C}{12}\delta^{\prime\prime\prime}\left(
w-z\right)  $. So we get%
\[
\left[  T\left(  z\right)  ,T\left(  w\right)  \right]  =-T^{\prime}\left(
z\right)  \delta\left(  w-z\right)  +2T\left(  z\right)  \delta^{\prime
}\left(  w-z\right)  +\dfrac{C}{12}\delta^{\prime\prime\prime}\left(
w-z\right)  .
\]


\textbf{Exercise:} Check that, if we interpret $L_{n}$ and $a_{m}$ as the
actions of $L_{n}\in\operatorname*{Vir}$ and $a_{m}\in\mathcal{A}$ on the
$\operatorname*{Vir}\ltimes\mathcal{A}$-module $F_{\mu}$, then the following
identity between maps $F_{\mu}\rightarrow F_{\mu}\left(  \left(  z\right)
\right)  \left(  \left(  w\right)  \right)  $ holds:
\[
\left[  T\left(  z\right)  ,a\left(  w\right)  \right]  =a\left(  z\right)
\delta^{\prime}\left(  w-z\right)  .
\]


Recall%
\[
:a_{m}a_{n}:\ =\ \left\{
\begin{array}
[c]{c}%
a_{m}a_{n},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq n;\\
a_{n}a_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>n
\end{array}
\right.  .
\]
So we can reasonably define the ``normal ordered'' product $\left.  :a\left(
z\right)  a\left(  w\right)  :\right.  $ to be
\[
\sum\limits_{\left(  n,m\right)  \in\mathbb{Z}^{2}}\left.  :a_{n}%
a_{m}:\right.  z^{-n-1}w^{-m-1}\in U\left(  \mathcal{A}\right)  \left[
\left[  z,z^{-1}\right]  \right]  \left[  \left[  w,w^{-1}\right]  \right]  .
\]
This definition of $\left.  :a\left(  z\right)  a\left(  w\right)  :\right.  $
is equivalent to the definition given in Problem 2 of Problem Set 3.

That $\left.  :a\left(  z\right)  a\left(  w\right)  :\right.  $ is
well-defined is not a surprise: the variables $z$ and $w$ are distinct, so
there are no terms to collect in the sum $\sum\limits_{\left(  n,m\right)
\in\mathbb{Z}^{2}}\left.  :a_{n}a_{m}:\right.  z^{-n-1}w^{-m-1}$, and thus
there is no danger of obtaining an infinite sum which makes no sense (like
what we would get if we would try to define $a\left(  z\right)  ^{2}%
$).\ \ \ \ \footnote{For the same reason, the product $a\left(  z\right)
a\left(  w\right)  $ (without normal ordering) is well-defined.} But it is
more interesting that (although we cannot define $a\left(  z\right)  ^{2}$) we
can define a ``normal ordered'' square $\left.  :a\left(  z\right)
^{2}:\right.  $ (or, what is the same, $\left.  :a\left(  z\right)  a\left(
z\right)  :\right.  $), although it will not be an element of $U\left(
\mathcal{A}\right)  \left[  \left[  z,z^{-1}\right]  \right]  $ but rather of
a suitable completion. We are not going to do elaborate on how to choose this
completion here; but for us it will be enough to notice that, if we
reinterpret the $a_{n}$ as endomorphisms of $F_{\mu}$ (using the action of
$\mathcal{A}$ on $F_{\mu}$) rather than elements of $U\left(  \mathcal{A}%
\right)  $, then the ``normal ordered'' square $\left.  :a\left(  z\right)
^{2}:\right.  $ is a well-defined element of $\left(  \operatorname*{End}%
F_{\mu}\right)  \left[  \left[  z,z^{-1}\right]  \right]  $. Namely:%
\begin{align*}
&  \left.  :a\left(  z\right)  ^{2}:\right. \\
&  =\sum\limits_{\left(  n,m\right)  \in\mathbb{Z}^{2}}\left.  :a_{n}%
a_{m}:\right.  z^{-n-1}z^{-m-1}=\sum\limits_{k\in\mathbb{Z}}\left(
\sum\limits_{\substack{\left(  n,m\right)  \in\mathbb{Z}^{2};\\n+m=k}}\left.
:a_{n}a_{m}:\right.  \right)  z^{-k-2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{this is how power series are always multiplied; but we don't yet}\\
\text{know that the sum }\sum\limits_{\substack{\left(  n,m\right)
\in\mathbb{Z}^{2};\\n+m=k}}\left.  :a_{n}a_{m}:\right.  \text{ makes sense for
all }k\\
\text{(although we will see in a few lines that it does)}%
\end{array}
\right) \\
&  =\sum\limits_{k\in\mathbb{Z}}\left(  \sum\limits_{m\in\mathbb{Z}}\left.
:a_{m}a_{k-m}:\right.  \right)  z^{-k-2}\ \ \ \ \ \ \ \ \ \ \left(
\text{here, we substituted }\left(  m,k-m\right)  \text{ for }\left(
n,m\right)  \right) \\
&  =\sum\limits_{n\in\mathbb{Z}}\left(  \sum\limits_{m\in\mathbb{Z}}\left.
:a_{-m}a_{n+m}:\right.  \right)  z^{-n-2}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we substituted }k\text{ by }n\text{ in the first sum,}\\
\text{and we substituted }m\text{ by }-m\text{ in the second sum}%
\end{array}
\right)  ,
\end{align*}
and the sums $\sum\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  $
are well-defined for all $n\in\mathbb{Z}$ (by Lemma \ref{lem.fockvir.welldef}
\textbf{(c)}). We can simplify this result if we also reinterpret the
$L_{n}\in\operatorname*{Vir}$ as endomorphisms of $F_{\mu}$ (using the action
of $\operatorname*{Vir}$ on $F_{\mu}$ that was introduced in Proposition
\ref{prop.fockvir.answer2}) rather than elements of $U\left(
\operatorname*{Vir}\right)  $. In fact, the ``series'' $T\left(  z\right)
=\sum\limits_{n\in\mathbb{Z}}L_{n}z^{-n-2}$ then becomes%
\begin{align*}
T\left(  z\right)   &  =\sum\limits_{n\in\mathbb{Z}}L_{n}z^{-n-2}%
=\sum\limits_{n\in\mathbb{Z}}\dfrac{1}{2}\left(  \sum\limits_{m\in\mathbb{Z}%
}\left.  :a_{-m}a_{n+m}:\right.  \right)  z^{-n-2}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{def.fockvir.def})}\right) \\
&  =\dfrac{1}{2}\underbrace{\sum\limits_{n\in\mathbb{Z}}\left(  \sum
\limits_{m\in\mathbb{Z}}\left.  :a_{-m}a_{n+m}:\right.  \right)  z^{-n-2}%
}_{=\left.  :a\left(  z\right)  ^{2}:\right.  }=\dfrac{1}{2}\left.  :a\left(
z\right)  ^{2}:\right.  .
\end{align*}


\begin{remark}
In Definition \ref{def.fockvir.normal}, we have defined the normal ordered
product $\left.  :a_{m}a_{n}:\right.  $ in the universal enveloping algebra of
the Heisenberg algebra. This is not the only situation in which we can define
a normal ordered product, but in other situations the definition can happen to
be different. For example, in Proposition \ref{prop.ramond.rep}, we will
define a normal ordered product (on a different algebra) which will not be
commutative, and not even ``super-commutative''. There is no general rule to
define normal ordered products; it is done on a case-by-case basis.

\textbf{However}, the definition of the normal ordered product of two
\textbf{quantum fields} given in Problem 2 of Problem Set 3 is general, i. e.,
it is defined not only for quantum fields over $U\left(  \mathcal{A}\right)  $.
\end{remark}

\textbf{Exercise 1.} For any $\beta\in\mathbb{C}$, the formula $T\left(
z\right)  =\dfrac{1}{2}\left.  :a\left(  z\right)  ^{2}:\right.  +\beta
a^{\prime}\left(  z\right)  $ defines a representation of $\operatorname*{Vir}%
$ on $F_{\mu}$ with $c=1-12\beta^{2}$.

\textbf{Exercise 2.} For any $\beta\in\mathbb{C}$, there is a homomorphism
$\varphi_{\beta}:\operatorname*{Vir}\rightarrow\operatorname*{Vir}%
\ltimes\mathcal{A}$ (a splitting of the projection $\operatorname*{Vir}%
\ltimes\mathcal{A}\rightarrow\operatorname*{Vir}$) given by%
\begin{align*}
\varphi_{\beta}\left(  L_{n}\right)   &  =L_{n}+\beta a_{n}%
,\ \ \ \ \ \ \ \ \ \ n\neq0;\\
\varphi_{\beta}\left(  L_{0}\right)   &  =L_{0}+\beta a_{0}+\dfrac{\beta^{2}%
}{2}K,\\
\varphi_{\beta}\left(  C\right)   &  =C.
\end{align*}


\textbf{Exercise 3.} If we twist the action of Exercise 1 by this map, we
recover the action of problem 1 of Homework 2 for $\beta=i\lambda$.

\subsubsection{Recognizing exponential series}

Here is a simple property of power series (actually, an algebraic analogue of
the well-known fact from analysis that the solutions of the differential
equation $f^{\prime}=\alpha f$ are scalar multiples of the function
$x\mapsto\exp\left(  \alpha x\right)  $):

\begin{proposition}
\label{prop.euler.recognizing-exp}Let $R$ be a commutative $\mathbb{Q}%
$-algebra. Let $U$ be an $R$-module. Let $\left(  \alpha_{1},\alpha_{2}%
,\alpha_{3},...\right)  $ be a sequence of elements of $R$. Let $P\in U\left[
\left[  x_{1},x_{2},x_{3},...\right]  \right]  $ is a formal power series with
coefficients in $U$ (where $x_{1},x_{2},x_{3},...$ are symbols) such that
every $i>0$ satisfies $\dfrac{\partial P}{\partial x_{i}}=\alpha_{i}P$. Then,
there exists some $f\in U$ such that $P=f\cdot\exp\left(  \sum\limits_{j>0}%
x_{j}\alpha_{j}\right)  $.
\end{proposition}

\begin{vershort}
The proof of Proposition \ref{prop.euler.recognizing-exp} is easy (just let
$f$ be the constant term of the power series $P$, and prove by induction that
every monomial of $P$ equals the corresponding monomial of $f\cdot\exp\left(
\sum\limits_{j>0}x_{j}\alpha_{j}\right)  $).
\end{vershort}

\begin{verlong}
\textit{Proof of Proposition \ref{prop.euler.recognizing-exp}.} Recall
Convention \ref{conv.fin}. For any power series $S\in U\left[  \left[
x_{1},x_{2},x_{3},...\right]  \right]  $ and every $\left(  m_{1},m_{2}%
,m_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
1,2,3,...\right\}  }$, define an element $\operatorname*{Coeff}%
\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)  }S\in U$ by setting
$\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)
}S=c_{\left(  m_{1},m_{2},m_{3},...\right)  }$, where the power series $S$ is
written in the form $S=\sum\limits_{\left(  n_{1},n_{2},n_{3},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }}c_{\left(
n_{1},n_{2},n_{3},...\right)  }x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...$ for
some $c_{\left(  n_{1},n_{2},n_{3},...\right)  }\in U$. This element
$\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)  }S$ is
called the \textit{coefficient of }$S$ \textit{before the monomial }%
$x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}}...$.

Write the power series $P$ in the form $P=\sum\limits_{\left(  n_{1}%
,n_{2},n_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
1,2,3,...\right\}  }}b_{\left(  n_{1},n_{2},n_{3},...\right)  }x_{1}^{n_{1}%
}x_{2}^{n_{2}}x_{3}^{n_{3}}...$ for some $b_{\left(  n_{1},n_{2}%
,n_{3},...\right)  }\in U$. Consider these $b_{\left(  n_{1},n_{2}%
,n_{3},...\right)  }$. Then, $\operatorname*{Coeff}\nolimits_{\left(
m_{1},m_{2},m_{3},...\right)  }P=b_{\left(  m_{1},m_{2},m_{3},...\right)  }$
(by the definition of $\operatorname*{Coeff}\nolimits_{\left(  m_{1}%
,m_{2},m_{3},...\right)  }P$).

We will now prove that for every $K\in\mathbb{N}$, the following holds:%
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{every }\left(  m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }\text{ such that }%
m_{1}+m_{2}+m_{3}+...=K\\
\text{satisfies }\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2}%
,m_{3},...\right)  }P=\dfrac{1}{m_{1}!m_{2}!m_{3}!...}\alpha_{1}^{m_{1}}%
\alpha_{2}^{m_{2}}\alpha_{3}^{m_{3}}...\operatorname*{Coeff}\nolimits_{\left(
0,0,0,...\right)  }P
\end{array}
\right)  \label{pf.euler.recognizing-exp.1}%
\end{equation}
\footnote{Here, the product $m_{1}!m_{2}!m_{3}!...$ is well-defined for every
$\left(  m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\left\{  1,2,3,...\right\}  }$ (because for every $\left(  m_{1}%
,m_{2},m_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
1,2,3,...\right\}  }$, only finitely many $i>0$ satisfy $m_{i}\neq0$, and thus
only finitely many $i>0$ satisfy $m_{i}!\neq1$).}

\textit{Proof of (\ref{pf.euler.recognizing-exp.1}):} We will prove
(\ref{pf.euler.recognizing-exp.1}) by induction over $K$:

\textit{Induction base:} Every $\left(  m_{1},m_{2},m_{3},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ such that
$m_{1}+m_{2}+m_{3}+...=0$ must satisfy $\left(  m_{1},m_{2},m_{3},...\right)
=\left(  0,0,0,...\right)  $. Hence, every $\left(  m_{1},m_{2},m_{3}%
,...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}
}$ such that $m_{1}+m_{2}+m_{3}+...=0$ must satisfy%
\begin{align*}
m_{1}!m_{2}!m_{3}!...  &  =0!0!0!...\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\left(  m_{1},m_{2},m_{3},...\right)  =\left(  0,0,0,...\right)  \right) \\
&  =1\cdot1\cdot1\cdot...=1.
\end{align*}
Also, every $\left(  m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ such that $m_{1}%
+m_{2}+m_{3}+...=0$ must satisfy%
\begin{align*}
\alpha_{1}^{m_{1}}\alpha_{2}^{m_{2}}\alpha_{3}^{m_{3}}...  &  =\alpha_{1}%
^{0}\alpha_{2}^{0}\alpha_{3}^{0}...\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\left(  m_{1},m_{2},m_{3},...\right)  =\left(  0,0,0,...\right)  \right) \\
&  =1\cdot1\cdot1\cdot...=1.
\end{align*}


Thus, every $\left(  m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ such that $m_{1}%
+m_{2}+m_{3}+...=0$ must also satisfy%
\begin{align*}
\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)  }P  &
=\operatorname*{Coeff}\nolimits_{\left(  0,0,0,...\right)  }%
P\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  m_{1},m_{2},m_{3}%
,...\right)  =\left(  0,0,0,...\right)  \right) \\
&  =\underbrace{1}_{\substack{=\dfrac{1}{m_{1}!m_{2}!m_{3}!...}\\\text{(since
}m_{1}!m_{2}!m_{3}!...=1\text{)}}}\cdot\underbrace{1}_{=\alpha_{1}^{m_{1}%
}\alpha_{2}^{m_{2}}\alpha_{3}^{m_{3}}...}\operatorname*{Coeff}%
\nolimits_{\left(  0,0,0,...\right)  }P\\
&  =\dfrac{1}{m_{1}!m_{2}!m_{3}!...}\alpha_{1}^{m_{1}}\alpha_{2}^{m_{2}}%
\alpha_{3}^{m_{3}}...\operatorname*{Coeff}\nolimits_{\left(  0,0,0,...\right)
}P.
\end{align*}
In other words, (\ref{pf.euler.recognizing-exp.1}) holds for $K=0$. This
completes the induction base.

\textit{Induction step:} Let $\kappa\in\mathbb{N}$. Assume that
(\ref{pf.euler.recognizing-exp.1}) holds for $K=\kappa$. We now must prove
that (\ref{pf.euler.recognizing-exp.1}) also holds for $K=\kappa+1$.

Let $\left(  m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}%
}^{\left\{  1,2,3,...\right\}  }$ be such that $m_{1}+m_{2}+m_{3}%
+...=\kappa+1$. Then, $m_{1}+m_{2}+m_{3}+...=\kappa+1\geq1>0$, so that there
exists at least one positive integer $i$ such that $m_{i}>0$. Consider this
$i$. Since $m_{i}>0$, we have $m_{i}-1\in\mathbb{N}$, so that $\left(
m_{1},m_{2},...,m_{i-1},m_{i}-1,m_{i+1},m_{i+2},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$.

We have%
\begin{align*}
&  \dfrac{\partial P}{\partial x_{i}}=\dfrac{\partial}{\partial x_{i}}%
P=\dfrac{\partial}{\partial x_{i}}\left(  \sum\limits_{\left(  n_{1}%
,n_{2},n_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
1,2,3,...\right\}  }}b_{\left(  n_{1},n_{2},n_{3},...\right)  }x_{1}^{n_{1}%
}x_{2}^{n_{2}}x_{3}^{n_{3}}...\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }P=\sum\limits_{\left(  n_{1}%
,n_{2},n_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
1,2,3,...\right\}  }}b_{\left(  n_{1},n_{2},n_{3},...\right)  }x_{1}^{n_{1}%
}x_{2}^{n_{2}}x_{3}^{n_{3}}...\right) \\
&  =\sum\limits_{\left(  n_{1},n_{2},n_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }}\left(  n_{i}+1\right)
b_{\left(  n_{1},n_{2},...,n_{i-1},n_{i}+1,n_{i+1},n_{i+2},...\right)  }%
x_{1}^{n_{1}}x_{2}^{n_{2}}x_{3}^{n_{3}}...
\end{align*}
(by the definition of $\dfrac{\partial}{\partial x_{i}}$), so that%
\begin{equation}
\operatorname*{Coeff}\nolimits_{\left(  k_{1},k_{2},k_{3},...\right)  }%
\dfrac{\partial P}{\partial x_{i}}=\left(  k_{i}+1\right)  b_{\left(
k_{1},k_{2},...,k_{i-1},k_{i}+1,k_{i+1},k_{i+2},...\right)  }
\label{pf.euler.recognizing-exp.14}%
\end{equation}
for every $\left(  k_{1},k_{2},k_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$.

Now, define $\left(  k_{1},k_{2},k_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ to be the sequence
$\left(  m_{1},m_{2},...,m_{i-1},m_{i}-1,m_{i+1},m_{i+2},...\right)  $. Then,
\begin{align*}
\left(  k_{1},k_{2},...,k_{i-1},k_{i}+1,k_{i+1},k_{i+2},...\right)   &
=\left(  m_{1},m_{2},...,m_{i-1},\underbrace{\left(  m_{i}-1\right)
+1}_{=m_{i}},m_{i+1},m_{i+2},...\right) \\
&  =\left(  m_{1},m_{2},...,m_{i-1},m_{i},m_{i+1},m_{i+2},...\right)  =\left(
m_{1},m_{2},m_{3},...\right)  .
\end{align*}
Hence, (\ref{pf.euler.recognizing-exp.14}) rewrites as%
\begin{equation}
\operatorname*{Coeff}\nolimits_{\left(  k_{1},k_{2},k_{3},...\right)  }%
\dfrac{\partial P}{\partial x_{i}}=\left(  k_{i}+1\right)  b_{\left(
m_{1},m_{2},m_{3},...\right)  }. \label{pf.euler.recognizing-exp.15}%
\end{equation}


Also, since $\left(  k_{1},k_{2},k_{3},...\right)  =\left(  m_{1}%
,m_{2},...,m_{i-1},m_{i}-1,m_{i+1},m_{i+2},...\right)  $, we have
\begin{align*}
k_{1}+k_{2}+k_{3}+...  &  =m_{1}+m_{2}+...+m_{i-1}+\left(  m_{i}-1\right)
+m_{i+1}+m_{i+2}+...\\
&  =\underbrace{\left(  m_{1}+m_{2}+...+m_{i-1}+m_{i}+m_{i+1}+m_{i+2}%
+...\right)  }_{=m_{1}+m_{2}+m_{3}+...=\kappa+1}-1=\kappa+1-1=\kappa.
\end{align*}
Hence, we can apply (\ref{pf.euler.recognizing-exp.1}) to $\kappa$ and
$\left(  k_{1},k_{2},k_{3},...\right)  $ instead of $K$ and $\left(
m_{1},m_{2},m_{3},...\right)  $ (since we have assumed that
(\ref{pf.euler.recognizing-exp.1}) holds for $K=\kappa$). As a consequence, we
obtain%
\begin{equation}
\operatorname*{Coeff}\nolimits_{\left(  k_{1},k_{2},k_{3},...\right)
}P=\dfrac{1}{k_{1}!k_{2}!k_{3}!...}\alpha_{1}^{k_{1}}\alpha_{2}^{k_{2}}%
\alpha_{3}^{k_{3}}...\operatorname*{Coeff}\nolimits_{\left(  0,0,0,...\right)
}P. \label{pf.euler.recognizing-exp.16}%
\end{equation}


Recall that $\left(  k_{1},k_{2},k_{3},...\right)  =\left(  m_{1}%
,m_{2},...,m_{i-1},m_{i}-1,m_{i+1},m_{i+2},...\right)  $. Thus, every positive
integer $j$ such that $j\neq i$ satisfies $k_{j}=m_{j}$. Hence, $\prod
\limits_{\substack{j>0;\\j\neq i}}\alpha_{j}^{k_{j}}=\prod
\limits_{\substack{j>0;\\j\neq i}}\alpha_{j}^{m_{j}}$ and $\prod
\limits_{\substack{j>0;\\j\neq i}}k_{j}!=\prod\limits_{\substack{j>0;\\j\neq
i}}m_{j}!$.

On the other hand, from $\left(  k_{1},k_{2},k_{3},...\right)  =\left(
m_{1},m_{2},...,m_{i-1},m_{i}-1,m_{i+1},m_{i+2},...\right)  $, we obtain
$k_{i}=m_{i}-1$, so that $k_{i}+1=m_{i}$. But%
\[
\alpha_{1}^{k_{1}}\alpha_{2}^{k_{2}}\alpha_{3}^{k_{3}}...=\prod\limits_{j>0}%
\alpha_{j}^{k_{j}}=\underbrace{\alpha_{i}^{k_{i}}}_{\substack{=\alpha
_{i}^{m_{i}-1}\\\text{(since }k_{i}=m_{i}-1\text{)}}}\cdot\underbrace{\prod
\limits_{\substack{j>0;\\j\neq i}}\alpha_{j}^{k_{j}}}_{=\prod
\limits_{\substack{j>0;\\j\neq i}}\alpha_{j}^{m_{j}}}=\alpha_{i}^{m_{i}%
-1}\cdot\prod\limits_{\substack{j>0;\\j\neq i}}\alpha_{j}^{m_{j}},
\]
so that%
\begin{equation}
\alpha_{i}\cdot\left(  \alpha_{1}^{k_{1}}\alpha_{2}^{k_{2}}\alpha_{3}^{k_{3}%
}...\right)  =\underbrace{\alpha_{i}\cdot\alpha_{i}^{m_{i}-1}}_{=\alpha
_{i}^{m_{i}}}\cdot\prod\limits_{\substack{j>0;\\j\neq i}}\alpha_{j}^{m_{j}%
}=\alpha_{i}^{m_{i}}\cdot\prod\limits_{\substack{j>0;\\j\neq i}}\alpha
_{j}^{m_{j}}=\prod\limits_{j>0}\alpha_{j}^{m_{j}}=\alpha_{1}^{m_{1}}\alpha
_{2}^{m_{2}}\alpha_{3}^{m_{3}}.... \label{pf.euler.recognizing-exp.18}%
\end{equation}
Besides,%
\begin{align}
k_{1}!k_{2}!k_{3}!...  &  =\prod\limits_{j>0}k_{j}!=\underbrace{k_{i}}%
_{=m_{i}-1}!\cdot\underbrace{\prod\limits_{\substack{j>0;\\j\neq i}}k_{j}%
!}_{=\prod\limits_{\substack{j>0;\\j\neq i}}m_{j}!}=\underbrace{\left(
m_{i}-1\right)  !}_{=\dfrac{m_{i}!}{m_{i}}}\cdot\prod
\limits_{\substack{j>0;\\j\neq i}}m_{j}!\nonumber\\
&  =\dfrac{1}{m_{i}}\cdot\underbrace{m_{i}!\cdot\prod
\limits_{\substack{j>0;\\j\neq i}}m_{j}!}_{=\prod\limits_{j>0}m_{j}%
!=m_{1}!m_{2}!m_{3}!...}=\dfrac{1}{m_{i}}m_{1}!m_{2}!m_{3}!....
\label{pf.euler.recognizing-exp.19}%
\end{align}
Now, $\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)
}P=b_{\left(  m_{1},m_{2},m_{3},...\right)  }$, so that%
\begin{align*}
m_{i}\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)  }P
&  =\underbrace{m_{i}}_{=k_{i}+1}b_{\left(  m_{1},m_{2},m_{3},...\right)
}=\left(  k_{i}+1\right)  b_{\left(  m_{1},m_{2},m_{3},...\right)  }\\
&  =\operatorname*{Coeff}\nolimits_{\left(  k_{1},k_{2},k_{3},...\right)
}\dfrac{\partial P}{\partial x_{i}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.euler.recognizing-exp.15})}\right) \\
&  =\operatorname*{Coeff}\nolimits_{\left(  k_{1},k_{2},k_{3},...\right)
}\left(  \alpha_{i}P\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }%
\dfrac{\partial P}{\partial x_{i}}=\alpha_{i}P\right) \\
&  =\alpha_{i}\underbrace{\operatorname*{Coeff}\nolimits_{\left(  k_{1}%
,k_{2},k_{3},...\right)  }P}_{\substack{=\dfrac{1}{k_{1}!k_{2}!k_{3}%
!...}\alpha_{1}^{k_{1}}\alpha_{2}^{k_{2}}\alpha_{3}^{k_{3}}%
...\operatorname*{Coeff}\nolimits_{\left(  0,0,0,...\right)  }P\\\text{(by
(\ref{pf.euler.recognizing-exp.16}))}}}\\
&  =\alpha_{i}\dfrac{1}{k_{1}!k_{2}!k_{3}!...}\alpha_{1}^{k_{1}}\alpha
_{2}^{k_{2}}\alpha_{3}^{k_{3}}...\operatorname*{Coeff}\nolimits_{\left(
0,0,0,...\right)  }P\\
&  =\underbrace{\dfrac{1}{k_{1}!k_{2}!k_{3}!...}}_{\substack{=\dfrac{1}%
{\dfrac{1}{m_{i}}m_{1}!m_{2}!m_{3}!...}\\\text{(by
(\ref{pf.euler.recognizing-exp.19}))}}}\cdot\underbrace{\alpha_{i}\cdot\left(
\alpha_{1}^{k_{1}}\alpha_{2}^{k_{2}}\alpha_{3}^{k_{3}}...\right)
}_{\substack{=\alpha_{1}^{m_{1}}\alpha_{2}^{m_{2}}\alpha_{3}^{m_{3}%
}...\\\text{(by (\ref{pf.euler.recognizing-exp.18}))}}}\operatorname*{Coeff}%
\nolimits_{\left(  0,0,0,...\right)  }P\\
&  =\dfrac{1}{\dfrac{1}{m_{i}}m_{1}!m_{2}!m_{3}!...}\alpha_{1}^{m_{1}}%
\alpha_{2}^{m_{2}}\alpha_{3}^{m_{3}}...\operatorname*{Coeff}\nolimits_{\left(
0,0,0,...\right)  }P\\
&  =m_{i}\dfrac{1}{m_{1}!m_{2}!m_{3}!...}\alpha_{1}^{m_{1}}\alpha_{2}^{m_{2}%
}\alpha_{3}^{m_{3}}...\operatorname*{Coeff}\nolimits_{\left(
0,0,0,...\right)  }P.
\end{align*}
We can divide this equality by $m_{i}$ (since $m_{i}>0$). As a result, we
obtain%
\[
\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)
}P=\dfrac{1}{m_{1}!m_{2}!m_{3}!...}\alpha_{1}^{m_{1}}\alpha_{2}^{m_{2}}%
\alpha_{3}^{m_{3}}...\operatorname*{Coeff}\nolimits_{\left(  0,0,0,...\right)
}P.
\]


Now, forget that we fixed $\left(  m_{1},m_{2},m_{3},...\right)  $. We thus
have proven that every $\left(  m_{1},m_{2},m_{3},...\right)  \in
\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ such that
$m_{1}+m_{2}+m_{3}+...=\kappa+1$ satisfies%
\[
\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)
}P=\dfrac{1}{m_{1}!m_{2}!m_{3}!...}\alpha_{1}^{m_{1}}\alpha_{2}^{m_{2}}%
\alpha_{3}^{m_{3}}...\operatorname*{Coeff}\nolimits_{\left(  0,0,0,...\right)
}P.
\]
In other words, (\ref{pf.euler.recognizing-exp.1}) holds for $K=\kappa+1$.
This completes the induction step. The induction proof of
(\ref{pf.euler.recognizing-exp.1}) is thus complete.

Now, we see that
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{every }\left(  m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }\text{ satisfies}\\
\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)
}P=\dfrac{1}{m_{1}!m_{2}!m_{3}!...}\alpha_{1}^{m_{1}}\alpha_{2}^{m_{2}}%
\alpha_{3}^{m_{3}}...\operatorname*{Coeff}\nolimits_{\left(  0,0,0,...\right)
}P
\end{array}
\right)  \label{pf.euler.recognizing-exp.28}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.euler.recognizing-exp.28}):} Let $\left(
m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
1,2,3,...\right\}  }$. Then, $m_{1}+m_{2}+m_{3}+...=m_{1}+m_{2}+m_{3}+...$.
Hence, applying (\ref{pf.euler.recognizing-exp.1}) to $K=m_{1}+m_{2}%
+m_{3}+...$, we obtain $\operatorname*{Coeff}\nolimits_{\left(  m_{1}%
,m_{2},m_{3},...\right)  }P=\dfrac{1}{m_{1}!m_{2}!m_{3}!...}\alpha_{1}^{m_{1}%
}\alpha_{2}^{m_{2}}\alpha_{3}^{m_{3}}...\operatorname*{Coeff}%
\nolimits_{\left(  0,0,0,...\right)  }P$. This proves
(\ref{pf.euler.recognizing-exp.28}).}

Now, define an element $f\in U$ by $f=\operatorname*{Coeff}\nolimits_{\left(
0,0,0,...\right)  }P$. Define a power series $Q\in U\left[  \left[
x_{1},x_{2},x_{3},...\right]  \right]  $ by $Q=f\cdot\exp\left(
\sum\limits_{j>0}x_{j}\alpha_{j}\right)  $. Then,%
\begin{align*}
\operatorname*{Coeff}\nolimits_{\left(  0,0,0,...\right)  }Q  &
=\operatorname*{Coeff}\nolimits_{\left(  0,0,0,...\right)  }\left(  f\cdot
\exp\left(  \sum\limits_{j>0}x_{j}\alpha_{j}\right)  \right) \\
&  =f\cdot\underbrace{\operatorname*{Coeff}\nolimits_{\left(
0,0,0,...\right)  }\left(  \exp\left(  \sum\limits_{j>0}x_{j}\alpha
_{j}\right)  \right)  }_{\substack{=1\\\text{(since the constant term of the
exponential series is }1\text{)}}}=f.
\end{align*}


It is easy to see that every integer $i>0$ satisfies%
\begin{equation}
\dfrac{\partial Q}{\partial x_{i}}=\alpha_{i}Q.
\label{pf.euler.recognizing-exp.35}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.euler.recognizing-exp.35}):} Let $i>0$ be
an integer. Since $R\left[  \left[  x_{1},x_{2},x_{3},...\right]  \right]  $
is a commutative $\mathbb{Q}$-algebra, we have%
\[
\exp\left(  \sum\limits_{j>0}x_{j}\alpha_{j}\right)  =\prod\limits_{j>0}%
\exp\left(  x_{j}\alpha_{j}\right)  =\exp\left(  x_{i}\alpha_{i}\right)
\cdot\prod\limits_{\substack{j>0;\\j\neq i}}\exp\left(  x_{j}\alpha
_{j}\right)  .
\]
Thus,%
\begin{align*}
&  \dfrac{\partial}{\partial x_{i}}\exp\left(  \sum\limits_{j>0}x_{j}%
\alpha_{j}\right) \\
&  =\dfrac{\partial}{\partial x_{i}}\left(  \exp\left(  x_{i}\alpha
_{i}\right)  \cdot\prod\limits_{\substack{j>0;\\j\neq i}}\exp\left(
x_{j}\alpha_{j}\right)  \right) \\
&  =\underbrace{\left(  \dfrac{\partial}{\partial x_{i}}\exp\left(
x_{i}\alpha_{i}\right)  \right)  }_{=\alpha_{i}\exp\left(  x_{i}\alpha
_{i}\right)  }\cdot\prod\limits_{\substack{j>0;\\j\neq i}}\exp\left(
x_{j}\alpha_{j}\right)  +\exp\left(  x_{i}\alpha_{i}\right)  \cdot
\underbrace{\dfrac{\partial}{\partial x_{i}}\left(  \prod
\limits_{\substack{j>0;\\j\neq i}}\exp\left(  x_{j}\alpha_{j}\right)  \right)
}_{\substack{=0\\\text{(since the variable }x_{i}\text{ never appears in
the}\\\text{power series }\prod\limits_{\substack{j>0;\\j\neq i}}\exp\left(
x_{j}\alpha_{j}\right)  \text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the Leibniz rule}\right) \\
&  =\alpha_{i}\underbrace{\exp\left(  x_{i}\alpha_{i}\right)  \cdot
\prod\limits_{\substack{j>0;\\j\neq i}}\exp\left(  x_{j}\alpha_{j}\right)
}_{=\exp\left(  \sum\limits_{j>0}x_{j}\alpha_{j}\right)  }+\underbrace{\exp
\left(  x_{i}\alpha_{i}\right)  \cdot0}_{=0}=\alpha_{i}\exp\left(
\sum\limits_{j>0}x_{j}\alpha_{j}\right)  .
\end{align*}
Now,%
\begin{align*}
\dfrac{\partial Q}{\partial x_{i}}  &  =\dfrac{\partial}{\partial x_{i}%
}Q=\dfrac{\partial}{\partial x_{i}}\left(  f\cdot\exp\left(  \sum
\limits_{j>0}x_{j}\alpha_{j}\right)  \right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }Q=f\cdot\exp\left(  \sum\limits_{j>0}x_{j}\alpha_{j}\right)
\right) \\
&  =f\cdot\underbrace{\dfrac{\partial}{\partial x_{i}}\exp\left(
\sum\limits_{j>0}x_{j}\alpha_{j}\right)  }_{=\alpha_{i}\exp\left(
\sum\limits_{j>0}x_{j}\alpha_{j}\right)  }=f\cdot\alpha_{i}\exp\left(
\sum\limits_{j>0}x_{j}\alpha_{j}\right)  =\alpha_{i}\cdot\underbrace{\left(
f\cdot\exp\left(  \sum\limits_{j>0}x_{j}\alpha_{j}\right)  \right)  }%
_{=Q}=\alpha_{i}Q.
\end{align*}
This proves (\ref{pf.euler.recognizing-exp.35}).}

Now, let us recollect what we have done. We have proven the property
(\ref{pf.euler.recognizing-exp.28}) for every power series $P\in U\left[
\left[  x_{1},x_{2},x_{3},...\right]  \right]  $ such that every $i>0$
satisfies $\dfrac{\partial P}{\partial x_{i}}=\alpha_{i}P$. Since we know that
$Q\in U\left[  \left[  x_{1},x_{2},x_{3},...\right]  \right]  $ also is a
power series such that every $i>0$ satisfies $\dfrac{\partial Q}{\partial
x_{i}}=\alpha_{i}Q$ (due to (\ref{pf.euler.recognizing-exp.28})), we can
therefore apply (\ref{pf.euler.recognizing-exp.28}) to $Q$ instead of $P$. As
a result, we obtain:%
\[
\left(
\begin{array}
[c]{c}%
\text{every }\left(  m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }\text{ satisfies}\\
\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)
}Q=\dfrac{1}{m_{1}!m_{2}!m_{3}!...}\alpha_{1}^{m_{1}}\alpha_{2}^{m_{2}}%
\alpha_{3}^{m_{3}}...\operatorname*{Coeff}\nolimits_{\left(  0,0,0,...\right)
}Q
\end{array}
\right)  .
\]
Hence, for every $\left(  m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$, we have%
\begin{align*}
\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)  }Q  &
=\dfrac{1}{m_{1}!m_{2}!m_{3}!...}\alpha_{1}^{m_{1}}\alpha_{2}^{m_{2}}%
\alpha_{3}^{m_{3}}...\underbrace{\operatorname*{Coeff}\nolimits_{\left(
0,0,0,...\right)  }Q}_{=f=\operatorname*{Coeff}\nolimits_{\left(
0,0,0,...\right)  }P}\\
&  =\dfrac{1}{m_{1}!m_{2}!m_{3}!...}\alpha_{1}^{m_{1}}\alpha_{2}^{m_{2}}%
\alpha_{3}^{m_{3}}...\operatorname*{Coeff}\nolimits_{\left(  0,0,0,...\right)
}P=\operatorname*{Coeff}\nolimits_{\left(  m_{1},m_{2},m_{3},...\right)  }P
\end{align*}
(by (\ref{pf.euler.recognizing-exp.28})). In other words, for every $\left(
m_{1},m_{2},m_{3},...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{
1,2,3,...\right\}  }$, the coefficient of the power series $Q$ before the
monomial $x_{1}^{m_{1}}x_{2}^{m_{2}}x_{3}^{m_{3}}...$ equals the coefficient
of the power series $P$ before the monomial $x_{1}^{m_{1}}x_{2}^{m_{2}}%
x_{3}^{m_{3}}...$. In other words, the coefficients of the power series $Q$
equal the corresponding coefficients of the power series $P$. Thus, the power
series $Q$ itself equals $P$, so that we have%
\[
P=Q=f\cdot\exp\left(  \sum\limits_{j>0}x_{j}\alpha_{j}\right)  .
\]
This proves Proposition \ref{prop.euler.recognizing-exp}.
\end{verlong}

\subsubsection{Homogeneous maps and equigraded series}

The discussion we will be doing now is only vaguely related to power series
(let alone quantum fields); it is meant as a preparation for a later proof
(namely, that of Theorem \ref{thm.euler}), where it will provide
``convergence'' assertions (in a certain sense).

A well-known nuisance in the theory of $\mathbb{Z}$-graded vector spaces is
the fact that the endomorphism ring of a $\mathbb{Z}$-graded vector space is
not (in general) $\mathbb{Z}$-graded. It does, however, contain a $\mathbb{Z}%
$-graded subring, which we will introduce now:

\begin{definition}
\label{def.hg}\textbf{(a)} Let $V$ and $W$ be two $\mathbb{Z}$-graded vector
spaces, with gradings $\left(  V\left[  n\right]  \right)  _{n\in\mathbb{Z}}$
and $\left(  W\left[  n\right]  \right)  _{n\in\mathbb{Z}}$, respectively. Let
$f:V\rightarrow W$ be a linear map. Let $m\in\mathbb{Z}$. Then, $f$ is said to
be a \textit{homogeneous linear map of degree }$m$ if every $n\in\mathbb{Z}$
satisfies $f\left(  V\left[  n\right]  \right)  \subseteq W\left[  n+m\right]
$.

(It is important not to confuse this notion of ``homogeneous linear maps of
degree $m$'' with the notion of ``homogeneous polynomial maps of degree $n$''
defined in Definition \ref{def.det.US.poly.hom} \textbf{(a)}; the former of
these notions is not a particular case of the latter.)

Note that the homogeneous linear maps of degree $0$ are exactly the graded
linear maps.

\textbf{(b)} Let $V$ and $W$ be two $\mathbb{Z}$-graded vector spaces. For
every $m\in\mathbb{Z}$, let $\operatorname*{Hom}\nolimits_{\operatorname{hg}%
=m}\left(  V,W\right)  $ denote the vector space of all homogeneous linear
maps $V\rightarrow W$ of degree $m$. This $\operatorname*{Hom}%
\nolimits_{\operatorname{hg}=m}\left(  V,W\right)  $ is a vector subspace of
$\operatorname*{Hom}\left(  V,W\right)  $ for every $m\in\mathbb{Z}$.
Moreover, $\bigoplus\limits_{m\in\mathbb{Z}}\operatorname*{Hom}%
\nolimits_{\operatorname{hg}=m}\left(  V,W\right)  $ is a well-defined
internal direct sum, and will be denoted by $\operatorname*{Hom}%
\nolimits_{\operatorname{hg}}\left(  V,W\right)  $. This $\operatorname*{Hom}%
\nolimits_{\operatorname{hg}}\left(  V,W\right)  $ is a vector subspace of
$\operatorname*{Hom}\left(  V,W\right)  $, and is canonically a $\mathbb{Z}%
$-graded vector space, with its $m$-th graded component being
$\operatorname*{Hom}\nolimits_{\operatorname{hg}=m}\left(  V,W\right)  $.

\textbf{(c)} Let $V$ be a $\mathbb{Z}$-graded vector space. Then, let
$\operatorname*{End}\nolimits_{\operatorname{hg}}V$ denote the $\mathbb{Z}%
$-graded vector subspace $\operatorname*{Hom}\nolimits_{\operatorname{hg}%
}\left(  V,V\right)  $ of $\operatorname*{Hom}\left(  V,V\right)
=\operatorname*{End}V$. Then, $\operatorname*{End}\nolimits_{\operatorname{hg}%
}V$ is a subalgebra of $\operatorname*{End}V$, and a $\mathbb{Z}$-graded
algebra. Moreover, the canonical action of $\operatorname*{End}%
\nolimits_{\operatorname{hg}}V$ on $V$ (obtained by restricting the action of
$\operatorname*{End}V$ on $V$ to $\operatorname*{End}%
\nolimits_{\operatorname{hg}}V$) makes $V$ into a $\mathbb{Z}$-graded
$\operatorname*{End}\nolimits_{\operatorname{hg}}V$-module.
\end{definition}

We next need a relatively simple notion for a special kind of power series. I
(Darij) call them ``equigraded power series'', though noone else seems to use
this nomenclature.

\begin{definition}
Let $B$ be a $\mathbb{Z}$-graded vector space, and $z$ a symbol. An element
$\sum\limits_{n\in\mathbb{Z}}b_{n}z^{n}$ of $B\left[  \left[  z,z^{-1}\right]
\right]  $ (with $b_{n}\in B$ for every $n\in\mathbb{Z}$) is said to be
\textit{equigraded} if every $n\in\mathbb{Z}$ satisfies $b_{n}\in B\left[
n\right]  $ (where $\left(  B\left[  m\right]  \right)  _{m\in\mathbb{Z}}$
denotes the grading on $B$). Since $B\left[  \left[  z\right]  \right]  $ and
$B\left(  \left(  z\right)  \right)  $ are vector subspaces of $B\left[
\left[  z,z^{-1}\right]  \right]  $, it clearly makes sense to speak of
equigraded elements of $B\left[  \left[  z\right]  \right]  $ or of $B\left(
\left(  z\right)  \right)  $. We will denote by $B\left[  \left[
z,z^{-1}\right]  \right]  _{\operatorname*{equi}}$ the set of all equigraded
elements of $B\left[  \left[  z,z^{-1}\right]  \right]  $. It is easy to see
that $B\left[  \left[  z,z^{-1}\right]  \right]  _{\operatorname*{equi}}$ is a
vector subspace of $B\left[  \left[  z,z^{-1}\right]  \right]  $.
\end{definition}

Elementary properties of equigraded elements are:

\begin{proposition}
\label{prop.equigraded.basics}\textbf{(a)} Let $B$ be a $\mathbb{Z}$-graded
vector space, and $z$ a symbol. Then,%
\begin{align*}
&  \left\{  f\in B\left[  z\right]  \ \mid\ f\text{ is equigraded}\right\}
,\ \ \ \ \ \ \ \ \ \ \left\{  f\in B\left[  z,z^{-1}\right]  \ \mid\ f\text{
is equigraded}\right\}  ,\\
&  \left\{  f\in B\left[  \left[  z\right]  \right]  \ \mid\ f\text{ is
equigraded}\right\}  ,\ \ \ \ \ \ \ \ \ \ \left\{  f\in B\left(  \left(
z\right)  \right)  \ \mid\ f\text{ is equigraded}\right\}  ,\\
&  \left\{  f\in B\left[  \left[  z,z^{-1}\right]  \right]  \ \mid\ f\text{ is
equigraded}\right\}  =B\left[  \left[  z,z^{-1}\right]  \right]
_{\operatorname*{equi}}%
\end{align*}
are vector spaces.

\textbf{(b)} Let $B$ be a $\mathbb{Z}$-graded algebra. Then, $\left\{  f\in
B\left[  \left[  z\right]  \right]  \ \mid\ f\text{ is equigraded}\right\}  $
is a subalgebra of $B\left[  \left[  z\right]  \right]  $ and closed with
respect to the usual topology on $B\left[  \left[  z\right]  \right]  $.

\textbf{(c)} Let $B$ be a $\mathbb{Z}$-graded algebra. If $f\in B\left[
\left[  z\right]  \right]  $ is an equigraded power series and invertible in
the ring $B\left[  \left[  z\right]  \right]  $, then $f^{-1}$ also is an
equigraded power series.
\end{proposition}

We will only use parts \textbf{(a)} and \textbf{(b)} of this proposition, and
these are completely straightforward to prove. (Part \textbf{(c)} is less
straightforward but still an easy exercise.)

Equigradedness of power series sometimes makes their actions on modules more
manageable. Here is an example:

\begin{proposition}
\label{prop.equigraded.fx}Let $A$ be a $\mathbb{Z}$-graded algebra, and let
$M$ be a $\mathbb{Z}$-graded $A$-module. Assume that $M$ is concentrated in
nonnegative degrees. Let $u$ be a symbol.

\textbf{(a)} It is clear that for any $f\in A\left[  \left[  u,u^{-1}\right]
\right]  $ and any $x\in M\left[  u,u^{-1}\right]  $, the product $fx$ is a
well-defined element of $M\left[  \left[  u,u^{-1}\right]  \right]  $.

\textbf{(b)} For any \textbf{equigraded} $f\in A\left[  \left[  u,u^{-1}%
\right]  \right]  $ and any $x\in M\left[  u,u^{-1}\right]  $, the product
$fx$ is a well-defined element of $M\left(  \left(  u\right)  \right)  $ (and
not only of $M\left[  \left[  u,u^{-1}\right]  \right]  $).

\textbf{(c)} For any \textbf{equigraded} $f\in A\left[  \left[  u^{-1}\right]
\right]  $ and any $x\in M\left[  u^{-1}\right]  $, the product $fx$ is a
well-defined element of $M\left[  u^{-1}\right]  $ (and not only of $M\left[
\left[  u^{-1}\right]  \right]  $).
\end{proposition}

\begin{vershort}
The proof of this proposition is quick and straightforward. (The only idea is
that for any fixed $x\in M\left[  u,u^{-1}\right]  $, any sufficiently
low-degree element of $A$ annihilates $x$ due to the \textquotedblleft
concentrated in nonnegative degrees\textquotedblright\ assumption, but
sufficiently low-degree monomials in $f$ come with sufficiently low-degree
coefficients due to $f$ being equigraded.)
\end{vershort}

\begin{verlong}
\textit{Proof of Proposition \ref{prop.equigraded.fx}.} Denote by $\left(
A\left[  m\right]  \right)  _{m\in\mathbb{Z}}$ the grading on $A$. Denote by
$\left(  M\left[  m\right]  \right)  _{m\in\mathbb{Z}}$ the $\mathbb{Z}%
$-grading on $M$.

\textbf{(a)} Part \textbf{(a)} of Proposition \ref{prop.equigraded.fx} is obvious.

\textbf{(b)} Let $f\in A\left[  \left[  u,u^{-1}\right]  \right]  $ be
equigraded. Let $x\in M\left[  u,u^{-1}\right]  $. We must show that $fx\in
M\left(  \left(  u\right)  \right)  $.

Since $M$ is concentrated in nonnegative degrees, we have $M=\bigoplus
\limits_{m\geq0}M\left[  m\right]  =\sum\limits_{m\geq0}M\left[  m\right]  $
(since direct sums are sums). Hence, $M\cdot u^{n}=\left(  \sum\limits_{m\geq
0}M\left[  m\right]  \right)  \cdot u^{n}=\sum\limits_{m\geq0}M\left[
m\right]  \cdot u^{n}$ for every $n\in\mathbb{Z}$. But
\begin{align*}
x  &  \in M\left[  u,u^{-1}\right]  =\sum\limits_{n\in\mathbb{Z}%
}\underbrace{M\cdot u^{n}}_{=\sum\limits_{m\geq0}M\left[  m\right]  \cdot
u^{n}}=\sum\limits_{n\in\mathbb{Z}}\sum\limits_{m\geq0}M\left[  m\right]
\cdot u^{n}=\sum\limits_{m\geq0}\underbrace{\sum\limits_{n\in\mathbb{Z}%
}M\left[  m\right]  \cdot u^{n}}_{=\left(  M\left[  m\right]  \right)  \left[
u,u^{-1}\right]  }\\
&  =\sum\limits_{m\geq0}\left(  M\left[  m\right]  \right)  \left[
u,u^{-1}\right]  .
\end{align*}
Hence, we can write the element $x$ in the form $x=\sum\limits_{m\geq0}x_{m}%
$\text{ for some family }$\left(  x_{m}\right)  _{m\in\mathbb{N}}$ of elements
of $M\left[  u,u^{-1}\right]  $ satisfying $\left(  x_{m}\in\left(  M\left[
m\right]  \right)  \left[  u,u^{-1}\right]  \text{ for every }m\in
\mathbb{N}\right)  $ and \newline$\left(  x_{m}=0\text{ for all but finitely
many }m\in\mathbb{N}\right)  $.

Since $\left(  x_{m}=0\text{ for all but finitely many }m\in\mathbb{N}\right)
$, there exists a finite set $S\subseteq\mathbb{N}$ such that every
$m\in\mathbb{N}\setminus S$ satisfies $x_{m}=0$. Consider this $S$.

Since $S$ is a finite set, the set $S$ has an upper bound. That is, there
exists a $t\in\mathbb{N}$ such that every $m\in S$ satisfies $m\leq t$.
Consider this $t$. Then, every $m\in\mathbb{N}$ such that $m>t$ satisfies
$m\in\mathbb{N}\setminus S$ (because otherwise, it would satisfy
$m\notin\mathbb{N}\setminus S$, thus $m\in S$, thus $m\leq t$, contradicting
$m>t$). Consequently, every $m\in\mathbb{N}$ such that $m>t$ satisfies
$x_{m}=0$ (since we know that every $m\in\mathbb{N}\setminus S$ satisfies
$x_{m}=0$). Thus, $\sum\limits_{\substack{m\geq0;\\m>t}}\underbrace{x_{m}%
}_{\substack{=0\\\text{(since }m>t\text{)}}}=\sum\limits_{\substack{m\geq
0;\\m>t}}0=0$. But%
\[
x=\sum\limits_{m\geq0}x_{m}=\sum\limits_{\substack{m\geq0;\\m\leq t}%
}x_{m}+\underbrace{\sum\limits_{\substack{m\geq0;\\m>t}}x_{m}}_{=0}%
=\sum\limits_{\substack{m\geq0;\\m\leq t}}x_{m}.
\]


Write $f\in A\left[  \left[  u,u^{-1}\right]  \right]  $ in the form
$f=\sum\limits_{n\in\mathbb{Z}}b_{n}u^{n}$. Then, every $n\in\mathbb{Z}$
satisfies $b_{n}\in A\left[  n\right]  $ (since $f$ is equigraded). Hence,
every $n\in\mathbb{Z}$ such that $n<-t$ satisfies%
\begin{align*}
b_{n}x  &  =b_{n}\sum\limits_{\substack{m\geq0;\\m\leq t}}x_{m}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }x=\sum\limits_{\substack{m\geq
0;\\m\leq t}}x_{m}\right) \\
&  =\sum\limits_{\substack{m\geq0;\\m\leq t}}\underbrace{b_{n}}_{\in A\left[
n\right]  }\underbrace{x_{m}}_{\in\left(  M\left[  m\right]  \right)  \left[
u,u^{-1}\right]  =\sum\limits_{k\in\mathbb{Z}}M\left[  m\right]  \cdot u^{k}%
}\in\sum\limits_{\substack{m\geq0;\\m\leq t}}\underbrace{\left(  A\left[
n\right]  \right)  \cdot\left(  \sum\limits_{k\in\mathbb{Z}}M\left[  m\right]
\cdot u^{k}\right)  }_{=\sum\limits_{k\in\mathbb{Z}}A\left[  n\right]  \cdot
M\left[  m\right]  \cdot u^{k}}\\
&  =\sum\limits_{\substack{m\geq0;\\m\leq t}}\sum\limits_{k\in\mathbb{Z}%
}\underbrace{A\left[  n\right]  \cdot M\left[  m\right]  }%
_{\substack{\subseteq M\left[  n+m\right]  \\\text{(since }M\text{ is a
}\mathbb{Z}\text{-graded }A\text{-module)}}}\cdot u^{k}\subseteq
\sum\limits_{\substack{m\geq0;\\m\leq t}}\sum\limits_{k\in\mathbb{Z}%
}\underbrace{M\left[  n+m\right]  }_{\substack{=0\\\text{(because from
}n<-t\text{ and }m\leq t\text{, we obtain}\\n+m<\left(  -t\right)  +t=0\text{,
so that }M\left[  n+m\right]  =0\\\text{(because }M\text{ is concentrated in
nonnegative}\\\text{degrees))}}}\cdot u^{k}\\
&  =\sum\limits_{\substack{m\geq0;\\m\leq t}}\sum\limits_{k\in\mathbb{Z}%
}0\cdot u^{k}=0.
\end{align*}
In other words, every $n\in\mathbb{Z}$ such that $n<-t$ satisfies $b_{n}x=0$.
Hence, from $f=\sum\limits_{n\in\mathbb{Z}}b_{n}u^{n}$, we conclude that
\begin{align*}
fx  &  =\sum\limits_{n\in\mathbb{Z}}b_{n}u^{n}x=\sum\limits_{n\in\mathbb{Z}%
}b_{n}xu^{n}=\sum\limits_{\substack{n\in\mathbb{Z};\\n<-t}}\underbrace{b_{n}%
x}_{\substack{=0\\\text{(since }n<-t\text{)}}}u^{n}+\sum
\limits_{\substack{n\in\mathbb{Z};\\n\geq-t}}b_{n}xu^{n}\\
&  =\underbrace{\sum\limits_{\substack{n\in\mathbb{Z};\\n<-t}}0u^{n}}%
_{=0}+\sum\limits_{\substack{n\in\mathbb{Z};\\n\geq-t}}b_{n}xu^{n}%
=\sum\limits_{\substack{n\in\mathbb{Z};\\n\geq-t}}b_{n}xu^{n}%
=\underbrace{\left(  \sum\limits_{\substack{n\in\mathbb{Z};\\n\geq-t}%
}b_{n}u^{n}\right)  }_{\in A\left(  \left(  u\right)  \right)  }%
\cdot\underbrace{x}_{\in M\left[  u,u^{-1}\right]  \subseteq M\left(  \left(
u\right)  \right)  }\\
&  \in A\left(  \left(  u\right)  \right)  \cdot M\left(  \left(  u\right)
\right)  \subseteq M\left(  \left(  u\right)  \right)  .
\end{align*}
This proves Proposition \ref{prop.equigraded.fx} \textbf{(b)}.

\textbf{(c)} Let $f\in A\left[  \left[  u^{-1}\right]  \right]  $ be
equigraded. Let $x\in M\left[  u^{-1}\right]  $. We must show that $fx\in
M\left[  u^{-1}\right]  $.

Since $f\in A\left[  \left[  u^{-1}\right]  \right]  \subseteq A\left[
\left[  u,u^{-1}\right]  \right]  $ and $x\in M\left[  u^{-1}\right]
\subseteq M\left[  u,u^{-1}\right]  $, we can apply Proposition
\ref{prop.equigraded.fx} \textbf{(b)} and obtain $fx\in M\left(  \left(
u\right)  \right)  $. Thus, we can write the Laurent series $fx$ in the form
$fx=\sum\limits_{n\in\mathbb{Z}}g_{n}u^{n}$ with $g_{n}$ being elements of $M$
such that $\left(  \text{only finitely many among the negative }n\in
\mathbb{Z}\text{ satisfy }g_{n}\neq0\right)  $. Consider these $g_{n}$. We
have%
\[
\sum\limits_{n\in\mathbb{Z}}g_{n}u^{n}=\underbrace{f}_{\in A\left[  \left[
u^{-1}\right]  \right]  }\underbrace{x}_{\in M\left[  u^{-1}\right]  \subseteq
M\left[  \left[  u^{-1}\right]  \right]  }\in A\left[  \left[  u^{-1}\right]
\right]  \cdot M\left[  \left[  u^{-1}\right]  \right]  \subseteq M\left[
\left[  u^{-1}\right]  \right]  .
\]
Hence, every positive $n\in\mathbb{Z}$ satisfies $g_{n}=0$ (because the
coefficient of any power series in $M\left[  \left[  u^{-1}\right]  \right]  $
before $u^{n}$ must be $0$ for any positive $n\in\mathbb{Z}$). Now,%
\begin{align*}
fx  &  =\sum\limits_{n\in\mathbb{Z}}g_{n}u^{n}=\sum\limits_{\substack{n\in
\mathbb{Z};\\n<0}}g_{n}u^{n}+g_{0}u^{0}+\sum\limits_{\substack{n\in
\mathbb{Z};\\n>0}}\underbrace{g_{n}}_{\substack{=0\\\text{(since }n\text{ is
positive)}}}u^{n}\\
&  =\sum\limits_{\substack{n\in\mathbb{Z};\\n<0}}g_{n}u^{n}+g_{0}%
u^{0}+\underbrace{\sum\limits_{\substack{n\in\mathbb{Z};\\n>0}}0u^{n}}%
_{=0}=\underbrace{\sum\limits_{\substack{n\in\mathbb{Z};\\n<0}}g_{n}u^{n}%
}_{\substack{\in M\left[  u^{-1}\right]  \\\text{(because only finitely
many}\\\text{among the negative }n\in\mathbb{Z}\\\text{satisfy }g_{n}%
\neq0\text{)}}}+\underbrace{g_{0}u^{0}}_{\in M\left[  u^{-1}\right]  }\in
M\left[  u^{-1}\right]  +M\left[  u^{-1}\right]  \subseteq M\left[
u^{-1}\right]  .
\end{align*}
This proves Proposition \ref{prop.equigraded.fx} \textbf{(c)}.
\end{verlong}

\subsection{\textbf{[unfinished]} More on unitary representations}

Let us consider the Verma modules of the Virasoro algebra.

\textbf{Last time:} $L_{\dfrac{\mu^{2}+\lambda^{2}}{2},1+12\lambda^{2}}$ is
unitary (for $\lambda,\mu\in\mathbb{R}$), so the $\operatorname*{Vir}$-module
$L_{h,c}$ is unitary if $c\geq1$ and $h\geq\dfrac{c-1}{24}$.

We can extend this as follows: $L_{0,1}^{\otimes m-1}\otimes L_{h,c}$ is
unitary and has a highest-weight vector $v_{0,1}^{\otimes m-1}\otimes v_{h,c}$
which has weight $\left(  h,c+m-1\right)  $. Hence, the representation
$L_{h,c+m-1}$ is unitary [why? use irreducibility of unitary modules and stuff].

Hence, $L_{h,c}$ is unitary if $c\geq m$ and $h\geq\dfrac{c-m}{24}$.

\begin{theorem}
In fact, $L_{h,c}$ is unitary if $c\geq1$ and $h\geq0$.
\end{theorem}

But this is harder to show.

This is still not an only-if. For example, $L_{0,0}$ is unitary (and $1$-dimensional).

\begin{proposition}
\label{prop.Lhc.unitary.triv}If $L_{h,c}$ is unitary, then $h\geq0$ and
$c\geq0$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.Lhc.unitary.triv}.} Assume that
$L_{h,c}$ is unitary. Then, $\left(  L_{-n}v_{h,c},L_{-n}v_{h,c}\right)
\geq0$ for every $n\in\mathbb{Z}$. But every positive $n\in\mathbb{Z}$
satisfies
\begin{align*}
\left(  L_{-n}v_{h,c},L_{-n}v_{h,c}\right)   &  =\left(  \underbrace{L_{n}%
L_{-n}}_{=\left[  L_{n},L_{-n}\right]  +L_{-n}L_{n}}v_{h,c},v_{h,c}\right)
=\left(  \underbrace{\left(  \left[  L_{n},L_{-n}\right]  +L_{-n}L_{n}\right)
v_{h,c}}_{\substack{=\left[  L_{n},L_{-n}\right]  v_{h,c}\\\text{(since
}L_{-n}L_{n}v_{h,c}=0\text{)}}},v_{h,c}\right) \\
&  =\left(  \underbrace{\left[  L_{n},L_{-n}\right]  }_{=2nL_{0}+\dfrac
{n^{3}-n}{12}C}v_{h,c},v_{h,c}\right)  =2nh+\dfrac{n^{3}-n}{12}c.
\end{align*}
Thus, $2nh+\dfrac{n^{3}-n}{12}c\geq0$ for every positive $n\in\mathbb{Z}$.
From this, by taking $n\rightarrow\infty$, we obtain $c\geq0$. By taking
$n=1$, we get $h\geq0$. This proves Proposition \ref{prop.Lhc.unitary.triv}.

\begin{definition}
\label{def.Cdelta}Let $\delta\in\left\{  0,\dfrac{1}{2}\right\}  $. Let
$C_{\delta}$ be the $\mathbb{C}$-algebra with generators $\left\{  \psi
_{j}\ \mid\ j\in\delta+\mathbb{Z}\right\}  $ and relations%
\[
\psi_{j}\psi_{k}+\psi_{k}\psi_{j}=\delta_{k,-j}\ \ \ \ \ \ \ \ \ \ \text{for
all }j,k\in\delta+\mathbb{Z}.
\]
This $\mathbb{C}$-algebra $C_{\delta}$ is an infinite-dimensional Clifford
algebra (namely, the Clifford algebra of the free vector space with basis
$\left\{  \psi_{j}\ \mid\ j\in\delta+\mathbb{Z}\right\}  $ and bilinear form
$\left(  \psi_{j},\psi_{k}\right)  \mapsto\dfrac{1}{2}\delta_{k,-j}$). The
algebra $C_{\delta}$ is called an \textit{algebra of free fermions}. For
$\delta=0$, it is called the \textit{Ramond sector}; for $\delta=\dfrac{1}{2}$
it is called \textit{Neveu-Schwarz sector}.

Let us now construct a representation $V_{\delta}$ of $C_{\delta}$: Let
$V_{\delta}$ be the $\mathbb{C}$-algebra $\wedge\left(  \xi_{n}\ \mid
\ n\in\left(  \delta+\mathbb{Z}\right)  _{\geq0}\right)  $. For any
$i\in\delta+\mathbb{Z}$, define an operator $\dfrac{\partial}{\partial\xi_{i}%
}:V_{\delta}\rightarrow V_{\delta}$ by%
\begin{align*}
&  \dfrac{\partial}{\partial\xi_{i}}\left(  \xi_{j_{1}}\wedge\xi_{j_{2}}%
\wedge...\wedge\xi_{j_{k}}\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  j_{1},j_{2},...,j_{k}\right\}
;\\
\left(  -1\right)  ^{\ell-1}\xi_{j_{1}}\wedge\xi_{j_{2}}\wedge...\wedge
\xi_{j_{\ell-1}}\wedge\xi_{j_{\ell+1}}\wedge\xi_{j_{\ell+2}}\wedge...\wedge
\xi_{j_{k}},\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  j_{1},j_{2}%
,...,j_{k}\right\}
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }j_{1}<j_{2}<...<j_{k}\text{ in
}\delta+\mathbb{Z}\right.  ,
\end{align*}
where, in the case when $i\in\left\{  j_{1},j_{2},...,j_{k}\right\}  $, we
denote by $\ell$ the element $u$ of $\left\{  1,2,...,k\right\}  $ satisfying
$j_{\ell}=u$. (Note the $\left(  -1\right)  ^{\ell-1}$ sign, which
distinguishes this ``differentiation'' from differentiation in the commutative
case. This is a particular case of the Koszul sign rule.)

Define an action of $C_{\delta}$ on $V_{\delta}$ by%
\begin{align*}
\psi_{-n}  &  \mapsto\xi_{n}\ \ \ \ \ \ \ \ \ \ \text{for }n<0;\\
\psi_{n}  &  \mapsto\dfrac{\partial}{\partial\xi_{n}}%
\ \ \ \ \ \ \ \ \ \ \text{for }n>0;\\
\psi_{0}  &  \mapsto\dfrac{1}{\sqrt{2}}\left(  \dfrac{\partial}{\partial
\xi_{0}}+\xi_{0}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{this is only
relevant if }\delta=0\right)  .
\end{align*}


This indeed defines a representation of $C_{\delta}$ (exercise!). This is an
infinite-dimensional analogue of the well-known spinor representation of
Clifford algebras.
\end{definition}

From Homework Set 2 problem 2, we know:

\begin{proposition}
\label{prop.ramond.rep}Let $\delta\in\left\{  0,\dfrac{1}{2}\right\}  $. For
every $k\in\mathbb{Z}$, define an endomorphism $L_{k}$ of $V_{\delta}$ by%
\[
L_{k}=\delta_{k,0}\dfrac{1-2\delta}{16}+\dfrac{1}{2}\sum\limits_{j\in
\delta+\mathbb{Z}}j\left.  :\psi_{-j}\psi_{j+k}:\right.  ,
\]
where the normal ordered product is defined as follows:%
\[
\left.  :\psi_{n}\psi_{m}:\right.  \ =\ \left\{
\begin{array}
[c]{c}%
-\psi_{m}\psi_{n},\ \ \ \ \ \ \ \ \ \ \text{if }m\leq n;\\
\psi_{n}\psi_{m},\ \ \ \ \ \ \ \ \ \ \text{if }m>n
\end{array}
\right.  .
\]
Then:

\textbf{(a)} Every $m\in\delta+\mathbb{Z}$ and $k\in\mathbb{Z}$ satisfy
$\left[  \psi_{m},L_{k}\right]  =\left(  m+\dfrac{k}{2}\right)  \psi_{m+k}$.

\textbf{(b)} Every $n\in\mathbb{Z}$ and $m\in\mathbb{Z}$ satisfy $\left[
L_{n},L_{m}\right]  =\left(  n-m\right)  L_{n+m}+\delta_{n,-m}\dfrac{m^{3}%
-m}{24}$. (Hence, $V_{\delta}$ is a representation of $\operatorname*{Vir}$
with central charge $c=\dfrac{1}{2}$).
\end{proposition}

Now this representation $V_{\delta}$ of $\operatorname*{Vir}$ is unitary. In
fact, consider the Hermitian form under which all monomials in $\psi_{i}$ are
orthonormal (positive definite). Then it is easy to see that $\psi_{j}^{\dag
}=\psi_{-j}$. Thus, $L_{n}^{\dag}=L_{-n}$.

But these representations $V_{\delta}$ are reducible. In fact, we can define a
$\left(  \mathbb{Z}\diagup2\mathbb{Z}\right)  $-grading on $V_{\delta}$ by
giving each $\xi_{n}$ the degree $\overline{1}$, and then the operators
$L_{n}$ preserve parity (i. e., degree under this grading), so that the
representation $V_{\delta}$ can be decomposed as a direct sum $V_{\delta
}=V_{\delta}^{+}\oplus V_{\delta}^{-}$, where $V_{\delta}^{+}$ is the set of
the even elements of $V_{\delta}$, and $V_{\delta}^{-}$ is the set of the odd
elements of $V_{\delta}$.

\begin{theorem}
These subrepresentations $V_{\delta}^{+}$ and $V_{\delta}^{-}$ are irreducible
Virasoro modules.
\end{theorem}

We will not prove this.

What are the highest weights of $V_{\delta}^{+}$ and $V_{\delta}^{-}$ ?

First consider the case $\delta=0$. The highest-weight vector of $V_{\delta
}^{+}$ is $1$, with weight $\left(  \dfrac{1}{16},\dfrac{1}{2}\right)  $. That
of $V_{\delta}^{-}$ is $\xi_{0}$, with weight $\left(  \dfrac{1}{16},\dfrac
{1}{2}\right)  $. Thus, $V_{\delta}^{+}\cong V_{\delta}^{-}$ by action of
$\psi_{0}$ (since $\psi_{0}^{2}=\dfrac{1}{2}$).

Now consider the case $\delta=\dfrac{1}{2}$. The highest-weight vector of
$V_{\delta}^{+}$ is $1$, with weight $\left(  0,\dfrac{1}{2}\right)  $. That
of $V_{\delta}^{-}$ is $\xi_{1/2}$, with weight $\left(  \dfrac{1}{2}%
,\dfrac{1}{2}\right)  $.

\begin{corollary}
The representation $L_{h,\dfrac{1}{2}}$ is unitary if $h=0$, $h=\dfrac{1}{16}$
or $h=\dfrac{1}{2}$. (In physics: Ising model.)
\end{corollary}

We will not prove:

\begin{proposition}
This is an only-if as well.
\end{proposition}

General answer for $c<1$: for $c=1-\dfrac{6}{\left(  m+2\right)  \left(
m+3\right)  }$ for $m\in\mathbb{N}$, there are finitely many $h$ where
$L_{h,c}$ is unitary. For other values of $c$, there are no such values.

\begin{definition}
The \textit{character }$\operatorname*{ch}\nolimits_{V}\left(  q\right)  $ of
a $\operatorname*{Vir}$-module $V$ from category $\mathcal{O}^{+}$ is
$\operatorname*{Tr}\nolimits_{V}\left(  q^{L_{0}}\right)  =\sum\left(  \dim
V_{\lambda}\right)  q^{\lambda}$ for $V_{\lambda}=$generalized eigenspace of
$L_{0}$ with eigenvalue $\lambda$.
\end{definition}

This is related to the old definition of character [how?]

What are the characters of the above modules? Since $V_{\delta}^{+}%
=\wedge\left(  \xi_{1},\xi_{2},\xi_{3},...\right)  ^{+}$, we have%
\[
\operatorname*{ch}\nolimits_{L_{\dfrac{1}{16},\dfrac{1}{2}}}\left(  q\right)
=q^{1/16}\left(  1+q\right)  \left(  1+q^{2}\right)  \left(  1+q^{3}\right)
...=q^{1/16}\prod\limits_{n\geq1}\left(  1+q^{n}\right)
\]
(because
\begin{align*}
2\operatorname*{ch}\nolimits_{L_{\dfrac{1}{16},\dfrac{1}{2}}}\left(  q\right)
&  =\operatorname*{ch}\nolimits_{V_{0}}\left(  q\right)  =q^{1/16}\left(
1+1\right)  \left(  1+q\right)  \left(  1+q^{2}\right)  \left(  1+q^{3}%
\right)  ...\\
&  =2q^{1/16}\left(  1+q\right)  \left(  1+q^{2}\right)  \left(
1+q^{3}\right)  ...
\end{align*}
).

Now%
\begin{align*}
\operatorname*{ch}\nolimits_{L_{0,\dfrac{1}{2}}}\left(  q\right)
+\operatorname*{ch}\nolimits_{L_{\dfrac{1}{2},\dfrac{1}{2}}}\left(  q\right)
&  =\operatorname*{ch}\nolimits_{V_{\dfrac{1}{2}}}\left(  q\right)  =\left(
1+q^{1/2}\right)  \left(  1+q^{3/2}\right)  \left(  1+q^{5/2}\right)  ...\\
&  =\prod\limits_{n\in\dfrac{1}{2}+\mathbb{N}}\left(  1+q^{n}\right)  .
\end{align*}
Thus, $\operatorname*{ch}\nolimits_{L_{0,\dfrac{1}{2}}}\left(  q\right)  $ is
the integer part of the product $\prod\limits_{n\in\dfrac{1}{2}+\mathbb{N}%
}\left(  1+q^{n}\right)  $, and $\operatorname*{ch}\nolimits_{L_{\dfrac{1}%
{2},\dfrac{1}{2}}}\left(  q\right)  $ is the half-integer part of the product
$\prod\limits_{n\in\dfrac{1}{2}+\mathbb{N}}\left(  1+q^{n}\right)  $.

With this, we conclude our study of $V_{\delta}$.

\begin{Convention}
The notation $\psi_{j}$ for the generators of $C_{\delta}$ introduced in
Definition \ref{def.Cdelta} will not be used in the following. (Instead, we
will use the notation $\psi_{j}$ for some completely different objects.)
\end{Convention}

\subsection{The Lie algebra \texorpdfstring{$\mathfrak{gl}_{\infty}$}
{gl-infinity} and its representations}

For every $n\in\mathbb{N}$, we can define a Lie algebra $\mathfrak{gl}_{n}$ of
$n\times n$-matrices over $\mathbb{C}$. One can wonder how this can be
generalized to the ``$n=\infty$ case'', i. e., to infinite matrices.
Obviously, not every pair of infinite matrices has a reasonable commutator
(because not any such pair can be multiplied), but there are certain
restrictions on infinite matrices which allow us to multiply them and form
their commutators. These restrictions can be used to define various Lie
algebras consisting of infinite matrices. We will be concerned with some such
Lie algebras; the first of them is $\mathfrak{gl}_{\infty}$:

\begin{definition}
\label{def.glinf.glinf}We define $\mathfrak{gl}_{\infty}$ to be the vector
space of infinite matrices whose rows and columns are labeled by integers (not
only positive integers!) such that only finitely many entries of the matrix
are nonzero. This vector space $\mathfrak{gl}_{\infty}$ is an associative
algebra \textit{without unit} (by matrix multiplication); we can thus make
$\mathfrak{gl}_{\infty}$ into a Lie algebra by the commutator in this
associative algebra.
\end{definition}

We will study the representations of this $\mathfrak{gl}_{\infty}$. The theory
of these representations will extend the well-known (Schur-Weyl) theory of
representations of $\mathfrak{gl}_{n}$.

\begin{definition}
\label{def.glinf.V}The \textit{vector representation} $V$ of $\mathfrak{gl}%
_{\infty}$ is defined as the vector space $\mathbb{C}^{\left(  \mathbb{Z}%
\right)  }=\left\{  \left(  x_{i}\right)  _{i\in\mathbb{Z}}\text{\ }%
\mid\ x_{i}\in\mathbb{C}\text{; only finitely many }x_{i}\text{ are
nonzero}\right\}  $. The Lie algebra $\mathfrak{gl}_{\infty}$ acts on the
vector representation $V$ in the obvious way: namely, for any $a\in
\mathfrak{gl}_{\infty}$ and $v\in V$, we let $a\rightharpoonup v$ be the
product of the matrix $a$ with the column vector $v$. Here, every element
$\left(  x_{i}\right)  _{i\in\mathbb{Z}}$ of $V$ is identified with the column
vector $\left(
\begin{array}
[c]{c}%
...\\
x_{-2}\\
x_{-1}\\
x_{0}\\
x_{1}\\
x_{2}\\
...
\end{array}
\right)  $.

For every $j\in\mathbb{Z}$, let $v_{j}$ be the vector $\left(  \delta
_{i,j}\right)  _{i\in\mathbb{Z}}\in V$. Then, $\left(  v_{j}\right)
_{j\in\mathbb{Z}}$ is a basis of the vector space $V$.
\end{definition}

\begin{Convention}
When we draw infinite matrices whose rows and columns are labeled by integers,
the index of the rows is supposed to increase as we go from left to right, and
the index of the columns is supposed to increase as we go from top to bottom.
\end{Convention}

\begin{remark}
In Definition \ref{def.glinf.V}, we used the following (very simple) fact: For
every $a\in\mathfrak{gl}_{\infty}$ and every $v\in V$, the product $av$ of the
matrix $a$ with the column vector $v$ is a well-defined element of $V$. This
fact can be generalized: If $a$ is an infinite matrix (whose rows and columns
are labeled by integers) such that every column of $a$ has only finitely many
nonzero entries, and $v$ is an element of $V$, then the product $av$ is a
well-defined element of $V$. However, this does \textbf{no longer} hold if we
drop the condition that every column of $a$ have only finitely many nonzero
entries. (For example, if $a$ would be the matrix whose all entries equal $1$,
then the product $av_{0}$ would \textbf{not} be an element of $V$, but rather
the element $\left(
\begin{array}
[c]{c}%
...\\
1\\
1\\
1\\
1\\
1\\
...
\end{array}
\right)  $ of the \textbf{larger} vector space $\mathbb{C}^{\mathbb{Z}%
}=\left\{  \left(  x_{i}\right)  _{i\in\mathbb{Z}}\text{\ }\mid\ x_{i}%
\in\mathbb{C}\right\}  $. Besides, the product $a\left(
\begin{array}
[c]{c}%
...\\
1\\
1\\
1\\
1\\
1\\
...
\end{array}
\right)  $ would not make any sense at all, not even in $\mathbb{C}%
^{\mathbb{Z}}$.)
\end{remark}

We can consider the representation $\wedge^{i}V$ of $\mathfrak{gl}_{\infty}$
for every $i\in\mathbb{N}$. More generally, we have the so-called
\textit{Schur modules}:

\begin{definition}
If $\pi\in\operatorname*{Irr}S_{n}$, then we can define a representation
$S_{\pi}\left(  V\right)  $ of $\mathfrak{gl}_{\infty}$ by $S_{\pi}\left(
V\right)  =\operatorname*{Hom}\nolimits_{S_{n}}\left(  \pi,V^{\otimes
n}\right)  $ (where $S_{n}$ acts on $V^{\otimes n}$ by permuting the
tensorands). This $S_{\pi}\left(  V\right)  $ is called the $\pi$\textit{-th
Schur module} of $V$.
\end{definition}

This definition mimics the well-known definition (or, more precisely, one of
the definitions) of the Schur modules of a finite-dimensional vector space.

\begin{proposition}
\label{prop.glinf.schur.irred}For every $\pi\in\operatorname*{Irr}S_{n}$, the
representation $S_{\pi}\left(  V\right)  $ of $\mathfrak{gl}_{\infty}$ is irreducible.
\end{proposition}

\textit{Proof of Proposition \ref{prop.glinf.schur.irred}.} The following is
not a self-contained proof; it is just a way to reduce Proposition
\ref{prop.glinf.schur.irred} to the similar fact about finite-dimensional
vector spaces (which is a well-known fact in the representation theory of
$\mathfrak{gl}_{m}$).

For every vector subspace $W\subseteq V$, we can canonically identify $S_{\pi
}\left(  W\right)  $ with a vector subspace of $S_{\pi}\left(  V\right)  $.

For every subset $I$ of $\mathbb{Z}$, let $W_{I}$ be the subset of $V$
generated by all $v_{i}$ with $i\in I$. Clearly, whenever two subsets $I$ and
$J$ of $\mathbb{Z}$ satisfy $I\subseteq J$, we have $W_{I}\subseteq W_{J}$.
Also, whenever $I$ is a finite subset of $\mathbb{Z}$, the vector space
$W_{I}$ is finite-dimensional.

For every tensor $u\in V^{\otimes n}$, there exists a finite subset $I$ of
$\mathbb{Z}$ such that $u\in\left(  W_{I}\right)  ^{\otimes n}$%
.\ \ \ \ \footnote{\textit{Proof.} The family $\left(  v_{i_{1}}\otimes
v_{i_{2}}\otimes...\otimes v_{i_{n}}\right)  _{\left(  i_{1},i_{2}%
,...,i_{n}\right)  \in\mathbb{Z}^{n}}$ is a basis of $V^{\otimes n}$ (since
$\left(  v_{i}\right)  _{i\in\mathbb{Z}}$ is a basis of $V$). Thus, we can
write the tensor $u\in V^{\otimes n}$ as a $\mathbb{C}$-linear combination of
finitely many tensors of the form $v_{i_{1}}\otimes v_{i_{2}}\otimes...\otimes
v_{i_{n}}$ with $\left(  i_{1},i_{2},...,i_{n}\right)  \in\mathbb{Z}^{n}$. Let
$I$ be the union of the sets $\left\{  i_{1},i_{2},...,i_{n}\right\}  $ over
all the tensors which appear in this linear combination. Since only finitely
many tensors appear in this linear combination, the set $I$ is finite. Every
tensor $v_{i_{1}}\otimes v_{i_{2}}\otimes...\otimes v_{i_{n}}$ which appears
in this linear combination satisfies $\left\{  i_{1},i_{2},...,i_{n}\right\}
\subseteq I$ (by the construction of $I$) and thus $v_{i_{1}}\otimes v_{i_{2}%
}\otimes...\otimes v_{i_{n}}\in\left(  W_{I}\right)  ^{\otimes n}$. Thus, $u$
must lie in $\left(  W_{I}\right)  ^{\otimes n}$, too (because $u$ is the
value of this linear combination). Hence, we have found a finite subset $I$ of
$\mathbb{Z}$ such that $u\in\left(  W_{I}\right)  ^{\otimes n}$. Qed.} Denote
this subset $I$ by $I\left(  u\right)  $. Thus, $u\in\left(  W_{I\left(
u\right)  }\right)  ^{\otimes n}$ for every $u\in V^{\otimes n}$.

For every $w\in S_{\pi}\left(  V\right)  $, there exists some finite subset
$I$ of $\mathbb{Z}$ such that $w\in S_{\pi}\left(  W_{I}\right)
$.\ \ \ \ \footnote{\textit{Proof.} Let $w\in S_{\pi}\left(  V\right)  $.
Then, $w\in S_{\pi}\left(  V\right)  =\operatorname*{Hom}\nolimits_{S_{n}%
}\left(  \pi,V^{\otimes n}\right)  $. But since $\pi$ is a finite-dimensional
vector space, the image $w\left(  \pi\right)  $ must be finite-dimensional.
Hence, $w\left(  \pi\right)  $ is a finite-dimensional vector subspace of
$V^{\otimes n}$. Thus, $w\left(  \pi\right)  $ is generated by some elements
$u_{1},u_{2},...,u_{k}\in V^{\otimes n}$. Let $I$ be the union $\bigcup
\limits_{j=1}^{k}I\left(  u_{j}\right)  $. Then, $I$ is finite (because for
every $j\in\left\{  1,2,...,k\right\}  $, the set $I\left(  u_{j}\right)  $ is
finite) and satisfies $I\left(  u_{j}\right)  \subseteq I$ for every
$j\in\left\{  1,2,...,k\right\}  $.
\par
Recall that every $u\in V^{\otimes n}$ satisfies $u\in\left(  W_{I\left(
u\right)  }\right)  ^{\otimes n}$. Thus, every $j\in\left\{
1,2,...,k\right\}  $ satisfies $u_{j}\in\left(  W_{I\left(  u_{j}\right)
}\right)  ^{\otimes n}\subseteq\left(  W_{I}\right)  ^{\otimes n}$ (since
$I\left(  u_{j}\right)  \subseteq I$ and thus $W_{I\left(  u_{j}\right)
}\subseteq W_{I}$). In other words, all $k$ elements $u_{1},u_{2},...,u_{k}$
lie in the vector space $\left(  W_{I}\right)  ^{\otimes n}$. Since the
elements $u_{1},u_{2},...,u_{k}$ generate the subspace $w\left(  \pi\right)
$, this yields that $w\left(  \pi\right)  \subseteq\left(  W_{I}\right)
^{\otimes n}$. Hence, the map $w:\pi\rightarrow V^{\otimes n}$ factors through
a map $\pi\rightarrow\left(  W_{I}\right)  ^{\otimes n}$. In other words,
$w\in\operatorname*{Hom}\nolimits_{S_{n}}\left(  \pi,V^{\otimes n}\right)  $
is contained in $\operatorname*{Hom}\nolimits_{S_{n}}\left(  \pi,\left(
W_{I}\right)  ^{\otimes n}\right)  =S_{\pi}\left(  W_{I}\right)  $, qed.}
Denote this subset $I$ by $I\left(  w\right)  $. Thus, $w\in S_{\pi}\left(
W_{I\left(  w\right)  }\right)  $ for every $w\in S_{\pi}\left(  V\right)  $.

Let $w$ and $w^{\prime}$ be two vectors in $S_{\pi}\left(  V\right)  $ such
that $w\neq0$. We are going to prove that $w^{\prime}\in U\left(
\mathfrak{gl}_{\infty}\right)  w$. Once this is proven, it will be obvious
that $S_{\pi}\left(  V\right)  $ is irreducible, and we will be done.

There exists a finite subset $I$ of $\mathbb{Z}$ such that $w\in S_{\pi
}\left(  W_{I}\right)  $ and $w^{\prime}\in S_{\pi}\left(  W_{I}\right)
$.\ \ \ \ \footnote{\textit{Proof.} Let $I=I\left(  w\right)  \cup I\left(
w^{\prime}\right)  $. Then, $I$ is a finite subset of $\mathbb{Z}$ (since
$I\left(  w\right)  $ and $I\left(  w^{\prime}\right)  $ are finite subsets of
$\mathbb{Z}$), and $I\left(  w\right)  \subseteq I$ and $I\left(  w^{\prime
}\right)  \subseteq I$. We have $w\in S_{\pi}\left(  W_{I\left(  w\right)
}\right)  \subseteq S_{\pi}\left(  W_{I}\right)  $ (since $I\left(  w\right)
\subseteq I$ and thus $W_{I\left(  w\right)  }\subseteq W_{I}$) and similarly
$w^{\prime}\in S_{\pi}\left(  W_{I}\right)  $. Thus, there exists a finite
subset $I$ of $\mathbb{Z}$ such that $w\in S_{\pi}\left(  W_{I}\right)  $ and
$w^{\prime}\in S_{\pi}\left(  W_{I}\right)  $, qed.} Consider this $I$.

Since $I$ is finite, the vector space $W_{I}$ is finite-dimensional. Thus, by
the analogue of Proposition \ref{prop.glinf.schur.irred} for representations
of $\mathfrak{gl}_{m}$, the representation $S_{\pi}\left(  W_{I}\right)  $ of
the Lie algebra $\mathfrak{gl}\left(  W_{I}\right)  $ is irreducible. Hence,
$w^{\prime}\in U\left(  \mathfrak{gl}\left(  W_{I}\right)  \right)  w$.

Now, we have a canonical injective Lie algebra homomorphism $\mathfrak{gl}%
\left(  W_{I}\right)  \rightarrow\mathfrak{gl}_{\infty}$\ \ \ \ \footnote{Here
is how it is defined: For every linear map $A\in\mathfrak{gl}\left(
W_{I}\right)  $, we define a linear map $A^{\prime}\in\mathfrak{gl}\left(
V\right)  $ by setting%
\[
A^{\prime}v_{i}=\left\{
\begin{array}
[c]{c}%
Av_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\in I;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin I
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for all }i\in\mathbb{Z}.
\]
This linear map $A^{\prime}$ is represented (with respect to the basis
$\left(  v_{i}\right)  _{i\in\mathbb{Z}}$ of $V$) by an infinite matrix whose
rows and columns are labeled by integers. This matrix lies in $\mathfrak{gl}%
_{\infty}$.
\par
Thus, we have assigned to every $A\in\mathfrak{gl}\left(  W_{I}\right)  $ a
matrix in $\mathfrak{gl}_{\infty}$. This defines an injective Lie algebra
homomorphism $\mathfrak{gl}\left(  W_{I}\right)  \rightarrow\mathfrak{gl}%
_{\infty}$.}. Thus, we can view $\mathfrak{gl}\left(  W_{I}\right)  $ as a Lie
subalgebra of $\mathfrak{gl}_{\infty}$ in a canonical way. Moreover, the
classical action $\mathfrak{gl}\left(  W_{I}\right)  \times S_{\pi}\left(
W_{I}\right)  \rightarrow S_{\pi}\left(  W_{I}\right)  $ of the Lie algebra
$\mathfrak{gl}\left(  W_{I}\right)  $ on the Schur module $S_{\pi}\left(
W_{I}\right)  $ can be viewed as the restriction of the action $\mathfrak{gl}%
_{\infty}\times S_{\pi}\left(  V\right)  \rightarrow S_{\pi}\left(  V\right)
$ to $\mathfrak{gl}\left(  W_{I}\right)  \times S_{\pi}\left(  W_{I}\right)
$. Hence, $U\left(  \mathfrak{gl}\left(  W_{I}\right)  \right)  w\subseteq
U\left(  \mathfrak{gl}_{\infty}\right)  w$. Since we know that $w^{\prime}\in
U\left(  \mathfrak{gl}\left(  W_{I}\right)  \right)  w$, we thus conclude
$w^{\prime}\in U\left(  \mathfrak{gl}_{\infty}\right)  w$. This completes the
proof of Proposition \ref{prop.glinf.schur.irred}.

On the other hand, we can define so-called \textit{highest-weight
representations}. Before we do so, let us make $\mathfrak{gl}_{\infty}$ into a
graded Lie algebra:

\begin{definition}
\label{def.glinf.grade}For every $i\in\mathbb{Z}$, let $\mathfrak{gl}_{\infty
}^{i}$ be the subspace of $\mathfrak{gl}_{\infty}$ which consists of matrices
which have nonzero entries only on the $i$-th diagonal. (The $i$\textit{-th
diagonal} consists of the entries in the $\left(  \alpha,\beta\right)  $-th
places with $\beta-\alpha=i$.)

Then, $\mathfrak{gl}_{\infty}=\bigoplus\limits_{i\in\mathbb{Z}}\mathfrak{gl}%
_{\infty}^{i}$, and this makes $\mathfrak{gl}_{\infty}$ into a $\mathbb{Z}%
$-graded Lie algebra. Note that $\mathfrak{gl}_{\infty}^{0}$ is abelian. Let
$\mathfrak{gl}_{\infty}=\mathfrak{n}_{-}\oplus\mathfrak{h}\oplus
\mathfrak{n}_{+}$ be the triangular decomposition of $\mathfrak{gl}_{\infty}$,
so that the subspace $\mathfrak{n}_{-}=\bigoplus\limits_{i<0}\mathfrak{gl}%
_{\infty}^{i}$ is the space of all strictly lower-triangular matrices in
$\mathfrak{gl}_{\infty}$, the subspace $\mathfrak{h}=\mathfrak{gl}_{\infty
}^{0}$ is the space of all diagonal matrices in $\mathfrak{gl}_{\infty}$, and
the subspace $\mathfrak{n}_{+}=\bigoplus\limits_{i>0}\mathfrak{gl}_{\infty
}^{i}$ is the space of all strictly upper-triangular matrices in
$\mathfrak{gl}_{\infty}$.
\end{definition}

\begin{definition}
For every $i,j\in\mathbb{Z}$, let $E_{i,j}$ be the matrix (with rows and
columns labeled by integers) whose $\left(  i,j\right)  $-th entry is $1$ and
whose all other entries are $0$. Then, $\left(  E_{i,j}\right)  _{\left(
i,j\right)  \in\mathbb{Z}^{2}}$ is a basis of the vector space $\mathfrak{gl}%
_{\infty}$.
\end{definition}

\begin{definition}
For every $\lambda\in\mathfrak{h}^{\ast}$, let $M_{\lambda}$ be the
highest-weight Verma module $M_{\lambda}^{+}$ (as defined in Definition
\ref{def.verma}). Let $J_{\lambda}=\operatorname*{Ker}\left(  \cdot
,\cdot\right)  \subseteq M_{\lambda}$ be the maximal proper graded submodule.
Let $L_{\lambda}$ be the quotient module $M_{\lambda}\diagup J_{\lambda
}=M_{\lambda}^{+}\diagup J_{\lambda}^{+}=L_{\lambda}^{+}$; then, $L_{\lambda}$
is irreducible (as we know).
\end{definition}

\begin{definition}
We can define an antilinear $\mathbb{R}$-antiinvolution $\dag:\mathfrak{gl}%
_{\infty}\rightarrow\mathfrak{gl}_{\infty}$ on $\mathfrak{gl}_{\infty}$ by
setting%
\[
E_{i,j}^{\dag}=E_{j,i}\ \ \ \ \ \ \ \ \ \ \text{for all }\left(  i,j\right)
\in\mathbb{Z}^{2}.
\]
(Thus, $\dag:\mathfrak{gl}_{\infty}\rightarrow\mathfrak{gl}_{\infty}$ is the
operator which transposes a matrix and then applies complex conjugation to
each of its entries.) Thus we can speak of Hermitian and unitary
$\mathfrak{gl}_{\infty}$-modules.
\end{definition}

A very important remark:

For the Lie algebra $\mathfrak{gl}_{n}$, the highest-weight modules are the
Schur modules up to tensoring with a power of the determinant module. (More
precisely: For $\mathfrak{gl}_{n}$, every finite-dimensional irreducible
representation and any unitary irreducible representation is of the form
$S_{\pi}\left(  V_{n}\right)  \otimes\left(  \wedge^{n}\left(  V_{n}^{\ast
}\right)  \right)  ^{\otimes j}$ for some partition $\pi$ and some
$j\in\mathbb{N}$, where $V_{n}$ is the $\mathfrak{gl}_{n}$-module
$\mathbb{C}^{n}$.)

Nothing like this is true for $\mathfrak{gl}_{\infty}$. Instead, exterior
powers of $V$ and highest-weight representations live ``in different worlds''.
This is because $V$ is composed of infinite-dimensional vectors which have
``no top or bottom''; $V$ has no highest or lowest weight and does not lie in
category $\mathcal{O}^{+}$ or $\mathcal{O}^{-}$.

This is important, because many beautiful properties of representations of
$\mathfrak{gl}_{n}$ come from the equality of the highest-weight and Schur
module representations.

A way to marry these two worlds is by considering so-called
\textit{semiinfinite wedges}.

\subsubsection{Semiinfinite wedges}

Let us first give an informal definition of semiinfinite wedges and the
semiinfinite wedge space $\wedge^{\dfrac{\infty}{2}}V$ (we will later define
these things formally):

An \textit{elementary semiinfinite wedge} will mean a formal infinite ``wedge
product'' $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ with $\left(
i_{0},i_{1},i_{2},...\right)  $ being a sequence of integers satisfying
$i_{0}>i_{1}>i_{2}>...$ and $i_{k+1}=i_{k}-1$ for all sufficiently large $k$.
(At the moment, we consider this wedge product $v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...$ just as a fancy symbol for the sequence $\left(
i_{0},i_{1},i_{2},...\right)  $.)

The \textit{semiinfinite wedge space} $\wedge^{\dfrac{\infty}{2}}V$ is defined
as the free vector space with basis given by elementary semiinfinite wedges.

Note that, despite the notation $\wedge^{\dfrac{\infty}{2}}V$, the
semiinfinite wedge space is not a functor in the vector space $V$. We could
replace our definition of $\wedge^{\dfrac{\infty}{2}}V$ by a somewhat more
functorial one, which doesn't use the basis $\left(  v_{i}\right)
_{i\in\mathbb{Z}}$ of $V$ anymore. But it would still need a topology on $V$
(which makes $V$ locally linearly compact), and some working with formal
Laurent series. It proceeds through the semiinfinite Grassmannian, and will
not be done in these lectures.\footnote{Some pointers to the more functorial
definition:
\par
Consider the field $\mathbb{C}\left(  \left(  t\right)  \right)  $ of formal
Laurent series over $\mathbb{C}$ as a $\mathbb{C}$-vector space.
\par
Let $\operatorname*{Gr}=\left\{  U\text{ vector subspace of }\mathbb{C}\left(
\left(  t\right)  \right)  \ \mid\ \left(
\begin{array}
[c]{c}%
U\supseteq t^{n}\mathbb{C}\left[  \left[  t\right]  \right]  \text{ and}\\
\dim\left(  U\diagup\left(  t^{n}\mathbb{C}\left[  \left[  t\right]  \right]
\right)  \right)  <\infty
\end{array}
\right)  \text{ for some sufficiently high }n\right\}  $.
\par
For every $U\in\operatorname*{Gr}$, define an integer $\operatorname*{sdim}U$
by $\operatorname*{sdim}U=\dim\left(  U\diagup\left(  t^{n}\mathbb{C}\left[
\left[  t\right]  \right]  \right)  \right)  -n$ for any $n\in\mathbb{Z}$
satisfying $U\supseteq t^{n}\mathbb{C}\left[  \left[  t\right]  \right]  $.
Note that this integer does not depend on $n$ as long as $n$ is sufficiently
high to satisfy $U\supseteq t^{n}\mathbb{C}\left[  \left[  t\right]  \right]
$.
\par
This Grassmannian $\operatorname*{Gr}$ is the disjoint union $\coprod
\operatorname*{Gr}\nolimits_{n}$.
\par
There is something called a determinant line bundle on $\operatorname*{Gr}$.
The space of semiinfinite wedges is then defined as the space of regular
sections of this line bundle (in the sense of algebraic geometry).
\par
See the book by Pressley and Segal about loop groups for explanations of these
matters.} For us, the definition using the basis will be enough.

The space $\wedge^{\dfrac{\infty}{2}}V$ is countably dimensional. More
precisely, we can write $\wedge^{\dfrac{\infty}{2}}V$ as
\begin{align*}
\wedge^{\dfrac{\infty}{2}}V  &  =\bigoplus\limits_{m\in\mathbb{Z}}%
\wedge^{\dfrac{\infty}{2},m}V,\ \ \ \ \ \ \ \ \ \ \text{where}\\
\wedge^{\dfrac{\infty}{2},m}V  &  =\operatorname*{span}\left\{  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\ \mid\ i_{k}+k=m\text{ for
sufficiently large }k\right\}  .
\end{align*}
The space $\wedge^{\dfrac{\infty}{2},m}V$ has basis $\left\{  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\ \mid\ i_{k}+k=m\text{ for sufficiently
large }k\right\}  $, which is easily seen to be countable. We will see later
that this basis can be naturally labeled by partitions (of all integers, not
just of $m$).

\subsubsection{The action of \texorpdfstring{$\mathfrak{gl}_{\infty}$}
{gl-infinity} on \texorpdfstring{$\wedge^{\dfrac{\infty}{2}}V$}{the
semi-infinite wedge space}}

For every $m\in\mathbb{Z}$, we want to define an action of the Lie algebra
$\mathfrak{gl}_{\infty}$ on the space $\wedge^{\dfrac{\infty}{2},m}V$ which is
given ``by the usual Leibniz rule'', i. e., satisfies the equation%
\[
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...
\]
for all $a\in\mathfrak{gl}_{\infty}$ and all elementary semiinfinite wedges
$v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ (where, of course,
$a\rightharpoonup v_{i_{k}}$ is the same as $av_{i_{k}}$ due to our definition
of the action of $\mathfrak{gl}_{\infty}$ on $V$). Of course, it is not
immediately clear how to interpret the infinite wedge products $v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup
v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...$ on the right
hand side of this equation, since they are (in general) not elementary
semiinfinite wedges anymore. We must find a reasonable definition for such
wedge products. What properties should a wedge product (infinite as it is)
satisfy? It should be multilinear\footnote{i. e., it should satisfy
\begin{align*}
&  b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(  \lambda
b+\lambda^{\prime}b^{\prime}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge...\\
&  =\lambda b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge b\wedge
b_{k+1}\wedge b_{k+2}\wedge...+\lambda^{\prime}b_{0}\wedge b_{1}%
\wedge...\wedge b_{k-1}\wedge b^{\prime}\wedge b_{k+1}\wedge b_{k+2}\wedge...
\end{align*}
for all $k\in\mathbb{N}$, $b_{0},b_{1},b_{2},...\in V$, $b,b^{\prime}\in V$
and $\lambda,\lambda^{\prime}\in\mathbb{C}$ for which the right hand side is
well-defined} and antisymmetric\footnote{i. e., a well-defined wedge product
$b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ should be $0$ whenever two of the
$b_{k}$ are equal}. These properties make it possible to compute any wedge
product of the form $b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ with $b_{0}%
,b_{1},b_{2},...$ being vectors in $V$ which satisfy%
\[
b_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for sufficiently large }i.
\]
In fact, whenever we are given such vectors $b_{0},b_{1},b_{2},...$, we can
compute the wedge product $b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ by the
following procedure:

\begin{itemize}
\item Find an integer $M\in\mathbb{N}$ such that every $i\geq M$ satisfies
$b_{i}=v_{m-i}$. (This $M$ exists by the condition that $b_{i}=v_{m-i}$ for
sufficiently large $i$.)

\item Expand each of the vectors $b_{0},b_{1},...,b_{M-1}$ as a $\mathbb{C}%
$-linear combination of the basis vectors $v_{\ell}$.

\item Using these expansions and the multilinearity of the wedge product,
reduce the computation of $b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ to the
computation of finitely many wedge products of basis vectors.

\item Each wedge product of basis vectors can now be computed as follows: If
two of the basis vectors are equal, then it must be $0$ (by antisymmetry of
the wedge product). If not, reorder the basis vectors in such a way that their
indices decrease (this is possible, because ``most'' of these basis vectors
are already in order, and only the first few must be reordered). Due to the
antisymmetry of the wedge product, the wedge product of the basis vectors
before reordering must be $\left(  -1\right)  ^{\pi}$ times the wedge product
of the basis vectors after reordering, where $\pi$ is the permutation which
corresponds to our reordering. But the wedge product of the basis vectors
after reordering is an elementary semiinfinite wedge, and thus we know how to
compute it.
\end{itemize}

This procedure is not exactly a formal definition, and it is not immediately
clear that the value of $b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ that it
computes is independent of, e. g., the choice of $M$. In the following
subsection (Subsection \ref{subsect.degress}), we will give a formal version
of this definition.

\subsubsection{\label{subsect.degress}The
\texorpdfstring{$\mathfrak{gl}_{\infty}$}{gl-infinity}-module
\texorpdfstring{$\wedge^{\dfrac{\infty}{2}}V$}{structure on the semi-infinite
wedge space}: a formal definition}

Before we formally define the value of $b_{0}\wedge b_{1}\wedge b_{2}%
\wedge...$, let us start from scratch and repeat the definitions of
$\wedge^{\dfrac{\infty}{2}}V$ and $\wedge^{\dfrac{\infty}{2},m}V$ in a cleaner
fashion than how we defined them above.

\begin{Warning}
Some of the nomenclature defined in the following (particularly, the notions
of ``$m$-degression'' and ``straying $m$-degression'') is mine (=Darij's). I
don't know whether there are established names for these things.
\end{Warning}

First, we introduce the notion of $m$\textit{-degressions} and formalize the
definitions of $\wedge^{\dfrac{\infty}{2}}V$ and $\wedge^{\dfrac{\infty}{2}%
,m}V$.

\begin{definition}
\label{def.glinf.m-deg}Let $m\in\mathbb{Z}$. An $m$\textit{-degression} will
mean a strictly decreasing sequence $\left(  i_{0},i_{1},i_{2},...\right)  $
of integers such that every sufficiently high $k\in\mathbb{N}$ satisfies
$i_{k}+k=m$. It is clear that any $m$-degression $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ automatically satisfies $i_{k}-i_{k+1}=1$ for all
sufficiently high $k$.

For any $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $, we introduce
a new symbol $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$. This symbol
is, for the time being, devoid of any meaning. The symbol $v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...$ will be called an \textit{elementary
semiinfinite wedge}.
\end{definition}

\begin{definition}
\textbf{(a)} Let $\wedge^{\dfrac{\infty}{2}}V$ denote the free $\mathbb{C}%
$-vector space with basis $\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)  _{m\in\mathbb{Z};\ \left(  i_{0},i_{1},i_{2},...\right)
\text{ is an }m\text{-degression}}$. We will refer to $\wedge^{\dfrac{\infty
}{2}}V$ as the \textit{semiinfinite wedge space}.

\textbf{(b)} For every $m\in\mathbb{Z}$, define a $\mathbb{C}$-vector subspace
$\wedge^{\dfrac{\infty}{2},m}V$ of $\wedge^{\dfrac{\infty}{2}}V$ by%
\[
\wedge^{\dfrac{\infty}{2},m}V=\operatorname*{span}\left\{  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\ \mid\ \left(  i_{0},i_{1},i_{2}%
,...\right)  \text{ is an }m\text{-degression}\right\}  .
\]
Clearly, $\wedge^{\dfrac{\infty}{2},m}V$ has basis $\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  _{\left(  i_{0},i_{1}%
,i_{2},...\right)  \text{ is an }m\text{-degression}}$.
\end{definition}

Obviously, $\wedge^{\dfrac{\infty}{2}}V=\bigoplus\limits_{m\in\mathbb{Z}%
}\wedge^{\dfrac{\infty}{2},m}V$.

Now, let us introduce the (more flexible) notion of \textit{straying }%
$m$\textit{-degressions}. This notion is obtained from the notion of
$m$-degressions by dropping the ``strictly decreasing'' condition:

\begin{definition}
\label{def.glinf.straym-deg}Let $m\in\mathbb{Z}$. A \textit{straying }%
$m$\textit{-degression} will mean a sequence $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ of integers such that every sufficiently high
$k\in\mathbb{N}$ satisfies $i_{k}+k=m$.
\end{definition}

As a consequence, a straying $m$-degression is strictly decreasing from some
point onwards, but needs not be strictly decreasing from the beginning (it can
``stray'', whence the name). A strictly decreasing straying $m$-degression is
exactly the same as an $m$-degression. Thus, every $m$-degression is a
straying $m$-degression.

\begin{definition}
Let $S$ be a (possibly infinite) set. Recall that a \textit{permutation} of
$S$ means a bijection from $S$ to $S$.

A \textit{finitary permutation} of $S$ means a bijection from $S$ to $S$ which
fixes all but finitely many elements of $S$. (Thus, all permutations of $S$
are finitary permutations if $S$ is finite.)
\end{definition}

Notice that the finitary permutations of a given set $S$ form a group (under composition).

\begin{definition}
Let $m\in\mathbb{Z}$. Let $\left(  i_{0},i_{1},i_{2},...\right)  $ be a
straying $m$-degression. If no two elements of this sequence $\left(
i_{0},i_{1},i_{2},...\right)  $ are equal, then there exists a unique finitary
permutation $\pi$ of $\mathbb{N}$ such that $\left(  i_{\pi^{-1}\left(
0\right)  },i_{\pi^{-1}\left(  1\right)  },i_{\pi^{-1}\left(  2\right)
},...\right)  $ is an $m$-degression. This finitary permutation $\pi$ is
called the \textit{straightening permutation} of $\left(  i_{0},i_{1}%
,i_{2},...\right)  $.
\end{definition}

\begin{definition}
\label{def.semiinfwedge.stray}Let $m\in\mathbb{Z}$. Let $\left(  i_{0}%
,i_{1},i_{2},...\right)  $ be a straying $m$-degression. We define the meaning
of the term $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ as follows:

- If some two elements of the sequence $\left(  i_{0},i_{1},i_{2},...\right)
$ are equal, then $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ is
defined to mean the element $0$ of $\wedge^{\dfrac{\infty}{2},m}V$.

- If no two elements of the sequence $\left(  i_{0},i_{1},i_{2},...\right)  $
are equal, then $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ is
defined to mean the element $\left(  -1\right)  ^{\pi}v_{i_{\pi^{-1}\left(
0\right)  }}\wedge v_{i_{\pi^{-1}\left(  1\right)  }}\wedge v_{i_{\pi
^{-1}\left(  2\right)  }}\wedge...$ of $\wedge^{\dfrac{\infty}{2},m}V$, where
$\pi$ is the straightening permutation of $\left(  i_{0},i_{1},i_{2}%
,...\right)  $.
\end{definition}

Note that whenever $\left(  i_{0},i_{1},i_{2},...\right)  $ is an
$m$-degression (not just a straying one), then the value of $v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...$ defined according to Definition
\ref{def.semiinfwedge.stray} is exactly the symbol $v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...$ of Definition \ref{def.glinf.m-deg} (because no
two elements of the sequence $\left(  i_{0},i_{1},i_{2},...\right)  $ are
equal, and the straightening permutation of $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ is $\operatorname*{id}$). Hence, Definition
\ref{def.semiinfwedge.stray} does not conflict with Definition
\ref{def.glinf.m-deg}.

\begin{definition}
\label{def.semiinfwedge}Let $m\in\mathbb{Z}$. Let $b_{0},b_{1},b_{2},...$ be
vectors in $V$ which satisfy%
\[
b_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for sufficiently large }i.
\]
Then, let us define the wedge product $b_{0}\wedge b_{1}\wedge b_{2}%
\wedge...\in\wedge^{\dfrac{\infty}{2},m}V$ as follows:

Find an integer $M\in\mathbb{N}$ such that every $i\geq M$ satisfies
$b_{i}=v_{m-i}$. (This $M$ exists by the condition that $b_{i}=v_{m-i}$ for
sufficiently large $i$.)

For every $i\in\left\{  0,1,...,M-1\right\}  $, write the vector $b_{i}$ as a
$\mathbb{C}$-linear combination $\sum\limits_{j\in\mathbb{Z}}\lambda
_{i,j}v_{j}$ (with $\lambda_{i,j}\in\mathbb{C}$ for all $j$).

Now, define $b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ to be the element%
\[
\sum\limits_{\left(  j_{0},j_{1},...,j_{M-1}\right)  \in\mathbb{Z}^{M}}%
\lambda_{0,j_{0}}\lambda_{1,j_{1}}...\lambda_{M-1,j_{M-1}}v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{M-1}}\wedge v_{m-M}\wedge v_{m-M-1}\wedge
v_{m-M-2}\wedge...
\]
of $\wedge^{\dfrac{\infty}{2},m}V$. Here, $v_{j_{0}}\wedge v_{j_{1}}%
\wedge...\wedge v_{j_{M-1}}\wedge v_{m-M}\wedge v_{m-M-1}\wedge v_{m-M-2}%
\wedge...$ is well-defined, since $\left(  j_{0},j_{1},...,j_{M-1}%
,m-M,m-M-1,m-M-2,...\right)  $ is a straying $m$-degression.

Note that this element $b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ is
well-defined (according to Proposition \ref{prop.semiinfwedge.welldef}
\textbf{(a)} below).

We refer to $b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ as the \textit{(infinite)
wedge product} of the vectors $b_{0}$, $b_{1}$, $b_{2}$, $...$.
\end{definition}

Note that, for any straying $m$-degression $\left(  i_{0},i_{1},i_{2}%
,...\right)  $, the value of $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...$ defined according to Definition \ref{def.semiinfwedge} equals the
value of $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ defined
according to Definition \ref{def.semiinfwedge.stray}. Hence, Definition
\ref{def.semiinfwedge} does not conflict with Definition
\ref{def.semiinfwedge.stray}.

We have the following easily verified properties of the infinite wedge product:

\begin{proposition}
\label{prop.semiinfwedge.welldef}Let $m\in\mathbb{Z}$. Let $b_{0},b_{1}%
,b_{2},...$ be vectors in $V$ which satisfy%
\[
b_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for sufficiently large }i.
\]


\textbf{(a)} The wedge product $b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ as
defined in Definition \ref{def.semiinfwedge} is well-defined (i. e., does not
depend on the choice of $M$).

\textbf{(b)} For any straying $m$-degression $\left(  i_{0},i_{1}%
,i_{2},...\right)  $, the value of $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...$ defined according to Definition \ref{def.semiinfwedge} equals the
value of $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ defined
according to Definition \ref{def.semiinfwedge.stray}.

\textbf{(c)} The infinite wedge product is multilinear. That is, we have%
\begin{align}
&  b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(  \lambda
b+\lambda^{\prime}b^{\prime}\right)  \wedge b_{k+1}\wedge b_{k+2}%
\wedge...\nonumber\\
&  =\lambda b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge b\wedge
b_{k+1}\wedge b_{k+2}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\lambda^{\prime}b_{0}\wedge b_{1}\wedge...\wedge
b_{k-1}\wedge b^{\prime}\wedge b_{k+1}\wedge b_{k+2}\wedge...
\label{glinf.semiinfwedge.multilinear}%
\end{align}
for all $k\in\mathbb{N}$, $b_{0},b_{1},b_{2},...\in V$, $b,b^{\prime}\in V$
and $\lambda,\lambda^{\prime}\in\mathbb{C}$ which satisfy $\left(
b_{i}=v_{m-i}\text{ for sufficiently large }i\right)  $.

\textbf{(d)} The infinite wedge product is antisymmetric. This means that if
$b_{0},b_{1},b_{2},...\in V$ are such that $\left(  b_{i}=v_{m-i}\text{ for
sufficiently large }i\right)  $ and $\left(  \text{two of the vectors }%
b_{0},b_{1},b_{2},...\text{ are equal}\right)  $, then%
\begin{equation}
b_{0}\wedge b_{1}\wedge b_{2}\wedge...=0. \label{glinf.semiinfwedge.antisym}%
\end{equation}
In other words, when (at least) two of the vectors forming a well-defined
infinite wedge product are equal, then this wedge product is $0$.

\textbf{(e)} As a consequence, the wedge product $b_{0}\wedge b_{1}\wedge
b_{2}\wedge...$ gets multiplied by $-1$ when we switch $b_{i}$ with $b_{j}$
for any two distinct $i\in\mathbb{N}$ and $j\in\mathbb{N}$.

\textbf{(f)} If $\pi$ is a finitary permutation of $\mathbb{N}$ and
$b_{0},b_{1},b_{2},...\in V$ are vectors such that $\left(  b_{i}%
=v_{m-i}\text{ for sufficiently large }i\right)  $, then the infinite wedge
product $b_{\pi\left(  0\right)  }\wedge b_{\pi\left(  1\right)  }\wedge
b_{\pi\left(  2\right)  }\wedge...$ is well-defined and satisfies
\begin{equation}
b_{\pi\left(  0\right)  }\wedge b_{\pi\left(  1\right)  }\wedge b_{\pi\left(
2\right)  }\wedge...=\left(  -1\right)  ^{\pi}\cdot b_{0}\wedge b_{1}\wedge
b_{2}\wedge.... \label{glinf.semiinfwedge.permut}%
\end{equation}

\end{proposition}

Now, we can define the action of $\mathfrak{gl}_{\infty}$ on $\wedge
^{\dfrac{\infty}{2},m}V$ just as we wanted to:

\begin{definition}
\label{def.glinf.semiinfwedge}Let $m\in\mathbb{Z}$. Define an action of the
Lie algebra $\mathfrak{gl}_{\infty}$ on the vector space $\wedge
^{\dfrac{\infty}{2},m}V$ by the equation%
\[
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...
\]
for all $a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions $\left(
i_{0},i_{1},i_{2},...\right)  $ (and by linear extension). (Recall that
$a\rightharpoonup v=av$ for every $a\in\mathfrak{gl}_{\infty}$ and $v\in V$,
due to how we defined the $\mathfrak{gl}_{\infty}$-module $V$.)
\end{definition}

Of course, this definition is only justified after showing that this indeed is
an action. But this is rather easy. Let us state this as a proposition:

\begin{proposition}
\label{prop.glinf.glinfact.welldef}Let $m\in\mathbb{Z}$. Then, Definition
\ref{def.glinf.semiinfwedge} really defines a representation of the Lie
algebra $\mathfrak{gl}_{\infty}$ on the vector space $\wedge^{\dfrac{\infty
}{2},m}V$. In other words, there exists one and only one action of the Lie
algebra $\mathfrak{gl}_{\infty}$ on the vector space $\wedge^{\dfrac{\infty
}{2},m}V$ such that all $a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions
$\left(  i_{0},i_{1},i_{2},...\right)  $ satisfy%
\[
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge....
\]

\end{proposition}

The proof of this proposition (using the multilinearity and the antisymmetry
of our wedge product) is rather straightforward and devoid of surprises. I
will show it nevertheless, if only because I assume every other text leaves it
to the reader. Due to its length, it is postponed until Subsection
\ref{subsect.degress.proofs}.

Proposition \ref{prop.glinf.glinfact.welldef} shows that the action of the Lie
algebra $\mathfrak{gl}_{\infty}$ on the vector space $\wedge^{\dfrac{\infty
}{2},m}V$ in Definition \ref{def.glinf.semiinfwedge} is well-defined. This
makes $\wedge^{\dfrac{\infty}{2},m}V$ into a $\mathfrak{gl}_{\infty}$-module.
Computations in this module can be somewhat simplified by the following
``comparably basis-free'' formula\footnote{I'm saying ``comparably'' because
the condition that $b_{i}=v_{m-i}$ for all sufficiently large $i$ is not
basis-free. But this should not come as a surprise, as the definition of
$\wedge^{\dfrac{\infty}{2},m}V$ itself is not basis-free to begin with.}:

\begin{proposition}
\label{prop.glinf.glinfact}Let $m\in\mathbb{Z}$. Let $b_{0},b_{1},b_{2},...$
be vectors in $V$ which satisfy%
\[
b_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all sufficiently large }i.
\]
Then, every $a\in\mathfrak{gl}_{\infty}$ satisfies%
\[
a\rightharpoonup\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right)
=\sum\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge....
\]

\end{proposition}

We can also explicitly describe this action on elementary matrices and
semiinfinite wedges:

\begin{proposition}
\label{prop.glinf.explicit}Let $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$. Let
$m\in\mathbb{Z}$. Let $\left(  i_{0},i_{1},i_{2},...\right)  $ be a straying
$m$-degression (so that $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge
...\in\wedge^{\dfrac{\infty}{2},m}V$).

\textbf{(a)} If $j\notin\left\{  i_{0},i_{1},i_{2},...\right\}  $, then
$E_{i,j}\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =0$.

\textbf{(b)} If there exists a \textbf{unique} $\ell\in\mathbb{N}$ such that
$j=i_{\ell}$, then for this $\ell$ we have%
\[
E_{i,j}\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}%
}\wedge v_{i}\wedge v_{i_{\ell+1}}\wedge v_{i_{\ell+2}}\wedge...
\]
(In words: If $v_{j}$ appears exactly once as a factor in the wedge product
$v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$, then $E_{i,j}%
\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  $ is the wedge product which is obtained from $v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ by replacing this factor by
$v_{i}$.)
\end{proposition}

Since we have given $\wedge^{\dfrac{\infty}{2},m}V$ a $\mathfrak{gl}_{\infty}%
$-module structure for every $m\in\mathbb{Z}$, it is clear that $\wedge
^{\dfrac{\infty}{2}}V=\bigoplus\limits_{m\in\mathbb{Z}}\wedge^{\dfrac{\infty
}{2},m}V$ also becomes a $\mathfrak{gl}_{\infty}$-module.

\subsubsection{\label{subsect.degress.proofs}Proofs}

Here are proofs of some of the unproven statements made in Subsection
\ref{subsect.degress}:

\textit{Proof of Proposition \ref{prop.glinf.glinfact.welldef}.} The first
thing we need to check is the following:

\begin{quote}
\textit{Assertion \ref{prop.glinf.glinfact.welldef}.0:} Let $a\in
\mathfrak{gl}_{\infty}$. Let $b_{0},b_{1},b_{2},...$ be vectors in $V$ which
satisfy%
\[
b_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all sufficiently large }i.
\]


\textbf{(a)} For every $k\in\mathbb{N}$, the infinite wedge product
$b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(  a\rightharpoonup
b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge...$ is well-defined.

\textbf{(b)} All but finitely many $k\in\mathbb{N}$ satisfy $b_{0}\wedge
b_{1}\wedge...\wedge b_{k-1}\wedge\left(  a\rightharpoonup b_{k}\right)
\wedge b_{k+1}\wedge b_{k+2}\wedge...=0$. (In other words, the sum%
\[
\sum\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge...
\]
converges in the discrete topology.)
\end{quote}

\begin{vershort}
The proof of Assertion \ref{prop.glinf.glinfact.welldef}.0 can easily be
supplied by the reader. (Part \textbf{(a)} is clear, since the property of the
sequence $\left(  b_{0},b_{1},b_{2},...\right)  $ to satisfy $\left(
b_{i}=v_{m-i}\text{ for all sufficiently large }i\right)  $ does not change if
we modify one entry of the sequence. Part \textbf{(b)} requires showing that
$b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(  a\rightharpoonup
b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge...=0$ for all sufficiently
large $k$; but this follows from $a\in\mathfrak{gl}_{\infty}$ being a matrix
with only finitely many nonzero entries, and from the condition that
$b_{i}=v_{m-i}$ for all sufficiently large $i$.)
\end{vershort}

\begin{verlong}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.0:} We know that
$b_{i}=v_{m-i}$ for all sufficiently large $i$. In other words, there exists
an $I\in\mathbb{N}$ such that%
\begin{equation}
\text{every integer }i\geq I\text{ satisfies }b_{i}=v_{m-i}.
\label{pf.glinf.glinfact.welldef.ass0.pf.2}%
\end{equation}
Fix such an $I$.

\textbf{(a)} Let $k\in\mathbb{N}$. Define a sequence $\left(  c_{0}%
,c_{1},c_{2},...\right)  $ of elements of $V$ by
\[
\left(  c_{i}=\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq k;\\
a\rightharpoonup b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i=k
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\mathbb{N}\right)  .
\]
Then, $\left(  c_{0},c_{1},c_{2},...\right)  =\left(  b_{0},b_{1}%
,...,b_{k-1},a\rightharpoonup b_{k},b_{k+1},b_{k+2},...\right)  $. Now, we
have%
\begin{equation}
c_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all sufficiently large }i.
\label{pf.glinf.glinfact.welldef.ass0.pf.0}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.glinf.glinfact.welldef.ass0.pf.0}):} For
every $i\in\mathbb{N}$ satisfying $i\geq\max\left\{  I,k+1\right\}  $, we have%
\begin{align*}
c_{i}  &  =\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq k;\\
a\rightharpoonup b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i=k
\end{array}
\right.  =b_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\neq k\text{ (because
}i\geq\max\left\{  I,k+1\right\}  \geq k+1>k\text{)}\right) \\
&  =v_{m-i}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.glinf.glinfact.welldef.ass0.pf.2}) (since }i\geq\max\left\{
I,k+1\right\}  \geq I\text{)}\right)  .
\end{align*}
Thus, for every sufficiently large $i$, we have $c_{i}=v_{m-i}$. This proves
(\ref{pf.glinf.glinfact.welldef.ass0.pf.0}).}

\textbf{(b)} We know that $\mathfrak{gl}_{\infty}$ is the vector space of all
infinite matrices whose rows and columns are labeled by integers such that
only finitely many entries of the matrix are nonzero. Since $a\in
\mathfrak{gl}_{\infty}$, this shows that only finitely many entries of the
matrix $a$ are nonzero. Hence, only finitely many columns of $a$ are nonzero.
Thus, for every sufficiently low integer $\ell$, the $\ell$-th column of $a$
is $0$. In other words, there exists an $L\in\mathbb{Z}$ such that%
\begin{equation}
\text{every integer }\ell\text{ such that }\ell\leq L\text{ satisfies }\left(
\text{the }\ell\text{-th column of }a\right)  =0.
\label{pf.glinf.glinfact.welldef.ass0.pf.1}%
\end{equation}
Consider such an $L$.

Now, let $k$ be any element of $\mathbb{N}$ satisfying $k\geq\max\left\{
I,m-L\right\}  $. Then, $k\geq\max\left\{  I,m-L\right\}  \geq I$, so that
$b_{k}=v_{m-k}$ (by (\ref{pf.glinf.glinfact.welldef.ass0.pf.2}), applied to
$i=k$). Moreover, $k\geq\max\left\{  I,m-L\right\}  \geq m-L$, so that
$m-k\leq L$. Hence, (\ref{pf.glinf.glinfact.welldef.ass0.pf.1}) (applied to
$\ell=m-k$) yields $\left(  \text{the }\left(  m-k\right)  \text{-th column of
}a\right)  =0$. Now,%
\[
a\rightharpoonup\underbrace{b_{k}}_{=v_{m-k}}=a\rightharpoonup v_{m-k}%
=av_{m-k}=\left(  \text{the }\left(  m-k\right)  \text{-th column of
}a\right)  =0,
\]
and thus%
\[
b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\underbrace{\left(
a\rightharpoonup b_{k}\right)  }_{=0}\wedge b_{k+1}\wedge b_{k+2}%
\wedge...=b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge0\wedge b_{k+1}\wedge
b_{k+2}\wedge...=0
\]
(due to the multilinearity of the infinite wedge product).

Now, forget that we fixed $k$. We thus have shown that every element $k$ of
$\mathbb{N}$ satisfying $k\geq\max\left\{  I,m-L\right\}  $ satisfies
$b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(  a\rightharpoonup
b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge...=0$. Since all but finitely
many $k\in\mathbb{N}$ satisfy $k\geq\max\left\{  I,m-L\right\}  $, this shows
that all but finitely many $k\in\mathbb{N}$ satisfy $b_{0}\wedge b_{1}%
\wedge...\wedge b_{k-1}\wedge\left(  a\rightharpoonup b_{k}\right)  \wedge
b_{k+1}\wedge b_{k+2}\wedge...=0$. In other words, the sum%
\[
\sum\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge...
\]
converges in the discrete topology. This proves Assertion
\ref{prop.glinf.glinfact.welldef}.0.
\end{verlong}

\begin{vershort}
Now that Assertion \ref{prop.glinf.glinfact.welldef}.0 is proven, we can make
the following definition:

For every $a\in\mathfrak{gl}_{\infty}$, let us define a $\mathbb{C}$-linear
map $F_{a}:\wedge^{\dfrac{\infty}{2},m}V\rightarrow\wedge^{\dfrac{\infty}%
{2},m}V$ as follows: For every $m$-degression $\left(  i_{0},i_{1}%
,i_{2},...\right)  $, set%
\begin{equation}
F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge... \label{pf.glinf.glinfact.welldef.pfshort.defFa}%
\end{equation}
\footnote{The right hand side of
(\ref{pf.glinf.glinfact.welldef.pfshort.defFa}) is indeed well-defined. This
follows from applying Assertion \ref{prop.glinf.glinfact.welldef}.0
\textbf{(b)} to $v_{i_{i}}$ instead of $b_{i}$.}. Thus, we have specified the
values of the map $F_{a}$ on the basis \newline$\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  _{\left(  i_{0},i_{1}%
,i_{2},...\right)  \text{ is an }m\text{-degression}}$ of $\wedge
^{\dfrac{\infty}{2},m}V$. Therefore, the map $F_{a}$ is uniquely determined
(and exists) by linearity.

We are going to prove various properties of this map now. First, we will prove
that the formula (\ref{pf.glinf.glinfact.welldef.pfshort.defFa}) which we used
to define $F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  $ for $m$-degressions $\left(  i_{0},i_{1},i_{2},...\right)
$ can also be applied when $\left(  i_{0},i_{1},i_{2},...\right)  $ is just a
straying $m$-degression:
\end{vershort}

\begin{verlong}
Now that Assertion \ref{prop.glinf.glinfact.welldef}.0 is proven, we can make
the following definition:

For every $a\in\mathfrak{gl}_{\infty}$, let us define a $\mathbb{C}$-linear
map $F_{a}:\wedge^{\dfrac{\infty}{2},m}V\rightarrow\wedge^{\dfrac{\infty}%
{2},m}V$ as follows: For every $m$-degression $\left(  i_{0},i_{1}%
,i_{2},...\right)  $, set%
\begin{equation}
F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge... \label{pf.glinf.glinfact.welldef.defFa}%
\end{equation}
\footnote{The right hand side of (\ref{pf.glinf.glinfact.welldef.defFa}) is
indeed well-defined. Here is why:
\par
Let $\left(  i_{0},i_{1},i_{2},...\right)  $ be an $m$-degression. Due to the
definition of an $m$-degression, every sufficiently high $k\in\mathbb{N}$
satisfies $i_{k}+k=m$. In other words, every sufficiently high $k\in
\mathbb{N}$ satisfies $i_{k}=m-k$. Hence, every sufficiently high
$k\in\mathbb{N}$ satisfies $v_{i_{k}}=v_{m-k}$. If we rename $k$ as $i$ in
this result, we obtain the following: Every sufficiently high $i\in\mathbb{N}$
satisfies $v_{i_{i}}=v_{m-i}$. In other words, $v_{i_{i}}=v_{m-i}$ for all
sufficiently large $i$. Hence, we can apply Assertion
\ref{prop.glinf.glinfact.welldef}.0 \textbf{(b)} to $v_{i_{i}}$ instead of
$b_{i}$. As a result, we conclude that all but finitely many $k\in\mathbb{N}$
satisfy $v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(
a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}%
\wedge...=0$. In other words, the sum $\sum\limits_{k\geq0}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}%
}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...$ converges in the
discrete topology. In other words, the right hand side of
(\ref{pf.glinf.glinfact.welldef.defFa}) is indeed well-defined, qed.}. Thus,
we have specified the values of the map $F_{a}$ on the basis \newline$\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  _{\left(
i_{0},i_{1},i_{2},...\right)  \text{ is an }m\text{-degression}}$ of
$\wedge^{\dfrac{\infty}{2},m}V$. Therefore, of course, the map $F_{a}$ is
uniquely determined (and exists) by linearity.

We will now explore, step by step, the properties of this map. First, we will
prove an assertion which extends the formula
(\ref{pf.glinf.glinfact.welldef.defFa}) to straying $m$-degressions:
\end{verlong}

\begin{quote}
\textit{Assertion \ref{prop.glinf.glinfact.welldef}.1:} Let $a\in
\mathfrak{gl}_{\infty}$. Then, every \textbf{straying} $m$-degression $\left(
j_{0},j_{1},j_{2},...\right)  $ satisfies%
\begin{equation}
F_{a}\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right)
=\sum\limits_{k\geq0}v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{k-1}%
}\wedge\left(  a\rightharpoonup v_{j_{k}}\right)  \wedge v_{j_{k+1}}\wedge
v_{j_{k+2}}\wedge.... \label{pf.glinf.glinfact.welldef.ass1}%
\end{equation}



\end{quote}

\begin{vershort}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.1 (sketched):}
Let $\left(  j_{0},j_{1},j_{2},...\right)  $ be a straying $m$-degression.
Thus, every sufficiently large $i\in\mathbb{N}$ satisfies $j_{i}+i=m$. We must
prove that (\ref{pf.glinf.glinfact.welldef.ass1}) holds.
\end{vershort}

\begin{verlong}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.1:} Let $\left(
j_{0},j_{1},j_{2},...\right)  $ be a straying $m$-degression. Thus, every
sufficiently large $i\in\mathbb{N}$ satisfies $j_{i}+i=m$. We must prove that
(\ref{pf.glinf.glinfact.welldef.ass1}) holds.
\end{verlong}

Now, we distinguish between two cases:

\textit{Case 1:} Some two elements of the sequence $\left(  j_{0},j_{1}%
,j_{2},...\right)  $ are equal.

\textit{Case 2:} No two elements of the sequence $\left(  j_{0},j_{1}%
,j_{2},...\right)  $ are equal.

\begin{vershort}
Let us first consider Case 1. In this case, some two elements of the sequence
$\left(  j_{0},j_{1},j_{2},...\right)  $ are equal. Hence, $v_{j_{0}}\wedge
v_{j_{1}}\wedge v_{j_{2}}\wedge...=0$ (by the definition of $v_{j_{0}}\wedge
v_{j_{1}}\wedge v_{j_{2}}\wedge...$), and thus the left hand side of
(\ref{pf.glinf.glinfact.welldef.ass1}) vanishes. We now need to show that so
does the right hand side.

We know that some two elements of the sequence $\left(  j_{0},j_{1}%
,j_{2},...\right)  $ are equal. Let $j_{p}$ and $j_{q}$ be two such elements,
with $p\neq q$. So we have $p\neq q$ and $j_{p}=j_{q}$.

The right hand side of (\ref{pf.glinf.glinfact.welldef.ass1}) is a sum over
all $k\geq0$. Each of its addends with $k\notin\left\{  p,q\right\}  $ is $0$
(because it is an infinite wedge product with two equal factors $v_{j_{p}}$
and $v_{j_{q}}$). So we need to check that the addend with $k=p$ and the
addend with $k=q$ cancel each other. In other words, we need to prove that%
\begin{align}
&  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{p+1}}\wedge v_{j_{p+2}}%
\wedge...\nonumber\\
&  =-v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{q-1}}\wedge\left(
a\rightharpoonup v_{j_{q}}\right)  \wedge v_{j_{q+1}}\wedge v_{j_{q+1}}%
\wedge.... \label{pf.glinf.glinfact.welldef.ass1.pfshort.0}%
\end{align}
We recall that an infinite wedge product of the form $b_{0}\wedge b_{1}\wedge
b_{2}\wedge...$ (where $b_{0},b_{1},b_{2},...$ are vectors in $V$ such that
$\left(  b_{i}=v_{m-i}\text{ for all sufficiently large }i\right)  $) gets
multiplied by $-1$ when we switch $b_{i}$ with $b_{j}$ for any two distinct
$i\in\mathbb{N}$ and $j\in\mathbb{N}$\ \ \ \ \footnote{This is a particular
case of Proposition \ref{prop.semiinfwedge.welldef} \textbf{(f)} (namely, the
case when $\pi$ is the transposition $\left(  i,j\right)  $).}. Thus, the
infinite wedge product
\[
v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{p+1}}\wedge v_{j_{p+2}}%
\wedge...\wedge v_{j_{q-1}}\wedge v_{j_{q}}\wedge v_{j_{q+1}}\wedge
v_{j_{q+2}}\wedge...
\]
gets multiplied by $-1$ when we switch $a\rightharpoonup v_{j_{p}}$ with
$v_{j_{q}}$ (since $p\in\mathbb{N}$ and $q\in\mathbb{N}$ are distinct). In
other words,%
\begin{align*}
&  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge v_{j_{q}}\wedge
v_{j_{p+1}}\wedge v_{j_{p+2}}\wedge...\wedge v_{j_{q-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{q+1}}\wedge v_{j_{q+1}}%
\wedge...\\
&  =-v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{p+1}}\wedge v_{j_{p+2}}%
\wedge...\wedge v_{j_{q-1}}\wedge v_{j_{q}}\wedge v_{j_{q+1}}\wedge
v_{j_{q+2}}\wedge....
\end{align*}
Thus,%
\begin{align}
&  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{p+1}}\wedge v_{j_{p+2}}%
\wedge...\wedge v_{j_{q-1}}\wedge v_{j_{q}}\wedge v_{j_{q+1}}\wedge
v_{j_{q+2}}\wedge...\nonumber\\
&  =-v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge v_{j_{q}%
}\wedge v_{j_{p+1}}\wedge v_{j_{p+2}}\wedge...\wedge v_{j_{q-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{q+1}}\wedge v_{j_{q+2}}%
\wedge.... \label{pf.glinf.glinfact.welldef.ass1.pfshort.1}%
\end{align}
Now,%
\begin{align*}
&  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{p+1}}\wedge v_{j_{p+2}}%
\wedge...\\
&  =v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{p+1}}\wedge v_{j_{p+2}}%
\wedge...\wedge v_{j_{q-1}}\wedge v_{j_{q}}\wedge v_{j_{q+1}}\wedge
v_{j_{q+2}}\wedge...\\
&  =-v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge v_{j_{q}%
}\wedge v_{j_{p+1}}\wedge v_{j_{p+2}}\wedge...\wedge v_{j_{q-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{q+1}}\wedge v_{j_{q+2}}%
\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.glinf.glinfact.welldef.ass1.pfshort.1})}\right) \\
&  =-v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge v_{j_{p}%
}\wedge v_{j_{p+1}}\wedge v_{j_{p+2}}\wedge...\wedge v_{j_{q-1}}\wedge\left(
a\rightharpoonup v_{j_{q}}\right)  \wedge v_{j_{q+1}}\wedge v_{j_{q+2}}%
\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }j_{q}=j_{p}\text{ and }j_{p}%
=j_{q}\right) \\
&  =-v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{q-1}}\wedge\left(
a\rightharpoonup v_{j_{q}}\right)  \wedge v_{j_{q+1}}\wedge v_{j_{q+2}}%
\wedge....
\end{align*}
This proves (\ref{pf.glinf.glinfact.welldef.ass1.pfshort.0}). The proof of
(\ref{pf.glinf.glinfact.welldef.ass1}) in Case 1 is thus complete.
\end{vershort}

\begin{verlong}
Let us consider Case 1 first. In this case, some two elements of the sequence
$\left(  j_{0},j_{1},j_{2},...\right)  $ are equal. Hence, $v_{j_{0}}\wedge
v_{j_{1}}\wedge v_{j_{2}}\wedge...=0$ (by the definition of $v_{j_{0}}\wedge
v_{j_{1}}\wedge v_{j_{2}}\wedge...$), and thus $F_{a}\left(  v_{j_{0}}\wedge
v_{j_{1}}\wedge v_{j_{2}}\wedge...\right)  =F_{a}\left(  0\right)  =0$ (since
$F_{a}$ is linear).

We know that some two elements of the sequence $\left(  j_{0},j_{1}%
,j_{2},...\right)  $ are equal. Let $j_{p}$ and $j_{q}$ be two such elements,
with $p\neq q$. So we have $p\neq q$ and $j_{p}=j_{q}$. WLOG assume that $p<q$
(otherwise, just switch $p$ with $q$).

We recall that an infinite wedge product of the form $b_{0}\wedge b_{1}\wedge
b_{2}\wedge...$ (where $b_{0},b_{1},b_{2},...$ are vectors in $V$ such that
$\left(  b_{i}=v_{m-i}\text{ for all sufficiently large }i\right)  $) gets
multiplied by $-1$ when we switch $b_{i}$ with $b_{j}$ for any two distinct
$i\in\mathbb{N}$ and $j\in\mathbb{N}$\ \ \ \ \footnote{This is a particular
case of Proposition \ref{prop.semiinfwedge.welldef} \textbf{(f)} (namely, the
case when $\pi$ is the transposition $\left(  i,j\right)  $).}. Thus, the
infinite wedge product
\[
v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{p+1}}\wedge v_{j_{p+2}}%
\wedge...\wedge v_{j_{q-1}}\wedge v_{j_{q}}\wedge v_{j_{q+1}}\wedge
v_{j_{q+2}}\wedge...
\]
gets multiplied by $-1$ when we switch $a\rightharpoonup v_{j_{p}}$ with
$v_{j_{q}}$ (since $p\in\mathbb{N}$ and $q\in\mathbb{N}$ are distinct). In
other words,%
\begin{align}
&  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge v_{j_{q}}\wedge
v_{j_{p+1}}\wedge v_{j_{p+2}}\wedge...\wedge v_{j_{q-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{q+1}}\wedge v_{j_{q+2}}%
\wedge...\nonumber\\
&  =-v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge\left(
a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{p+1}}\wedge v_{j_{p+2}}%
\wedge...\wedge v_{j_{q-1}}\wedge v_{j_{q}}\wedge v_{j_{q+1}}\wedge
v_{j_{q+2}}\wedge.... \label{pf.glinf.glinfact.welldef.ass1.pf.1}%
\end{align}


On the other hand, for every $k\in\mathbb{N}$ satisfying $k\neq p$ and $k\neq
q$, we have
\begin{equation}
v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{k-1}}\wedge\left(
a\rightharpoonup v_{j_{k}}\right)  \wedge v_{j_{k+1}}\wedge v_{j_{k+2}}%
\wedge...=0 \label{pf.glinf.glinfact.welldef.ass1.pf.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.glinf.glinfact.welldef.ass1.pf.2}):} Let
$k\in\mathbb{N}$ satisfy $k\neq p$ and $k\neq q$. Since $k\neq p$ and $k\neq
q$, both vectors $v_{j_{p}}$ and $v_{j_{q}}$ appear as factors in the wedge
product $v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{k-1}}\wedge\left(
a\rightharpoonup v_{j_{k}}\right)  \wedge v_{j_{k+1}}\wedge v_{j_{k+2}}%
\wedge...$. These two vectors $v_{j_{p}}$ and $v_{j_{q}}$ are equal (since
$j_{p}=j_{q}$).
\par
We know that when (at least) two of the vectors forming a well-defined
infinite wedge product are equal, then this wedge product is $0$. Since two of
the vectors forming the well-defined infinite wedge product $v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{k-1}}\wedge\left(  a\rightharpoonup v_{j_{k}%
}\right)  \wedge v_{j_{k+1}}\wedge v_{j_{k+2}}\wedge...$ are equal (namely,
the vectors $v_{j_{p}}$ and $v_{j_{q}}$), this yields that the wedge product
$v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{k-1}}\wedge\left(
a\rightharpoonup v_{j_{k}}\right)  \wedge v_{j_{k+1}}\wedge v_{j_{k+2}}%
\wedge...$ is $0$. This proves (\ref{pf.glinf.glinfact.welldef.ass1.pf.2}).}.

On the other hand, $p$ and $q$ are two distinct elements of $\mathbb{N}$.
Hence,
\begin{align*}
&  \sum\limits_{k\geq0}v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{k-1}%
}\wedge\left(  a\rightharpoonup v_{j_{k}}\right)  \wedge v_{j_{k+1}}\wedge
v_{j_{k+2}}\wedge...\\
&  =\sum\limits_{\substack{k\geq0;\\k\neq p;\ k\neq q}}\underbrace{v_{j_{0}%
}\wedge v_{j_{1}}\wedge...\wedge v_{j_{k-1}}\wedge\left(  a\rightharpoonup
v_{j_{k}}\right)  \wedge v_{j_{k+1}}\wedge v_{j_{k+2}}\wedge...}%
_{\substack{=0\\\text{(by (\ref{pf.glinf.glinfact.welldef.ass1.pf.2}) (since
}k\neq p\text{ and }k\neq q\text{))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{p-1}}\wedge\left(  a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{p+1}%
}\wedge v_{j_{p+2}}\wedge...}_{\substack{=v_{j_{0}}\wedge v_{j_{1}}%
\wedge...\wedge v_{j_{p-1}}\wedge\left(  a\rightharpoonup v_{j_{p}}\right)
\wedge v_{j_{p+1}}\wedge v_{j_{p+2}}\wedge...\wedge v_{j_{q-1}}\wedge
v_{j_{q}}\wedge v_{j_{q+1}}\wedge v_{j_{q+2}}\wedge...\\=-v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge v_{j_{q}}\wedge v_{j_{p+1}}\wedge
v_{j_{p+2}}\wedge...\wedge v_{j_{q-1}}\wedge\left(  a\rightharpoonup v_{j_{p}%
}\right)  \wedge v_{j_{q+1}}\wedge v_{j_{q+2}}\wedge...\\\text{(by
(\ref{pf.glinf.glinfact.welldef.ass1.pf.1}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{q-1}}\wedge\left(  a\rightharpoonup v_{j_{q}}\right)  \wedge v_{j_{q+1}%
}\wedge v_{j_{q+2}}\wedge...}_{\substack{=v_{j_{0}}\wedge v_{j_{1}}%
\wedge...\wedge v_{j_{p-1}}\wedge v_{j_{p}}\wedge v_{j_{p+1}}\wedge
v_{j_{p+2}}\wedge...\wedge v_{j_{q-1}}\wedge\left(  a\rightharpoonup v_{j_{q}%
}\right)  \wedge v_{j_{q+1}}\wedge v_{j_{q+2}}\wedge...\\=v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{p-1}}\wedge v_{j_{q}}\wedge v_{j_{p+1}}\wedge
v_{j_{p+2}}\wedge...\wedge v_{j_{q-1}}\wedge\left(  a\rightharpoonup v_{j_{p}%
}\right)  \wedge v_{j_{q+1}}\wedge v_{j_{q+2}}\wedge...\\\text{(since }%
j_{p}=j_{q}\text{ and }j_{q}=j_{p}\text{)}}}\\
&  =\underbrace{\sum\limits_{\substack{k\geq0;\\k\neq p;\ k\neq q}}0}_{=0}\\
&  \ \ \ \ \ \ \ \ \ \ -v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}%
}\wedge v_{j_{q}}\wedge v_{j_{p+1}}\wedge v_{j_{p+2}}\wedge...\wedge
v_{j_{q-1}}\wedge\left(  a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{q+1}%
}\wedge v_{j_{q+2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ +v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{p-1}%
}\wedge v_{j_{q}}\wedge v_{j_{p+1}}\wedge v_{j_{p+2}}\wedge...\wedge
v_{j_{q-1}}\wedge\left(  a\rightharpoonup v_{j_{p}}\right)  \wedge v_{j_{q+1}%
}\wedge v_{j_{q+2}}\wedge...\\
&  =0=F_{a}\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right)
\end{align*}
(since we have shown that $F_{a}\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge
v_{j_{2}}\wedge...\right)  =0$). Thus, (\ref{pf.glinf.glinfact.welldef.ass1})
is proven in Case 1.
\end{verlong}

Now, let us consider Case 2. In this case, no two elements of the sequence
$\left(  j_{0},j_{1},j_{2},...\right)  $ are equal. Thus, the straightening
permutation of the straying $m$-degression $\left(  j_{0},j_{1},j_{2}%
,...\right)  $ is well-defined. Let $\pi$ be this straightening permutation.
Then, $\left(  j_{\pi^{-1}\left(  0\right)  },j_{\pi^{-1}\left(  1\right)
},j_{\pi^{-1}\left(  2\right)  },...\right)  $ is an $m$-degression.

Let $\sigma=\pi^{-1}$. Then, $\sigma$ is a finitary permutation of
$\mathbb{N}$, thus a bijective map $\mathbb{N}\rightarrow\mathbb{N}$. From
$\sigma=\pi^{-1}$, we obtain $\sigma\pi=\operatorname*{id}$, thus $\left(
-1\right)  ^{\sigma\pi}=1$.

We know that $\left(  j_{\pi^{-1}\left(  0\right)  },j_{\pi^{-1}\left(
1\right)  },j_{\pi^{-1}\left(  2\right)  },...\right)  $ is an $m$-degression.
Since $\pi^{-1}=\sigma$, this rewrites as follows: The sequence $\left(
j_{\sigma\left(  0\right)  },j_{\sigma\left(  1\right)  },j_{\sigma\left(
2\right)  },...\right)  $ is an $m$-degression.

By the definition of $v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...$ (in
Definition \ref{def.semiinfwedge.stray}), we have%
\[
v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...=\left(  -1\right)  ^{\pi
}v_{j_{\pi^{-1}\left(  0\right)  }}\wedge v_{j_{\pi^{-1}\left(  1\right)  }%
}\wedge v_{j_{\pi^{-1}\left(  2\right)  }}\wedge...=\left(  -1\right)  ^{\pi
}v_{j_{\sigma\left(  0\right)  }}\wedge v_{j_{\sigma\left(  1\right)  }}\wedge
v_{j_{\sigma\left(  2\right)  }}\wedge...
\]
(since $\pi^{-1}=\sigma$). Thus,%
\begin{align*}
&  F_{a}\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right) \\
&  =F_{a}\left(  \left(  -1\right)  ^{\pi}v_{j_{\sigma\left(  0\right)  }%
}\wedge v_{j_{\sigma\left(  1\right)  }}\wedge v_{j_{\sigma\left(  2\right)
}}\wedge...\right)  =\left(  -1\right)  ^{\pi}\cdot F_{a}\left(
v_{j_{\sigma\left(  0\right)  }}\wedge v_{j_{\sigma\left(  1\right)  }}\wedge
v_{j_{\sigma\left(  2\right)  }}\wedge...\right)
\end{align*}
(since $F_{a}$ is linear). Multiplying this equality with $\left(  -1\right)
^{\sigma}$, we obtain%
\begin{align}
&  \left(  -1\right)  ^{\sigma}\cdot F_{a}\left(  v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\right) \nonumber\\
&  =\underbrace{\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\pi}%
}_{=\left(  -1\right)  ^{\sigma\pi}=1}\cdot F_{a}\left(  v_{j_{\sigma\left(
0\right)  }}\wedge v_{j_{\sigma\left(  1\right)  }}\wedge v_{j_{\sigma\left(
2\right)  }}\wedge...\right)  =F_{a}\left(  v_{j_{\sigma\left(  0\right)  }%
}\wedge v_{j_{\sigma\left(  1\right)  }}\wedge v_{j_{\sigma\left(  2\right)
}}\wedge...\right) \nonumber\\
&  =\sum\limits_{k\geq0}v_{j_{\sigma\left(  0\right)  }}\wedge v_{j_{\sigma
\left(  1\right)  }}\wedge...\wedge v_{j_{\sigma\left(  k-1\right)  }}%
\wedge\left(  a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k+1\right)  }}\wedge v_{j_{\sigma\left(  k+2\right)  }%
}\wedge...\label{pf.glinf.glinfact.welldef.ass1.pf.4}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by the definition of }F_{a}\left(  v_{j_{\sigma\left(  0\right)  }%
}\wedge v_{j_{\sigma\left(  1\right)  }}\wedge v_{j_{\sigma\left(  2\right)
}}\wedge...\right)  \text{,}\\
\text{since }\left(  j_{\sigma\left(  0\right)  },j_{\sigma\left(  1\right)
},j_{\sigma\left(  2\right)  },...\right)  \text{ is an }m\text{-degression}%
\end{array}
\right)  .\nonumber
\end{align}


\begin{vershort}
On the other hand, for every $k\in\mathbb{N}$, we have%
\begin{align}
&  v_{j_{\sigma\left(  0\right)  }}\wedge v_{j_{\sigma\left(  1\right)  }%
}\wedge...\wedge v_{j_{\sigma\left(  k-1\right)  }}\wedge\left(
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k+1\right)  }}\wedge v_{j_{\sigma\left(  k+2\right)  }%
}\wedge...\nonumber\\
&  =\left(  -1\right)  ^{\sigma}\cdot v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(  a\rightharpoonup
v_{j_{\sigma\left(  k\right)  }}\right)  \wedge v_{j_{\sigma\left(  k\right)
+1}}\wedge v_{j_{\sigma\left(  k\right)  +2}}\wedge....
\label{pf.glinf.glinfact.welldef.ass1.pfshort.5}%
\end{align}
\footnote{\textit{Proof of (\ref{pf.glinf.glinfact.welldef.ass1.pfshort.5}):}
Let $k\in\mathbb{N}$. Define a sequence $\left(  c_{0},c_{1},c_{2},...\right)
$ of elements of $V$ by%
\[
\left(  c_{0},c_{1},c_{2},...\right)  =\left(  v_{j_{0}},v_{j_{1}%
},...,v_{j_{\sigma\left(  k\right)  -1}},a\rightharpoonup v_{j_{\sigma\left(
k\right)  }},v_{j_{\sigma\left(  k\right)  +1}},v_{j_{\sigma\left(  k\right)
+2}},...\right)  .
\]
Then,%
\[
c_{0}\wedge c_{1}\wedge c_{2}\wedge...=v_{j_{0}}\wedge v_{j_{1}}%
\wedge...\wedge v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k\right)  +1}}\wedge v_{j_{\sigma\left(  k\right)  +2}%
}\wedge....
\]
\par
But according to Proposition \ref{prop.semiinfwedge.welldef} \textbf{(f)}
(applied to $\left(  c_{0},c_{1},c_{2},...\right)  $ instead of $\left(
b_{0},b_{1},b_{2},...\right)  $), the infinite wedge product $c_{\sigma\left(
0\right)  }\wedge c_{\sigma\left(  1\right)  }\wedge c_{\sigma\left(
2\right)  }\wedge...$ is well-defined and satisfies
\[
c_{\sigma\left(  0\right)  }\wedge c_{\sigma\left(  1\right)  }\wedge
c_{\sigma\left(  2\right)  }\wedge...=\left(  -1\right)  ^{\sigma}\cdot
c_{0}\wedge c_{1}\wedge c_{2}\wedge....
\]
\par
But it is easy to see that
\[
\left(  c_{\sigma\left(  0\right)  },c_{\sigma\left(  1\right)  }%
,c_{\sigma\left(  2\right)  },...\right)  =\left(  v_{j_{\sigma\left(
0\right)  }},v_{j_{\sigma\left(  1\right)  }},...,v_{j_{\sigma\left(
k-1\right)  }},a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}%
,v_{j_{\sigma\left(  k+1\right)  }},v_{j_{\sigma\left(  k+2\right)  }%
},...\right)  ,
\]
so that%
\[
c_{\sigma\left(  0\right)  }\wedge c_{\sigma\left(  1\right)  }\wedge
c_{\sigma\left(  2\right)  }\wedge...=v_{j_{\sigma\left(  0\right)  }}\wedge
v_{j_{\sigma\left(  1\right)  }}\wedge...\wedge v_{j_{\sigma\left(
k-1\right)  }}\wedge\left(  a\rightharpoonup v_{j_{\sigma\left(  k\right)  }%
}\right)  \wedge v_{j_{\sigma\left(  k+1\right)  }}\wedge v_{j_{\sigma\left(
k+2\right)  }}\wedge....
\]
Hence,%
\begin{align*}
&  v_{j_{\sigma\left(  0\right)  }}\wedge v_{j_{\sigma\left(  1\right)  }%
}\wedge...\wedge v_{j_{\sigma\left(  k-1\right)  }}\wedge\left(
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k+1\right)  }}\wedge v_{j_{\sigma\left(  k+2\right)  }%
}\wedge...\\
&  =c_{\sigma\left(  0\right)  }\wedge c_{\sigma\left(  1\right)  }\wedge
c_{\sigma\left(  2\right)  }\wedge...=\left(  -1\right)  ^{\sigma}%
\cdot\underbrace{c_{0}\wedge c_{1}\wedge c_{2}\wedge...}_{=v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k\right)  +1}}\wedge v_{j_{\sigma\left(  k\right)  +2}%
}\wedge...}\\
&  =\left(  -1\right)  ^{\sigma}\cdot v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(  a\rightharpoonup
v_{j_{\sigma\left(  k\right)  }}\right)  \wedge v_{j_{\sigma\left(  k\right)
+1}}\wedge v_{j_{\sigma\left(  k\right)  +2}}\wedge....
\end{align*}
This proves (\ref{pf.glinf.glinfact.welldef.ass1.pfshort.5}).} Hence,
(\ref{pf.glinf.glinfact.welldef.ass1.pf.4}) becomes%
\begin{align*}
&  \left(  -1\right)  ^{\sigma}\cdot F_{a}\left(  v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\right) \\
&  =\sum\limits_{k\geq0}\underbrace{v_{j_{\sigma\left(  0\right)  }}\wedge
v_{j_{\sigma\left(  1\right)  }}\wedge...\wedge v_{j_{\sigma\left(
k-1\right)  }}\wedge\left(  a\rightharpoonup v_{j_{\sigma\left(  k\right)  }%
}\right)  \wedge v_{j_{\sigma\left(  k+1\right)  }}\wedge v_{j_{\sigma\left(
k+2\right)  }}\wedge...}_{\substack{=\left(  -1\right)  ^{\sigma}\cdot
v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{\sigma\left(  k\right)  -1}%
}\wedge\left(  a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)
\wedge v_{j_{\sigma\left(  k\right)  +1}}\wedge v_{j_{\sigma\left(  k\right)
+2}}\wedge...\\\text{(by (\ref{pf.glinf.glinfact.welldef.ass1.pfshort.5}))}%
}}\\
&  =\sum\limits_{k\geq0}\left(  -1\right)  ^{\sigma}\cdot v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k\right)  +1}}\wedge v_{j_{\sigma\left(  k\right)  +2}%
}\wedge....
\end{align*}
Dividing this equality by $\left(  -1\right)  ^{\sigma}$, we obtain%
\begin{align*}
&  F_{a}\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right) \\
&  =\sum\limits_{k\geq0}v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(  a\rightharpoonup
v_{j_{\sigma\left(  k\right)  }}\right)  \wedge v_{j_{\sigma\left(  k\right)
+1}}\wedge v_{j_{\sigma\left(  k\right)  +2}}\wedge...\\
&  =\sum\limits_{k\geq0}v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{k-1}%
}\wedge\left(  a\rightharpoonup v_{j_{k}}\right)  \wedge v_{j_{k+1}}\wedge
v_{j_{k+2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }k\text{ for }%
\sigma\left(  k\right)  \text{ in the sum (since }\sigma\text{ is
bijective)}\right)  .
\end{align*}
Thus, (\ref{pf.glinf.glinfact.welldef.ass1}) is proven in Case 2.
\end{vershort}

\begin{verlong}
On the other hand, for every $k\in\mathbb{N}$, we have%
\begin{align}
&  v_{j_{\sigma\left(  0\right)  }}\wedge v_{j_{\sigma\left(  1\right)  }%
}\wedge...\wedge v_{j_{\sigma\left(  k-1\right)  }}\wedge\left(
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k+1\right)  }}\wedge v_{j_{\sigma\left(  k+2\right)  }%
}\wedge...\nonumber\\
&  =\left(  -1\right)  ^{\sigma}\cdot v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(  a\rightharpoonup
v_{j_{\sigma\left(  k\right)  }}\right)  \wedge v_{j_{\sigma\left(  k\right)
+1}}\wedge v_{j_{\sigma\left(  k\right)  +2}}\wedge....
\label{pf.glinf.glinfact.welldef.ass1.pf.5}%
\end{align}
\footnote{\textit{Proof of (\ref{pf.glinf.glinfact.welldef.ass1.pf.5}):} Let
$k\in\mathbb{N}$. Define a sequence $\left(  c_{0},c_{1},c_{2},...\right)  $
of elements of $V$ by%
\[
\left(  c_{i}=\left\{
\begin{array}
[c]{l}%
v_{j_{i}},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\sigma\left(  k\right)  ;\\
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }},\ \ \ \ \ \ \ \ \ \ \text{if
}i=\sigma\left(  k\right)
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\mathbb{N}\right)  .
\]
Then, $\left(  c_{0},c_{1},c_{2},...\right)  =\left(  v_{j_{0}},v_{j_{1}%
},...,v_{j_{\sigma\left(  k\right)  -1}},a\rightharpoonup v_{j_{\sigma\left(
k\right)  }},v_{j_{\sigma\left(  k\right)  +1}},v_{j_{\sigma\left(  k\right)
+2}},...\right)  $.
\par
It is easy to see that $c_{i}=v_{m-i}$ for sufficiently large $i$.
\par
(\textit{Proof:} We know that every sufficiently large $i\in\mathbb{N}$
satisfies $j_{i}+i=m$. In other words, there exists a $K\in\mathbb{N}$ such
that every $i\geq K$ satisfies $j_{i}+i=m$. Consider this $K$.
\par
Let $i\in\mathbb{N}$ be such that $i\geq\max\left\{  K,\sigma\left(  k\right)
+1\right\}  $. Then, $i\geq\max\left\{  K,\sigma\left(  k\right)  +1\right\}
\geq K$. Hence, $j_{i}+i=m$ (since we know that every $i\geq K$ satisfies
$j_{i}+i=m$), so that $j_{i}=m-i$. But also, $i\geq\max\left\{  K,\sigma
\left(  k\right)  +1\right\}  \geq\sigma\left(  k\right)  +1>\sigma\left(
k\right)  $, so that $i\neq\sigma\left(  k\right)  $. Now, by the definition
of $c_{i}$, we have%
\begin{align*}
c_{i}  &  =\left\{
\begin{array}
[c]{l}%
v_{j_{i}},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\sigma\left(  k\right)  ;\\
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }},\ \ \ \ \ \ \ \ \ \ \text{if
}i=\sigma\left(  k\right)
\end{array}
\right.  =v_{j_{i}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\neq\sigma\left(
k\right)  \right) \\
&  =v_{m-i}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j_{i}=m-i\right)  .
\end{align*}
\par
Now, forget that we fixed $i$. We thus have shown that $c_{i}=v_{m-i}$ for
every $i\in\mathbb{N}$ such that $i\geq\max\left\{  K,\sigma\left(  k\right)
+1\right\}  $. Hence, $c_{i}=v_{m-i}$ for sufficiently large $i$, qed.)
\par
We have $c_{i}=v_{m-i}$ for sufficiently large $i$. Hence, the infinite wedge
product $c_{0}\wedge c_{1}\wedge c_{2}\wedge...$ is well-defined. Since
$\left(  c_{0},c_{1},c_{2},...\right)  =\left(  v_{j_{0}},v_{j_{1}%
},...,v_{j_{\sigma\left(  k\right)  -1}},a\rightharpoonup v_{j_{\sigma\left(
k\right)  }},v_{j_{\sigma\left(  k\right)  +1}},v_{j_{\sigma\left(  k\right)
+2}},...\right)  $, we have%
\[
c_{0}\wedge c_{1}\wedge c_{2}\wedge...=v_{j_{0}}\wedge v_{j_{1}}%
\wedge...\wedge v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k\right)  +1}}\wedge v_{j_{\sigma\left(  k\right)  +2}%
}\wedge....
\]
\par
But according to Proposition \ref{prop.semiinfwedge.welldef} \textbf{(f)}
(applied to $\left(  c_{0},c_{1},c_{2},...\right)  $ instead of $\left(
b_{0},b_{1},b_{2},...\right)  $), the infinite wedge product $c_{\sigma\left(
0\right)  }\wedge c_{\sigma\left(  1\right)  }\wedge c_{\sigma\left(
2\right)  }\wedge...$ is well-defined and satisfies
\[
c_{\sigma\left(  0\right)  }\wedge c_{\sigma\left(  1\right)  }\wedge
c_{\sigma\left(  2\right)  }\wedge...=\left(  -1\right)  ^{\sigma}\cdot
c_{0}\wedge c_{1}\wedge c_{2}\wedge....
\]
\par
On the other hand, define a sequence $\left(  d_{0},d_{1},d_{2},...\right)  $
of elements of $V$ by%
\begin{equation}
\left(  d_{i}=\left\{
\begin{array}
[c]{l}%
v_{j_{\sigma\left(  i\right)  }},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq k;\\
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }},\ \ \ \ \ \ \ \ \ \ \text{if
}i=k
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\mathbb{N}\right)  .
\label{pf.glinf.glinfact.welldef.ass1.pf.5.pf.d}%
\end{equation}
Then, $\left(  d_{0},d_{1},d_{2},...\right)  =\left(  v_{j_{\sigma\left(
0\right)  }},v_{j_{\sigma\left(  1\right)  }},...,v_{j_{\sigma\left(
k-1\right)  }},a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}%
,v_{j_{\sigma\left(  k+1\right)  }},v_{j_{\sigma\left(  k+2\right)  }%
},...\right)  $.
\par
But every $i\in\mathbb{N}$ satisfies%
\begin{align*}
c_{\sigma\left(  i\right)  }  &  =\left\{
\begin{array}
[c]{l}%
v_{j_{\sigma\left(  i\right)  }},\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
i\right)  \neq\sigma\left(  k\right)  ;\\
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }},\ \ \ \ \ \ \ \ \ \ \text{if
}\sigma\left(  i\right)  =\sigma\left(  k\right)
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }%
c_{\sigma\left(  i\right)  }\right) \\
&  =\left\{
\begin{array}
[c]{l}%
v_{j_{\sigma\left(  i\right)  }},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq k;\\
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }},\ \ \ \ \ \ \ \ \ \ \text{if
}i=k
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\sigma\left(  i\right)  \neq\sigma\left(  k\right)  \text{ is
equivalent to }i\neq k\text{ (since }\sigma\text{ is bijective), and since}\\
\sigma\left(  i\right)  =\sigma\left(  k\right)  \text{ is equivalent to
}i=k\text{ (since }\sigma\text{ is bijective)}%
\end{array}
\right) \\
&  =d_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.glinf.glinfact.welldef.ass1.pf.5.pf.d})}\right)  .
\end{align*}
Thus,
\begin{align*}
\left(  c_{\sigma\left(  0\right)  },c_{\sigma\left(  1\right)  }%
,c_{\sigma\left(  2\right)  },...\right)   &  =\left(  d_{0},d_{1}%
,d_{2},...\right) \\
&  =\left(  v_{j_{\sigma\left(  0\right)  }},v_{j_{\sigma\left(  1\right)  }%
},...,v_{j_{\sigma\left(  k-1\right)  }},a\rightharpoonup v_{j_{\sigma\left(
k\right)  }},v_{j_{\sigma\left(  k+1\right)  }},v_{j_{\sigma\left(
k+2\right)  }},...\right)  ,
\end{align*}
so that%
\[
c_{\sigma\left(  0\right)  }\wedge c_{\sigma\left(  1\right)  }\wedge
c_{\sigma\left(  2\right)  }\wedge...=v_{j_{\sigma\left(  0\right)  }}\wedge
v_{j_{\sigma\left(  1\right)  }}\wedge...\wedge v_{j_{\sigma\left(
k-1\right)  }}\wedge\left(  a\rightharpoonup v_{j_{\sigma\left(  k\right)  }%
}\right)  \wedge v_{j_{\sigma\left(  k+1\right)  }}\wedge v_{j_{\sigma\left(
k+2\right)  }}\wedge....
\]
Hence,%
\begin{align*}
&  v_{j_{\sigma\left(  0\right)  }}\wedge v_{j_{\sigma\left(  1\right)  }%
}\wedge...\wedge v_{j_{\sigma\left(  k-1\right)  }}\wedge\left(
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k+1\right)  }}\wedge v_{j_{\sigma\left(  k+2\right)  }%
}\wedge...\\
&  =c_{\sigma\left(  0\right)  }\wedge c_{\sigma\left(  1\right)  }\wedge
c_{\sigma\left(  2\right)  }\wedge...=\left(  -1\right)  ^{\sigma}%
\cdot\underbrace{c_{0}\wedge c_{1}\wedge c_{2}\wedge...}_{=v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k\right)  +1}}\wedge v_{j_{\sigma\left(  k\right)  +2}%
}\wedge...}\\
&  =\left(  -1\right)  ^{\sigma}\cdot v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(  a\rightharpoonup
v_{j_{\sigma\left(  k\right)  }}\right)  \wedge v_{j_{\sigma\left(  k\right)
+1}}\wedge v_{j_{\sigma\left(  k\right)  +2}}\wedge....
\end{align*}
This proves (\ref{pf.glinf.glinfact.welldef.ass1.pf.5}).} Hence,
(\ref{pf.glinf.glinfact.welldef.ass1.pf.4}) becomes%
\begin{align*}
&  \left(  -1\right)  ^{\sigma}\cdot F_{a}\left(  v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\right) \\
&  =\sum\limits_{k\geq0}\underbrace{v_{j_{\sigma\left(  0\right)  }}\wedge
v_{j_{\sigma\left(  1\right)  }}\wedge...\wedge v_{j_{\sigma\left(
k-1\right)  }}\wedge\left(  a\rightharpoonup v_{j_{\sigma\left(  k\right)  }%
}\right)  \wedge v_{j_{\sigma\left(  k+1\right)  }}\wedge v_{j_{\sigma\left(
k+2\right)  }}\wedge...}_{\substack{=\left(  -1\right)  ^{\sigma}\cdot
v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{\sigma\left(  k\right)  -1}%
}\wedge\left(  a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)
\wedge v_{j_{\sigma\left(  k\right)  +1}}\wedge v_{j_{\sigma\left(  k\right)
+2}}\wedge...\\\text{(by (\ref{pf.glinf.glinfact.welldef.ass1.pf.5}))}}}\\
&  =\sum\limits_{k\geq0}\left(  -1\right)  ^{\sigma}\cdot v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k\right)  +1}}\wedge v_{j_{\sigma\left(  k\right)  +2}%
}\wedge...\\
&  =\left(  -1\right)  ^{\sigma}\cdot\sum\limits_{k\geq0}v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(
a\rightharpoonup v_{j_{\sigma\left(  k\right)  }}\right)  \wedge
v_{j_{\sigma\left(  k\right)  +1}}\wedge v_{j_{\sigma\left(  k\right)  +2}%
}\wedge....
\end{align*}
Dividing this equality by $\left(  -1\right)  ^{\sigma}$, we obtain%
\begin{align*}
&  F_{a}\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right) \\
&  =\sum\limits_{k\geq0}v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{\sigma\left(  k\right)  -1}}\wedge\left(  a\rightharpoonup
v_{j_{\sigma\left(  k\right)  }}\right)  \wedge v_{j_{\sigma\left(  k\right)
+1}}\wedge v_{j_{\sigma\left(  k\right)  +2}}\wedge...\\
&  =\sum\limits_{k\geq0}v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{k-1}%
}\wedge\left(  a\rightharpoonup v_{j_{k}}\right)  \wedge v_{j_{k+1}}\wedge
v_{j_{k+2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }k\text{ for }%
\sigma\left(  k\right)  \text{ in the sum (since }\sigma\text{ is
bijective)}\right)  .
\end{align*}
Thus, (\ref{pf.glinf.glinfact.welldef.ass1}) is proven in Case 2.
\end{verlong}

\begin{vershort}
We have now proven (\ref{pf.glinf.glinfact.welldef.ass1}) in each of the two
Cases 1 and 2, hence in all situations. In other words, Assertion
\ref{prop.glinf.glinfact.welldef}.1 is proven.
\end{vershort}

\begin{verlong}
Hence, (\ref{pf.glinf.glinfact.welldef.ass1}) is proven in each of the two
Cases 1 and 2. Since these two cases cover all possibilities, this shows that
(\ref{pf.glinf.glinfact.welldef.ass1}) always holds. This completes the proof
of (\ref{pf.glinf.glinfact.welldef.ass1}). In other words, Assertion
\ref{prop.glinf.glinfact.welldef}.1 is proven.
\end{verlong}

Our next goal is the following assertion:

\begin{quote}
\textit{Assertion \ref{prop.glinf.glinfact.welldef}.2:} Let $a\in
\mathfrak{gl}_{\infty}$. Let $b_{0},b_{1},b_{2},...$ be vectors in $V$ which
satisfy%
\[
b_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all sufficiently large }i.
\]
Then,%
\[
F_{a}\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right)  =\sum
\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge....
\]



\end{quote}

\begin{vershort}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.2 (sketched):} We
have $b_{i}=v_{m-i}$ for all sufficiently large $i$. In other words, there
exists a $K\in\mathbb{N}$ such that every $i\geq K$ satisfies $b_{i}=v_{m-i}$.
Fix such a $K$.

We have to prove the equality%
\begin{equation}
F_{a}\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right)  =\sum
\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge....
\label{pf.glinf.glinfact.welldef.ass2.pfshort}%
\end{equation}
This equality is clearly linear in \textbf{each} of the variables $b_{0}$,
$b_{1}$, $...$, $b_{K-1}$ (and also in each of the variables $b_{K}$,
$b_{K+1}$, $b_{K+2}$, $...$, but we don't care about them). Hence, in proving
it, we can WLOG assume that each of the vectors $b_{0}$, $b_{1}$, $...$,
$b_{K-1}$ belongs to the basis $\left(  v_{j}\right)  _{j\in\mathbb{Z}}$ of
$V$.\ \ \ \ \footnote{Note that this assumption is allowed because $b_{0}$,
$b_{1}$, $...$, $b_{K-1}$ are \textbf{finitely many} vectors. In contrast, if
we wanted to WLOG assume that each of the (infinitely many) vectors $b_{0}$,
$b_{1}$, $b_{2}$, $...$ belongs to the basis $\left(  v_{j}\right)
_{j\in\mathbb{Z}}$ of $V$, then we would have to need more justification for
such an assumption.} Assume this. Of course, the remaining vectors $b_{K}$,
$b_{K+1}$, $b_{K+2}$, $...$ also belong to the basis $\left(  v_{j}\right)
_{j\in\mathbb{Z}}$ of $V$ (because every $i\geq K$ satisfies $b_{i}=v_{m-i}$).
Hence, all the vectors $b_{0}$, $b_{1}$, $b_{2}$, $...$ belong to the basis
$\left(  v_{j}\right)  _{j\in\mathbb{Z}}$ of $V$. Hence, there exists a
sequence $\left(  j_{0},j_{1},j_{2},...\right)  \in\mathbb{Z}^{\mathbb{N}}$
such that every $i\in\mathbb{N}$ satisfies $b_{i}=v_{j_{i}}$. Therefore, the
equality that we need to prove, (\ref{pf.glinf.glinfact.welldef.ass2.pfshort}%
), will immediately follow from Assertion \ref{prop.glinf.glinfact.welldef}.1
once we can show that $\left(  j_{0},j_{1},j_{2},...\right)  $ is a straying
$m$-degression. But the latter is obvious (since every $i\geq K$ satisfies
$v_{j_{i}}=b_{i}=v_{m-i}$ and thus $j_{i}=m-i$, so that $j_{i}+i=m$). Hence,
(\ref{pf.glinf.glinfact.welldef.ass2.pfshort}) is proven. That is, Assertion
\ref{prop.glinf.glinfact.welldef}.2 is proven.
\end{vershort}

\begin{verlong}
Rather than prove this directly, we will show the following assertion first:

\textit{Assertion \ref{prop.glinf.glinfact.welldef}.3:} Let $K$ be a
nonnegative integer. Let $a\in\mathfrak{gl}_{\infty}$. Let $b_{0},b_{1}%
,b_{2},...$ be vectors in $V$ which satisfy%
\begin{equation}
b_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all sufficiently large }i.
\label{pf.glinf.glinfact.welldef.ass3.stand}%
\end{equation}
Assume also that%
\begin{equation}
b_{i}\in\left\{  v_{z}\ \mid\ z\in\mathbb{Z}\right\}
\ \ \ \ \ \ \ \ \ \ \text{for all integers }i\geq K.
\label{pf.glinf.glinfact.welldef.ass3.hook}%
\end{equation}
Then,%
\begin{equation}
F_{a}\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right)  =\sum
\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge....
\label{pf.glinf.glinfact.welldef.ass3.state}%
\end{equation}


Notice that Assertion \ref{prop.glinf.glinfact.welldef}.3 differs from
Assertion \ref{prop.glinf.glinfact.welldef}.2 in the presence of an additional
condition (namely, (\ref{pf.glinf.glinfact.welldef.ass3.hook})). This
condition will turn out to be harmless\footnote{In fact, it is easy to see
that for every fixed sequence $\left(  b_{0},b_{1},b_{2},...\right)  $ of
vectors in $V$ which satisfy $\left(  b_{i}=v_{m-i}\text{ for all sufficiently
large }i\right)  $, there exists a $K\in\mathbb{N}$ for which this condition
(\ref{pf.glinf.glinfact.welldef.ass3.hook}) is satisfied. We will explain this
in more detail later.}, but it allows us to induct over the variable $K$.

\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.3:} We will prove
Assertion \ref{prop.glinf.glinfact.welldef}.3 by induction over $K$:

\textit{Induction base:} To complete the induction base, we need to show that
Assertion \ref{prop.glinf.glinfact.welldef}.3 holds for $K=0$. So, assume that
$K=0$. Let $a\in\mathfrak{gl}_{\infty}$. Let $b_{0},b_{1},b_{2},...$ be
vectors in $V$ which satisfy (\ref{pf.glinf.glinfact.welldef.ass3.stand}) and
(\ref{pf.glinf.glinfact.welldef.ass3.hook}). We need to prove that
(\ref{pf.glinf.glinfact.welldef.ass3.state}) holds.

We know that (\ref{pf.glinf.glinfact.welldef.ass3.stand}) holds. In other
words, there exists an $N\in\mathbb{N}$ such that every $i\geq N$ satisfies
$b_{i}=v_{m-i}$. Fix such an $N$.

Notice that%
\begin{equation}
b_{i}\in\left\{  v_{z}\ \mid\ z\in\mathbb{Z}\right\}
\ \ \ \ \ \ \ \ \ \ \text{for all }i\in\mathbb{N}.
\label{pf.glinf.glinfact.welldef.ass3.pf.indbase}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.glinf.glinfact.welldef.ass3.pf.indbase}):}
Let $i\in\mathbb{N}$. Then, $i\geq0=K$. Thus (according to
(\ref{pf.glinf.glinfact.welldef.ass3.hook})), we have $b_{i}\in\left\{
v_{z}\ \mid\ z\in\mathbb{Z}\right\}  $. This proves
(\ref{pf.glinf.glinfact.welldef.ass3.pf.indbase}).}

For every $i\in\mathbb{N}$, let us define an integer $j_{i}$ as follows: If
$i\geq N$, set $j_{i}=m-i$. Otherwise, set $j_{i}$ to be an integer
$z\in\mathbb{Z}$ such that $b_{i}=v_{z}$ (such an integer $z$ exists, because
(\ref{pf.glinf.glinfact.welldef.ass3.pf.indbase}) shows that $b_{i}\in\left\{
v_{z}\ \mid\ z\in\mathbb{Z}\right\}  $). Thus, we have defined an integer
$j_{i}$ for every $i\in\mathbb{N}$.

It is now rather clear that%
\begin{equation}
b_{i}=v_{j_{i}}\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\mathbb{N}.
\label{pf.glinf.glinfact.welldef.ass3.pf.indbase.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.glinf.glinfact.welldef.ass3.pf.indbase.2}%
):} Let $i\in\mathbb{N}$. We need to then show that $b_{i}=v_{j_{i}}$. We
distinguish between two cases:
\par
\textit{Case 1:} We have $i\geq N$.
\par
\textit{Case 2:} We don't have $i\geq N$.
\par
Let us first consider Case 1. In this case, $i\geq N$. Hence, $j_{i}=m-i$ (by
the definition of $j_{i}$). Thus, $m-i=j_{i}$, so that $v_{m-i}=v_{j_{i}}$.
But on the other hand, $b_{i}=v_{m-i}$ (since we know that every $i\geq N$
satisfies $b_{i}=v_{m-i}$). Thus, $b_{i}=v_{m-i}=v_{j_{i}}$. This proves
$b_{i}=v_{j_{i}}$ in Case 1.
\par
Now let us consider Case 2. In this case, we don't have $i\geq N$. Hence,
$j_{i}$ is an integer $z\in\mathbb{Z}$ such that $b_{i}=v_{z}$ (by the
definition of $j_{i}$). Hence, $b_{i}=v_{j_{i}}$. Thus, $b_{i}=v_{j_{i}}$ is
proven in Case 2.
\par
We have now proven $b_{i}=v_{j_{i}}$ in each of the Cases 1 and 2. Hence,
$b_{i}=v_{j_{i}}$ always holds (since Cases 1 and 2 cover all possibilities).
Thus, (\ref{pf.glinf.glinfact.welldef.ass3.pf.indbase.2}) is proven.}

Also,%
\begin{equation}
\text{every sufficiently high }k\in\mathbb{N}\text{ satisfies }j_{k}+k=m.
\label{pf.glinf.glinfact.welldef.ass3.pf.indbase.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.glinf.glinfact.welldef.ass3.pf.indbase.3}%
):} For every $k\in\mathbb{N}$ such that $k\geq N$, we have $j_{k}=m-k$ (by
the definition of $j_{k}$). In other words, for every $k\in\mathbb{N}$ such
that $k\geq N$, we have $j_{k}+k=m$. Thus, for every sufficiently high
$k\in\mathbb{N}$, we have $j_{k}+k=m$. This proves
(\ref{pf.glinf.glinfact.welldef.ass3.pf.indbase.3}).} Hence, $\left(
j_{0},j_{1},j_{2},...\right)  $ is a straying $m$-degression. Thus, Assertion
\ref{prop.glinf.glinfact.welldef}.1 yields%
\[
F_{a}\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right)
=\sum\limits_{k\geq0}v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{k-1}%
}\wedge\left(  a\rightharpoonup v_{j_{k}}\right)  \wedge v_{j_{k+1}}\wedge
v_{j_{k+2}}\wedge....
\]


But due to (\ref{pf.glinf.glinfact.welldef.ass3.pf.indbase.2}), we have
$\left(  b_{0},b_{1},b_{2},...\right)  =\left(  v_{j_{0}},v_{j_{1}},v_{j_{2}%
},...\right)  $. Therefore, $b_{0}\wedge b_{1}\wedge b_{2}\wedge...=v_{j_{0}%
}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...$. Hence,%
\begin{align*}
F_{a}\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right)   &  =F_{a}\left(
v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\right) \\
&  =\sum\limits_{k\geq0}\underbrace{v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{k-1}}\wedge\left(  a\rightharpoonup v_{j_{k}}\right)  \wedge v_{j_{k+1}%
}\wedge v_{j_{k+2}}\wedge...}_{\substack{=b_{0}\wedge b_{1}\wedge...\wedge
b_{k-1}\wedge\left(  a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge
b_{k+2}\wedge...\\\text{(since }\left(  v_{j_{0}},v_{j_{1}},v_{j_{2}%
},...\right)  =\left(  b_{0},b_{1},b_{2},...\right)  \text{)}}}\\
&  =\sum\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge....
\end{align*}
In other words, (\ref{pf.glinf.glinfact.welldef.ass3.state}) holds. We have
thus shown that Assertion \ref{prop.glinf.glinfact.welldef}.3 holds for $K=0$.
This completes the induction base.

\textit{Induction step:} Let $\kappa\in\mathbb{N}$. Assume that Assertion
\ref{prop.glinf.glinfact.welldef}.3 holds for $K=\kappa$. We now need to prove
that Assertion \ref{prop.glinf.glinfact.welldef}.3 holds for $K=\kappa+1$.

Let $a\in\mathfrak{gl}_{\infty}$. Let $b_{0},b_{1},b_{2},...$ be vectors in
$V$ which satisfy (\ref{pf.glinf.glinfact.welldef.ass3.stand}), and satisfy
(\ref{pf.glinf.glinfact.welldef.ass3.hook}) for $K=\kappa+1$. We need to prove
that (\ref{pf.glinf.glinfact.welldef.ass3.state}) holds.

We know that the vectors $b_{0},b_{1},b_{2},...$ satisfy
(\ref{pf.glinf.glinfact.welldef.ass3.hook}) for $K=\kappa+1$. In other words,%
\begin{equation}
b_{i}\in\left\{  v_{z}\ \mid\ z\in\mathbb{Z}\right\}
\ \ \ \ \ \ \ \ \ \ \text{for all integers }i\geq\kappa+1.
\label{pf.glinf.glinfact.welldef.ass3.indstep.hook}%
\end{equation}


We have $b_{\kappa}\in V$, while $\left(  v_{j}\right)  _{j\in\mathbb{Z}}$ is
a basis of the vector space $V$. Thus, we can write the vector $b_{\kappa}$ in
the form%
\[
b_{\kappa}=\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}v_{\ell}%
\]
for some family $\left(  \beta_{\ell}\right)  _{\ell\in\mathbb{Z}}%
\in\mathbb{C}^{\mathbb{Z}}$ such that all but finitely many $\ell\in
\mathbb{Z}$ satisfy $\beta_{\ell}=0$. Consider this family $\left(
\beta_{\ell}\right)  _{\ell\in\mathbb{Z}}$. Since all but finitely many
$\ell\in\mathbb{Z}$ satisfy $\beta_{\ell}=0$, the sum $\sum\limits_{\ell
\in\mathbb{Z}}\beta_{\ell}v_{\ell}$ has only finitely many nonzero addends,
and thus can be treated as a finite sum in regard to algebraic
transformations. In particular, the following computation is allowed:%
\begin{align}
&  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\nonumber\\
&  =b_{0}\wedge b_{1}\wedge...\wedge b_{\kappa-1}\wedge b_{\kappa}\wedge
b_{\kappa+1}\wedge b_{\kappa+2}\wedge...\nonumber\\
&  =b_{0}\wedge b_{1}\wedge...\wedge b_{\kappa-1}\wedge\left(  \sum
\limits_{\ell\in\mathbb{Z}}\beta_{\ell}v_{\ell}\right)  \wedge b_{\kappa
+1}\wedge b_{\kappa+2}\wedge...\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}b_{\kappa}=\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}v_{\ell}\right)
\nonumber\\
&  =\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}b_{0}\wedge b_{1}%
\wedge...\wedge b_{\kappa-1}\wedge v_{\ell}\wedge b_{\kappa+1}\wedge
b_{\kappa+2}\wedge...\label{pf.glinf.glinfact.welldef.ass3.indstep.1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{due to the multilinearity of the infinite
wedge product}\right)  .\nonumber
\end{align}


Now, for every $\ell\in\mathbb{Z}$, let us define a sequence $\left(
b_{\ell,0},b_{\ell,1},b_{\ell,2},...\right)  $ of vectors in $V$ by%
\[
\left(  b_{\ell,i}=\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\mathbb{N}\right)  .
\]
Then, for every $\ell\in\mathbb{Z}$, we have $\left(  b_{\ell,0},b_{\ell
,1},b_{\ell,2},...\right)  =\left(  b_{0},b_{1},...,b_{\kappa-1},v_{\ell
},b_{\kappa+1},b_{\kappa+2},...\right)  $. Hence, for every $\ell\in
\mathbb{Z}$, we have%
\[
b_{\ell,0}\wedge b_{\ell,1}\wedge b_{\ell,2}\wedge...=b_{0}\wedge b_{1}%
\wedge...\wedge b_{\kappa-1}\wedge v_{\ell}\wedge b_{\kappa+1}\wedge
b_{\kappa+2}\wedge....
\]
Thus, (\ref{pf.glinf.glinfact.welldef.ass3.indstep.1}) becomes%
\begin{align*}
&  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\\
&  =\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}\underbrace{b_{0}\wedge
b_{1}\wedge...\wedge b_{\kappa-1}\wedge v_{\ell}\wedge b_{\kappa+1}\wedge
b_{\kappa+2}\wedge...}_{=b_{\ell,0}\wedge b_{\ell,1}\wedge b_{\ell,2}%
\wedge...}\\
&  =\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}b_{\ell,0}\wedge b_{\ell
,1}\wedge b_{\ell,2}\wedge....
\end{align*}


Applying the map $F_{a}$ to this equality, we obtain%
\begin{align}
&  F_{a}\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right) \nonumber\\
&  =F_{a}\left(  \sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}b_{\ell,0}\wedge
b_{\ell,1}\wedge b_{\ell,2}\wedge...\right)  =\sum\limits_{\ell\in\mathbb{Z}%
}\beta_{\ell}F_{a}\left(  b_{\ell,0}\wedge b_{\ell,1}\wedge b_{\ell,2}%
\wedge...\right) \label{pf.glinf.glinfact.welldef.ass3.indstep.2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the map }F_{a}\text{ is linear, and since the sum }\sum
\limits_{\ell\in\mathbb{Z}}\beta_{\ell}b_{\ell,0}\wedge b_{\ell,1}\wedge
b_{\ell,2}\wedge...\\
\text{has only finitely many nonzero addends}\\
\text{(because all but finitely many }\ell\in\mathbb{Z}\text{ satisfy }%
\beta_{\ell}=0\text{)}%
\end{array}
\right)  .\nonumber
\end{align}


Now, fix an $\ell\in\mathbb{N}$. The sequence $\left(  b_{\ell,0},b_{\ell
,1},b_{\ell,2},...\right)  \in V^{\mathbb{N}}$ satisfies%
\begin{equation}
b_{\ell,i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all sufficiently large }i.
\label{pf.glinf.glinfact.welldef.ass3.indstep.standk+1}%
\end{equation}
\footnote{\textit{Proof of
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.standk+1}):} We know that the
vectors $b_{0},b_{1},b_{2},...$ satisfy
(\ref{pf.glinf.glinfact.welldef.ass3.stand}). In other words, there exists an
$I\in\mathbb{N}$ such that every integer $i>I$ satisfies $b_{i}=v_{m-i}$.
Consider such an $I$.
\par
Let $i\in\mathbb{N}$ be such that $i>\max\left\{  \kappa,I\right\}  $. Then,
$i>\max\left\{  \kappa,I\right\}  \geq I$, so that $b_{i}=v_{m-i}$ (since we
know that every integer $i>I$ satisfies $b_{i}=v_{m-i}$). Also, $i>\max
\left\{  \kappa,I\right\}  \geq\kappa$, and thus $i\neq\kappa$. By the
definition of $b_{\ell,i}$, we have%
\begin{align*}
b_{\ell,i}  &  =\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right.  =b_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\neq\kappa\right) \\
&  =v_{m-i}.
\end{align*}
\par
Now, forget that we fixed $i$. We thus have shown that every $i\in\mathbb{N}$
such that $i>\max\left\{  \kappa,I\right\}  $ satisfies $b_{\ell,i}=v_{m-i}$.
Thus, every sufficiently large $i$ satisfies $b_{\ell,i}=v_{m-i}$. This proves
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.standk+1}).} Moreover, the
sequence $\left(  b_{\ell,0},b_{\ell,1},b_{\ell,2},...\right)  \in
V^{\mathbb{N}}$ satisfies
\begin{equation}
b_{\ell,i}\in\left\{  v_{z}\ \mid\ z\in\mathbb{Z}\right\}
\ \ \ \ \ \ \ \ \ \ \text{for all integers }i\geq\kappa.
\label{pf.glinf.glinfact.welldef.ass3.indstep.hookk+1}%
\end{equation}
\footnote{\textit{Proof of
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.hookk+1}):} Let $i$ be an integer
such that $i\geq\kappa$.
\par
If $i=\kappa$, then%
\begin{align*}
b_{\ell,i}  &  =\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }b_{\ell
,i}\right) \\
&  =v_{\ell}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i=\kappa\right) \\
&  \in\left\{  v_{z}\ \mid\ z\in\mathbb{Z}\right\}
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\ell\in\mathbb{Z}\right)  .
\end{align*}
Hence, if $i=\kappa$, then
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.hookk+1}) is true. Thus, for the
rest of the proof of (\ref{pf.glinf.glinfact.welldef.ass3.indstep.hookk+1}),
we can WLOG assume that $i\neq\kappa$. Assume this.
\par
Since $i\geq\kappa$ and $i\neq\kappa$, we have $i>\kappa$. Since $i$ and
$\kappa$ are integers, this shows that $i\geq\kappa+1$. By the definition of
$b_{\ell,i}$, we have%
\begin{align*}
b_{\ell,i}  &  =\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right.  =b_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\neq\kappa\right) \\
&  \in\left\{  v_{z}\ \mid\ z\in\mathbb{Z}\right\}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.hook}), since }i\geq
\kappa+1\right)  .
\end{align*}
This proves (\ref{pf.glinf.glinfact.welldef.ass3.indstep.hookk+1}).}

But we have assumed (as the induction hypothesis) that Assertion
\ref{prop.glinf.glinfact.welldef}.3 holds for $K=\kappa$. Hence, we can apply
Assertion \ref{prop.glinf.glinfact.welldef}.3 to $\left(  b_{\ell,0}%
,b_{\ell,1},b_{\ell,2},...\right)  $ and $\kappa$ instead of $\left(
b_{0},b_{1},b_{2},...\right)  $ and $K$ (because we know that the sequence
$\left(  b_{\ell,0},b_{\ell,1},b_{\ell,2},...\right)  $ satisfies
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.standk+1}) and
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.hookk+1})). As a result, we
conclude that%
\begin{align}
&  F_{a}\left(  b_{\ell,0}\wedge b_{\ell,1}\wedge b_{\ell,2}\wedge...\right)
\nonumber\\
&  =\sum\limits_{k\geq0}b_{\ell,0}\wedge b_{\ell,1}\wedge...\wedge
b_{\ell,k-1}\wedge\left(  a\rightharpoonup b_{\ell,k}\right)  \wedge
b_{\ell,k+1}\wedge b_{\ell,k+2}\wedge....
\label{pf.glinf.glinfact.welldef.ass3.indstep.7}%
\end{align}


Now, forget that we have fixed $\ell$. We have thus proven
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.7}) for every $\ell\in\mathbb{N}%
$. Now, (\ref{pf.glinf.glinfact.welldef.ass3.indstep.2}) becomes%
\begin{align}
&  F_{a}\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right) \nonumber\\
&  =\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}\underbrace{F_{a}\left(
b_{\ell,0}\wedge b_{\ell,1}\wedge b_{\ell,2}\wedge...\right)  }%
_{\substack{=\sum\limits_{k\geq0}b_{\ell,0}\wedge b_{\ell,1}\wedge...\wedge
b_{\ell,k-1}\wedge\left(  a\rightharpoonup b_{\ell,k}\right)  \wedge
b_{\ell,k+1}\wedge b_{\ell,k+2}\wedge...\\\text{(by
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.7}))}}}\nonumber\\
&  =\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}\sum\limits_{k\geq0}b_{\ell
,0}\wedge b_{\ell,1}\wedge...\wedge b_{\ell,k-1}\wedge\left(  a\rightharpoonup
b_{\ell,k}\right)  \wedge b_{\ell,k+1}\wedge b_{\ell,k+2}\wedge...\nonumber\\
&  =\sum\limits_{k\geq0}\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}b_{\ell
,0}\wedge b_{\ell,1}\wedge...\wedge b_{\ell,k-1}\wedge\left(  a\rightharpoonup
b_{\ell,k}\right)  \wedge b_{\ell,k+1}\wedge b_{\ell,k+2}\wedge....
\label{pf.glinf.glinfact.welldef.ass3.indstep.8}%
\end{align}
It remains to prove that the right hand side of
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.7}) equals the right hand side of
(\ref{pf.glinf.glinfact.welldef.ass3.state}).

So our next goal is to show that every $k\in\mathbb{N}$ satisfies%
\begin{align}
&  \sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}b_{\ell,0}\wedge b_{\ell
,1}\wedge...\wedge b_{\ell,k-1}\wedge\left(  a\rightharpoonup b_{\ell
,k}\right)  \wedge b_{\ell,k+1}\wedge b_{\ell,k+2}\wedge...\nonumber\\
&  =b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(  a\rightharpoonup
b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge....
\label{pf.glinf.glinfact.welldef.ass3.indstep.13}%
\end{align}


\textit{Proof of (\ref{pf.glinf.glinfact.welldef.ass3.indstep.13}):} Fix
$k\in\mathbb{N}$. We need to prove that
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.13}) holds.

Let us define a sequence $\left(  \widetilde{b}_{0},\widetilde{b}%
_{1},\widetilde{b}_{2},...\right)  $ of vectors in $V$ by%
\[
\left(  \widetilde{b}_{i}=\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq k;\\
a\rightharpoonup b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i=k
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\mathbb{N}\right)  .
\]
Then, $\left(  \widetilde{b}_{0},\widetilde{b}_{1},\widetilde{b}%
_{2},...\right)  =\left(  b_{0},b_{1},...,b_{k-1},a\rightharpoonup
b_{k},b_{k+1},b_{k+2},...\right)  $.

For every $\ell\in\mathbb{Z}$, let us define a sequence $\left(
\widetilde{b}_{\ell,0},\widetilde{b}_{\ell,1},\widetilde{b}_{\ell
,2},...\right)  $ of vectors in $V$ by%
\[
\left(  \widetilde{b}_{\ell,i}=\left\{
\begin{array}
[c]{c}%
b_{\ell,i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq k;\\
a\rightharpoonup b_{\ell,i},\ \ \ \ \ \ \ \ \ \ \text{if }i=k
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\mathbb{N}\right)  .
\]
Then, for every $\ell\in\mathbb{Z}$, we have $\left(  \widetilde{b}_{\ell
,0},\widetilde{b}_{\ell,1},\widetilde{b}_{\ell,2},...\right)  =\left(
b_{\ell,0},b_{\ell,1},...,b_{\ell,k-1},a\rightharpoonup b_{\ell,k}%
,b_{\ell,k+1},b_{\ell,k+2},...\right)  $. Hence, for every $\ell\in\mathbb{Z}%
$, we have%
\begin{align}
&  \widetilde{b}_{\ell,0}\wedge\widetilde{b}_{\ell,1}\wedge\widetilde{b}%
_{\ell,2}\wedge...\nonumber\\
&  =b_{\ell,0}\wedge b_{\ell,1}\wedge...\wedge b_{\ell,k-1}\wedge\left(
a\rightharpoonup b_{\ell,k}\right)  \wedge b_{\ell,k+1}\wedge b_{\ell
,k+2}\wedge.... \label{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.0}%
\end{align}


Furthermore, for every $\ell\in\mathbb{Z}$, define an element $w_{\ell}$ of
$V$ by $w_{\ell}=\left\{
\begin{array}
[c]{c}%
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa\neq k;\\
a\rightharpoonup v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa=k
\end{array}
\right.  $. Since the sum $\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}v_{\ell
}$ has only finitely many nonzero addends, we have%
\[
\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}\left(  a\rightharpoonup v_{\ell
}\right)  =a\rightharpoonup\underbrace{\left(  \sum\limits_{\ell\in\mathbb{Z}%
}\beta_{\ell}v_{\ell}\right)  }_{=b_{\kappa}}=a\rightharpoonup b_{\kappa}.
\]
Now, the sum $\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}w_{\ell}$ has only
finitely many nonzero addends (since all but finitely many $\ell\in\mathbb{Z}$
satisfy $\beta_{\ell}=0$), and equals%
\begin{align}
\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}w_{\ell}  &  =\sum\limits_{\ell
\in\mathbb{Z}}\beta_{\ell}\left\{
\begin{array}
[c]{c}%
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa\neq k;\\
a\rightharpoonup v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa=k
\end{array}
\right. \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }w_{\ell}=\left\{
\begin{array}
[c]{c}%
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa\neq k;\\
a\rightharpoonup v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa=k
\end{array}
\right.  \text{ for every }\ell\in\mathbb{Z}\right) \nonumber\\
&  =\left\{
\begin{array}
[c]{c}%
\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}v_{\ell}%
,\ \ \ \ \ \ \ \ \ \ \text{if }\kappa\neq k;\\
\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}\left(  a\rightharpoonup v_{\ell
}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }\kappa=k
\end{array}
\right.  =\left\{
\begin{array}
[c]{c}%
b_{\kappa},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa\neq k;\\
a\rightharpoonup b_{\kappa},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa=k
\end{array}
\right. \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}v_{\ell}=b_{\kappa
}\text{ if }\kappa\neq k\text{,}\\
\text{and since }\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}\left(
a\rightharpoonup v_{\ell}\right)  =a\rightharpoonup b_{\kappa}\text{ if
}\kappa=k
\end{array}
\right) \nonumber\\
&  =\widetilde{b}_{\kappa}\ \ \ \ \ \ \ \ \ \ \left(  \text{since the
definition of }\widetilde{b}_{\kappa}\text{ yields }\widetilde{b}_{\kappa
}=\left\{
\begin{array}
[c]{c}%
b_{\kappa},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa\neq k;\\
a\rightharpoonup b_{\kappa},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa=k
\end{array}
\right.  \right)  . \label{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.1}%
\end{align}


Now, fix $\ell\in\mathbb{Z}$. It is easy to see that
\begin{equation}
\widetilde{b}_{\ell,i}=\left\{
\begin{array}
[c]{c}%
\widetilde{b}_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
w_{\ell}\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\mathbb{N}.
\label{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.2}%
\end{equation}
\footnote{\textit{Proof of
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.2}):} Let $i\in\mathbb{N}$.
We distinguish between two cases:
\par
\textit{Case 1:} We have $i\neq k$.
\par
\textit{Case 2:} We have $i=k$.
\par
Let us consider Case 1 first. In this case, $i\neq k$. By the definition of
$\widetilde{b}_{\ell,i}$, we have%
\begin{align}
\widetilde{b}_{\ell,i}  &  =\left\{
\begin{array}
[c]{c}%
b_{\ell,i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq k;\\
a\rightharpoonup b_{\ell,i},\ \ \ \ \ \ \ \ \ \ \text{if }i=k
\end{array}
\right.  =b_{\ell,i}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\neq k\right)
\nonumber\\
&  =\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }b_{\ell
,i}\right)  . \label{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.2.pf.1}%
\end{align}
But the definition of $\widetilde{b}_{i}$ yields $\widetilde{b}_{i}=\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq k;\\
a\rightharpoonup b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i=k
\end{array}
\right.  =b_{i}$ (since $i\neq k$). Thus, $b_{i}=\widetilde{b}_{i}$. Also, if
$i=\kappa$, then $\kappa\neq k$ (since $i\neq k$). Hence, if $i=\kappa$, then%
\begin{align*}
w_{\ell}  &  =\left\{
\begin{array}
[c]{c}%
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa\neq k;\\
a\rightharpoonup v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa=k
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }w_{\ell
}\right) \\
&  =v_{\ell}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\kappa\neq k\right)  .
\end{align*}
Hence, if $i=\kappa$, then $v_{\ell}=w_{\ell}$. Now,
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.2.pf.1}) becomes%
\begin{align*}
\widetilde{b}_{\ell,i}  &  =\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right.  =\left\{
\begin{array}
[c]{c}%
\widetilde{b}_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
w_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because we have }b_{i}=\widetilde{b}_{i}\text{ if }i\neq\kappa\text{
(this was proven above),}\\
\text{and because we have }v_{\ell}=w_{\ell}\text{ if }i=\kappa\text{ (this
was proven above)}%
\end{array}
\right)  .
\end{align*}
Thus, (\ref{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.2}) is proven in Case
1.
\par
Let us now consider Case 2. In this case, $i=k$. By the definition
$\widetilde{b}_{\ell,i}$, we have%
\begin{align}
\widetilde{b}_{\ell,i}  &  =\left\{
\begin{array}
[c]{c}%
b_{\ell,i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq k;\\
a\rightharpoonup b_{\ell,i},\ \ \ \ \ \ \ \ \ \ \text{if }i=k
\end{array}
\right.  =a\rightharpoonup b_{\ell,i}\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}i=k\right) \nonumber\\
&  =a\rightharpoonup\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right. \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }b_{\ell,i}=\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right.  \text{ (by the definition of }b_{\ell,i}\text{)}\right) \nonumber\\
&  =\left\{
\begin{array}
[c]{c}%
a\rightharpoonup b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
a\rightharpoonup v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right.  . \label{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.2.pf.2}%
\end{align}
But the definition of $\widetilde{b}_{i}$ yields $\widetilde{b}_{i}=\left\{
\begin{array}
[c]{c}%
b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq k;\\
a\rightharpoonup b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i=k
\end{array}
\right.  =a\rightharpoonup b_{i}$ (since $i=k$). Thus, $a\rightharpoonup
b_{i}=\widetilde{b}_{i}$. Also, if $i=\kappa$, then $\kappa=k$ (since $i=k$).
Hence, if $i=\kappa$, then%
\begin{align*}
w_{\ell}  &  =\left\{
\begin{array}
[c]{c}%
v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa\neq k;\\
a\rightharpoonup v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\kappa=k
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }w_{\ell
}\right) \\
&  =a\rightharpoonup v_{\ell}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }%
\kappa=k\right)  .
\end{align*}
Hence, if $i=\kappa$, then $a\rightharpoonup v_{\ell}=w_{\ell}$. Now,
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.2.pf.2}) becomes%
\begin{align*}
\widetilde{b}_{\ell,i}  &  =\left\{
\begin{array}
[c]{c}%
a\rightharpoonup b_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
a\rightharpoonup v_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right.  =\left\{
\begin{array}
[c]{c}%
\widetilde{b}_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i\neq\kappa;\\
w_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }i=\kappa
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because we have }a\rightharpoonup b_{i}=\widetilde{b}_{i}\text{ if
}i\neq\kappa\text{ (this was proven above),}\\
\text{and because we have }a\rightharpoonup v_{\ell}=w_{\ell}\text{ if
}i=\kappa\text{ (this was proven above)}%
\end{array}
\right)  .
\end{align*}
Thus, (\ref{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.2}) is proven in Case
2.
\par
We have thus proven (\ref{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.2}) in
each of the Cases 1 and 2. Since these two Cases cover all possibilities, this
yields that (\ref{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.2}) always
holds, qed.} In other words, $\left(  \widetilde{b}_{\ell,0},\widetilde{b}%
_{\ell,1},\widetilde{b}_{\ell,2},...\right)  =\left(  \widetilde{b}%
_{0},\widetilde{b}_{1},...,\widetilde{b}_{\kappa-1},w_{\ell},\widetilde{b}%
_{\kappa+1},\widetilde{b}_{\kappa+2},...\right)  $. Thus,%
\[
\widetilde{b}_{\ell,0}\wedge\widetilde{b}_{\ell,1}\wedge\widetilde{b}_{\ell
,2}\wedge...=\widetilde{b}_{0}\wedge\widetilde{b}_{1}\wedge...\wedge
\widetilde{b}_{\kappa-1}\wedge w_{\ell}\wedge\widetilde{b}_{\kappa+1}%
\wedge\widetilde{b}_{\kappa+2}\wedge....
\]
Compared with (\ref{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.0}), this
yields%
\begin{align}
&  b_{\ell,0}\wedge b_{\ell,1}\wedge...\wedge b_{\ell,k-1}\wedge\left(
a\rightharpoonup b_{\ell,k}\right)  \wedge b_{\ell,k+1}\wedge b_{\ell
,k+2}\wedge...\nonumber\\
&  =\widetilde{b}_{0}\wedge\widetilde{b}_{1}\wedge...\wedge\widetilde{b}%
_{\kappa-1}\wedge w_{\ell}\wedge\widetilde{b}_{\kappa+1}\wedge\widetilde{b}%
_{\kappa+2}\wedge.... \label{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.4}%
\end{align}


Now, forget that we fixed $\ell$. We thus have proven that
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.4}) holds for every
$\ell\in\mathbb{Z}$.

Now,%
\begin{align*}
&  \sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}\underbrace{b_{\ell,0}\wedge
b_{\ell,1}\wedge...\wedge b_{\ell,k-1}\wedge\left(  a\rightharpoonup
b_{\ell,k}\right)  \wedge b_{\ell,k+1}\wedge b_{\ell,k+2}\wedge...}%
_{\substack{=\widetilde{b}_{0}\wedge\widetilde{b}_{1}\wedge...\wedge
\widetilde{b}_{\kappa-1}\wedge w_{\ell}\wedge\widetilde{b}_{\kappa+1}%
\wedge\widetilde{b}_{\kappa+2}\wedge...\\\text{(by
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.4}))}}}\\
&  =\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}\widetilde{b}_{0}%
\wedge\widetilde{b}_{1}\wedge...\wedge\widetilde{b}_{\kappa-1}\wedge w_{\ell
}\wedge\widetilde{b}_{\kappa+1}\wedge\widetilde{b}_{\kappa+2}\wedge...\\
&  =\widetilde{b}_{0}\wedge\widetilde{b}_{1}\wedge...\wedge\widetilde{b}%
_{\kappa-1}\wedge\underbrace{\left(  \sum\limits_{\ell\in\mathbb{Z}}%
\beta_{\ell}w_{\ell}\right)  }_{\substack{=\widetilde{b}_{\kappa}\\\text{(by
(\ref{pf.glinf.glinfact.welldef.ass3.indstep.13.pf.1}))}}}\wedge
\widetilde{b}_{\kappa+1}\wedge\widetilde{b}_{\kappa+2}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the infinite wedge product is multilinear, and since the sum}\\
\sum\limits_{\ell\in\mathbb{Z}}\beta_{\ell}w_{\ell}\text{ has only finitely
many nonzero addends}%
\end{array}
\right) \\
&  =\widetilde{b}_{0}\wedge\widetilde{b}_{1}\wedge...\wedge\widetilde{b}%
_{\kappa-1}\wedge\widetilde{b}_{\kappa}\wedge\widetilde{b}_{\kappa+1}%
\wedge\widetilde{b}_{\kappa+2}\wedge...\\
&  =\widetilde{b}_{0}\wedge\widetilde{b}_{1}\wedge\widetilde{b}_{2}\wedge...\\
&  =b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(  a\rightharpoonup
b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  \widetilde{b}%
_{0},\widetilde{b}_{1},\widetilde{b}_{2},...\right)  =\left(  b_{0}%
,b_{1},...,b_{k-1},a\rightharpoonup b_{k},b_{k+1},b_{k+2},...\right)  \right)
.
\end{align*}
This proves (\ref{pf.glinf.glinfact.welldef.ass3.indstep.13}).

Now, (\ref{pf.glinf.glinfact.welldef.ass3.indstep.8}) becomes%
\begin{align*}
&  F_{a}\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right) \\
&  =\sum\limits_{k\geq0}\underbrace{\sum\limits_{\ell\in\mathbb{Z}}\beta
_{\ell}b_{\ell,0}\wedge b_{\ell,1}\wedge...\wedge b_{\ell,k-1}\wedge\left(
a\rightharpoonup b_{\ell,k}\right)  \wedge b_{\ell,k+1}\wedge b_{\ell
,k+2}\wedge...}_{\substack{=b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}%
\wedge\left(  a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge
b_{k+2}\wedge...\\\text{(by (\ref{pf.glinf.glinfact.welldef.ass3.indstep.13}%
))}}}\\
&  =\sum\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge....
\end{align*}
In other words, (\ref{pf.glinf.glinfact.welldef.ass3.state}) holds.

Now, forget that we fixed $a$ and $b_{0},b_{1},b_{2},...$. We thus have shown
the following result:
\[
\left(
\begin{array}
[c]{c}%
\text{If }a\text{ is an element of }\mathfrak{gl}_{\infty}\text{, and }%
b_{0},b_{1},b_{2},...\text{ are vectors in }V\text{ which satisfy
(\ref{pf.glinf.glinfact.welldef.ass3.stand}),}\\
\text{and satisfy (\ref{pf.glinf.glinfact.welldef.ass3.hook}) for }%
K=\kappa+1\text{, then (\ref{pf.glinf.glinfact.welldef.ass3.state}) holds}%
\end{array}
\right)  .
\]
In other words, we have shown that Assertion \ref{prop.glinf.glinfact.welldef}%
.3 holds for $K=\kappa+1$. This completes the inductive proof of Assertion
\ref{prop.glinf.glinfact.welldef}.3.

\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.2:} By the
assumptions, we know that $b_{i}=v_{m-i}$ for all sufficiently large $i$. In
other words, there exists a $K\in\mathbb{N}$ such that every $i\geq K$
satisfies $b_{i}=v_{m-i}$. Fix such a $K$. Then, every integer $i\geq K$
satisfies $b_{i}=v_{m-i}\in\left\{  v_{z}\ \mid\ z\in\mathbb{Z}\right\}  $
(since $m-i\in\mathbb{Z}$). In other words,
(\ref{pf.glinf.glinfact.welldef.ass3.hook}) is satisfied. Thus, we can apply
Assertion \ref{prop.glinf.glinfact.welldef}.3, and conclude that
\[
F_{a}\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right)  =\sum
\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge....
\]
This proves Assertion \ref{prop.glinf.glinfact.welldef}.2.
\end{verlong}

Next, here's something obvious that we are going to use a few times in the proof:

\begin{quote}
\textit{Assertion \ref{prop.glinf.glinfact.welldef}.4:} Let $f$ and $g$ be two
endomorphisms of the $\mathbb{C}$-vector space $\wedge^{\dfrac{\infty}{2},m}%
V$. If every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $ satisfies
$f\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =g\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $, then $f=g$.
\end{quote}

\begin{vershort}
This follows from the fact that $\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  _{\left(  i_{0},i_{1},i_{2},...\right)  \text{ is
an }m\text{-degression}}$ is a basis of the $\mathbb{C}$-vector space
$\wedge^{\dfrac{\infty}{2},m}V$.
\end{vershort}

\begin{verlong}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.4:} Assume that
every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $ satisfies
$f\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =g\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $. In other words,
the two maps $f$ and $g$ are equal to each other on every element of the
family $\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
_{\left(  i_{0},i_{1},i_{2},...\right)  \text{ is an }m\text{-degression}}$.

Let us recall the following fact from linear algebra: If $\mathfrak{A}$ and
$\mathfrak{B}$ are two $\mathbb{C}$-vector spaces, and $S$ is a basis of
$\mathfrak{A}$, and $\mathfrak{f}:\mathfrak{A}\rightarrow\mathfrak{B}$ and
$\mathfrak{g}:\mathfrak{A}\rightarrow\mathfrak{B}$ are two $\mathbb{C}$-linear
maps such that $f$ and $g$ are equal to each other on every element of $S$,
then $\mathfrak{f}=\mathfrak{g}$. Applying this fact to $\mathfrak{A}%
=\wedge^{\dfrac{\infty}{2},m}V$, $\mathfrak{B}=\wedge^{\dfrac{\infty}{2},m}V$,
$S=\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
_{\left(  i_{0},i_{1},i_{2},...\right)  \text{ is an }m\text{-degression}}$,
$\mathfrak{f}=f$ and $\mathfrak{g}=g$, we conclude that $f=g$. This proves
Assertion \ref{prop.glinf.glinfact.welldef}.4.
\end{verlong}

Next, we notice the following easy fact:

\begin{quote}
\textit{Assertion \ref{prop.glinf.glinfact.welldef}.5:} Let $a\in
\mathfrak{gl}_{\infty}$ and $b\in\mathfrak{gl}_{\infty}$. Let $\lambda
\in\mathbb{C}$ and $\mu\in\mathbb{C}$. Then, $\lambda F_{a}+\mu F_{b}%
=F_{\lambda a+\mu b}$ in the Lie algebra $\mathfrak{gl}\left(  \wedge
^{\dfrac{\infty}{2},m}V\right)  $.
\end{quote}

\begin{vershort}
This follows very quickly from the linearity of the definition of $F_{a}$ with
respect to $a$ (the details are left to the reader).
\end{vershort}

\begin{verlong}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.5:} Both maps
$F_{a}$ and $F_{b}$ are $\mathbb{C}$-linear (by their definitions). Hence, the
map $\lambda F_{a}+\mu F_{b}$ is $\mathbb{C}$-linear. On the other hand, the
map $F_{\lambda a+\mu b}$ is $\mathbb{C}$-linear (by its definition). Now,
every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $ satisfies%
\begin{align}
&  \left(  \lambda F_{a}+\mu F_{b}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  =\lambda\underbrace{F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)  }_{\substack{=\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\\text{(by the definition of
}F_{a}\text{)}}}+\mu\underbrace{F_{b}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  }_{\substack{=\sum\limits_{k\geq0}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}%
}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\\text{(by the
definition of }F_{b}\text{)}}}\nonumber\\
&  =\lambda\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\mu\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...
\label{pf.glinf.glinfact.welldef.ass4.1}%
\end{align}
and%
\begin{align}
&  F_{\lambda a+\mu b}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right) \nonumber\\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  \underbrace{\left(  \lambda a+\mu b\right)  \rightharpoonup
v_{i_{k}}}_{=\lambda a\rightharpoonup v_{i_{k}}+\mu b\rightharpoonup v_{i_{k}%
}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }F_{\lambda a+\mu
b}\right) \nonumber\\
&  =\sum\limits_{k\geq0}\underbrace{v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  \lambda a\rightharpoonup v_{i_{k}}+\mu
b\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}%
\wedge...}_{\substack{=\lambda v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...+\mu v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\\\text{(by Proposition
\ref{prop.semiinfwedge.welldef} \textbf{(c)})}}}\nonumber\\
&  =\sum\limits_{k\geq0}\left(  \lambda v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\right. \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left.  +\mu v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}%
}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\right) \nonumber\\
&  =\lambda\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\mu\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge....
\label{pf.glinf.glinfact.welldef.ass4.2}%
\end{align}
Hence, every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $ satisfies%
\begin{align*}
&  \left(  \lambda F_{a}+\mu F_{b}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right) \\
&  =\lambda\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ +\mu\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.glinf.glinfact.welldef.ass4.1})}\right) \\
&  =F_{\lambda a+\mu b}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.glinf.glinfact.welldef.ass4.2})}\right)  .
\end{align*}
Hence, Assertion \ref{prop.glinf.glinfact.welldef}.4 (applied to $f=\lambda
F_{a}+\mu F_{b}$ and $g=F_{\lambda a+\mu b}$) yields that $\lambda F_{a}+\mu
F_{b}=F_{\lambda a+\mu b}$. This proves Assertion
\ref{prop.glinf.glinfact.welldef}.5.
\end{verlong}

Here is something rather simple:

\begin{quote}
\textit{Assertion \ref{prop.glinf.glinfact.welldef}.6:} Let $i\in\mathbb{Z}$
and $j\in\mathbb{Z}$. Let $m\in\mathbb{Z}$. Let $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ be a straying $m$-degression.

\textbf{(a)} For every $\ell\in\mathbb{N}$, the sequence $\left(  i_{0}%
,i_{1},...,i_{\ell-1},i,i_{\ell+1},i_{\ell+2},...\right)  $ is a straying $m$-degression.

\textbf{(b)} If $j\notin\left\{  i_{0},i_{1},i_{2},...\right\}  $, then
$F_{E_{i,j}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=0$.

\textbf{(c)} If there exists a \textbf{unique} $\ell\in\mathbb{N}$ such that
$j=i_{\ell}$, then we have%
\[
F_{E_{i,j}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{L-1}}\wedge v_{i}\wedge
v_{i_{L+1}}\wedge v_{i_{L+2}}\wedge...,
\]
where $L$ is the unique $\ell\in\mathbb{N}$ such that $j=i_{\ell}$.


\end{quote}

The proof of Assertion \ref{prop.glinf.glinfact.welldef}.6 is as
straightforward as one would expect: it is a matter of substituting
$a=E_{i,j}$ and $b_{k}=v_{i_{k}}$ into Assertion
\ref{prop.glinf.glinfact.welldef}.2 and taking care of the few addends which
are not $0$.

\begin{verlong}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.6:} By the
definition of $E_{i,j}$, we have%
\begin{equation}
E_{i,j}v_{u}=\delta_{j,u}v_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
u\in\mathbb{Z}. \label{pf.glinf.glinfact.welldef.ass6.pf.Eij}%
\end{equation}


We have%
\begin{equation}
v_{i_{i}}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all sufficiently large }i.
\label{pf.glinf.glinfact.welldef.ass6.pf.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.glinf.glinfact.welldef.ass6.pf.1}):} We
know that $\left(  i_{0},i_{1},i_{2},...\right)  $ is a straying
$m$-degression. By the definition of a straying $m$-degression, this rewrites
as follows: Every sufficiently high $k\in\mathbb{N}$ satisfies $i_{k}+k=m$. In
other words, every sufficiently high $k\in\mathbb{N}$ satisfies $i_{k}=m-k$.
Hence, every sufficiently high $k\in\mathbb{N}$ satisfies $v_{i_{k}}=v_{m-k}$.
Renaming the variable $k$ as $i$ in this result, we conclude: Every
sufficiently high $i\in\mathbb{N}$ satisfies $v_{i_{i}}=v_{m-i}$. This proves
(\ref{pf.glinf.glinfact.welldef.ass6.pf.1}).}. Hence, Assertion
\ref{prop.glinf.glinfact.welldef}.2 (applied to $a=E_{i,j}$ and $b_{k}%
=v_{i_{k}}$) yields%
\begin{align}
&  F_{E_{i,j}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge
...\right) \nonumber\\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\underbrace{\left(  E_{i,j}\rightharpoonup v_{i_{k}}\right)
}_{\substack{=E_{i,j}v_{i_{k}}=\delta_{j,i_{k}}v_{i}\\\text{(by
(\ref{pf.glinf.glinfact.welldef.ass6.pf.Eij}), applied to }u=i_{k}\text{)}%
}}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\nonumber\\
&  =\sum\limits_{k\geq0}\underbrace{v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  \delta_{j,i_{k}}v_{i}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...}_{\substack{=\delta_{j,i_{k}}\cdot v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i}\wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\\\text{(since the infinite wedge product is
multilinear)}}}\nonumber\\
&  =\sum\limits_{k\geq0}\delta_{j,i_{k}}\cdot v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge v_{i}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}%
}\wedge.... \label{pf.glinf.glinfact.welldef.ass6.pf.2}%
\end{align}


\textbf{(a)} Let $\ell\in\mathbb{N}$. We know that $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ is a straying $m$-degression. By the definition of a
straying $m$-degression, this rewrites as follows: Every sufficiently high
$k\in\mathbb{N}$ satisfies $i_{k}+k=m$. In other words, there exists a
$K\in\mathbb{N}$ such that every integer $k\geq K$ satisfies $i_{k}+k=m$.
Consider this $k$.

Define a sequence $\left(  i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime
},...\right)  $ of integers by%
\[
\left(  i_{k}^{\prime}=\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq\ell;\\
i,\ \ \ \ \ \ \ \ \ \ \text{if }k=\ell
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{N}\right)  .
\]
Then, $\left(  i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right)
=\left(  i_{0},i_{1},...,i_{\ell-1},i,i_{\ell+1},i_{\ell+2},...\right)  $.

Now, let $k$ be any integer such that $k\geq\max\left\{  \ell+1,K\right\}  $.
Then, $k\geq\max\left\{  \ell+1,K\right\}  \geq K$, so that $i_{k}+k=m$. And
since $k\geq\max\left\{  \ell+1,K\right\}  \geq\ell+1$, we have $k\in
\mathbb{N}$ (since $\ell\in\mathbb{N}$). Moreover, since $k\geq\ell+1>\ell$,
we have $k\neq\ell$. Now, by the definition of $i_{k}^{\prime}$, we have%
\[
i_{k}^{\prime}=\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq\ell;\\
i,\ \ \ \ \ \ \ \ \ \ \text{if }k=\ell
\end{array}
\right.  =i_{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\neq\ell\right)  ,
\]
so that $i_{k}^{\prime}+k=i_{k}+k=m$. Now, forget that we fixed $k$. We thus
have proven that every integer $k$ satisfying $k\geq\max\left\{
\ell+1,K\right\}  $ satisfies $i_{k}^{\prime}+k=m$. In other words, the
sequence $\left(  i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right)  $
is a straying $m$-degression. Since $\left(  i_{0}^{\prime},i_{1}^{\prime
},i_{2}^{\prime},...\right)  =\left(  i_{0},i_{1},...,i_{\ell-1},i,i_{\ell
+1},i_{\ell+2},...\right)  $, this rewrites as follows: The sequence $\left(
i_{0},i_{1},...,i_{\ell-1},i,i_{\ell+1},i_{\ell+2},...\right)  $ is a straying
$m$-degression. This proves Assertion \ref{prop.glinf.glinfact.welldef}.6
\textbf{(a)}.

\textbf{(b)} Assume that $j\notin\left\{  i_{0},i_{1},i_{2},...\right\}  $.
Then,
\begin{equation}
\text{every }k\in\mathbb{N}\text{ satisfies }\delta_{j,i_{k}}=0
\label{pf.glinf.glinfact.welldef.ass6.pf.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.glinf.glinfact.welldef.ass6.pf.3}):} Let
$k\in\mathbb{N}$. Then, $j\neq i_{k}$ (because otherwise, we would have
$j=i_{k}\in\left\{  i_{0},i_{1},i_{2},...\right\}  $, which contradicts
$j\notin\left\{  i_{0},i_{1},i_{2},...\right\}  $). Thus, $\delta_{j,i_{k}}%
=0$. This proves (\ref{pf.glinf.glinfact.welldef.ass6.pf.3}).}. Now,
(\ref{pf.glinf.glinfact.welldef.ass6.pf.2}) becomes%
\begin{align*}
&  F_{E_{i,j}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge
...\right) \\
&  =\sum\limits_{k\geq0}\underbrace{\delta_{j,i_{k}}}_{\substack{=0\\\text{(by
(\ref{pf.glinf.glinfact.welldef.ass6.pf.3}))}}}\cdot v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{k-1}}\wedge v_{i}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}%
}\wedge...\\
&  =\sum\limits_{k\geq0}0\cdot v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge v_{i}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...=0.
\end{align*}
This proves Assertion \ref{prop.glinf.glinfact.welldef}.6 \textbf{(b)}.

\textbf{(c)} Assume that there exists a \textbf{unique} $\ell\in\mathbb{N}$
such that $j=i_{\ell}$. Denote this $\ell$ by $L$.

Recall that $L$ is an $\ell\in\mathbb{N}$ such that $j=i_{\ell}$. Hence,
$j=i_{L}$.

Recall that $L$ is a \textbf{unique} $\ell\in\mathbb{N}$ such that $j=i_{\ell
}$. From the uniqueness in this statement, we conclude that there exists no
$k\in\mathbb{N}$ satisfying $j=i_{k}$ and $k\neq L$. In other words, no
$k\in\mathbb{N}$ satisfying $k\neq L$ can satisfy $j=i_{k}$. In other words,
every $k\in\mathbb{N}$ satisfying $k\neq L$ satisfies $j\neq i_{k}$. Hence,
\begin{equation}
\text{every }k\in\mathbb{N}\text{ satisfying }k\neq L\text{ satisfies }%
\delta_{j,i_{k}}=0. \label{pf.glinf.glinfact.welldef.ass6.pf.4}%
\end{equation}


Now, (\ref{pf.glinf.glinfact.welldef.ass6.pf.2}) becomes%
\begin{align*}
&  F_{E_{i,j}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge
...\right) \\
&  =\sum\limits_{k\geq0}\delta_{j,i_{k}}\cdot v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge v_{i}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}%
}\wedge...\\
&  =\underbrace{\delta_{j,i_{L}}}_{\substack{=1\\\text{(since }j=i_{L}%
\text{)}}}\cdot v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{L-1}}\wedge
v_{i}\wedge v_{i_{L+1}}\wedge v_{i_{L+2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{k\geq0;\\k\neq L}%
}\underbrace{\delta_{j,i_{k}}}_{\substack{=0\\\text{(by
(\ref{pf.glinf.glinfact.welldef.ass6.pf.4}))}}}\cdot v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{k-1}}\wedge v_{i}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}%
}\wedge...\\
&  =\underbrace{1\cdot v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{L-1}%
}\wedge v_{i}\wedge v_{i_{L+1}}\wedge v_{i_{L+2}}\wedge...}_{=v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{L-1}}\wedge v_{i}\wedge v_{i_{L+1}}\wedge
v_{i_{L+2}}\wedge...}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum\limits_{\substack{k\geq0;\\k\neq
L}}0\cdot v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge
v_{i}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...}_{=0}\\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{L-1}}\wedge v_{i}\wedge
v_{i_{L+1}}\wedge v_{i_{L+2}}\wedge....
\end{align*}
This proves Assertion \ref{prop.glinf.glinfact.welldef}.6 \textbf{(c)}.
\end{verlong}

Now here is something less obvious:

\begin{quote}
\textit{Assertion \ref{prop.glinf.glinfact.welldef}.7:} Every $a\in
\mathfrak{gl}_{\infty}$ and $b\in\mathfrak{gl}_{\infty}$ satisfy $\left[
F_{a},F_{b}\right]  =F_{\left[  a,b\right]  }$ in the Lie algebra
$\mathfrak{gl}\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  $.
\end{quote}

There are two possible approaches to proving Assertion
\ref{prop.glinf.glinfact.welldef}.7.

\begin{landscape}
\textit{First proof of Assertion \ref{prop.glinf.glinfact.welldef}.7
(sketched):} In order to prove Assertion \ref{prop.glinf.glinfact.welldef}.7,
it is enough to show that%
\begin{equation}
\left[  F_{a},F_{b}\right]  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)  =F_{\left[  a,b\right]  }\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)
\label{pf.glinf.glinfact.welldef.ass7.pf.short.2.goal}%
\end{equation}
for every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $. (Indeed,
once this is done, $\left[  F_{a},F_{b}\right]  =F_{\left[  a,b\right]  }$
will follow from Assertion \ref{prop.glinf.glinfact.welldef}.4.) So let
$\left(  i_{0},i_{1},i_{2},...\right)  $ be any $m$-degression. Then,%
\begin{align*}
&  F_{a}\left(  F_{b}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \right)  \\
&  =F_{a}\left(  \sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\right)  \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }F_{b}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)\text{ is defined as } \sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge... \right)  \\
&  =\sum\limits_{k\geq0}F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\right)  \\
&  =\sum\limits_{q\geq0}\underbrace{F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{q-1}}\wedge\left(  b\rightharpoonup v_{i_{q}}\right)
\wedge v_{i_{q+1}}\wedge v_{i_{q+2}}\wedge...\right)  }_{\substack{=\sum
\limits_{k\geq0}\left\{
\begin{array}
[c]{l}%
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(
b\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}%
\wedge...\wedge v_{i_{q-1}}\wedge\left(  a\rightharpoonup v_{i_{q}}\right)
\wedge v_{i_{q+1}}\wedge v_{i_{q+2}},\ \ \ \ \ \ \ \ \ \ \text{if }k<q;\\
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{q-1}}\wedge\left(  \left(
ab\right)  \rightharpoonup v_{i_{q}}\right)  \wedge v_{i_{q+1}}\wedge
v_{i_{q+2}}\wedge...,\ \ \ \ \ \ \ \ \ \ \text{if }k=q;\\
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{q-1}}\wedge\left(
a\rightharpoonup v_{i_{q}}\right)  \wedge v_{i_{q+1}}\wedge v_{i_{q+2}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}},\ \ \ \ \ \ \ \ \ \ \text{if }k>q
\end{array}
\right.  \\\text{(by an application of Assertion
\ref{prop.glinf.glinfact.welldef}.2)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}k\text{ as }q\right)  \\
&  =\sum\limits_{q\geq0}\sum\limits_{k\geq0}\left\{
\begin{array}
[c]{l}%
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(
b\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}%
\wedge...\wedge v_{i_{q-1}}\wedge\left(  a\rightharpoonup v_{i_{q}}\right)
\wedge v_{i_{q+1}}\wedge v_{i_{q+2}}\wedge...,\ \ \ \ \ \ \ \ \ \ \text{if
}k<q;\\
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{q-1}}\wedge\left(  \left(
ab\right)  \rightharpoonup v_{i_{q}}\right)  \wedge v_{i_{q+1}}\wedge
v_{i_{q+2}}\wedge...,\ \ \ \ \ \ \ \ \ \ \text{if }k=q;\\
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{q-1}}\wedge\left(
a\rightharpoonup v_{i_{q}}\right)  \wedge v_{i_{q+1}}\wedge v_{i_{q+2}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...,\ \ \ \ \ \ \ \ \ \ \text{if
}k>q
\end{array}
\right.
\end{align*}%
\begin{align}
&  =\sum\limits_{q\geq0}\sum\limits_{\substack{k\geq0;\\k<q}}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}%
}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\wedge v_{i_{q-1}%
}\wedge\left(  a\rightharpoonup v_{i_{q}}\right)  \wedge v_{i_{q+1}}\wedge
v_{i_{q+2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{q\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{q-1}}\wedge\left(  \left(  ab\right)  \rightharpoonup
v_{i_{q}}\right)  \wedge v_{i_{q+1}}\wedge v_{i_{q+2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{q\geq0}\sum\limits_{\substack{k\geq
0;\\k>q}}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{q-1}}\wedge\left(
a\rightharpoonup v_{i_{q}}\right)  \wedge v_{i_{q+1}}\wedge v_{i_{q+2}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  b\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\nonumber\\
&  =\sum\limits_{q\geq0}\sum\limits_{\substack{p\geq0;\\p<q}}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{p-1}}\wedge\left(  b\rightharpoonup v_{i_{p}%
}\right)  \wedge v_{i_{p+1}}\wedge v_{i_{p+2}}\wedge...\wedge v_{i_{q-1}%
}\wedge\left(  a\rightharpoonup v_{i_{q}}\right)  \wedge v_{i_{q+1}}\wedge
v_{i_{q+2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  \left(  ab\right)  \rightharpoonup
v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{p\geq0}\sum\limits_{\substack{q\geq
0;\\q>p}}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{p-1}}\wedge\left(
a\rightharpoonup v_{i_{p}}\right)  \wedge v_{i_{p+1}}\wedge v_{i_{p+2}}%
\wedge...\wedge v_{i_{q-1}}\wedge\left(  b\rightharpoonup v_{i_{q}}\right)
\wedge v_{i_{q+1}}\wedge v_{i_{q+2}}\wedge
...\label{pf.glinf.glinfact.welldef.ass7.pf.short.1}%
\end{align}
\footnote{In the last step of this computation, we did the following
substitutions:
\par
-- We renamed the index $k$ as $p$ in the second sum.
\par
-- We renamed the index $q$ as $k$ in the third sum.
\par
-- We switched the meanings of the indices $p$ and $q$ in the fourth and fifth
sums.}. Similarly,%
\begin{align}
& F_{b}\left(  F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \right)  \nonumber\\
& =\sum\limits_{q\geq0}\sum\limits_{\substack{p\geq0;\\p<q}}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{p-1}}\wedge\left(  a\rightharpoonup v_{i_{p}%
}\right)  \wedge v_{i_{p+1}}\wedge v_{i_{p+2}}\wedge...\wedge v_{i_{q-1}%
}\wedge\left(  b\rightharpoonup v_{i_{q}}\right)  \wedge v_{i_{q+1}}\wedge
v_{i_{q+2}}\wedge...\nonumber\\
& \ \ \ \ \ \ \ \ \ \ +\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  \left(  ba\right)  \rightharpoonup
v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\nonumber\\
& \ \ \ \ \ \ \ \ \ \ +\sum\limits_{p\geq0}\sum\limits_{\substack{q\geq
0;\\q>p}}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{p-1}}\wedge\left(
b\rightharpoonup v_{i_{p}}\right)  \wedge v_{i_{p+1}}\wedge v_{i_{p+2}}%
\wedge...\wedge v_{i_{q-1}}\wedge\left(  a\rightharpoonup v_{i_{q}}\right)
\wedge v_{i_{q+1}}\wedge v_{i_{q+2}}\wedge
....\label{pf.glinf.glinfact.welldef.ass7.pf.short.2}%
\end{align}
\end{landscape}


Now, let us subtract (\ref{pf.glinf.glinfact.welldef.ass7.pf.short.2}) from
(\ref{pf.glinf.glinfact.welldef.ass7.pf.short.1}). I am claiming that the
first term on the right hand side of
(\ref{pf.glinf.glinfact.welldef.ass7.pf.short.2}) cancels against the third
term on the right hand side of
(\ref{pf.glinf.glinfact.welldef.ass7.pf.short.1}). Indeed, in order to see
this, one needs to check that one can interchange the order of summation in
the sum%
\[
\sum\limits_{q\geq0}\sum\limits_{\substack{p\geq0;\\p<q}}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{p-1}}\wedge\left(  b\rightharpoonup v_{i_{p}%
}\right)  \wedge v_{i_{p+1}}\wedge v_{i_{p+2}}\wedge...\wedge v_{i_{q-1}%
}\wedge\left(  a\rightharpoonup v_{i_{q}}\right)  \wedge v_{i_{q+1}}\wedge
v_{i_{q+2}}\wedge...,
\]
i. e., replace $\sum\limits_{q\geq0}\sum\limits_{\substack{p\geq0;\\p<q}}$ by
$\sum\limits_{p\geq0}\sum\limits_{\substack{q\geq0;\\q>p}}$. This is easy to
see (indeed, one must show that \newline$v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{p-1}}\wedge\left(  b\rightharpoonup v_{i_{p}}\right)
\wedge v_{i_{p+1}}\wedge v_{i_{p+2}}\wedge...\wedge v_{i_{q-1}}\wedge\left(
a\rightharpoonup v_{i_{q}}\right)  \wedge v_{i_{q+1}}\wedge v_{i_{q+2}}%
\wedge...=0$ for all but finitely many \textbf{pairs} $\left(  i,j\right)
\in\mathbb{N}^{2}$), but not trivial a priori\footnote{Here is a cautionary
tale on why one cannot always interchange summation in infinite sums. Define a
family $\left(  \alpha_{p,q}\right)  _{\left(  p,q\right)  \in\mathbb{N}^{2}}$
of integers by $\alpha_{p,q}=\left\{
\begin{array}
[c]{c}%
1,\text{ if }p=q;\\
-1,\text{ if }p=q+1
\end{array}
\right.  $. Then, every $q\in\mathbb{N}$ satisfies $\sum\limits_{p\geq0}%
\alpha_{p,q}=0$. Hence, $\sum\limits_{q\geq0}\sum\limits_{p\geq0}\alpha
_{p,q}=0$. On the other hand, every $p\in\mathbb{N}$ satisfies $\sum
\limits_{q\geq0}\alpha_{p,q}=\delta_{p,0}$. Hence, $\sum\limits_{p\geq0}%
\sum\limits_{q\geq0}\alpha_{p,q}=1\neq0=\sum\limits_{q\geq0}\sum
\limits_{p\geq0}\alpha_{p,q}$. So the two summation signs in this situation
cannot be interchanged, even though all sums (both inner and outer) converge
in the discrete topology. Generally, for a family $\left(  \lambda
_{p,q}\right)  _{\left(  p,q\right)  \in\mathbb{N}^{2}}$ of elements of an
additive group, we are guaranteed to have $\sum\limits_{p\geq0}\sum
\limits_{q\geq0}\lambda_{p,q}=\sum\limits_{q\geq0}\sum\limits_{p\geq0}%
\lambda_{p,q}$ if the \textbf{double sum }$\sum\limits_{\left(  p,q\right)
\in\mathbb{N}^{2}}\lambda_{p,q}$ still converges in the discrete topology
(this is analogous to Fubini's theorem). But the double sum $\sum
\limits_{\left(  p,q\right)  \in\mathbb{N}^{2}}\alpha_{p,q}$ does not converge
in the discrete topology, so $\sum\limits_{p\geq0}\sum\limits_{q\geq0}%
\alpha_{p,q}\neq\sum\limits_{q\geq0}\sum\limits_{p\geq0}\alpha_{p,q}$ should
not come as a surprise.}. So we know that the first term on the right hand
side of (\ref{pf.glinf.glinfact.welldef.ass7.pf.short.2}) cancels against the
third term on the right hand side of
(\ref{pf.glinf.glinfact.welldef.ass7.pf.short.1}). Similarly, the third term
on the right hand side of (\ref{pf.glinf.glinfact.welldef.ass7.pf.short.2})
cancels against the first term on the right hand side of
(\ref{pf.glinf.glinfact.welldef.ass7.pf.short.1}). Thus, when we subtract
(\ref{pf.glinf.glinfact.welldef.ass7.pf.short.2}) from
(\ref{pf.glinf.glinfact.welldef.ass7.pf.short.1}), on the right hand side only
the second terms of both equations remain, and we obtain%
\begin{align*}
&  F_{a}\left(  F_{b}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \right)  -F_{b}\left(  F_{a}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right) \\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  \left(  ab\right)  \rightharpoonup v_{i_{k}}\right)  \wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ -\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  \left(  ba\right)  \rightharpoonup
v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  \left(  ab-ba\right)  \rightharpoonup v_{i_{k}}\right)  \wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the multilinearity of the infinite
wedge product}\right) \\
&  =F_{ab-ba}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge
...\right)  =F_{\left[  a,b\right]  }\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  .
\end{align*}
This proves (\ref{pf.glinf.glinfact.welldef.ass7.pf.short.2.goal}), and thus
Assertion \ref{prop.glinf.glinfact.welldef}.7. Filling the details of this
proof is left to the reader.

\begin{vershort}
\textit{Second proof of Assertion \ref{prop.glinf.glinfact.welldef}.7
(sketched):} Due to Assertion \ref{prop.glinf.glinfact.welldef}.5, the value
of $F_{c}$ for $c\in\mathfrak{gl}_{\infty}$ depends $\mathbb{C}$-linearly on
$c$.

But we must prove the equality $\left[  F_{a},F_{b}\right]  =F_{\left[
a,b\right]  }$ for all $a\in\mathfrak{gl}_{\infty}$ and $b\in\mathfrak{gl}%
_{\infty}$. This equality is $\mathbb{C}$-linear in $a$ and $b$ (since the
value of $F_{c}$ for $c\in\mathfrak{gl}_{\infty}$ depends $\mathbb{C}%
$-linearly on $c$), so it is enough to show it only when $a$ and $b$ belong to
the basis $\left(  E_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$
of $\mathfrak{gl}_{\infty}$. But in this case, one can check this equality by
verifying that every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $
satisfies%
\[
\left[  F_{a},F_{b}\right]  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)  =F_{\left[  a,b\right]  }\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  .
\]
This can be done (using Assertion \ref{prop.glinf.glinfact.welldef}.6) by a
straightforward distinction of cases (the cases depend on whether some indices
belong to $\left\{  i_{0},i_{1},i_{2},...\right\}  $ or not, and whether some
indices are equal or not). The reader should not have much of a trouble
supplying these arguments, but they are as unenlightening as one would expect.
There \textbf{is} a somewhat better way to do this verification (better in the
sense that less cases have to be considered) by means of exploiting some
symmetry; this relies on checking the following assertion:
\end{vershort}

\begin{verlong}
We will soon show a second proof of Assertion
\ref{prop.glinf.glinfact.welldef}.7 in more detail. This proof relies
substantially on the following assertion:
\end{verlong}

\begin{quote}
\textit{Assertion \ref{prop.glinf.glinfact.welldef}.8:} Let $r$, $s$, $u$ and
$v$ be integers. Let $m\in\mathbb{Z}$. Let $\left(  i_{0},i_{1},i_{2}%
,...\right)  $ be an $m$-degression. Let $I$ denote the set $\left\{
i_{0},i_{1},i_{2},...\right\}  $.

\textbf{(a)} If $v\notin I$, then
\[
\left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =0.
\]


\textbf{(b)} If $s=v$, then
\[
\left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =0.
\]


\textbf{(c)} Assume that $s\neq v$. Let $\mathbf{w}:\mathbb{Z}\rightarrow
\mathbb{Z}$ be the function defined by
\[
\left(  \mathbf{w}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=v;\\
k,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for all }k\in\mathbb{Z}\right)  .
\]
\footnote{Here, the term $\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=v;\\
k,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  $ makes sense, since $s\neq v$.} Then, $\left(  \mathbf{w}\left(
i_{0}\right)  ,\mathbf{w}\left(  i_{1}\right)  ,\mathbf{w}\left(
i_{2}\right)  ,...\right)  $ is a straying $m$-degression, and satisfies
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\left[  s\in I\right]  \cdot\left[  v\in I\right]  \cdot v_{\mathbf{w}%
\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge....
\end{align*}


Here, whenever $\mathcal{A}$ is an assertion, we denote by $\left[
\mathcal{A}\right]  $ the integer $\left\{
\begin{array}
[c]{l}%
1,\text{ if }\mathcal{A}\text{ is true;}\\
0,\text{ if }\mathcal{A}\text{ is wrong}%
\end{array}
\right.  $.
\end{quote}

\begin{vershort}
The proof of this assertion, as well as the derivation of Assertion
\ref{prop.glinf.glinfact.welldef}.7 from it (Assertion
\ref{prop.glinf.glinfact.welldef}.8 must be applied twice), is left to the reader.
\end{vershort}

\begin{verlong}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.8:} We know that
$\left(  i_{0},i_{1},i_{2},...\right)  $ is an $m$-degression, hence a
strictly decreasing straying $m$-degression (since the $m$-degressions are
exactly the strictly decreasing straying $m$-degressions). Since $\left(
i_{0},i_{1},i_{2},...\right)  $ is strictly decreasing, the numbers $i_{0}$,
$i_{1}$, $i_{2}$, $...$ are pairwise distinct. Hence,%
\begin{equation}
\left\{  i_{0},i_{1},...,i_{w-1},i_{w+1},i_{w+2},...\right\}  =I\setminus
\left\{  i_{w}\right\}  \ \ \ \ \ \ \ \ \ \ \text{for every }w\in\mathbb{N}.
\label{pf.glinf.glinfact.welldef.8.pf.allbutone}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.glinf.glinfact.welldef.8.pf.allbutone}):}
Let $w\in\mathbb{N}$. We have $i_{w}\neq i_{p}$ for every $p\in\mathbb{N}$
satisfying $w\neq p$ (since the numbers $i_{0}$, $i_{1}$, $i_{2}$, $...$ are
pairwise distinct). Thus,%
\begin{align*}
i_{w}  &  \notin\left\{  i_{p}\ \mid\ \underbrace{p\in\mathbb{N};\ w\neq
p}_{\substack{\text{this is equivalent to}\\p\in\left\{  q\in\mathbb{N}%
\ \mid\ w\neq q\right\}  }}\right\}  =\left\{  i_{p}\ \mid\ p\in
\underbrace{\left\{  q\in\mathbb{N}\ \mid\ w\neq q\right\}  }_{=\left\{
0,1,...,w-1,w+1,w+2,...\right\}  }\right\} \\
&  =\left\{  i_{p}\ \mid\ p\in\left\{  0,1,...,w-1,w+1,w+2,...\right\}
\right\}  =\left\{  i_{0},i_{1},...,i_{w-1},i_{w+1},i_{w+2},...\right\}  .
\end{align*}
Combined with $\left\{  i_{0},i_{1},...,i_{w-1},i_{w+1},i_{w+2},...\right\}
\subseteq\left\{  i_{0},i_{1},i_{2},...\right\}  $, this yields that
\[
\left\{  i_{0},i_{1},...,i_{w-1},i_{w+1},i_{w+2},...\right\}  \subseteq
\left\{  i_{0},i_{1},i_{2},...\right\}  \setminus\left\{  i_{w}\right\}  .
\]
Combined with%
\begin{align*}
&  \underbrace{\left\{  i_{0},i_{1},i_{2},...\right\}  }_{\substack{=\left\{
i_{0},i_{1},...,i_{w-1},i_{w},i_{w+1},i_{w+2},...\right\}  \\=\left\{
i_{0},i_{1},...,i_{w-1},i_{w+1},i_{w+2},...\right\}  \cup\left\{
i_{w}\right\}  }}\setminus\left\{  i_{w}\right\} \\
&  =\left(  \left\{  i_{0},i_{1},...,i_{w-1},i_{w+1},i_{w+2},...\right\}
\cup\left\{  i_{w}\right\}  \right)  \setminus\left\{  i_{w}\right\} \\
&  =\underbrace{\left(  \left\{  i_{0},i_{1},...,i_{w-1},i_{w+1}%
,i_{w+2},...\right\}  \setminus\left\{  i_{w}\right\}  \right)  }%
_{\subseteq\left\{  i_{0},i_{1},...,i_{w-1},i_{w+1},i_{w+2},...\right\}  }%
\cup\underbrace{\left(  \left\{  i_{w}\right\}  \setminus\left\{
i_{w}\right\}  \right)  }_{=\varnothing}\\
&  \subseteq\left\{  i_{0},i_{1},...,i_{w-1},i_{w+1},i_{w+2},...\right\}
\cup\varnothing=\left\{  i_{0},i_{1},...,i_{w-1},i_{w+1},i_{w+2},...\right\}
,
\end{align*}
this yields $\left\{  i_{0},i_{1},...,i_{w-1},i_{w+1},i_{w+2},...\right\}
=\left\{  i_{0},i_{1},i_{2},...\right\}  \setminus\left\{  i_{w}\right\}  $.
Since $\left\{  i_{0},i_{1},i_{2},...\right\}  =I$, this rewrites as $\left\{
i_{0},i_{1},...,i_{w-1},i_{w+1},i_{w+2},...\right\}  =I\setminus\left\{
i_{w}\right\}  $. Thus, (\ref{pf.glinf.glinfact.welldef.8.pf.allbutone}) is
proven.}

\textbf{(a)} Assume that $v\notin I$. Thus, $v\notin I=\left\{  i_{0}%
,i_{1},i_{2},...\right\}  $. Thus, $F_{E_{u,v}}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =0$ (by Assertion
\ref{prop.glinf.glinfact.welldef}.6 \textbf{(b)}, applied to $i=u$ and $j=v$)
and $F_{E_{r,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =0$ (by Assertion \ref{prop.glinf.glinfact.welldef}.6
\textbf{(b)}, applied to $i=r$ and $j=v$). Hence,%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =F_{E_{r,s}}\left(  \underbrace{F_{E_{u,v}}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }_{=0}\right)  -\delta
_{s,u}\underbrace{F_{E_{r,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)  }_{=0}\\
&  =\underbrace{F_{E_{r,s}}\left(  0\right)  }_{\substack{=0\\\text{(since
}F_{E_{r,s}}\text{ is linear)}}}-\underbrace{\delta_{s,u}0}_{=0}=0-0=0.
\end{align*}
This proves Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(a)}.

\textbf{(b)} Assume that $s=v$. If $v\notin I$, then Assertion
\ref{prop.glinf.glinfact.welldef}.8 \textbf{(b)} is obviously
true\footnote{\textit{Proof.} Assume that $v\notin I$. Then, due to Assertion
\ref{prop.glinf.glinfact.welldef}.8 \textbf{(a)}, we have $\left(  F_{E_{r,s}%
}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  =0$. In other words, Assertion
\ref{prop.glinf.glinfact.welldef}.8 \textbf{(b)} is true, qed.}. Hence, for
the rest of the proof of Assertion \ref{prop.glinf.glinfact.welldef}.8
\textbf{(b)}, we can WLOG assume that we don't have $v\notin I$. Assume this.

So we have $v\in I$ (since we don't have $v\notin I$). Thus, there exists a
\textbf{unique} $\ell\in\mathbb{N}$ such that $v=i_{\ell}$%
.\ \ \ \ \footnote{\textit{Proof:} Since $v\in I=\left\{  i_{0},i_{1}%
,i_{2},...\right\}  $, there exists at least one $\ell\in\mathbb{N}$ such that
$v=i_{\ell}$.
\par
Now, let $\ell_{1}$ and $\ell_{2}$ be two elements $\ell$ of $\mathbb{N}$ such
that $v=i_{\ell}$. Then, $v=i_{\ell_{1}}$ (since $\ell_{1}$ is an element
$\ell$ of $\mathbb{N}$ such that $v=i_{\ell}$) and $v=i_{\ell_{2}}$
(similarly). Hence, $i_{\ell_{1}}=v=i_{\ell_{2}}$. If we had $\ell_{1}\neq
\ell_{2}$, then we would have $i_{\ell_{1}}\neq i_{\ell_{2}}$ (since the
numbers $i_{0}$, $i_{1}$, $i_{2}$, $...$ are pairwise distinct), which would
contradict $i_{\ell_{1}}=i_{\ell_{2}}$. Thus, we cannot have $\ell_{1}\neq
\ell_{2}$. Hence, $\ell_{1}=\ell_{2}$.
\par
Now, forget that we fixed $\ell_{1}$ and $\ell_{2}$. We thus have proven that
if $\ell_{1}$ and $\ell_{2}$ be two elements $\ell$ of $\mathbb{N}$ such that
$v=i_{\ell}$, then $\ell_{1}=\ell_{2}$. In other words, there exists at most
one $\ell\in\mathbb{N}$ such that $v=i_{\ell}$. Hence, there exists a
\textbf{unique} $\ell\in\mathbb{N}$ such that $v=i_{\ell}$ (since we also know
that there exists at least one $\ell\in\mathbb{N}$ such that $v=i_{\ell}$),
qed.} Denote this $\ell$ by $L$. Then, $v=i_{L}$ (since $L$ is an $\ell
\in\mathbb{N}$ such that $v=i_{\ell}$). Now, define a sequence $\left(
i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right)  $ by%
\[
\left(  i_{k}^{\prime}=\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{N}\right)  .
\]
Then, $\left(  i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right)
=\left(  i_{0},i_{1},...,i_{L-1},u,i_{L+1},i_{L+2},...\right)  $. Thus,
\begin{align}
\left\{  i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right\}   &
=\left\{  i_{0},i_{1},...,i_{L-1},u,i_{L+1},i_{L+2},...\right\} \nonumber\\
&  =\left\{  u\right\}  \cup\underbrace{\left\{  i_{0},i_{1},...,i_{L-1}%
,i_{L+1},i_{L+2},...\right\}  }_{\substack{=I\setminus\left\{  i_{L}\right\}
\\\text{(by (\ref{pf.glinf.glinfact.welldef.8.pf.allbutone}), applied to
}w=L\text{)}}}\nonumber\\
&  =\left\{  u\right\}  \cup\left(  I\setminus\left\{  \underbrace{i_{L}}%
_{=v}\right\}  \right)  =\left\{  u\right\}  \cup\left(  I\setminus\left\{
v\right\}  \right)  . \label{pf.glinf.glinfact.welldef.8.pf.b.1}%
\end{align}


By the definition of $i_{L}^{\prime}$, we have $i_{L}^{\prime}=\left\{
\begin{array}
[c]{c}%
i_{L},\ \ \ \ \ \ \ \ \ \ \text{if }L\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }L=L
\end{array}
\right.  =u$ (since $L=L$).

On the other hand, $L$ is the \textbf{unique} $\ell\in\mathbb{N}$ such that
$v=i_{\ell}$. Hence, Assertion \ref{prop.glinf.glinfact.welldef}.6
\textbf{(c)} (applied to $u$ and $v$ instead of $i$ and $j$) yields
\begin{align}
F_{E_{u,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{L-1}}\wedge v_{u}\wedge
v_{i_{L+1}}\wedge v_{i_{L+2}}\wedge...\nonumber\\
&  =v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}}\wedge v_{i_{2}^{\prime}}%
\wedge...\label{pf.glinf.glinfact.welldef.8.pf.b.2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{L-1}%
,u,i_{L+1},i_{L+2},...\right)  =\left(  i_{0}^{\prime},i_{1}^{\prime}%
,i_{2}^{\prime},...\right)  \right)  .\nonumber
\end{align}


Moreover, $\left(  i_{0},i_{1},...,i_{L-1},u,i_{L+1},i_{L+2},...\right)  $ is
a straying $m$-degression (by Assertion \ref{prop.glinf.glinfact.welldef}.6
\textbf{(a)}, applied to $u$, $u$ and $L$ instead of $i$, $j$ and $\ell$). In
other words, $\left(  i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right)
$ is a straying $m$-degression (since $\left(  i_{0}^{\prime},i_{1}^{\prime
},i_{2}^{\prime},...\right)  =\left(  i_{0},i_{1},...,i_{L-1},u,i_{L+1}%
,i_{L+2},...\right)  $).

We now distinguish between two cases:

\textit{Case 1:} We have $s=u$.

\textit{Case 2:} We don't have $s=u$.

Let us first consider Case 1. In this case, $s=u$. Hence, $s=u=i_{L}^{\prime}%
$. Thus,
\begin{equation}
\text{there exists at least one }\ell\in\mathbb{N}\text{ satisfying }%
s=i_{\ell}^{\prime} \label{pf.glinf.glinfact.welldef.8.pf.b.case1.1}%
\end{equation}
(namely, $\ell=L$). But there exists at most one $\ell\in\mathbb{N}$
satisfying $s=i_{\ell}^{\prime}$\ \ \ \ \footnote{\textit{Proof.} Let $\ell
\in\mathbb{N}$ satisfy $s=i_{\ell}^{\prime}$. We will prove that $\ell=L$.
\par
Assume (for the sake of contradiction) that $\ell\neq L$. Then, by the
definition of $i_{\ell}^{\prime}$, we have $i_{\ell}^{\prime}=\left\{
\begin{array}
[c]{c}%
i_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\ell\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }\ell=L
\end{array}
\right.  =i_{\ell}$ (since $\ell\neq L$). But since $\ell\in\mathbb{N}$ and
$\ell\neq L$, we have $\ell\in\mathbb{N}\setminus\left\{  L\right\}  =\left\{
0,1,...,L-1,L+1,L+2,...\right\}  $, so that $i_{L}\in\left\{  i_{0}%
,i_{1},...,i_{L-1},i_{L+1},i_{L+2},...\right\}  =I\setminus\left\{
i_{L}\right\}  $ (by (\ref{pf.glinf.glinfact.welldef.8.pf.allbutone}), applied
to $w=L$). Thus, $s=i_{\ell}^{\prime}=i_{\ell}\in I\setminus\left\{
i_{L}\right\}  =I\setminus\left\{  v\right\}  $ (since $i_{L}=v$).
Consequently, $s\neq v$. This contradicts $s=v$. This contradiction shows that
our assumption (that $\ell\neq L$) was wrong. Hence, $\ell=L$.
\par
Now forget that we fixed $\ell$. We thus have shown that every $\ell
\in\mathbb{N}$ satisfying $s=i_{\ell}^{\prime}$ must satisfy $\ell=L$. In
other words, every $\ell\in\mathbb{N}$ satisfying $s=i_{\ell}^{\prime}$ must
equal $L$. Hence, there exists at most one $\ell\in\mathbb{N}$ satisfying
$s=i_{\ell}^{\prime}$, qed.}. Combined with
(\ref{pf.glinf.glinfact.welldef.8.pf.b.case1.1}), this yields that there
exists a \textbf{unique }$\ell\in\mathbb{N}$ such that $s=i_{\ell}^{\prime}$.
This $\ell$ is $L$ (because $s=i_{L}^{\prime}$). Therefore, we can apply
Assertion \ref{prop.glinf.glinfact.welldef}.6 \textbf{(c)} to $i_{k}^{\prime}%
$, $r$ and $s$ instead of $i_{k}$, $i$ and $j$. As a result, we obtain%
\[
F_{E_{r,s}}\left(  v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}}\wedge
v_{i_{2}^{\prime}}\wedge...\right)  =v_{i_{0}^{\prime}}\wedge v_{i_{1}%
^{\prime}}\wedge...\wedge v_{i_{L-1}^{\prime}}\wedge v_{r}\wedge
v_{i_{L+1}^{\prime}}\wedge v_{i_{L+2}^{\prime}}\wedge....
\]
Thus,%
\begin{align}
&  v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}}\wedge...\wedge v_{i_{L-1}%
^{\prime}}\wedge v_{r}\wedge v_{i_{L+1}^{\prime}}\wedge v_{i_{L+2}^{\prime}%
}\wedge...\nonumber\\
&  =F_{E_{r,s}}\left(  \underbrace{v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}%
}\wedge v_{i_{2}^{\prime}}\wedge...}_{\substack{=F_{E_{u,v}}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.b.2}))}}}\right) \nonumber\\
&  =F_{E_{r,s}}\left(  F_{E_{u,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  \right)  .
\label{pf.glinf.glinfact.welldef.8.pf.b.case1.3}%
\end{align}


On the other hand, define a sequence $\left(  i_{0}^{\prime\prime}%
,i_{1}^{\prime\prime},i_{2}^{\prime\prime},...\right)  $ by%
\[
\left(  i_{k}^{\prime\prime}=\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{N}\right)  .
\]
Then, $\left(  i_{0}^{\prime\prime},i_{1}^{\prime\prime},i_{2}^{\prime\prime
},...\right)  =\left(  i_{0},i_{1},...,i_{L-1},r,i_{L+1},i_{L+2},...\right)  $.

By the definition of $i_{L}^{\prime\prime}$, we have $i_{L}^{\prime\prime
}=\left\{
\begin{array}
[c]{c}%
i_{L},\ \ \ \ \ \ \ \ \ \ \text{if }L\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }L=L
\end{array}
\right.  =r$ (since $L=L$).

But we have%
\begin{equation}
i_{k}^{\prime\prime}=\left\{
\begin{array}
[c]{c}%
i_{k}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{N}
\label{pf.glinf.glinfact.welldef.8.pf.b.case1.5}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.glinf.glinfact.welldef.8.pf.b.case1.5}):}
Let $k\in\mathbb{N}$.
\par
Notice that $\left\{
\begin{array}
[c]{c}%
i_{L}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }L\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }L=L
\end{array}
\right.  =r$ (since $L=L$). Thus, $i_{L}^{\prime\prime}=r=\left\{
\begin{array}
[c]{c}%
i_{L}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }L\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }L=L
\end{array}
\right.  $. In other words, (\ref{pf.glinf.glinfact.welldef.8.pf.b.case1.5})
holds in the case when $k=L$. Hence, for the rest of the proof of
(\ref{pf.glinf.glinfact.welldef.8.pf.b.case1.5}), we can WLOG assume that
$k=L$ does not hold. Let us assume this.
\par
We have assumed that $k=L$ does not hold. In other words, $k\neq L$. By the
definition of $i_{k}^{\prime\prime}$, we have $i_{k}^{\prime\prime}=\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  =i_{k}$ (since $k\neq L$). But%
\begin{align*}
\left\{
\begin{array}
[c]{c}%
i_{k}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.   &  =i_{k}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\neq
L\right) \\
&  =\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }i_{k}%
^{\prime}\right) \\
&  =i_{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\neq L\right)  .
\end{align*}
Compared with $i_{k}^{\prime\prime}=i_{k}$, this yields $i_{k}^{\prime\prime
}=\left\{
\begin{array}
[c]{c}%
i_{k}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  $. This proves (\ref{pf.glinf.glinfact.welldef.8.pf.b.case1.5}).}.
Hence, $\left(  i_{0}^{\prime\prime},i_{1}^{\prime\prime},i_{2}^{\prime\prime
},...\right)  =\left(  i_{0}^{\prime},i_{1}^{\prime},...,i_{L-1}^{\prime
},r,i_{L+1}^{\prime},i_{L+2}^{\prime},...\right)  $.

Recall that $L$ is the \textbf{unique} $\ell\in\mathbb{N}$ such that
$v=i_{\ell}$. Hence, Assertion \ref{prop.glinf.glinfact.welldef}.6
\textbf{(c)} (applied to $r$ and $v$ instead of $i$ and $j$) yields
\begin{align}
F_{E_{r,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{L-1}}\wedge v_{r}\wedge
v_{i_{L+1}}\wedge v_{i_{L+2}}\wedge...\nonumber\\
&  =v_{i_{0}^{\prime\prime}}\wedge v_{i_{1}^{\prime\prime}}\wedge
v_{i_{2}^{\prime\prime}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{L-1}%
,r,i_{L+1},i_{L+2},...\right)  =\left(  i_{0}^{\prime\prime},i_{1}%
^{\prime\prime},i_{2}^{\prime\prime},...\right)  \right) \nonumber\\
&  =v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}}\wedge...\wedge v_{i_{L-1}%
^{\prime}}\wedge v_{r}\wedge v_{i_{L+1}^{\prime}}\wedge v_{i_{L+2}^{\prime}%
}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0}^{\prime\prime}%
,i_{1}^{\prime\prime},i_{2}^{\prime\prime},...\right)  =\left(  i_{0}^{\prime
},i_{1}^{\prime},...,i_{L-1}^{\prime},r,i_{L+1}^{\prime},i_{L+2}^{\prime
},...\right)  \right) \nonumber\\
&  =F_{E_{r,s}}\left(  F_{E_{u,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  \right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.glinf.glinfact.welldef.8.pf.b.case1.3})}\right)  .
\label{pf.glinf.glinfact.welldef.8.pf.b.case1.9}%
\end{align}


Now,%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =F_{E_{r,s}}\left(  F_{E_{u,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  \right)  -\underbrace{\delta_{s,u}}%
_{\substack{=1\\\text{(since }s=u\text{)}}}\underbrace{F_{E_{r,v}}\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }%
_{\substack{=F_{E_{r,s}}\left(  F_{E_{u,v}}\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  \right)  \\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.b.case1.9}))}}}\\
&  =F_{E_{r,s}}\left(  F_{E_{u,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  \right)  -F_{E_{r,s}}\left(  F_{E_{u,v}}\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right) \\
&  =0.
\end{align*}
Thus, Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(b)} is proven in
Case 1.

Let us now consider Case 2. In this case, we don't have $s=u$. Thus, $s\neq
u$. Hence, $s\notin\left\{  u\right\}  $. Combined with $s=v\notin
I\setminus\left\{  v\right\}  $, this yields $s\notin\left\{  u\right\}
\cup\left(  I\setminus\left\{  v\right\}  \right)  =\left\{  i_{0}^{\prime
},i_{1}^{\prime},i_{2}^{\prime},...\right\}  $ (by
(\ref{pf.glinf.glinfact.welldef.8.pf.b.1})). Hence, Assertion
\ref{prop.glinf.glinfact.welldef}.6 \textbf{(b)} (applied to $i_{k}^{\prime}$,
$r$ and $s$ instead of $i_{k}$, $i$ and $j$) yields
\begin{equation}
F_{E_{r,s}}\left(  v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}}\wedge
v_{i_{2}^{\prime}}\wedge...\right)  =0.
\label{pf.glinf.glinfact.welldef.8.pf.b.case2.1}%
\end{equation}
Thus,%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =F_{E_{r,s}}\left(  \underbrace{F_{E_{u,v}}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }_{\substack{=v_{i_{0}^{\prime}%
}\wedge v_{i_{1}^{\prime}}\wedge v_{i_{2}^{\prime}}\wedge...\\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.b.case2.1}))}}}\right)
-\underbrace{\delta_{s,u}}_{\substack{=0\\\text{(since }s\neq u\text{)}%
}}F_{E_{r,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
\\
&  =\underbrace{F_{E_{r,s}}\left(  v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}%
}\wedge v_{i_{2}^{\prime}}\wedge...\right)  }_{\substack{=0\\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.b.case2.1}))}}}-\underbrace{0F_{E_{r,v}%
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }%
_{=0}=0-0=0.
\end{align*}
Thus, Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(b)} is proven in
Case 2.

Hence, Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(b)} is proven in
each of the Cases 1 and 2. Thus, Assertion \ref{prop.glinf.glinfact.welldef}.8
\textbf{(b)} is proven in all cases (since Cases 1 and 2 cover all possible situations).

\textbf{(c)} It is easy to see that $\left(  \mathbf{w}\left(  i_{0}\right)
,\mathbf{w}\left(  i_{1}\right)  ,\mathbf{w}\left(  i_{2}\right)  ,...\right)
$ is a straying $m$-degression\footnote{\textit{Proof.} Recall that $\left(
i_{0},i_{1},i_{2},...\right)  $ is a straying $m$-degression. Due to the
definition of a straying $m$-degression, this means that every sufficiently
high $k\in\mathbb{N}$ satisfies $i_{k}+k=m$. Thus, we know that every
sufficiently high $k\in\mathbb{N}$ satisfies $i_{k}+k=m$. In other words,
there exists a $K\in\mathbb{N}$ such that every nonnegative integer $k\geq K$
satisfies $i_{k}+k=m$. Consider this $k$.
\par
Now, let $k$ be a nonnegative integer such that $k\geq\max\left\{
K,m-s+1,m-v+1\right\}  $. Then, $k\geq\max\left\{  K,m-s+1,m-v+1\right\}  \geq
K$. Hence, $i_{k}+k=m$, so that $i_{k}=m-k$. Moreover, $k\geq\max\left\{
K,m-s+1,m-v+1\right\}  \geq m-s+1>m-s$, so that $m-k<s$. Thus, $m-k\neq s$.
Also, $k\geq\max\left\{  K,m-s+1,m-v+1\right\}  \geq m-v+1>m-v$, so that
$m-k<v$. Hence, $m-k\neq v$. Since $m-k\neq s$ and $m-k\neq v$, we have
neither $m-k=s$ nor $m-k=v$. Now,%
\begin{align*}
\mathbf{w}\left(  \underbrace{i_{k}}_{=m-k}\right)   &  =\mathbf{w}\left(
m-k\right)  =\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }m-k=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }m-k=v;\\
m-k,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }%
\mathbf{w}\left(  m-k\right)  \right) \\
&  =m-k\ \ \ \ \ \ \ \ \ \ \left(  \text{since we have neither }m-k=s\text{
nor }m-k=v\right)
\end{align*}
and thus $\mathbf{w}\left(  i_{k}\right)  +k=m$.
\par
Now, forget that we fixed $k$. We thus have shown that every nonnegative
integer $k\geq\max\left\{  K,m-s+1,m-v+1\right\}  $ satisfies $\mathbf{w}%
\left(  i_{k}\right)  +k=m$. Hence, every sufficiently high $k\in\mathbb{N}$
satisfies $\mathbf{w}\left(  i_{k}\right)  +k=m$. According the definition of
a straying $m$-degression, this means precisely that $\left(  \mathbf{w}%
\left(  i_{0}\right)  ,\mathbf{w}\left(  i_{1}\right)  ,\mathbf{w}\left(
i_{2}\right)  ,...\right)  $ is a straying $m$-degression. We have thus proven
that $\left(  \mathbf{w}\left(  i_{0}\right)  ,\mathbf{w}\left(  i_{1}\right)
,\mathbf{w}\left(  i_{2}\right)  ,...\right)  $ is a straying $m$-degression,
qed.}. Thus, in order to prove Assertion \ref{prop.glinf.glinfact.welldef}.8
\textbf{(c)}, it only remains to show that%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\left[  s\in I\right]  \cdot\left[  v\in I\right]  \cdot v_{\mathbf{w}%
\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge....
\end{align*}


If $v\notin I$, then Assertion \ref{prop.glinf.glinfact.welldef}.8
\textbf{(c)} is obviously true\footnote{\textit{Proof.} Assume that $v\notin
I$. Then, due to Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(a)},
we have $\left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =0$.
\par
But since $v\notin I$, the assertion $v\in I$ does not hold. Hence, $\left[
v\in I\right]  =0$. Thus,
\[
\left[  s\in I\right]  \cdot\underbrace{\left[  v\in I\right]  }_{=0}\cdot
v_{\mathbf{w}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)
}\wedge v_{\mathbf{w}\left(  i_{2}\right)  }\wedge...=0.
\]
Thus,%
\[
\left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =0=\left[  s\in
I\right]  \cdot\left[  v\in I\right]  \cdot v_{\mathbf{w}\left(  i_{0}\right)
}\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge v_{\mathbf{w}\left(
i_{2}\right)  }\wedge....
\]
In other words, Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(c)} is
true, qed.}. Hence, for the rest of the proof of Assertion
\ref{prop.glinf.glinfact.welldef}.8 \textbf{(c)}, we can WLOG assume that we
don't have $v\notin I$. Assume this.

So we have $v\in I$ (since we don't have $v\notin I$). Thus, there exists a
\textbf{unique} $\ell\in\mathbb{N}$ such that $v=i_{\ell}$%
.\ \ \ \ \footnote{\textit{Proof:} Since $v\in I=\left\{  i_{0},i_{1}%
,i_{2},...\right\}  $, there exists at least one $\ell\in\mathbb{N}$ such that
$v=i_{\ell}$.
\par
Now, let $\ell_{1}$ and $\ell_{2}$ be two elements $\ell$ of $\mathbb{N}$ such
that $v=i_{\ell}$. Then, $v=i_{\ell_{1}}$ (since $\ell_{1}$ is an element
$\ell$ of $\mathbb{N}$ such that $v=i_{\ell}$) and $v=i_{\ell_{2}}$
(similarly). Hence, $i_{\ell_{1}}=v=i_{\ell_{2}}$. If we had $\ell_{1}\neq
\ell_{2}$, then we would have $i_{\ell_{1}}\neq i_{\ell_{2}}$ (since the
numbers $i_{0}$, $i_{1}$, $i_{2}$, $...$ are pairwise distinct), which would
contradict $i_{\ell_{1}}=i_{\ell_{2}}$. Thus, we cannot have $\ell_{1}\neq
\ell_{2}$. Hence, $\ell_{1}=\ell_{2}$.
\par
Now, forget that we fixed $\ell_{1}$ and $\ell_{2}$. We thus have proven that
if $\ell_{1}$ and $\ell_{2}$ be two elements $\ell$ of $\mathbb{N}$ such that
$v=i_{\ell}$, then $\ell_{1}=\ell_{2}$. In other words, there exists at most
one $\ell\in\mathbb{N}$ such that $v=i_{\ell}$. Hence, there exists a
\textbf{unique} $\ell\in\mathbb{N}$ such that $v=i_{\ell}$ (since we also know
that there exists at least one $\ell\in\mathbb{N}$ such that $v=i_{\ell}$),
qed.} Denote this $\ell$ by $L$. Then, $v=i_{L}$ (since $L$ is an $\ell
\in\mathbb{N}$ such that $v=i_{\ell}$). Now, define a sequence $\left(
i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right)  $ by%
\[
\left(  i_{k}^{\prime}=\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{N}\right)  .
\]
Then, $\left(  i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right)
=\left(  i_{0},i_{1},...,i_{L-1},u,i_{L+1},i_{L+2},...\right)  $. Thus,
\begin{align}
\left\{  i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right\}   &
=\left\{  i_{0},i_{1},...,i_{L-1},u,i_{L+1},i_{L+2},...\right\} \nonumber\\
&  =\left\{  u\right\}  \cup\underbrace{\left\{  i_{0},i_{1},...,i_{L-1}%
,i_{L+1},i_{L+2},...\right\}  }_{\substack{=I\setminus\left\{  i_{L}\right\}
\\\text{(by (\ref{pf.glinf.glinfact.welldef.8.pf.allbutone}), applied to
}w=L\text{)}}}\nonumber\\
&  =\left\{  u\right\}  \cup\left(  I\setminus\left\{  \underbrace{i_{L}}%
_{=v}\right\}  \right)  =\left\{  u\right\}  \cup\left(  I\setminus\left\{
v\right\}  \right)  . \label{pf.glinf.glinfact.welldef.8.pf.c.1}%
\end{align}


By the definition of $i_{L}^{\prime}$, we have $i_{L}^{\prime}=\left\{
\begin{array}
[c]{c}%
i_{L},\ \ \ \ \ \ \ \ \ \ \text{if }L\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }L=L
\end{array}
\right.  =u$ (since $L=L$).

On the other hand, $L$ is the \textbf{unique} $\ell\in\mathbb{N}$ such that
$v=i_{\ell}$. Hence, Assertion \ref{prop.glinf.glinfact.welldef}.6
\textbf{(c)} (applied to $u$ and $v$ instead of $i$ and $j$) yields
\begin{align}
F_{E_{u,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{L-1}}\wedge v_{u}\wedge
v_{i_{L+1}}\wedge v_{i_{L+2}}\wedge...\nonumber\\
&  =v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}}\wedge v_{i_{2}^{\prime}}%
\wedge...\label{pf.glinf.glinfact.welldef.8.pf.c.2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{L-1}%
,u,i_{L+1},i_{L+2},...\right)  =\left(  i_{0}^{\prime},i_{1}^{\prime}%
,i_{2}^{\prime},...\right)  \right)  .\nonumber
\end{align}


Moreover, $\left(  i_{0},i_{1},...,i_{L-1},u,i_{L+1},i_{L+2},...\right)  $ is
a straying $m$-degression (by Assertion \ref{prop.glinf.glinfact.welldef}.6
\textbf{(a)}, applied to $u$, $u$ and $L$ instead of $i$, $j$ and $\ell$). In
other words, $\left(  i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right)
$ is a straying $m$-degression (since $\left(  i_{0}^{\prime},i_{1}^{\prime
},i_{2}^{\prime},...\right)  =\left(  i_{0},i_{1},...,i_{L-1},u,i_{L+1}%
,i_{L+2},...\right)  $).

We now distinguish between two cases:

\textit{Case 1:} We have $s=u$.

\textit{Case 2:} We don't have $s=u$.

Let us first consider Case 1. In this case, $s=u$. Hence, $s=u=i_{L}^{\prime}$.

Let us define a sequence $\left(  i_{0}^{\prime\prime},i_{1}^{\prime\prime
},i_{2}^{\prime\prime},...\right)  $ by%
\[
\left(  i_{k}^{\prime\prime}=\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{N}\right)  .
\]
Then, $\left(  i_{0}^{\prime\prime},i_{1}^{\prime\prime},i_{2}^{\prime\prime
},...\right)  =\left(  i_{0},i_{1},...,i_{L-1},r,i_{L+1},i_{L+2},...\right)  $.

By the definition of $i_{L}^{\prime\prime}$, we have $i_{L}^{\prime\prime
}=\left\{
\begin{array}
[c]{c}%
i_{L},\ \ \ \ \ \ \ \ \ \ \text{if }L\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }L=L
\end{array}
\right.  =r$ (since $L=L$).

But $L$ is the \textbf{unique} $\ell\in\mathbb{N}$ such that $v=i_{\ell}$.
Hence, Assertion \ref{prop.glinf.glinfact.welldef}.6 \textbf{(c)} (applied to
$r$ and $v$ instead of $i$ and $j$) yields
\begin{align}
F_{E_{r,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{L-1}}\wedge v_{r}\wedge
v_{i_{L+1}}\wedge v_{i_{L+2}}\wedge...\nonumber\\
&  =v_{i_{0}^{\prime\prime}}\wedge v_{i_{1}^{\prime\prime}}\wedge
v_{i_{2}^{\prime\prime}}\wedge
...\label{pf.glinf.glinfact.welldef.8.pf.c.case1.o.1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{L-1}%
,r,i_{L+1},i_{L+2},...\right)  =\left(  i_{0}^{\prime\prime},i_{1}%
^{\prime\prime},i_{2}^{\prime\prime},...\right)  \right)  .\nonumber
\end{align}


Now, we distinguish between the following two subcases:

\textit{Subcase 1.1:} We have $s\in I$.

\textit{Subcase 1.2:} We don't have $s\in I$.

Let us consider Subcase 1.1 first. In this subcase, we have $s\in I$. Thus,
there exists a \textbf{unique} $\ell\in\mathbb{N}$ such that $s=i_{\ell}%
$.\ \ \ \ \footnote{\textit{Proof:} Since $s\in I=\left\{  i_{0},i_{1}%
,i_{2},...\right\}  $, there exists at least one $\ell\in\mathbb{N}$ such that
$s=i_{\ell}$.
\par
Now, let $\ell_{1}$ and $\ell_{2}$ be two elements $\ell$ of $\mathbb{N}$ such
that $s=i_{\ell}$. Then, $s=i_{\ell_{1}}$ (since $\ell_{1}$ is an element
$\ell$ of $\mathbb{N}$ such that $s=i_{\ell}$) and $s=i_{\ell_{2}}$
(similarly). Hence, $i_{\ell_{1}}=s=i_{\ell_{2}}$. If we had $\ell_{1}\neq
\ell_{2}$, then we would have $i_{\ell_{1}}\neq i_{\ell_{2}}$ (since the
numbers $i_{0}$, $i_{1}$, $i_{2}$, $...$ are pairwise distinct), which would
contradict $i_{\ell_{1}}=i_{\ell_{2}}$. Thus, we cannot have $\ell_{1}\neq
\ell_{2}$. Hence, $\ell_{1}=\ell_{2}$.
\par
Now, forget that we fixed $\ell_{1}$ and $\ell_{2}$. We thus have proven that
if $\ell_{1}$ and $\ell_{2}$ be two elements $\ell$ of $\mathbb{N}$ such that
$s=i_{\ell}$, then $\ell_{1}=\ell_{2}$. In other words, there exists at most
one $\ell\in\mathbb{N}$ such that $s=i_{\ell}$. Hence, there exists a
\textbf{unique} $\ell\in\mathbb{N}$ such that $s=i_{\ell}$ (since we also know
that there exists at least one $\ell\in\mathbb{N}$ such that $s=i_{\ell}$),
qed.} Denote this $\ell$ by $M$. Thus, $s=i_{M}$ (since $M$ is an $\ell
\in\mathbb{N}$ such that $s=i_{\ell}$). Since $i_{M}=s\neq v=i_{L}$, we have
$M\neq L$. Now, by the definition of $i_{M}^{\prime}$, we have%
\begin{align*}
i_{M}^{\prime}  &  =\left\{
\begin{array}
[c]{c}%
i_{M},\ \ \ \ \ \ \ \ \ \ \text{if }M\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }M=L
\end{array}
\right.  =i_{M}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }M\neq L\right) \\
&  =s=u=i_{L}^{\prime}.
\end{align*}
Hence, $v_{i_{M}^{\prime}}=v_{i_{L}^{\prime}}$. Since $M\neq L$, this yields
that two of the vectors $v_{i_{0}^{\prime}},v_{i_{1}^{\prime}},v_{i_{2}%
^{\prime}},...$ are equal (namely, the vectors $v_{i_{M}^{\prime}}$ and
$v_{i_{L}^{\prime}}$). Hence, (\ref{glinf.semiinfwedge.antisym}) (applied to
$b_{k}=v_{i_{k}^{\prime}}$) yields that $v_{i_{0}^{\prime}}\wedge
v_{i_{1}^{\prime}}\wedge v_{i_{2}^{\prime}}\wedge...=0$. Thus,
(\ref{pf.glinf.glinfact.welldef.8.pf.c.2}) becomes%
\begin{equation}
F_{E_{u,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}}\wedge v_{i_{2}^{\prime}}%
\wedge...=0. \label{pf.glinf.glinfact.welldef.8.pf.c.case1.1.2}%
\end{equation}


Let $S_{\infty}$ be the group of all finitary permutations of $\mathbb{N}$.
Let $\tau\in S_{\infty}$ be the transposition $\left(  L,M\right)  $ (this is
well-defined because $M\neq L$). Clearly, $\left(  -1\right)  ^{\tau}=-1$
(since $\tau$ is a transposition).

Since $\left(  \mathbf{w}\left(  i_{0}\right)  ,\mathbf{w}\left(
i_{1}\right)  ,\mathbf{w}\left(  i_{2}\right)  ,...\right)  $ is a straying
$m$-degression, we know that every sufficiently high $k\in\mathbb{N}$
satisfies $\mathbf{w}\left(  i_{k}\right)  +k=m$ (by the definition of a
``straying $m$-degression''). In other words, we have $\mathbf{w}\left(
i_{k}\right)  +k=m$ for sufficiently large $k$. Renaming $k$ as $i$ in this
assertion, we conclude: We have $\mathbf{w}\left(  i_{i}\right)  +i=m$ for
sufficiently large $i$. In other words, $\mathbf{w}\left(  i_{i}\right)  =m-i$
for sufficiently large $i$. Therefore, $v_{\mathbf{w}\left(  i_{i}\right)
}=v_{m-i}$ for sufficiently large $i$. Hence, applying Proposition
\ref{prop.semiinfwedge.welldef} \textbf{(f)} to $b_{k}=v_{\mathbf{w}\left(
i_{k}\right)  }$ and $\pi=\tau$, we obtain%
\begin{align}
v_{\mathbf{w}\left(  i_{\tau\left(  0\right)  }\right)  }\wedge v_{\mathbf{w}%
\left(  i_{\tau\left(  1\right)  }\right)  }\wedge v_{\mathbf{w}\left(
i_{\tau\left(  2\right)  }\right)  }\wedge...  &  =\underbrace{\left(
-1\right)  ^{\tau}}_{=-1}\cdot v_{\mathbf{w}\left(  i_{0}\right)  }\wedge
v_{\mathbf{w}\left(  i_{1}\right)  }\wedge v_{\mathbf{w}\left(  i_{2}\right)
}\wedge...\nonumber\\
&  =-v_{\mathbf{w}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(
i_{1}\right)  }\wedge v_{\mathbf{w}\left(  i_{2}\right)  }\wedge....
\label{pf.glinf.glinfact.welldef.8.pf.c.case1.1.4}%
\end{align}
But every $k\in\mathbb{N}$ satisfies $i_{k}^{\prime\prime}=\mathbf{w}\left(
i_{\tau\left(  k\right)  }\right)  $\ \ \ \ \footnote{\textit{Proof.} Let
$k\in\mathbb{N}$. We have to prove that $i_{k}^{\prime\prime}=\mathbf{w}%
\left(  i_{\tau\left(  k\right)  }\right)  $.
\par
Notice that $\tau\left(  M\right)  =L$ (since $\tau=\left(  L,M\right)  $).
Thus,%
\begin{align*}
\mathbf{w}\left(  i_{\tau\left(  M\right)  }\right)   &  =\mathbf{w}\left(
\underbrace{i_{L}}_{=v}\right)  =\mathbf{w}\left(  v\right)  =\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }v=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }v=v;\\
v,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mathbf{w}\left(
v\right)  \right) \\
&  =u\ \ \ \ \ \ \ \ \ \ \left(  \text{since }v=v\right)  .
\end{align*}
Compared with%
\begin{align*}
i_{M}^{\prime\prime}  &  =\left\{
\begin{array}
[c]{c}%
i_{M},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }i_{M}%
^{\prime\prime}\right) \\
&  =i_{M}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }M\neq L\right) \\
&  =s=u\ \ \ \ \ \ \ \ \ \ \left(  \text{since we are in Case 1}\right)  ,
\end{align*}
this yields $i_{M}^{\prime\prime}=\mathbf{w}\left(  i_{\tau\left(  M\right)
}\right)  $. In other words, $i_{k}^{\prime\prime}=\mathbf{w}\left(
i_{\tau\left(  k\right)  }\right)  $ holds if $k=M$. Hence, we have proven
$i_{k}^{\prime\prime}=\mathbf{w}\left(  i_{\tau\left(  k\right)  }\right)  $
in the case when $k=M$. Thus, for the rest of the proof of $i_{k}%
^{\prime\prime}=\mathbf{w}\left(  i_{\tau\left(  k\right)  }\right)  $, we can
WLOG assume that we don't have $k=M$. Assume this. So we know that we don't
have $k=M$. In other words, $k\neq M$.
\par
Since $\tau=\left(  L,M\right)  $, we have $\tau\left(  L\right)  =M$. Hence,%
\begin{align*}
\mathbf{w}\left(  i_{\tau\left(  L\right)  }\right)   &  =\mathbf{w}\left(
\underbrace{i_{M}}_{=s}\right)  =\mathbf{w}\left(  s\right)  =\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }s=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }s=v;\\
s,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mathbf{w}\left(
s\right)  \right) \\
&  =r\ \ \ \ \ \ \ \ \ \ \left(  \text{since }s=s\right)  .
\end{align*}
Compared with $i_{L}^{\prime\prime}=r$, this yields $i_{L}^{\prime\prime
}=\mathbf{w}\left(  i_{\tau\left(  L\right)  }\right)  $. In other words,
$i_{k}^{\prime\prime}=\mathbf{w}\left(  i_{\tau\left(  k\right)  }\right)  $
holds if $k=L$. Hence, we have proven $i_{k}^{\prime\prime}=\mathbf{w}\left(
i_{\tau\left(  k\right)  }\right)  $ in the case when $k=L$. Thus, for the
rest of the proof of $i_{k}^{\prime\prime}=\mathbf{w}\left(  i_{\tau\left(
k\right)  }\right)  $, we can WLOG assume that we don't have $k=L$. Assume
this. So we know that we don't have $k=L$. In other words, $k\neq L$.
\par
Since $k\neq L$ and $k\neq M$, we have $k\notin\left\{  L,M\right\}  $, thus
$k\in\mathbb{N}\setminus\left\{  L,M\right\}  $ (since $k\in\mathbb{N}$).
\par
Now, by the definition of $i_{k}^{\prime\prime}$, we have%
\begin{align*}
i_{k}^{\prime\prime}  &  =\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }i_{k}%
^{\prime\prime}\right) \\
&  =i_{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\neq L\right)  .
\end{align*}
\par
On the other hand, the numbers $i_{0}$, $i_{1}$, $i_{2}$, $...$ are pairwise
distinct. Thus, $i_{\alpha}\neq i_{\beta}$ for any $\alpha\in\mathbb{N}$ and
$\beta\in\mathbb{N}$ such that $\alpha\neq\beta$. Applying this to $\alpha=k$
and $\beta=L$, we obtain $i_{k}\neq i_{L}$ (since $k\neq L$). Since $i_{L}=v$,
this rewrites as $i_{k}\neq v$.
\par
Again, recall that $i_{\alpha}\neq i_{\beta}$ for any $\alpha\in\mathbb{N}$
and $\beta\in\mathbb{N}$ such that $\alpha\neq\beta$. Applying this to
$\alpha=k$ and $\beta=M$, we obtain $i_{k}\neq i_{M}$ (since $k\neq M$). Since
$i_{M}=s$, this rewrites as $i_{k}\neq s$.
\par
Since $\tau$ is the transposition $\left(  L,M\right)  $, we know that $\tau$
leaves any element of $\mathbb{N}$ other than $L$ and $M$ fixed. In other
words, $\tau\left(  \alpha\right)  =\alpha$ for every $\alpha\in
\mathbb{N}\setminus\left\{  L,M\right\}  $. Applying this to $\alpha=k$, we
obtain $\tau\left(  k\right)  =k$ (since $k\in\mathbb{N}\setminus\left\{
L,M\right\}  $).
\par
Since $i_{k}\neq s$ and $i_{k}\neq v$, we have neither $i_{k}=s$ nor $i_{k}%
=v$. Now,%
\begin{align*}
\mathbf{w}\left(  i_{\tau\left(  k\right)  }\right)   &  =\mathbf{w}\left(
i_{k}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\tau\left(  k\right)
=k\right) \\
&  =\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}=v;\\
i_{k},\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }%
\mathbf{w}\left(  i_{k}\right)  \right) \\
&  =i_{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since neither }i_{k}=s\text{ nor
}i_{k}=v\right)  .
\end{align*}
Thus, $i_{k}^{\prime\prime}=i_{k}=\mathbf{w}\left(  i_{\tau\left(  k\right)
}\right)  $, qed.}. In other words, $\left(  i_{0}^{\prime\prime}%
,i_{1}^{\prime\prime},i_{2}^{\prime\prime},...\right)  =\left(  \mathbf{w}%
\left(  i_{\tau\left(  0\right)  }\right)  ,\mathbf{w}\left(  i_{\tau\left(
1\right)  }\right)  ,\mathbf{w}\left(  i_{\tau\left(  2\right)  }\right)
,...\right)  $. Hence,%
\begin{align}
&  v_{i_{0}^{\prime\prime}}\wedge v_{i_{1}^{\prime\prime}}\wedge
v_{i_{2}^{\prime\prime}}\wedge...\nonumber\\
&  =v_{\mathbf{w}\left(  i_{\tau\left(  0\right)  }\right)  }\wedge
v_{\mathbf{w}\left(  i_{\tau\left(  1\right)  }\right)  }\wedge v_{\mathbf{w}%
\left(  i_{\tau\left(  2\right)  }\right)  }\wedge...\nonumber\\
&  =-v_{\mathbf{w}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(
i_{1}\right)  }\wedge v_{\mathbf{w}\left(  i_{2}\right)  }\wedge
...\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.glinf.glinfact.welldef.8.pf.c.case1.1.4})}\right)  .
\label{pf.glinf.glinfact.welldef.8.pf.c.case1.1.6}%
\end{align}


Now,%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =F_{E_{r,s}}\left(  \underbrace{F_{E_{u,v}}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }_{\substack{=0\\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.c.case1.1.2}))}}}\right)
-\underbrace{\delta_{s,u}}_{\substack{=1\\\text{(since }s=u\text{)}%
}}\underbrace{F_{E_{r,v}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)  }_{\substack{=v_{i_{0}^{\prime\prime}}\wedge v_{i_{1}%
^{\prime\prime}}\wedge v_{i_{2}^{\prime\prime}}\wedge...\\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.c.case1.o.1}))}}}\\
&  =\underbrace{F_{E_{r,s}}\left(  0\right)  }_{\substack{=0\\\text{(since
}F_{E_{r,s}}\text{ is a}\\\text{linear map)}}}-v_{i_{0}^{\prime\prime}}\wedge
v_{i_{1}^{\prime\prime}}\wedge v_{i_{2}^{\prime\prime}}\wedge
...=-\underbrace{v_{i_{0}^{\prime\prime}}\wedge v_{i_{1}^{\prime\prime}}\wedge
v_{i_{2}^{\prime\prime}}\wedge...}_{\substack{=-v_{\mathbf{w}\left(
i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge...\\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.c.case1.1.6}))}}}\\
&  =-\left(  -v_{\mathbf{w}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(
i_{1}\right)  }\wedge v_{\mathbf{w}\left(  i_{2}\right)  }\wedge...\right)
=v_{\mathbf{w}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)
}\wedge v_{\mathbf{w}\left(  i_{2}\right)  }\wedge....
\end{align*}
Compared with%
\begin{align*}
&  \underbrace{\left[  s\in I\right]  }_{\substack{=1\\\text{(since }s\in
I\text{)}}}\cdot\underbrace{\left[  v\in I\right]  }%
_{\substack{=1\\\text{(since }v\in I\text{)}}}\cdot v_{\mathbf{w}\left(
i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge...\\
&  =v_{\mathbf{w}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(
i_{1}\right)  }\wedge v_{\mathbf{w}\left(  i_{2}\right)  }\wedge...,
\end{align*}
this yields%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\left[  s\in I\right]  \cdot\left[  v\in I\right]  \cdot v_{\mathbf{w}%
\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge....
\end{align*}
Thus, Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(c)} is proven in
Subcase 1.1.

Let us now consider Subcase 1.2. In this subcase, we don't have $s\in I$.
Thus, $s\notin I$.

Let us define a sequence $\left(  j_{0},j_{1},j_{2},...\right)  $ by%
\[
\left(  j_{k}=\left\{
\begin{array}
[c]{c}%
i_{k}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{N}\right)  .
\]
Then, $\left(  j_{0},j_{1},j_{2},...\right)  =\left(  i_{0}^{\prime}%
,i_{1}^{\prime},...,i_{L-1}^{\prime},r,i_{L+1}^{\prime},i_{L+2}^{\prime
},...\right)  $.

But $j_{k}=i_{k}^{\prime\prime}$ for every $k\in\mathbb{N}$%
\ \ \ \ \footnote{\textit{Proof.} Let $k\in\mathbb{N}$. We need to prove that
$j_{k}=i_{k}^{\prime\prime}$.
\par
First, notice that the definition of $j_{L}$ yields%
\begin{align*}
j_{L}  &  =\left\{
\begin{array}
[c]{c}%
i_{L}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }L\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }L=L
\end{array}
\right.  =r\ \ \ \ \ \ \ \ \ \ \left(  \text{since }L=L\right) \\
&  =i_{L}^{\prime\prime}.
\end{align*}
In other words, $j_{k}=i_{k}^{\prime\prime}$ holds for $k=L$. Thus,
$j_{k}=i_{k}^{\prime\prime}$ is proven in the case when $k=L$. Hence, for the
rest of the proof of $j_{k}=i_{k}^{\prime\prime}$, we can WLOG assume that we
don't have $k=L$. Assume this.
\par
We don't have $k=L$. In other words, $k\neq L$. Now, the definition of
$i_{k}^{\prime\prime}$ yields%
\[
i_{k}^{\prime\prime}=\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  =i_{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\neq L\right)  .
\]
On the other hand, from the definition of $j_{k}$, we obtain%
\begin{align*}
j_{k}  &  =\left\{
\begin{array}
[c]{c}%
i_{k}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  =i_{k}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\neq
L\right) \\
&  =\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }i_{k}%
^{\prime}\right) \\
&  =i_{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\neq L\right) \\
&  =i_{k}^{\prime\prime},
\end{align*}
qed.}. In other words, $\left(  j_{0},j_{1},j_{2},...\right)  =\left(
i_{0}^{\prime\prime},i_{1}^{\prime\prime},i_{2}^{\prime\prime},...\right)  $.
Hence,%
\begin{equation}
\left(  i_{0}^{\prime\prime},i_{1}^{\prime\prime},i_{2}^{\prime\prime
},...\right)  =\left(  j_{0},j_{1},j_{2},...\right)  =\left(  i_{0}^{\prime
},i_{1}^{\prime},...,i_{L-1}^{\prime},r,i_{L+1}^{\prime},i_{L+2}^{\prime
},...\right)  . \label{pf.glinf.glinfact.welldef.8.pf.c.case1.2.0}%
\end{equation}


Now, recall that $s=i_{L}^{\prime}$. Thus,%
\begin{equation}
\text{there exists at least one }\ell\in\mathbb{N}\text{ satisfying }%
s=i_{\ell}^{\prime} \label{pf.glinf.glinfact.welldef.8.pf.c.case1.2.1}%
\end{equation}
(namely, $\ell=L$). But there exists at most one $\ell\in\mathbb{N}$
satisfying $s=i_{\ell}^{\prime}$\ \ \ \ \footnote{\textit{Proof.} Let $\ell
\in\mathbb{N}$ satisfy $s=i_{\ell}^{\prime}$. We will prove that $\ell=L$.
\par
Assume (for the sake of contradiction) that $\ell\neq L$. Then, by the
definition of $i_{\ell}^{\prime}$, we have $i_{\ell}^{\prime}=\left\{
\begin{array}
[c]{c}%
i_{\ell},\ \ \ \ \ \ \ \ \ \ \text{if }\ell\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }\ell=L
\end{array}
\right.  =i_{\ell}$ (since $\ell\neq L$). Thus, $s=i_{\ell}^{\prime}=i_{\ell
}\in\left\{  i_{0},i_{1},i_{2},...\right\}  =I$, contradicting $s\notin I$.
This contradiction shows that our assumption (that $\ell\neq L$) was wrong.
Hence, $\ell=L$.
\par
Now forget that we fixed $\ell$. We thus have shown that every $\ell
\in\mathbb{N}$ satisfying $s=i_{\ell}^{\prime}$ must satisfy $\ell=L$. In
other words, every $\ell\in\mathbb{N}$ satisfying $s=i_{\ell}^{\prime}$ must
equal $L$. Hence, there exists at most one $\ell\in\mathbb{N}$ satisfying
$s=i_{\ell}^{\prime}$, qed.}. Combined with
(\ref{pf.glinf.glinfact.welldef.8.pf.c.case1.2.1}), this yields that there
exists a \textbf{unique }$\ell\in\mathbb{N}$ such that $s=i_{\ell}^{\prime}$.
This $\ell$ is $L$ (because $s=i_{L}^{\prime}$). Therefore, we can apply
Assertion \ref{prop.glinf.glinfact.welldef}.6 \textbf{(c)} to $i_{k}^{\prime}%
$, $r$ and $s$ instead of $i_{k}$, $i$ and $j$. As a result, we obtain%
\begin{align}
F_{E_{r,s}}\left(  v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}}\wedge
v_{i_{2}^{\prime}}\wedge...\right)   &  =v_{i_{0}^{\prime}}\wedge
v_{i_{1}^{\prime}}\wedge...\wedge v_{i_{L-1}^{\prime}}\wedge v_{r}\wedge
v_{i_{L+1}^{\prime}}\wedge v_{i_{L+2}^{\prime}}\wedge...\nonumber\\
&  =v_{i_{0}^{\prime\prime}}\wedge v_{i_{1}^{\prime\prime}}\wedge
v_{i_{2}^{\prime\prime}}\wedge
...\label{pf.glinf.glinfact.welldef.8.pf.c.case1.2.5}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\left(  i_{0}^{\prime},i_{1}^{\prime},...,i_{L-1}^{\prime
},r,i_{L+1}^{\prime},i_{L+2}^{\prime},...\right)  =\left(  i_{0}^{\prime
\prime},i_{1}^{\prime\prime},i_{2}^{\prime\prime},...\right) \\
\text{(by (\ref{pf.glinf.glinfact.welldef.8.pf.c.case1.2.0}))}%
\end{array}
\right)  .\nonumber
\end{align}
Now,%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =F_{E_{r,s}}\left(  \underbrace{F_{E_{u,v}}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }_{\substack{=v_{i_{0}^{\prime}%
}\wedge v_{i_{1}^{\prime}}\wedge v_{i_{2}^{\prime}}\wedge...\\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.c.2}))}}}\right)  -\underbrace{\delta
_{s,u}}_{\substack{=1\\\text{(since }s=u\text{)}}}\underbrace{F_{E_{r,v}%
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
}_{\substack{=v_{i_{0}^{\prime\prime}}\wedge v_{i_{1}^{\prime\prime}}\wedge
v_{i_{2}^{\prime\prime}}\wedge...\\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.c.case1.o.1}))}}}\\
&  =\underbrace{F_{E_{r,s}}\left(  v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}%
}\wedge v_{i_{2}^{\prime}}\wedge...\right)  }_{\substack{=v_{i_{0}%
^{\prime\prime}}\wedge v_{i_{1}^{\prime\prime}}\wedge v_{i_{2}^{\prime\prime}%
}\wedge...\\\text{(by (\ref{pf.glinf.glinfact.welldef.8.pf.c.case1.2.5}))}%
}}-v_{i_{0}^{\prime\prime}}\wedge v_{i_{1}^{\prime\prime}}\wedge
v_{i_{2}^{\prime\prime}}\wedge...\\
&  =v_{i_{0}^{\prime\prime}}\wedge v_{i_{1}^{\prime\prime}}\wedge
v_{i_{2}^{\prime\prime}}\wedge...-v_{i_{0}^{\prime\prime}}\wedge
v_{i_{1}^{\prime\prime}}\wedge v_{i_{2}^{\prime\prime}}\wedge...=0.
\end{align*}
Compared with%
\[
\underbrace{\left[  s\in I\right]  }_{\substack{=0\\\text{(since we don't have
}s\in I\text{)}}}\cdot\left[  v\in I\right]  \cdot v_{\mathbf{w}\left(
i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge...=0,
\]
this yields%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\left[  s\in I\right]  \cdot\left[  v\in I\right]  \cdot v_{\mathbf{w}%
\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge....
\end{align*}
Thus, Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(c)} is proven in
Subcase 1.2.

We have now proven Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(c)}
in each of the Subcases 1.1 and 1.2. Since these Subcases 1.1 and 1.2 cover
the whole Case 1, this yields that Assertion \ref{prop.glinf.glinfact.welldef}%
.8 \textbf{(c)} is proven in the whole Case 1.

Let us now consider Case 2. In this case, we don't have $s=u$. Thus, $s\neq u$.

Now, we distinguish between the following two subcases:

\textit{Subcase 2.1:} We have $s\in I$.

\textit{Subcase 2.2:} We don't have $s\in I$.

Let us first consider Subcase 2.1. In this subcase, we have $s\in I$. Recall
also that $s\neq u$, and the definition of $i_{k}^{\prime}$. It is now easy to
see that there exists a \textbf{unique} $\ell\in\mathbb{N}$ such that
$s=i_{\ell}^{\prime}$.\ \ \ \ \footnote{\textit{Proof:} Since $s\in I=\left\{
i_{0},i_{1},i_{2},...\right\}  $, there exists at least one $\ell\in
\mathbb{N}$ such that $s=i_{\ell}$. Denote this $\ell$ by $M$. Then,
$M\in\mathbb{N}$ satisfies $s=i_{M}$.
\par
Since $i_{M}=s\neq v=i_{L}$, we have $M\neq L$. Now, by the definition of
$i_{M}^{\prime}$, we have%
\begin{align*}
i_{M}^{\prime}  &  =\left\{
\begin{array}
[c]{c}%
i_{M},\ \ \ \ \ \ \ \ \ \ \text{if }M\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }M=L
\end{array}
\right.  =i_{M}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }M\neq L\right) \\
&  =s.
\end{align*}
Thus, $s=i_{M}^{\prime}$. Hence, there exists at least one $\ell\in\mathbb{N}$
such that $s=i_{\ell}^{\prime}$ (namely, $\ell=M$).
\par
Now, let $\ell_{1}$ and $\ell_{2}$ be two elements $\ell$ of $\mathbb{N}$ such
that $s=i_{\ell}^{\prime}$. Then, $s=i_{\ell_{1}}^{\prime}$ (since $\ell_{1}$
is an element $\ell$ of $\mathbb{N}$ such that $s=i_{\ell}^{\prime}$). Hence,
$i_{\ell_{1}}^{\prime}=s\neq u=i_{L}^{\prime}$, so that $\ell_{1}\neq L$. Now,
by the definition of $i_{\ell_{1}}^{\prime}$, we have%
\[
i_{\ell_{1}}^{\prime}=\left\{
\begin{array}
[c]{c}%
i_{\ell_{1}},\ \ \ \ \ \ \ \ \ \ \text{if }\ell_{1}\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }\ell_{1}=L
\end{array}
\right.  =i_{\ell_{1}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\ell_{1}\neq
L\right)  .
\]
Thus, $i_{\ell_{1}}=i_{\ell_{1}}^{\prime}=s$. The same argument, applied to
$\ell_{2}$ instead of $\ell_{1}$, shows that $i_{\ell_{2}}=s$. Thus,
$i_{\ell_{1}}=s=i_{\ell_{2}}$. Now, if we had $\ell_{1}\neq\ell_{2}$, then we
would have $i_{\ell_{1}}\neq i_{\ell_{2}}$ (since the numbers $i_{0}$, $i_{1}%
$, $i_{2}$, $...$ are pairwise distinct), which would contradict $i_{\ell_{1}%
}=i_{\ell_{2}}$. Thus, we cannot have $\ell_{1}\neq\ell_{2}$. Hence, $\ell
_{1}=\ell_{2}$.
\par
Now, forget that we fixed $\ell_{1}$ and $\ell_{2}$. We thus have proven that
if $\ell_{1}$ and $\ell_{2}$ be two elements $\ell$ of $\mathbb{N}$ such that
$s=i_{\ell}^{\prime}$, then $\ell_{1}=\ell_{2}$. In other words, there exists
at most one $\ell\in\mathbb{N}$ such that $s=i_{\ell}^{\prime}$. Hence, there
exists a \textbf{unique} $\ell\in\mathbb{N}$ such that $s=i_{\ell}^{\prime}$
(since we also know that there exists at least one $\ell\in\mathbb{N}$ such
that $s=i_{\ell}^{\prime}$), qed.} Denote this $\ell$ by $M$. Thus,
$s=i_{M}^{\prime}$ (since $M$ is an $\ell\in\mathbb{N}$ such that $s=i_{\ell
}^{\prime}$). Since $i_{M}^{\prime}=s\neq u=i_{L}^{\prime}$, we have $M\neq
L$. Now, by the definition of $i_{M}^{\prime}$, we have%
\[
i_{M}^{\prime}=\left\{
\begin{array}
[c]{c}%
i_{M},\ \ \ \ \ \ \ \ \ \ \text{if }M\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }M=L
\end{array}
\right.  =i_{M}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }M\neq L\right)  .
\]
Hence, $i_{M}=i_{M}^{\prime}=s$.

Let us define a sequence $\left(  i_{0}^{\prime\prime},i_{1}^{\prime\prime
},i_{2}^{\prime\prime},...\right)  $ by%
\[
\left(  i_{k}^{\prime\prime}=\left\{
\begin{array}
[c]{c}%
i_{k}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq M;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=M
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{N}\right)  .
\]
Then,
\begin{equation}
\left(  i_{0}^{\prime\prime},i_{1}^{\prime\prime},i_{2}^{\prime\prime
},...\right)  =\left(  i_{0}^{\prime},i_{1}^{\prime},...,i_{M-1}^{\prime
},r,i_{M+1}^{\prime},i_{M+2}^{\prime},...\right)  .
\label{pf.glinf.glinfact.welldef.8.pf.c.case2.1.1}%
\end{equation}


By the definition of $i_{M}^{\prime\prime}$, we have $i_{M}^{\prime\prime
}=\left\{
\begin{array}
[c]{c}%
i_{M}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }M\neq M;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }M=M
\end{array}
\right.  =r$ (since $M=M$).

But every $k\in\mathbb{N}$ satisfies $i_{k}^{\prime\prime}=\mathbf{w}\left(
i_{k}\right)  $\ \ \ \ \footnote{\textit{Proof.} Let $k\in\mathbb{N}$. We have
to prove that $i_{k}^{\prime\prime}=\mathbf{w}\left(  i_{k}\right)  $.
\par
If $k=M$, then%
\begin{align*}
i_{k}^{\prime\prime}  &  =i_{M}^{\prime\prime}=r=\mathbf{w}\left(  s\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the definition of }\mathbf{w}%
\left(  s\right)  \text{ yields }\mathbf{w}\left(  s\right)  =\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }s=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }s=v;\\
s,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  =r\text{ (since }s=s\text{)}\right) \\
&  =\mathbf{w}\left(  i_{M}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}s=i_{M}\right) \\
&  =\mathbf{w}\left(  i_{k}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}M=k\right)  .
\end{align*}
Hence, if $k=M$, then $i_{k}^{\prime\prime}=\mathbf{w}\left(  i_{k}\right)  $
is proven. Thus, for the rest of the proof of $i_{k}^{\prime\prime}%
=\mathbf{w}\left(  i_{k}\right)  $, we can WLOG assume that we don't have
$k=M$. Assume this. So we know that we don't have $k=M$. In other words,
$k\neq M$.
\par
If $k=L$, then%
\begin{align*}
i_{k}^{\prime\prime}  &  =i_{L}^{\prime\prime}=\left\{
\begin{array}
[c]{c}%
i_{L}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }L\neq M;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }L=M
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }i_{L}%
^{\prime\prime}\right) \\
&  =i_{L}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }L\neq M\text{
(since }M\neq L\text{)}\right) \\
&  =u=\mathbf{w}\left(  v\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the definition of }\mathbf{w}%
\left(  v\right)  \text{ yields }\mathbf{w}\left(  v\right)  =\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }v=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }v=v;\\
v,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  =u\text{ (since }v=v\text{)}\right) \\
&  =\mathbf{w}\left(  i_{L}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}v=i_{L}\right) \\
&  =\mathbf{w}\left(  i_{k}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}L=k\right)  .
\end{align*}
Hence, if $k=L$, then $i_{k}^{\prime\prime}=\mathbf{w}\left(  i_{k}\right)  $
is proven. Thus, for the rest of the proof of $i_{k}^{\prime\prime}%
=\mathbf{w}\left(  i_{k}\right)  $, we can WLOG assume that we don't have
$k=L$. Assume this. So we know that we don't have $k=L$. In other words,
$k\neq L$.
\par
Now, by the definition of $i_{k}^{\prime\prime}$, we have%
\begin{align*}
i_{k}^{\prime\prime}  &  =\left\{
\begin{array}
[c]{c}%
i_{k}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq M;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=M
\end{array}
\right.  =i_{k}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\neq
M\right) \\
&  =\left\{
\begin{array}
[c]{c}%
i_{k},\ \ \ \ \ \ \ \ \ \ \text{if }k\neq L;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=L
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }i_{k}%
^{\prime}\right) \\
&  =i_{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\neq L\right)  .
\end{align*}
\par
On the other hand, the numbers $i_{0}$, $i_{1}$, $i_{2}$, $...$ are pairwise
distinct. Thus, $i_{\alpha}\neq i_{\beta}$ for any $\alpha\in\mathbb{N}$ and
$\beta\in\mathbb{N}$ such that $\alpha\neq\beta$. Applying this to $\alpha=k$
and $\beta=L$, we obtain $i_{k}\neq i_{L}$ (since $k\neq L$). Since $i_{L}=v$,
this rewrites as $i_{k}\neq v$.
\par
Again, recall that $i_{\alpha}\neq i_{\beta}$ for any $\alpha\in\mathbb{N}$
and $\beta\in\mathbb{N}$ such that $\alpha\neq\beta$. Applying this to
$\alpha=k$ and $\beta=M$, we obtain $i_{k}\neq i_{M}$ (since $k\neq M$). Since
$i_{M}=s$, this rewrites as $i_{k}\neq s$.
\par
Since $i_{k}\neq s$ and $i_{k}\neq v$, we have neither $i_{k}=s$ nor $i_{k}%
=v$. Now, by the definition of $\mathbf{w}\left(  i_{k}\right)  $, we have%
\[
\mathbf{w}\left(  i_{k}\right)  =\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}=v;\\
i_{k},\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  =i_{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since neither }i_{k}=s\text{
nor }i_{k}=v\right)  .
\]
Thus, $i_{k}^{\prime\prime}=i_{k}=\mathbf{w}\left(  i_{k}\right)  $, qed.}. In
other words, $\left(  i_{0}^{\prime\prime},i_{1}^{\prime\prime},i_{2}%
^{\prime\prime},...\right)  =\left(  \mathbf{w}\left(  i_{0}\right)
,\mathbf{w}\left(  i_{1}\right)  ,\mathbf{w}\left(  i_{2}\right)  ,...\right)
$. Hence, (\ref{pf.glinf.glinfact.welldef.8.pf.c.case2.1.1}) becomes%
\begin{align*}
&  \left(  i_{0}^{\prime},i_{1}^{\prime},...,i_{M-1}^{\prime},r,i_{M+1}%
^{\prime},i_{M+2}^{\prime},...\right) \\
&  =\left(  i_{0}^{\prime\prime},i_{1}^{\prime\prime},i_{2}^{\prime\prime
},...\right)  =\left(  \mathbf{w}\left(  i_{0}\right)  ,\mathbf{w}\left(
i_{1}\right)  ,\mathbf{w}\left(  i_{2}\right)  ,...\right)  .
\end{align*}


Now, recall that $M$ is the unique $\ell\in\mathbb{N}$ such that $s=i_{\ell
}^{\prime}$. Hence, we can apply Assertion \ref{prop.glinf.glinfact.welldef}.6
\textbf{(c)} to $M$, $i_{k}^{\prime}$, $r$ and $s$ instead of $L$, $i_{k}$,
$i$ and $j$. As a result, we obtain%
\begin{align}
F_{E_{r,s}}\left(  v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}}\wedge
v_{i_{2}^{\prime}}\wedge...\right)   &  =v_{i_{0}^{\prime}}\wedge
v_{i_{1}^{\prime}}\wedge...\wedge v_{i_{M-1}^{\prime}}\wedge v_{r}\wedge
v_{i_{M+1}^{\prime}}\wedge v_{i_{M+2}^{\prime}}\wedge...\nonumber\\
&  =v_{\mathbf{w}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(
i_{1}\right)  }\wedge v_{\mathbf{w}\left(  i_{2}\right)  }\wedge
...\label{pf.glinf.glinfact.welldef.8.pf.c.case2.1.3}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0}^{\prime}%
,i_{1}^{\prime},...,i_{L-1}^{\prime},r,i_{L+1}^{\prime},i_{L+2}^{\prime
},...\right)  =\left(  \mathbf{w}\left(  i_{0}\right)  ,\mathbf{w}\left(
i_{1}\right)  ,\mathbf{w}\left(  i_{2}\right)  ,...\right)  \right)
.\nonumber
\end{align}


Now,%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =F_{E_{r,s}}\left(  \underbrace{F_{E_{u,v}}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }_{\substack{=v_{i_{0}^{\prime}%
}\wedge v_{i_{1}^{\prime}}\wedge v_{i_{2}^{\prime}}\wedge...\\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.c.2}))}}}\right)  -\underbrace{\delta
_{s,u}}_{\substack{=0\\\text{(since }s\neq u\text{)}}}F_{E_{r,v}}\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\underbrace{F_{E_{r,s}}\left(  v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}%
}\wedge v_{i_{2}^{\prime}}\wedge...\right)  }_{\substack{=v_{\mathbf{w}\left(
i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge...\\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.c.case2.1.3}))}}}-\underbrace{0F_{E_{r,v}%
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }_{=0}\\
&  =v_{\mathbf{w}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(
i_{1}\right)  }\wedge v_{\mathbf{w}\left(  i_{2}\right)  }\wedge
...-0=v_{\mathbf{w}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(
i_{1}\right)  }\wedge v_{\mathbf{w}\left(  i_{2}\right)  }\wedge....
\end{align*}
Compared with%
\begin{align*}
&  \underbrace{\left[  s\in I\right]  }_{\substack{=1\\\text{(since }s\in
I\text{)}}}\cdot\underbrace{\left[  v\in I\right]  }%
_{\substack{=1\\\text{(since }v\in I\text{)}}}\cdot v_{\mathbf{w}\left(
i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge...\\
&  =v_{\mathbf{w}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(
i_{1}\right)  }\wedge v_{\mathbf{w}\left(  i_{2}\right)  }\wedge...,
\end{align*}
this yields%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\left[  s\in I\right]  \cdot\left[  v\in I\right]  \cdot v_{\mathbf{w}%
\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge....
\end{align*}
Thus, Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(c)} is proven in
Subcase 2.1.

Finally, let us consider Subcase 2.2. In this subcase, we don't have $s\in I$.
Recall also that $s\neq u$. Now, it is easy to see that $s\notin\left\{
i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right\}  $%
\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Then, $s\in\left\{
i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right\}  $. Thus,
$s\in\left\{  i_{0}^{\prime},i_{1}^{\prime},i_{2}^{\prime},...\right\}
=\left\{  u\right\}  \cup\left(  I\setminus\left\{  v\right\}  \right)  $ (by
(\ref{pf.glinf.glinfact.welldef.8.pf.c.1})). Combining this with $s\neq u$, we
obtain%
\[
s\in\left(  \left\{  u\right\}  \cup\left(  I\setminus\left\{  v\right\}
\right)  \right)  \setminus\left\{  u\right\}  =\underbrace{\left(  \left\{
u\right\}  \setminus\left\{  u\right\}  \right)  }_{=\varnothing}%
\cup\underbrace{\left(  \left(  I\setminus\left\{  v\right\}  \right)
\setminus\left\{  u\right\}  \right)  }_{\subseteq I}\subseteq\varnothing\cup
I\subseteq I.
\]
This contradicts the fact that we don't have $s\in I$. This contradiction
shows that our assumption was wrong, qed.}. Hence, Assertion
\ref{prop.glinf.glinfact.welldef}.6 \textbf{(b)} (applied to $i_{k}^{\prime}$,
$r$ and $s$ instead of $i_{k}$, $i$ and $j$) yields
\begin{equation}
F_{E_{r,s}}\left(  v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}}\wedge
v_{i_{2}^{\prime}}\wedge...\right)  =0.
\label{pf.glinf.glinfact.welldef.8.pf.c.case2.2.1}%
\end{equation}


Now,%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =F_{E_{r,s}}\left(  \underbrace{F_{E_{u,v}}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }_{\substack{=v_{i_{0}^{\prime}%
}\wedge v_{i_{1}^{\prime}}\wedge v_{i_{2}^{\prime}}\wedge...\\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.c.2}))}}}\right)  -\underbrace{\delta
_{s,u}}_{\substack{=0\\\text{(since }s\neq u\text{)}}}F_{E_{r,v}}\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\underbrace{F_{E_{r,s}}\left(  v_{i_{0}^{\prime}}\wedge v_{i_{1}^{\prime}%
}\wedge v_{i_{2}^{\prime}}\wedge...\right)  }_{\substack{=0\\\text{(by
(\ref{pf.glinf.glinfact.welldef.8.pf.c.case2.2.1}))}}}-\underbrace{0F_{E_{r,v}%
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }_{=0}\\
&  =0-0=0.
\end{align*}
Compared with%
\begin{align*}
&  \underbrace{\left[  s\in I\right]  }_{\substack{=0\\\text{(since we don't
have }s\in I\text{)}}}\cdot\left[  v\in I\right]  \cdot v_{\mathbf{w}\left(
i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge...\\
&  =0,
\end{align*}
this yields%
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\left[  s\in I\right]  \cdot\left[  v\in I\right]  \cdot v_{\mathbf{w}%
\left(  i_{0}\right)  }\wedge v_{\mathbf{w}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}\left(  i_{2}\right)  }\wedge....
\end{align*}
Thus, Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(c)} is proven in
Subcase 2.2.

We have now proven Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(c)}
in each of the Subcases 2.1 and 2.2. Since these Subcases 2.1 and 2.2 cover
the whole Case 2, this yields that Assertion \ref{prop.glinf.glinfact.welldef}%
.8 \textbf{(c)} is proven in the whole Case 2.

Altogether, we have now proven Assertion \ref{prop.glinf.glinfact.welldef}.8
\textbf{(c)} in each of the two Cases 1 and 2. Since these two Cases 1 and 2
cover all possibilities, this yields that Assertion
\ref{prop.glinf.glinfact.welldef}.8 \textbf{(c)} always holds. This completes
the proof of Assertion \ref{prop.glinf.glinfact.welldef}.8 \textbf{(c)}.

Now that Assertion \ref{prop.glinf.glinfact.welldef}.8 is completely proven,
let us prove Assertion \ref{prop.glinf.glinfact.welldef}.7:

\textit{Second proof of Assertion \ref{prop.glinf.glinfact.welldef}.7: }Let us
recall that
\begin{equation}
E_{r,s}E_{u,v}=\delta_{s,u}E_{r,v}\ \ \ \ \ \ \ \ \ \ \text{for any integers
}r\text{, }s\text{, }u\text{ and }v.
\label{pf.glinf.glinfact.welldef.7.pf.Emult}%
\end{equation}
(In fact, this is a property of elementary matrices that can be directly
proven in the same way as the completely analogous property of finite-size
elementary matrices.)

Let $a\in\mathfrak{gl}_{\infty}$ and $b\in\mathfrak{gl}_{\infty}$.

Since $\left(  E_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$ is a
basis of the vector space $\mathfrak{gl}_{\infty}$, we can write $a$ in the
form $a=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\alpha_{i,j}%
E_{i,j}$ for some family $\left(  \alpha_{i,j}\right)  _{\left(  i,j\right)
\in\mathbb{Z}^{2}}\in\mathbb{C}^{\mathbb{Z}^{2}}$ such that $\left(  \text{all
but finitely many }\left(  i,j\right)  \in\mathbb{Z}^{2}\text{ satisfy }%
\alpha_{i,j}=0\right)  $. Consider this family $\left(  \alpha_{i,j}\right)
_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\in\mathbb{C}^{\mathbb{Z}^{2}}$.

Since $\left(  E_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$ is a
basis of the vector space $\mathfrak{gl}_{\infty}$, we can write $b$ in the
form $b=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\beta_{i,j}E_{i,j}$
for some family $\left(  \beta_{i,j}\right)  _{\left(  i,j\right)
\in\mathbb{Z}^{2}}\in\mathbb{C}^{\mathbb{Z}^{2}}$ such that $\left(  \text{all
but finitely many }\left(  i,j\right)  \in\mathbb{Z}^{2}\text{ satisfy }%
\beta_{i,j}=0\right)  $. Consider this family $\left(  \beta_{i,j}\right)
_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\in\mathbb{C}^{\mathbb{Z}^{2}}$.

We are now going to prove that $\left[  F_{E_{r,s}},F_{E_{u,v}}\right]
=F_{\left[  E_{r,s},E_{u,v}\right]  }$ for all $\left(  r,s\right)
\in\mathbb{Z}^{2}$ and $\left(  u,v\right)  \in\mathbb{Z}^{2}$. This will
easily yield Assertion \ref{prop.glinf.glinfact.welldef}.7.

Let $\left(  r,s\right)  \in\mathbb{Z}^{2}$ and $\left(  u,v\right)
\in\mathbb{Z}^{2}$ be arbitrary. Then,%
\begin{align}
\left[  E_{r,s},E_{u,v}\right]   &  =\underbrace{E_{r,s}E_{u,v}}%
_{\substack{=\delta_{s,u}E_{r,v}\\\text{(by
(\ref{pf.glinf.glinfact.welldef.7.pf.Emult}))}}}-\underbrace{E_{u,v}E_{r,s}%
}_{\substack{=\delta_{v,r}E_{u,s}\\\text{(by
(\ref{pf.glinf.glinfact.welldef.7.pf.Emult}), applied}\\\text{to }u\text{,
}v\text{, }r\text{ and }s\text{ instead of }r\text{, }s\text{, }u\text{ and
}v\text{)}}}=\delta_{s,u}E_{r,v}-\delta_{v,r}E_{u,s}%
\label{pf.glinf.glinfact.welldef.7.pf.Elie}\\
&  =\delta_{s,u}E_{r,v}+\left(  -\delta_{v,r}\right)  E_{u,s}.\nonumber
\end{align}
Hence,%
\[
F_{\left[  E_{r,s},E_{u,v}\right]  }=F_{\delta_{s,u}E_{r,v}+\left(
-\delta_{v,r}\right)  E_{u,s}}=\delta_{s,u}F_{E_{r,v}}+\left(  -\delta
_{v,r}\right)  F_{E_{u,s}}%
\]
(since Assertion \ref{prop.glinf.glinfact.welldef}.5 (applied to $\delta
_{s,u}$, $E_{r,v}$, $-\delta_{v,r}$ and $E_{u,s}$ instead of $\lambda$, $a$,
$\mu$ and $b$) yields $\delta_{s,u}F_{E_{r,v}}+\left(  -\delta_{v,r}\right)
F_{E_{u,s}}=F_{\delta_{s,u}E_{r,v}+\left(  -\delta_{v,r}\right)  E_{u,s}}$),
so that%
\begin{align}
&  \underbrace{\left[  F_{E_{r,s}},F_{E_{u,v}}\right]  }_{=F_{E_{r,s}}\circ
F_{E_{u,v}}-F_{E_{u,v}}\circ F_{E_{r,s}}}-\underbrace{F_{\left[
E_{r,s},E_{u,v}\right]  }}_{=\delta_{s,u}F_{E_{r,v}}+\left(  -\delta
_{v,r}\right)  F_{E_{u,s}}}\nonumber\\
&  =\left(  F_{E_{r,s}}\circ F_{E_{u,v}}-F_{E_{u,v}}\circ F_{E_{r,s}}\right)
-\left(  \delta_{s,u}F_{E_{r,v}}+\left(  -\delta_{v,r}\right)  F_{E_{u,s}%
}\right) \nonumber\\
&  =\left(  F_{E_{r,s}}\circ F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)
-\underbrace{\left(  F_{E_{u,v}}\circ F_{E_{r,s}}+\left(  -\delta
_{v,r}\right)  F_{E_{u,s}}\right)  }_{=F_{E_{u,v}}\circ F_{E_{r,s}}%
-\delta_{v,r}F_{E_{u,s}}}\nonumber\\
&  =\left(  F_{E_{r,s}}\circ F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)
-\left(  F_{E_{u,v}}\circ F_{E_{r,s}}-\delta_{v,r}F_{E_{u,s}}\right)  .
\label{pf.glinf.glinfact.welldef.7.pf.Elie.applied.1}%
\end{align}
Now,%
\begin{align}
&  \left[  F_{E_{r,s}},F_{E_{u,v}}\right]  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  -F_{\left[  E_{r,s},E_{u,v}\right]
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  =\underbrace{\left(  \left[  F_{E_{r,s}},F_{E_{u,v}}\right]  -F_{\left[
E_{r,s},E_{u,v}\right]  }\right)  }_{\substack{=\left(  F_{E_{r,s}}F_{E_{u,v}%
}-\delta_{s,u}F_{E_{r,v}}\right)  -\left(  F_{E_{u,v}}F_{E_{r,s}}-\delta
_{v,r}F_{E_{u,s}}\right)  \\\text{(by
(\ref{pf.glinf.glinfact.welldef.7.pf.Elie.applied.1}))}}}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  =\left(  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)
-\left(  F_{E_{u,v}}F_{E_{r,s}}-\delta_{v,r}F_{E_{u,s}}\right)  \right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  =\left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ -\left(  F_{E_{u,v}}F_{E_{r,s}}-\delta_{v,r}F_{E_{u,s}%
}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  .
\label{pf.glinf.glinfact.welldef.7.pf.Elie.applied}%
\end{align}


Let $\left(  i_{0},i_{1},i_{2},...\right)  $ be an $m$-degression. We are now
going to prove the following claim:%
\begin{equation}
\left[  F_{E_{r,s}},F_{E_{u,v}}\right]  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  =F_{\left[  E_{r,s},E_{u,v}\right]
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  .
\label{pf.glinf.glinfact.welldef.7.pf.claim}%
\end{equation}


\textit{Proof of (\ref{pf.glinf.glinfact.welldef.7.pf.claim}):} We distinguish
between two cases:

\textit{Case 1:} We have $s=v$.

\textit{Case 2:} We have $s\neq v$.

Let us first consider Case 1. In this case, $s=v$. Now,
(\ref{pf.glinf.glinfact.welldef.7.pf.Elie.applied}) becomes%
\begin{align*}
&  \left[  F_{E_{r,s}},F_{E_{u,v}}\right]  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  -F_{\left[  E_{r,s},E_{u,v}\right]
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\underbrace{\left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
}_{\substack{=0\\\text{(by Assertion \ref{prop.glinf.glinfact.welldef}.8
\textbf{(b)}}\\\text{(since }s=v\text{))}}}\\
&  \ \ \ \ \ \ \ \ \ \ -\underbrace{\left(  F_{E_{u,v}}F_{E_{r,s}}%
-\delta_{v,r}F_{E_{u,s}}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  }_{\substack{=0\\\text{(by Assertion
\ref{prop.glinf.glinfact.welldef}.8 \textbf{(b)}}\\\text{(applied to }u\text{,
}v\text{, }r\text{, }s\text{ instead of }r\text{, }s\text{, }u\text{,
}v\text{) (since }v=s\text{))}}}\\
&  =0-0=0.
\end{align*}
Hence, $\left[  F_{E_{r,s}},F_{E_{u,v}}\right]  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =F_{\left[  E_{r,s},E_{u,v}\right]
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $. Thus,
(\ref{pf.glinf.glinfact.welldef.7.pf.claim}) is proven in Case 1.

Now, let us consider Case 2. In this case, $s\neq v$. Thus, $v\neq s$. Let
$\mathbf{w}_{1}:\mathbb{Z}\rightarrow\mathbb{Z}$ be the function defined by
\[
\left(  \mathbf{w}_{1}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=v;\\
k,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for all }k\in\mathbb{Z}\right)  .
\]
\footnote{Here, the term $\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=v;\\
k,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  $ makes sense, since $s\neq v$.} Let $\mathbf{w}_{2}:\mathbb{Z}%
\rightarrow\mathbb{Z}$ be the function defined by
\[
\left(  \mathbf{w}_{2}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=v;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=s;\\
k,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for all }k\in\mathbb{Z}\right)  .
\]
\footnote{Here, the term $\left\{
\begin{array}
[c]{c}%
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=v;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=s;\\
k,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  $ makes sense, since $s\neq v$.} Clearly, $\mathbf{w}_{1}%
=\mathbf{w}_{2}$\ \ \ \ \footnote{This is because every $k\in\mathbb{Z}$
satisfies%
\begin{align*}
\mathbf{w}_{1}\left(  k\right)   &  =\left\{
\begin{array}
[c]{c}%
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=s;\\
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=v;\\
k,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  =\left\{
\begin{array}
[c]{c}%
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=v;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=s;\\
k,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  =\mathbf{w}_{2}\left(  k\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathbf{w}_{2}\left(  k\right)
=\left\{
\begin{array}
[c]{c}%
u,\ \ \ \ \ \ \ \ \ \ \text{if }k=v;\\
r,\ \ \ \ \ \ \ \ \ \ \text{if }k=s;\\
k,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  \right)  .
\end{align*}
}.

Now, recall that $s\neq v$. Hence, Assertion \ref{prop.glinf.glinfact.welldef}%
.8 \textbf{(c) }(applied to $\mathbf{w}_{1}$ instead of $\mathbf{w}$) yields
that $\left(  \mathbf{w}_{1}\left(  i_{0}\right)  ,\mathbf{w}_{1}\left(
i_{1}\right)  ,\mathbf{w}_{1}\left(  i_{2}\right)  ,...\right)  $ is a
straying $m$-degression, and satisfies
\begin{align*}
&  \left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\left[  s\in I\right]  \cdot\left[  v\in I\right]  \cdot v_{\mathbf{w}%
_{1}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}_{1}\left(  i_{1}\right)
}\wedge v_{\mathbf{w}_{1}\left(  i_{2}\right)  }\wedge....
\end{align*}


On the other hand, $v\neq s$. Hence, Assertion
\ref{prop.glinf.glinfact.welldef}.8 \textbf{(c) }(applied to $\mathbf{w}_{2}$,
$u$, $v$, $r$, $s$ instead of $\mathbf{w}$, $r$, $s$, $u$, $v$) yields that
$\left(  \mathbf{w}_{2}\left(  i_{0}\right)  ,\mathbf{w}_{2}\left(
i_{1}\right)  ,\mathbf{w}_{2}\left(  i_{2}\right)  ,...\right)  $ is a
straying $m$-degression, and satisfies
\begin{align*}
&  \left(  F_{E_{u,v}}F_{E_{r,s}}-\delta_{v,r}F_{E_{u,s}}\right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\left[  v\in I\right]  \cdot\left[  s\in I\right]  \cdot v_{\mathbf{w}%
_{2}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}_{2}\left(  i_{1}\right)
}\wedge v_{\mathbf{w}_{2}\left(  i_{2}\right)  }\wedge....
\end{align*}


Now, (\ref{pf.glinf.glinfact.welldef.7.pf.Elie.applied}) becomes%
\begin{align*}
&  \left[  F_{E_{r,s}},F_{E_{u,v}}\right]  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  -F_{\left[  E_{r,s},E_{u,v}\right]
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\underbrace{\left(  F_{E_{r,s}}F_{E_{u,v}}-\delta_{s,u}F_{E_{r,v}}\right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  }_{=\left[
s\in I\right]  \cdot\left[  v\in I\right]  \cdot v_{\mathbf{w}_{1}\left(
i_{0}\right)  }\wedge v_{\mathbf{w}_{1}\left(  i_{1}\right)  }\wedge
v_{\mathbf{w}_{1}\left(  i_{2}\right)  }\wedge...}\\
&  \ \ \ \ \ \ \ \ \ \ -\underbrace{\left(  F_{E_{u,v}}F_{E_{r,s}}%
-\delta_{v,r}F_{E_{u,s}}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  }_{=\left[  v\in I\right]  \cdot\left[  s\in
I\right]  \cdot v_{\mathbf{w}_{2}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}%
_{2}\left(  i_{1}\right)  }\wedge v_{\mathbf{w}_{2}\left(  i_{2}\right)
}\wedge...}\\
&  =\underbrace{\left[  s\in I\right]  \cdot\left[  v\in I\right]  }_{=\left[
v\in I\right]  \cdot\left[  s\in I\right]  }\cdot\underbrace{v_{\mathbf{w}%
_{1}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}_{1}\left(  i_{1}\right)
}\wedge v_{\mathbf{w}_{1}\left(  i_{2}\right)  }\wedge...}%
_{\substack{=v_{\mathbf{w}_{2}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}%
_{2}\left(  i_{1}\right)  }\wedge v_{\mathbf{w}_{2}\left(  i_{2}\right)
}\wedge...\\\text{(since }\mathbf{w}_{1}=\mathbf{w}_{2}\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ -\left[  v\in I\right]  \cdot\left[  s\in I\right]
\cdot v_{\mathbf{w}_{2}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}_{2}\left(
i_{1}\right)  }\wedge v_{\mathbf{w}_{2}\left(  i_{2}\right)  }\wedge...\\
&  =\left[  v\in I\right]  \cdot\left[  s\in I\right]  \cdot v_{\mathbf{w}%
_{2}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}_{2}\left(  i_{1}\right)
}\wedge v_{\mathbf{w}_{2}\left(  i_{2}\right)  }\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ -\left[  v\in I\right]  \cdot\left[  s\in I\right]
\cdot v_{\mathbf{w}_{2}\left(  i_{0}\right)  }\wedge v_{\mathbf{w}_{2}\left(
i_{1}\right)  }\wedge v_{\mathbf{w}_{2}\left(  i_{2}\right)  }\wedge...\\
&  =0.
\end{align*}
Hence, $\left[  F_{E_{r,s}},F_{E_{u,v}}\right]  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =F_{\left[  E_{r,s},E_{u,v}\right]
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $. Thus,
(\ref{pf.glinf.glinfact.welldef.7.pf.claim}) is proven in Case 1.

Now, we have proven (\ref{pf.glinf.glinfact.welldef.7.pf.claim}) in each of
the two Cases 1 and 2. Since these two Cases cover all possibilities, this
yields that (\ref{pf.glinf.glinfact.welldef.7.pf.claim}) always holds. Thus,
(\ref{pf.glinf.glinfact.welldef.7.pf.claim}) is proven.

Now, forget that we fixed $\left(  i_{0},i_{1},i_{2},...\right)  $. We have
thus proven that every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $
satisfies (\ref{pf.glinf.glinfact.welldef.7.pf.claim}). Hence, Assertion
\ref{prop.glinf.glinfact.welldef}.4 (applied to $\left[  F_{E_{r,s}%
},F_{E_{u,v}}\right]  $ and $F_{\left[  E_{r,s},E_{u,v}\right]  }$ instead of
$f$ and $g$) allows us to conclude that $\left[  F_{E_{r,s}},F_{E_{u,v}%
}\right]  =F_{\left[  E_{r,s},E_{u,v}\right]  }$.

Now, forget that we fixed $\left(  r,s\right)  $ and $\left(  u,v\right)  $.
We have thus proven that%
\begin{equation}
\left[  F_{E_{r,s}},F_{E_{u,v}}\right]  =F_{\left[  E_{r,s},E_{u,v}\right]
}\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  r,s\right)  \in\mathbb{Z}%
^{2}\text{ and }\left(  u,v\right)  \in\mathbb{Z}^{2}.
\label{pf.glinf.glinfact.welldef.7.pf.onbasis}%
\end{equation}


Let us now notice that the map%
\begin{align*}
\mathfrak{gl}_{\infty}  &  \rightarrow\mathfrak{gl}\left(  \wedge
^{\dfrac{\infty}{2},m}V\right)  ,\\
c  &  \mapsto F_{c}%
\end{align*}
is $\mathbb{C}$-linear (indeed, this follows immediately from Assertion
\ref{prop.glinf.glinfact.welldef}.5). Hence, whenever $S$ is a set, $\left(
c_{s}\right)  _{s\in S}$ is a family of elements of $\mathfrak{gl}_{\infty}$,
and $\left(  \gamma_{s}\right)  _{s\in S}$ is a family of elements of
$\mathbb{C}$ such that $\left(  \text{all but finitely many }s\in S\text{
satisfy }\gamma_{s}=0\right)  $, then%
\begin{equation}
F_{\sum\limits_{s\in S}\gamma_{s}c_{s}}=\sum\limits_{s\in S}\gamma_{s}%
F_{c_{s}}. \label{pf.glinf.glinfact.welldef.7.pf.linearity}%
\end{equation}


But we have $b=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\beta
_{i,j}E_{i,j}$. Thus,%
\[
\left[  a,b\right]  =\left[  a,\ \sum\limits_{\left(  i,j\right)
\in\mathbb{Z}^{2}}\beta_{i,j}E_{i,j}\right]  =\sum\limits_{\left(  i,j\right)
\in\mathbb{Z}^{2}}\beta_{i,j}\left[  a,E_{i,j}\right]
\]
(since the Lie bracket is $\mathbb{C}$-bilinear), so that%
\[
F_{\left[  a,b\right]  }=F_{\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}%
^{2}}\beta_{i,j}\left[  a,E_{i,j}\right]  }=\sum\limits_{\left(  i,j\right)
\in\mathbb{Z}^{2}}\beta_{i,j}F_{\left[  a,E_{i,j}\right]  }%
\]
(by (\ref{pf.glinf.glinfact.welldef.7.pf.linearity}), applied to
$\mathbb{Z}^{2}$, $\left(  i,j\right)  $, $\left[  a,E_{i,j}\right]  $ and
$\beta_{i,j}$ in lieu of $S$, $s$, $c_{s}$ and $\gamma_{s}$ (since all but
finitely many $\left(  i,j\right)  \in\mathbb{Z}^{2}$ satisfy $\beta_{i,j}%
=0$)). Hence,%
\begin{equation}
F_{\left[  a,b\right]  }=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}%
}\beta_{i,j}F_{\left[  a,E_{i,j}\right]  }=\sum\limits_{\left(  u,v\right)
\in\mathbb{Z}^{2}}\beta_{u,v}F_{\left[  a,E_{u,v}\right]  }
\label{pf.glinf.glinfact.welldef.7.pf.1}%
\end{equation}
(here, we renamed the index $\left(  i,j\right)  $ as $\left(  u,v\right)  $
in the sum). On the other hand, $a=\sum\limits_{\left(  i,j\right)
\in\mathbb{Z}^{2}}\alpha_{i,j}E_{i,j}$. Hence, every $\left(  u,v\right)
\in\mathbb{Z}^{2}$ satisfies%
\[
\left[  a,E_{u,v}\right]  =\left[  \sum\limits_{\left(  i,j\right)
\in\mathbb{Z}^{2}}\alpha_{i,j}E_{i,j},E_{u,v}\right]  =\sum\limits_{\left(
i,j\right)  \in\mathbb{Z}^{2}}\alpha_{i,j}\left[  E_{i,j},E_{u,v}\right]
\]
(since the Lie bracket is $\mathbb{C}$-bilinear). Therefore, every $\left(
u,v\right)  \in\mathbb{Z}^{2}$ satisfies
\begin{equation}
F_{\left[  a,E_{u,v}\right]  }=F_{\sum\limits_{\left(  i,j\right)
\in\mathbb{Z}^{2}}\alpha_{i,j}\left[  E_{i,j},E_{u,v}\right]  }=\sum
\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\alpha_{i,j}F_{\left[
E_{i,j},E_{u,v}\right]  } \label{pf.glinf.glinfact.welldef.7.pf.2}%
\end{equation}
(by (\ref{pf.glinf.glinfact.welldef.7.pf.linearity}), applied to
$\mathbb{Z}^{2}$, $\left(  i,j\right)  $, $\left[  E_{i,j},E_{u,v}\right]  $
and $\alpha_{i,j}$ in lieu of $S$, $s$, $c_{s}$ and $\gamma_{s}$ (since all
but finitely many $\left(  i,j\right)  \in\mathbb{Z}^{2}$ satisfy
$\alpha_{i,j}=0$)). Thus, (\ref{pf.glinf.glinfact.welldef.7.pf.1}) becomes%
\begin{equation}
F_{\left[  a,b\right]  }=\sum\limits_{\left(  u,v\right)  \in\mathbb{Z}^{2}%
}\beta_{u,v}\underbrace{F_{\left[  a,E_{u,v}\right]  }}_{=\sum\limits_{\left(
i,j\right)  \in\mathbb{Z}^{2}}\alpha_{i,j}F_{\left[  E_{i,j},E_{u,v}\right]
}}=\sum\limits_{\left(  u,v\right)  \in\mathbb{Z}^{2}}\beta_{u,v}%
\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\alpha_{i,j}F_{\left[
E_{i,j},E_{u,v}\right]  }. \label{pf.glinf.glinfact.welldef.7.pf.left}%
\end{equation}


On the other hand, recall that $a=\sum\limits_{\left(  i,j\right)
\in\mathbb{Z}^{2}}\alpha_{i,j}E_{i,j}$, so that%
\begin{equation}
F_{a}=F_{\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\alpha
_{i,j}E_{i,j}}=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\alpha
_{i,j}F_{E_{i,j}} \label{pf.glinf.glinfact.welldef.7.pf.3}%
\end{equation}
(by (\ref{pf.glinf.glinfact.welldef.7.pf.linearity}), applied to
$\mathbb{Z}^{2}$, $\left(  i,j\right)  $, $E_{i,j}$ and $\alpha_{i,j}$ in lieu
of $S$, $s$, $c_{s}$ and $\gamma_{s}$ (since all but finitely many $\left(
i,j\right)  \in\mathbb{Z}^{2}$ satisfy $\alpha_{i,j}=0$)). Also,
$b=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\beta_{i,j}E_{i,j}$, so
that%
\[
F_{b}=F_{\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\beta_{i,j}%
E_{i,j}}=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\beta
_{i,j}F_{E_{i,j}}%
\]
(by (\ref{pf.glinf.glinfact.welldef.7.pf.linearity}), applied to
$\mathbb{Z}^{2}$, $\left(  i,j\right)  $, $E_{i,j}$ and $\beta_{i,j}$ in lieu
of $S$, $s$, $c_{s}$ and $\gamma_{s}$ (since all but finitely many $\left(
i,j\right)  \in\mathbb{Z}^{2}$ satisfy $\beta_{i,j}=0$)). Thus,%
\begin{equation}
F_{b}=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\beta_{i,j}%
F_{E_{i,j}}=\sum\limits_{\left(  u,v\right)  \in\mathbb{Z}^{2}}\beta
_{u,v}F_{E_{u,v}} \label{pf.glinf.glinfact.welldef.7.pf.4}%
\end{equation}
(here, we renamed the index $\left(  i,j\right)  $ as $\left(  u,v\right)  $
in the sum). From (\ref{pf.glinf.glinfact.welldef.7.pf.3}) and
(\ref{pf.glinf.glinfact.welldef.7.pf.4}), we obtain%
\begin{align*}
\left[  F_{a},F_{b}\right]   &  =\left[  \sum\limits_{\left(  i,j\right)
\in\mathbb{Z}^{2}}\alpha_{i,j}F_{E_{i,j}},\ \sum\limits_{\left(  u,v\right)
\in\mathbb{Z}^{2}}\beta_{u,v}F_{E_{u,v}}\right] \\
&  =\sum\limits_{\left(  u,v\right)  \in\mathbb{Z}^{2}}\beta_{u,v}%
\underbrace{\left[  \sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}%
\alpha_{i,j}F_{E_{i,j}},\ F_{E_{u,v}}\right]  }_{\substack{=\sum
\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\alpha_{i,j}\left[  F_{E_{i,j}%
},F_{E_{u,v}}\right]  \\\text{(since the Lie bracket is }\mathbb{C}%
\text{-bilinear)}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since the Lie bracket is
}\mathbb{C}\text{-bilinear}\right) \\
&  =\sum\limits_{\left(  u,v\right)  \in\mathbb{Z}^{2}}\beta_{u,v}%
\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\alpha_{i,j}%
\underbrace{\left[  F_{E_{i,j}},F_{E_{u,v}}\right]  }_{\substack{=F_{\left[
E_{i,j},E_{u,v}\right]  }\\\text{(by
(\ref{pf.glinf.glinfact.welldef.7.pf.onbasis}), applied to }r=i\text{ and
}s=j\text{)}}}\\
&  =\sum\limits_{\left(  u,v\right)  \in\mathbb{Z}^{2}}\beta_{u,v}%
\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\alpha_{i,j}F_{\left[
E_{i,j},E_{u,v}\right]  }=F_{\left[  a,b\right]  }\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.glinf.glinfact.welldef.7.pf.left})}\right)  .
\end{align*}
This proves Assertion \ref{prop.glinf.glinfact.welldef}.7.
\end{verlong}

We are now ready for the endgame:

\begin{quote}
\textit{Assertion \ref{prop.glinf.glinfact.welldef}.9:} There exists
\textbf{at least} one action of the Lie algebra $\mathfrak{gl}_{\infty}$ on
the vector space $\wedge^{\dfrac{\infty}{2},m}V$ such that all $a\in
\mathfrak{gl}_{\infty}$ and all $m$-degressions $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ satisfy%
\begin{equation}
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge....
\label{pf.glinf.glinfact.welldef.9}%
\end{equation}


\textit{Assertion \ref{prop.glinf.glinfact.welldef}.10:} There exists
\textbf{at most} one action of the Lie algebra $\mathfrak{gl}_{\infty}$ on the
vector space $\wedge^{\dfrac{\infty}{2},m}V$ such that all $a\in
\mathfrak{gl}_{\infty}$ and all $m$-degressions $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ satisfy%
\begin{equation}
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge....
\label{pf.glinf.glinfact.welldef.10}%
\end{equation}



\end{quote}

\begin{vershort}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.9:} Let $\rho$ be
the map
\begin{align*}
\mathfrak{gl}_{\infty}  &  \rightarrow\mathfrak{gl}\left(  \wedge
^{\dfrac{\infty}{2},m}V\right)  ,\\
c  &  \mapsto F_{c}.
\end{align*}
This map $\rho$ is $\mathbb{C}$-linear (by Assertion
\ref{prop.glinf.glinfact.welldef}.5) and hence a Lie algebra homomorphism (by
Assertion \ref{prop.glinf.glinfact.welldef}.7). Hence, $\rho$ is an action of
the Lie algebra $\mathfrak{gl}_{\infty}$ on the vector space $\wedge
^{\dfrac{\infty}{2},m}V$. Let us write this action in infix notation (i. e.,
let us write $c\rightharpoonup w$ for $\left(  \rho\left(  c\right)  \right)
w$ whenever $c\in\mathfrak{gl}_{\infty}$ and $w\in\wedge^{\dfrac{\infty}{2}%
,m}V$). Then, all $c\in\mathfrak{gl}_{\infty}$ and $w\in\wedge^{\dfrac{\infty
}{2},m}V$ satisfy%
\[
c\rightharpoonup w=\underbrace{\left(  \rho\left(  c\right)  \right)
}_{\substack{=F_{c}\\\text{(by the definition of }\rho\left(  c\right)
\text{)}}}w=F_{c}\left(  w\right)  .
\]
Hence, all $a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions $\left(
i_{0},i_{1},i_{2},...\right)  $ satisfy%
\begin{align*}
&  a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right) \\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...
\end{align*}
(by the definition of $F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)  $). In other words, all $a\in\mathfrak{gl}_{\infty}$ and
all $m$-degressions $\left(  i_{0},i_{1},i_{2},...\right)  $ satisfy
(\ref{pf.glinf.glinfact.welldef.9}).

We have thus constructed an action of the Lie algebra $\mathfrak{gl}_{\infty}$
on the vector space $\wedge^{\dfrac{\infty}{2},m}V$ such that all
$a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions $\left(  i_{0}%
,i_{1},i_{2},...\right)  $ satisfy (\ref{pf.glinf.glinfact.welldef.9}).
Therefore, there exists \textbf{at least} one such action. This proves
Assertion \ref{prop.glinf.glinfact.welldef}.9.
\end{vershort}

\begin{verlong}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.9:} Let $\rho$ be
the map%
\begin{align*}
\mathfrak{gl}_{\infty}  &  \rightarrow\mathfrak{gl}\left(  \wedge
^{\dfrac{\infty}{2},m}V\right)  ,\\
c  &  \mapsto F_{c}.
\end{align*}
Any $a\in\mathfrak{gl}_{\infty}$, $b\in\mathfrak{gl}_{\infty}$, $\lambda
\in\mathbb{C}$ and $\mu\in\mathbb{C}$ satisfy%
\begin{align*}
&  \lambda\underbrace{\rho\left(  a\right)  }_{\substack{=F_{a}\\\text{(by the
definition of }\rho\left(  a\right)  \text{)}}}+\mu\underbrace{\rho\left(
b\right)  }_{\substack{=F_{b}\\\text{(by the definition of }\rho\left(
b\right)  \text{)}}}\\
&  =\lambda F_{a}+\mu F_{b}=F_{\lambda a+\mu b}\ \ \ \ \ \ \ \ \ \ \left(
\text{by Assertion \ref{prop.glinf.glinfact.welldef}.5}\right) \\
&  =\rho\left(  \lambda a+\mu b\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since the definition of }\rho\left(  \lambda a+\mu b\right)  \text{
yields }\rho\left(  \lambda a+\mu b\right)  =F_{\lambda a+\mu b}\right)  .
\end{align*}
In other words, the map $\rho$ is $\mathbb{C}$-linear. Moreover, any
$a\in\mathfrak{gl}_{\infty}$ and $b\in\mathfrak{gl}_{\infty}$ satisfy%
\begin{align*}
&  \left[  \underbrace{\rho\left(  a\right)  }_{\substack{=F_{a}\\\text{(by
the definition of }\rho\left(  a\right)  \text{)}}},\underbrace{\rho\left(
b\right)  }_{\substack{=F_{b}\\\text{(by the definition of }\rho\left(
b\right)  \text{)}}}\right] \\
&  =\left[  F_{a},F_{b}\right]  =F_{\left[  a,b\right]  }%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Assertion
\ref{prop.glinf.glinfact.welldef}.7}\right) \\
&  =\rho\left(  \left[  a,b\right]  \right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since the definition of }\rho\left(  \left[  a,b\right]  \right)  \text{
yields }\rho\left(  \left[  a,b\right]  \right)  =F_{\left[  a,b\right]
}\right)  .
\end{align*}
Thus, $\rho$ is a Lie algebra homomorphism (since $\rho$ is $\mathbb{C}%
$-linear). As a consequence, $\rho$ is an action of the Lie algebra
$\mathfrak{gl}_{\infty}$ on the vector space $\wedge^{\dfrac{\infty}{2},m}V$.
Let us write this action in infix notation (i. e., let us write
$c\rightharpoonup w$ for $\left(  \rho\left(  c\right)  \right)  w$ whenever
$c\in\mathfrak{gl}_{\infty}$ and $w\in\wedge^{\dfrac{\infty}{2},m}V$). Then,
all $c\in\mathfrak{gl}_{\infty}$ and $w\in\wedge^{\dfrac{\infty}{2},m}V$
satisfy%
\begin{equation}
c\rightharpoonup w=\underbrace{\left(  \rho\left(  c\right)  \right)
}_{\substack{=F_{c}\\\text{(by the definition of }\rho\left(  c\right)
\text{)}}}w=F_{c}\left(  w\right)  . \label{pf.glinf.glinfact.welldef.9.pf.1}%
\end{equation}
Hence, all $a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions $\left(
i_{0},i_{1},i_{2},...\right)  $ satisfy%
\begin{align*}
&  a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right) \\
&  =F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.glinf.glinfact.welldef.9.pf.1}),
applied to }c=a\text{ and }w=v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right) \\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.glinf.glinfact.welldef.defFa})}\right)  .
\end{align*}
In other words, all $a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions
$\left(  i_{0},i_{1},i_{2},...\right)  $ satisfy
(\ref{pf.glinf.glinfact.welldef.9}).

We have thus constructed an action of the Lie algebra $\mathfrak{gl}_{\infty}$
on the vector space $\wedge^{\dfrac{\infty}{2},m}V$ such that all
$a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions $\left(  i_{0}%
,i_{1},i_{2},...\right)  $ satisfy (\ref{pf.glinf.glinfact.welldef.9}).
Therefore, there exists \textbf{at least} one action of the Lie algebra
$\mathfrak{gl}_{\infty}$ on the vector space $\wedge^{\dfrac{\infty}{2},m}V$
such that all $a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions $\left(
i_{0},i_{1},i_{2},...\right)  $ satisfy (\ref{pf.glinf.glinfact.welldef.9}).
This proves Assertion \ref{prop.glinf.glinfact.welldef}.9.
\end{verlong}

\begin{vershort}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.10:} Given an
action of the Lie algebra $\mathfrak{gl}_{\infty}$ on the vector space
$\wedge^{\dfrac{\infty}{2},m}V$ such that all $a\in\mathfrak{gl}_{\infty}$ and
all $m$-degressions $\left(  i_{0},i_{1},i_{2},...\right)  $ satisfy
(\ref{pf.glinf.glinfact.welldef.10}), it is clear that the value of
$a\rightharpoonup w$ is uniquely determined for every $a\in\mathfrak{gl}%
_{\infty}$ and $w\in\wedge^{\dfrac{\infty}{2},m}V$ (by the bilinearity of the
action, because $w$ can be written as a $\mathbb{C}$-linear combination of
elementary semiinfinite wedges $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...$). Hence, there exists \textbf{at most} one such action. This
proves Assertion \ref{prop.glinf.glinfact.welldef}.10.
\end{vershort}

\begin{verlong}
\textit{Proof of Assertion \ref{prop.glinf.glinfact.welldef}.10:} Define a
linear map $\rho:\mathfrak{gl}_{\infty}\rightarrow\mathfrak{gl}\left(
\wedge^{\dfrac{\infty}{2},m}V\right)  $ as in the proof of Assertion
\ref{prop.glinf.glinfact.welldef}.9. (However, here we are \textbf{not} going
to write $c\rightharpoonup w$ for $\left(  \rho\left(  c\right)  \right)  w$
whenever $c\in\mathfrak{gl}_{\infty}$ and $w\in\wedge^{\dfrac{\infty}{2},m}V$).

Let $\widetilde{\rho}:\mathfrak{gl}_{\infty}\rightarrow\mathfrak{gl}\left(
\wedge^{\dfrac{\infty}{2},m}V\right)  $ be an action of the Lie algebra
$\mathfrak{gl}_{\infty}$ on the vector space $\wedge^{\dfrac{\infty}{2},m}V$
such that all $a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions $\left(
i_{0},i_{1},i_{2},...\right)  $ satisfy (\ref{pf.glinf.glinfact.welldef.10}).
Let us write this action in infix notation (i. e., let us write
$c\rightharpoonup w$ for $\left(  \widetilde{\rho}\left(  c\right)  \right)
w$ whenever $c\in\mathfrak{gl}_{\infty}$ and $w\in\wedge^{\dfrac{\infty}{2}%
,m}V$). (Of course, the term $a\rightharpoonup\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $ in
(\ref{pf.glinf.glinfact.welldef.10}) has to be interpreted according to this
infix notation, i. e., it has to be read as $\left(  \widetilde{\rho}\left(
a\right)  \right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  $, not as $\left(  \rho\left(  a\right)  \right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $.)

Let $a\in\mathfrak{gl}_{\infty}$. We know that all $m$-degressions $\left(
i_{0},i_{1},i_{2},...\right)  $ satisfy (\ref{pf.glinf.glinfact.welldef.10}).
In other words, all $m$-degressions $\left(  i_{0},i_{1},i_{2},...\right)  $
satisfy (\ref{pf.glinf.glinfact.welldef.10}). In other words, all
$m$-degressions $\left(  i_{0},i_{1},i_{2},...\right)  $ satisfy%
\[
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge....
\]
Thus, every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $ satisfies%
\begin{align*}
&  \left(  \widetilde{\rho}\left(  a\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right) \\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...\\
&  =F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.glinf.glinfact.welldef.defFa}%
)}\right)  .
\end{align*}
Since $\widetilde{\rho}\left(  a\right)  $ and $F_{a}$ are two endomorphisms
of the $\mathbb{C}$-vector space $\wedge^{\dfrac{\infty}{2},m}V$, this shows
that $\widetilde{\rho}\left(  a\right)  =F_{a}$ (according to Assertion
\ref{prop.glinf.glinfact.welldef}.4). But $\rho\left(  a\right)  =F_{a}$ (by
the definition of $\rho\left(  a\right)  $). Hence, $\widetilde{\rho}\left(
a\right)  =\rho\left(  a\right)  $.

Now, forget that we fixed $a$. We thus have proven that $\widetilde{\rho
}\left(  a\right)  =\rho\left(  a\right)  $ for every $a\in\mathfrak{gl}%
_{\infty}$. In other words, $\widetilde{\rho}=\rho$.

Now, forget that we fixed $\widetilde{\rho}$. We have thus proven that
whenever $\widetilde{\rho}$ is an action of the Lie algebra $\mathfrak{gl}%
_{\infty}$ on the vector space $\wedge^{\dfrac{\infty}{2},m}V$ such that all
$a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions $\left(  i_{0}%
,i_{1},i_{2},...\right)  $ satisfy (\ref{pf.glinf.glinfact.welldef.10}), then
we must have $\widetilde{\rho}=\rho$. In other words, every action of the Lie
algebra $\mathfrak{gl}_{\infty}$ on the vector space $\wedge^{\dfrac{\infty
}{2},m}V$ such that all $a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions
$\left(  i_{0},i_{1},i_{2},...\right)  $ satisfy
(\ref{pf.glinf.glinfact.welldef.10}) must be equal to $\rho$. Hence, there
exists \textbf{at most} one action of the Lie algebra $\mathfrak{gl}_{\infty}$
on the vector space $\wedge^{\dfrac{\infty}{2},m}V$ such that all
$a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions $\left(  i_{0}%
,i_{1},i_{2},...\right)  $ satisfy (\ref{pf.glinf.glinfact.welldef.10}). This
proves Assertion \ref{prop.glinf.glinfact.welldef}.10.
\end{verlong}

\begin{vershort}
Combining Assertion \ref{prop.glinf.glinfact.welldef}.9 with Assertion
\ref{prop.glinf.glinfact.welldef}.10, we see that there exists \textbf{one and
only one} action of the Lie algebra $\mathfrak{gl}_{\infty}$ on the vector
space $\wedge^{\dfrac{\infty}{2},m}V$ such that all $a\in\mathfrak{gl}%
_{\infty}$ and all $m$-degressions $\left(  i_{0},i_{1},i_{2},...\right)  $
satisfy%
\[
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...
\]
In other words, Proposition \ref{prop.glinf.glinfact.welldef} is proven.
\end{vershort}

\begin{verlong}
We now know that there exists \textbf{one and only one} action of the Lie
algebra $\mathfrak{gl}_{\infty}$ on the vector space $\wedge^{\dfrac{\infty
}{2},m}V$ such that all $a\in\mathfrak{gl}_{\infty}$ and all $m$-degressions
$\left(  i_{0},i_{1},i_{2},...\right)  $ satisfy%
\[
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...
\]
(because Assertion \ref{prop.glinf.glinfact.welldef}.9 shows that there exists
\textbf{at least} one such action, and because Assertion
\ref{prop.glinf.glinfact.welldef}.10 shows that there exists \textbf{at most}
one such action). Hence, Definition \ref{def.glinf.semiinfwedge} really
defines a representation of the Lie algebra $\mathfrak{gl}_{\infty}$ on the
vector space $\wedge^{\dfrac{\infty}{2},m}V$. Proposition
\ref{prop.glinf.glinfact.welldef} is thus proven.
\end{verlong}

\begin{vershort}
\textit{Proof of Proposition \ref{prop.glinf.glinfact} and Proposition
\ref{prop.glinf.explicit}.} Both Proposition \ref{prop.glinf.glinfact} and
Proposition \ref{prop.glinf.explicit} boil down to facts that have been proven
during our proof of Proposition \ref{prop.glinf.glinfact.welldef} (indeed,
Proposition \ref{prop.glinf.glinfact} boils down to Assertion
\ref{prop.glinf.glinfact.welldef}.2, and Proposition \ref{prop.glinf.explicit}
to parts \textbf{(b)} and \textbf{(c)} of Assertion
\ref{prop.glinf.glinfact.welldef}.6).
\end{vershort}

\begin{verlong}
\textit{Proof of Proposition \ref{prop.glinf.glinfact}.} Let $\rho
:\mathfrak{gl}_{\infty}\rightarrow\mathfrak{gl}\left(  \wedge^{\dfrac{\infty
}{2},m}V\right)  $ be the action of the Lie algebra $\mathfrak{gl}_{\infty}$
on the vector space $\wedge^{\dfrac{\infty}{2},m}V$ defined in Definition
\ref{def.glinf.semiinfwedge}. Then,%
\begin{equation}
c\rightharpoonup w=\left(  \rho\left(  c\right)  \right)
w\ \ \ \ \ \ \ \ \ \ \text{for every }c\in\mathfrak{gl}_{\infty}\text{ and
}w\in\wedge^{\dfrac{\infty}{2},m}V. \label{pf.glinf.glinfact.taut}%
\end{equation}


For every $a\in\mathfrak{gl}_{\infty}$, define the map $F_{a}:\wedge
^{\dfrac{\infty}{2},m}V\rightarrow\wedge^{\dfrac{\infty}{2},m}V$ as in the
proof of Proposition \ref{prop.glinf.glinfact.welldef}.

Let $a\in\mathfrak{gl}_{\infty}$. Let $\left(  i_{0},i_{1},i_{2},...\right)  $
be an $m$-degression. Then, applying (\ref{pf.glinf.glinfact.taut}) to $c=a$
and $w=v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$, we obtain
\[
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\left(  \rho\left(  a\right)  \right)  \left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  ,
\]
so that%
\begin{align*}
&  \left(  \rho\left(  a\right)  \right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right) \\
&  =a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right) \\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...\\
&  =F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.glinf.glinfact.welldef.defFa}%
)}\right)  .
\end{align*}


Now, forget that we fixed $\left(  i_{0},i_{1},i_{2},...\right)  $. We thus
have shown that every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $
satisfies $\left(  \rho\left(  a\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =F_{a}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $. Hence, applying Assertion
\ref{prop.glinf.glinfact.welldef}.4 to $\rho\left(  a\right)  $ and $F_{a}$
instead of $f$ and $g$, we obtain $\rho\left(  a\right)  =F_{a}$ (since
$\rho\left(  a\right)  $ and $F_{a}$ are two endomorphisms of the $\mathbb{C}%
$-vector space $\wedge^{\dfrac{\infty}{2},m}V$).

Now, applying (\ref{pf.glinf.glinfact.taut}) to $c=a$ and $w=b_{0}\wedge
b_{1}\wedge b_{2}\wedge...$, we obtain%
\begin{align*}
&  a\rightharpoonup\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right) \\
&  =\underbrace{\left(  \rho\left(  a\right)  \right)  }_{=F_{a}}\left(
b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right) \\
&  =F_{a}\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right) \\
&  =\sum\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{due to the Assertion
\ref{prop.glinf.glinfact.welldef}.2 in the proof of Proposition
\ref{prop.glinf.glinfact.welldef}}\right)  .
\end{align*}
This proves Proposition \ref{prop.glinf.glinfact}.

\textit{Proof of Proposition \ref{prop.glinf.explicit}.} Let $\rho
:\mathfrak{gl}_{\infty}\rightarrow\mathfrak{gl}\left(  \wedge^{\dfrac{\infty
}{2},m}V\right)  $ be the action of the Lie algebra $\mathfrak{gl}_{\infty}$
on the vector space $\wedge^{\dfrac{\infty}{2},m}V$ defined in Definition
\ref{def.glinf.semiinfwedge}. Then,%
\begin{equation}
c\rightharpoonup w=\left(  \rho\left(  c\right)  \right)
w\ \ \ \ \ \ \ \ \ \ \text{for every }c\in\mathfrak{gl}_{\infty}\text{ and
}w\in\wedge^{\dfrac{\infty}{2},m}V. \label{pf.glinf.explicit.taut}%
\end{equation}


For every $a\in\mathfrak{gl}_{\infty}$, define the map $F_{a}:\wedge
^{\dfrac{\infty}{2},m}V\rightarrow\wedge^{\dfrac{\infty}{2},m}V$ as in the
proof of Proposition \ref{prop.glinf.glinfact.welldef}.

Let $a\in\mathfrak{gl}_{\infty}$. Let $\left(  i_{0},i_{1},i_{2},...\right)  $
be an $m$-degression. Then, applying (\ref{pf.glinf.glinfact.taut}) to $c=a$
and $w=v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$, we obtain
\[
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\left(  \rho\left(  a\right)  \right)  \left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  ,
\]
so that%
\begin{align*}
&  \left(  \rho\left(  a\right)  \right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right) \\
&  =a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right) \\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...\\
&  =F_{a}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.glinf.glinfact.welldef.defFa}%
)}\right)  .
\end{align*}


Now, forget that we fixed $\left(  i_{0},i_{1},i_{2},...\right)  $. We thus
have shown that every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $
satisfies $\left(  \rho\left(  a\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =F_{a}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $. Hence, applying Assertion
\ref{prop.glinf.glinfact.welldef}.4 to $\rho\left(  a\right)  $ and $F_{a}$
instead of $f$ and $g$, we obtain $\rho\left(  a\right)  =F_{a}$ (since
$\rho\left(  a\right)  $ and $F_{a}$ are two endomorphisms of the $\mathbb{C}%
$-vector space $\wedge^{\dfrac{\infty}{2},m}V$).

Now, forget that we fixed $a$. We thus have shown that
\[
\rho\left(  a\right)  =F_{a}\ \ \ \ \ \ \ \ \ \ \text{for every }%
a\in\mathfrak{gl}_{\infty}.
\]
Now, every $a\in\mathfrak{gl}_{\infty}$ and $w\in\wedge^{\dfrac{\infty}{2}%
,m}V$ satisfy%
\begin{align}
a\rightharpoonup w  &  =\underbrace{\left(  \rho\left(  a\right)  \right)
}_{=F_{a}}w\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.glinf.explicit.taut}%
), applied to }c=a\right) \nonumber\\
&  =F_{a}\left(  w\right)  . \label{pf.glinf.explicit.rewriter}%
\end{align}


\textbf{(a)} If $j\notin\left\{  i_{0},i_{1},i_{2},...\right\}  $, then
\begin{align*}
&  E_{i,j}\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right) \\
&  =F_{E_{i,j}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.glinf.explicit.rewriter}), applied to }a=E_{i,j}\text{ and
}w=v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{due to the Assertion
\ref{prop.glinf.glinfact.welldef}.6 \textbf{(b)} in the proof of Proposition
\ref{prop.glinf.glinfact.welldef}}\right)  .
\end{align*}
This proves Proposition \ref{prop.glinf.explicit} \textbf{(a)}.

\textbf{(b)} Assume that there exists a \textbf{unique} $\ell\in\mathbb{N}$
such that $j=i_{\ell}$. Then, for this $\ell$, we have%
\begin{align*}
&  E_{i,j}\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right) \\
&  =F_{E_{i,j}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.glinf.explicit.rewriter}), applied to }a=E_{i,j}\text{ and
}w=v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\wedge v_{i}\wedge
v_{i_{\ell+1}}\wedge v_{i_{\ell+2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{due to the Assertion \ref{prop.glinf.glinfact.welldef}.6 \textbf{(c)} in
the proof of Proposition \ref{prop.glinf.glinfact.welldef}}\\
\text{(applied to }L=\ell\text{)}%
\end{array}
\right)  .
\end{align*}
This proves Proposition \ref{prop.glinf.explicit} \textbf{(b)}.

Proposition \ref{prop.glinf.explicit} is thus completely proven.
\end{verlong}

\subsubsection{Properties of \texorpdfstring{$\wedge^{\dfrac{\infty}{2},m}V$}
{the semi-infinite wedge space}}

There is an easy way to define a grading on $\wedge^{\dfrac{\infty}{2},m}V$.
To do it, we notice that:

\begin{proposition}
\label{prop.glinf.wedge.grading}For every $m$-degression $\left(  i_{0}%
,i_{1},i_{2},...\right)  $, the sequence $\left(  i_{k}+k-m\right)  _{k\geq0}$
is a partition (i. e., a nonincreasing sequence of nonnegative integers such
that all but finitely many of its elements are $0$). In particular, every
integer $k\geq0$ satisfies $i_{k}+k-m\geq0$, and only finitely many integers
$k\geq0$ satisfy $i_{k}+k-m\neq0$. Hence, the sum $\sum\limits_{k\geq0}\left(
i_{k}+k-m\right)  $ is well-defined and equals a nonnegative integer.
\end{proposition}

The proof of this is very easy and left to the reader. As a consequence of
this proposition, we have:

\begin{definition}
\label{def.glinf.wedge.grading}Let $m\in\mathbb{Z}$. We define a grading on
the $\mathbb{C}$-vector space $\wedge^{\dfrac{\infty}{2},m}V$ by setting%
\begin{align*}
\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[  d\right]   &
=\left\langle v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\ \mid
\ \left(  i_{0},i_{1},i_{2},...\right)  \text{ is an }m\text{-degression
}\phantom{\sum\limits_{k}}\right. \\
&
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left.
\text{satisfying }\sum\limits_{k\geq0}\left(  i_{k}+k-m\right)
=-d\right\rangle \\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for every }d\in\mathbb{Z}\right.  .
\end{align*}
In other words, we define a grading on the $\mathbb{C}$-vector space
$\wedge^{\dfrac{\infty}{2},m}V$ by setting%
\[
\deg\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=-\sum\limits_{k}\left(  i_{k}+k-m\right)
\]
for every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $.

This grading satisfies $\wedge^{\dfrac{\infty}{2},m}V=\bigoplus\limits_{d\leq
0}\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[  d\right]  $ (since
Proposition \ref{prop.glinf.wedge.grading} yields that $\sum\limits_{k\geq
0}\left(  i_{k}+k-m\right)  $ is nonnegative for every $m$-degression $\left(
i_{0},i_{1},i_{2},...\right)  $). In other words, $\wedge^{\dfrac{\infty}%
{2},m}V$ is nonpositively graded.
\end{definition}

Note that, for every given $m\in\mathbb{Z}$, the $m$-degressions are in a
1-to-1 correspondence with the partitions. This correspondence maps any
$m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $ to the sequence
$\left(  i_{k}+k-m\right)  _{k\geq0}$ (this sequence is a partition due to
Proposition \ref{prop.glinf.wedge.grading}). The degree $\deg\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $ of the semiinfinite wedge
$v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ equals minus the sum of
the parts of this partition.

It is easy to check that:

\begin{proposition}
Let $m\in\mathbb{Z}$. With the grading defined in Definition
\ref{def.glinf.wedge.grading}, the $\mathfrak{gl}_{\infty}$-module
$\wedge^{\dfrac{\infty}{2},m}V$ is graded (where the grading on $\mathfrak{gl}%
_{\infty}$ is the one from Definition \ref{def.glinf.grade}).
\end{proposition}

Let us say more about this module:

\begin{proposition}
\label{prop.Lomegam}Let $m\in\mathbb{Z}$. The graded $\mathfrak{gl}_{\infty}%
$-module $\wedge^{\dfrac{\infty}{2},m}V$ is the irreducible highest-weight
representation $L_{\omega_{m}}$ of $\mathfrak{gl}_{\infty}$ with highest
weight $\omega_{m}=\left(  ...,1,1,0,0,...\right)  $, where the last $1$ is on
place $m$ and the first $0$ is on place $m+1$. Moreover, $L_{\omega_{m}}$ is unitary.
\end{proposition}

Before we prove this, let us define the vectors that will turn out to be the
highest-weight vectors:

\begin{definition}
\label{def.psim}For every $m\in\mathbb{Z}$, we denote by $\psi_{m}$ the vector
$v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\in\wedge^{\dfrac{\infty}{2},m}V$.
(This is well-defined since the infinite sequence $\left(
m,m-1,m-2,...\right)  $ is an $m$-degression.)

(Let us repeat that we are no longer using the notations of Definition
\ref{def.Cdelta}, so that this $\psi_{m}$ has nothing to do with the $\psi
_{j}$ from Definition \ref{def.Cdelta}.)

Note that $\psi_{m}\in\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[
0\right]  $ by the definition of the grading on $\wedge^{\dfrac{\infty}{2}%
,m}V$.
\end{definition}

\textit{Proof of Proposition \ref{prop.Lomegam}.} It is easy to see that
$\mathfrak{n}_{+}\cdot\psi_{m}=0$. (In fact, if $E_{i,j}\in\mathfrak{n}_{+}$
then $i<j$ and thus indices are replaced by smaller indices when computing
$E_{i,j}\rightharpoonup\psi_{m}$... For an alternative proof, just use the
fact that $\psi_{m}\in\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[
0\right]  $ and that $\wedge^{\dfrac{\infty}{2},m}V$ is concentrated in
nonpositive degrees.) Moreover, every $h\in\mathfrak{h}$ satisfies $h\psi
_{m}=\omega_{m}\left(  h\right)  \psi_{m}$ (in fact, test at $h=E_{i,i}$).
Also, $\psi_{m}$ generates the $\mathfrak{gl}_{\infty}$-module $\wedge
^{\dfrac{\infty}{2},m}V$. Thus, $\wedge^{\dfrac{\infty}{2},m}V$ is a
highest-weight representation with highest weight $\omega_{m}$ (and
highest-weight vector $\psi_{m}$).

Next let us prove that it is unitary. This will yield that it is
irreducible.\footnote{We could also show the irreducibility more directly, by
showing that every sum of wedges can be used to get back $\psi_{m}$.}

The unitarity is because the form in which the wedges are orthonormal is
$\dag$-invariant. Thus, irreducible. (We used Lemma \ref{lem.unitrick}.)
Proposition \ref{prop.Lomegam} is proven.

\begin{corollary}
\label{cor.lomegam.unit}For every finite sum $\sum\limits_{i\in\mathbb{Z}%
}k_{i}\omega_{i}$ with $k_{i}\in\mathbb{N}$, the representation $L_{\sum
\limits_{i\in\mathbb{Z}}k_{i}\omega_{i}}$ is unitary.
\end{corollary}

\textit{Proof.} Take the module $\bigotimes\limits_{i}L_{\omega_{i}}^{\otimes
k_{i}}$, and let $v$ be the tensor product of their respective highest-weight
vectors. Let $L$ be the submodule generated by $v$. Then, $L$ is a
highest-weight module, and is unitary since it is a submodule of a unitary
module. Hence it is irreducible, and thus $L\cong L_{\sum\limits_{i}%
k_{i}\omega_{i}}$, qed.

\subsection{\texorpdfstring{$\overline{\mathfrak{a}_{\infty}}$}
{a-infinity-bar}}

The Lie algebra $\mathfrak{gl}_{\infty}$ is fairly small (it doesn't even
contain the identity matrix) - too small for several applications. Here is a
larger Lie algebra with roughly similar properties:

\begin{definition}
We define $\overline{\mathfrak{a}_{\infty}}$ to be the vector space of
infinite matrices with rows and columns labeled by integers (not only positive
integers) such that only finitely many \textbf{diagonals} are nonzero. This is
an associative algebra with $1$ (due to Remark \ref{rmk.ainf.mult}
\textbf{(a)} below), and thus, by the commutator, a Lie algebra.
\end{definition}

We can think of the elements of $\overline{\mathfrak{a}_{\infty}}$ as
difference operators:

Consider $V$ as the space of sequences\footnote{In the following,
``sequences'' means ``sequences labeled by integers''.} with finitely many
nonzero entries. One very important endomorphism of $V$ is defined as follows:

\begin{definition}
\label{def.shiftoperator}Let $T:V\rightarrow V$ be the linear map given by
\[
\left(  Tx\right)  _{n}=x_{n+1}\ \ \ \ \ \ \ \ \ \ \text{for all }x\in V\text{
and }n\in\mathbb{Z}.
\]
This map $T$ is called the \textit{shift operator}. It satisfies
$Tv_{i+1}=v_{i}$ for every $i\in\mathbb{Z}$.

We can also write $T$ in the form $T=\sum\limits_{i\in\mathbb{Z}}E_{i,i+1}$,
where the sum is infinite but makes sense entrywise (i. e., for every $\left(
a,b\right)  \in\mathbb{Z}^{2}$, there are only finitely many $i\in\mathbb{Z}$
for which the matrix $E_{i,i+1}$ has nonzero $\left(  a,b\right)  $-th entry).
\end{definition}

Note that:

\begin{proposition}
\label{prop.shiftoperator.Tj}The shift operator $T$ is invertible. Every
$j\in\mathbb{Z}$ satisfies $T^{j}=\sum\limits_{i\in\mathbb{Z}}E_{i,i+j}$.
\end{proposition}

A \textit{difference operator} is an operator of the form $A=\sum
\limits_{i=p}^{q}\gamma_{i}\left(  n\right)  T^{i}$, where $p$ and $q$ are
some integers, and $\gamma_{i}:\mathbb{Z}\rightarrow\mathbb{C}$ are some
functions.\footnote{The sum $\sum\limits_{i=p}^{q}\gamma_{i}\left(  n\right)
T^{i}$ has to be understood as the linear map $X:V\rightarrow V$ given by%
\[
\left(  Xx\right)  _{n}=\sum\limits_{i=p}^{q}\gamma_{i}\left(  n\right)
x_{n+i}\ \ \ \ \ \ \ \ \ \ \text{for all }x\in V\text{ and }n\in\mathbb{Z}.
\]
} Then, $\overline{\mathfrak{a}_{\infty}}$ is the algebra of all such
operators. (These operators also act on the space of \textit{all} sequences,
not only on the space of sequences with finitely many nonzero entries.) In
particular, $T\in\overline{\mathfrak{a}_{\infty}}$, and $T^{i}\in
\overline{\mathfrak{a}_{\infty}}$ for every $i\in\mathbb{Z}$.

Note that $\overline{\mathfrak{a}_{\infty}}$ is no longer countably
dimensional. The family $\left(  E_{i,j}\right)  _{\left(  i,j\right)
\in\mathbb{Z}^{2}}$ is no longer a vector space basis, but it is a topological
basis in an appropriately defined topology.

Let us make a remark on multiplication of infinite matrices:

\begin{remark}
\label{rmk.ainf.mult}\textbf{(a)} For every $A\in\overline{\mathfrak{a}%
_{\infty}}$ and $B\in\overline{\mathfrak{a}_{\infty}}$, the matrix $AB$ is
well-defined and lies in $\overline{\mathfrak{a}_{\infty}}$.

\textbf{(b)} For every $A\in\overline{\mathfrak{a}_{\infty}}$ and
$B\in\mathfrak{gl}_{\infty}$, the matrix $AB$ is well-defined and lies in
$\mathfrak{gl}_{\infty}$.
\end{remark}

\textit{Proof of Remark \ref{rmk.ainf.mult}.} \textbf{(a)} Let $A\in
\overline{\mathfrak{a}_{\infty}}$ and $B\in\overline{\mathfrak{a}_{\infty}}$.
Write the matrix $A$ in the form $\left(  a_{i,j}\right)  _{\left(
i,j\right)  \in\mathbb{Z}^{2}}$, and write the matrix $B$ in the form $\left(
b_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$.

Since $A\in\overline{\mathfrak{a}_{\infty}}$, only finitely many diagonals of
$A$ are nonzero. Hence, there exists a finite subset $\mathfrak{A}$ of
$\mathbb{Z}$ such that%
\begin{equation}
\text{for every }u\in\mathbb{Z}\diagdown\mathfrak{A}\text{, the }u\text{-th
diagonal of }A\text{ is zero.} \label{pf.ainf.mult.1}%
\end{equation}
Consider this $\mathfrak{A}$.

Since $B\in\overline{\mathfrak{a}_{\infty}}$, only finitely many diagonals of
$B$ are nonzero. Hence, there exists a finite subset $\mathfrak{B}$ of
$\mathbb{Z}$ such that%
\begin{equation}
\text{for every }v\in\mathbb{Z}\diagdown\mathfrak{B}\text{, the }v\text{-th
diagonal of }B\text{ is zero.} \label{pf.ainf.mult.2}%
\end{equation}
Consider this $\mathfrak{B}$.

For every $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$, the infinite sum
$\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}$ has a well-defined value, because
all but finitely many addends of this sum are zero\footnote{\textit{Proof.}
Every $k\in\mathbb{Z}$ such that $k-i\notin\mathfrak{A}$ satisfies $a_{i,k}=0$
(because $k-i\notin\mathfrak{A}$, so that $k-i\in\mathbb{Z}\diagdown
\mathfrak{A}$, and thus (\ref{pf.ainf.mult.1}) (applied to $u=k-i$) yields
that the $\left(  k-i\right)  $-th diagonal of $A$ is zero, and thus $a_{i,k}$
(being an entry in this diagonal) must be $=0$). Hence, every $k\in\mathbb{Z}$
such that $k-i\notin\mathfrak{A}$ satisfies $a_{i,k}b_{k,j}=0b_{k,j}=0$. Since
$\mathfrak{A}$ is a finite set, all but finitely many $k\in\mathbb{Z}$ satisfy
$k-i\notin\mathfrak{A}$, and thus all but finitely many $k\in\mathbb{Z}$
satisfy $a_{i,k}b_{k,j}=0$ (because every $k\in\mathbb{Z}$ such that
$k-i\notin\mathfrak{A}$ satisfies $a_{i,k}b_{k,j}=0$). In other words, all but
finitely many addends of the sum $\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}$
are zero, qed.}. Hence, the matrix $AB$ is well-defined (because the matrix
$AB$ is defined as the matrix whose $\left(  i,j\right)  $-th entry is
$\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}$ for all $\left(  i,j\right)
\in\mathbb{Z}^{2}$), and satisfies%
\[
\left(  \left(  i,j\right)  \text{-th entry of the matrix }AB\right)
=\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}%
\]
for any $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$.

Now we must show that $AB\in\overline{\mathfrak{a}_{\infty}}$.

Let $\mathfrak{A}+\mathfrak{B}$ denote the set $\left\{  a+b\ \mid\ \left(
a,b\right)  \in\mathfrak{A}\times\mathfrak{B}\right\}  $. Clearly,
$\mathfrak{A}+\mathfrak{B}$ is a finite set (since $\mathfrak{A}$ and
$\mathfrak{B}$ are finite). Now, for any $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$
satisfying $j-i\notin\mathfrak{A}+\mathfrak{B}$, every $k\in\mathbb{Z}$
satisfies $a_{i,k}b_{k,j}=0$\ \ \ \ \footnote{\textit{Proof.} Let
$i\in\mathbb{Z}$ and $j\in\mathbb{Z}$ satisfy $j-i\notin\mathfrak{A}%
+\mathfrak{B}$, and let $k\in\mathbb{Z}$. Assume that $a_{i,k}b_{k,j}\neq0$.
Then, $a_{i,k}\neq0$ and $b_{k,j}\neq0$.
\par
Since $a_{i,k}$ is an entry of the $\left(  k-i\right)  $-th diagonal of $A$,
we see that some entry of the $\left(  k-i\right)  $-th diagonal of $A$ is
nonzero (since $a_{i,k}\neq0$). Hence, the $\left(  k-i\right)  $-th diagonal
of $A$ is nonzero. Thus, $k-i\notin\mathbb{Z}\diagdown\mathfrak{A}$ (because
otherwise, we would have $k-i\in\mathbb{Z}\diagdown\mathfrak{A}$, so that
(\ref{pf.ainf.mult.1}) (applied to $u=k-i$) would yield that the $\left(
k-i\right)  $-th diagonal of $A$ is zero, contradicting the fact that it is
nonzero), so that $k-i\in\mathfrak{A}$.
\par
Since $b_{k,j}$ is an entry of the $\left(  j-k\right)  $-th diagonal of $B$,
we see that some entry of the $\left(  j-k\right)  $-th diagonal of $B$ is
nonzero (since $b_{k,j}\neq0$). Hence, the $\left(  j-k\right)  $-th diagonal
of $B$ is nonzero. Thus, $j-k\notin\mathbb{Z}\diagdown\mathfrak{B}$ (because
otherwise, we would have $j-k\in\mathbb{Z}\diagdown\mathfrak{B}$, so that
(\ref{pf.ainf.mult.2}) (applied to $v=j-k$) would yield that the $\left(
j-k\right)  $-th diagonal of $B$ is zero, contradicting the fact that it is
nonzero), so that $j-k\in\mathfrak{B}$.
\par
Now, $j-i=\underbrace{\left(  k-i\right)  }_{\in\mathfrak{A}}%
+\underbrace{\left(  j-k\right)  }_{\in\mathfrak{B}}\in\mathfrak{A}%
+\mathfrak{B}$. This contradicts $j-i\notin\mathfrak{A}+\mathfrak{B}$. Thus,
our assumption that $a_{i,k}b_{k,j}\neq0$ must have been wrong. Hence,
$a_{i,k}b_{k,j}=0$, qed.}. Thus, for any $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$
satisfying $j-i\notin\mathfrak{A}+\mathfrak{B}$, we have%
\[
\left(  \left(  i,j\right)  \text{-th entry of the matrix }AB\right)
=\sum\limits_{k\in\mathbb{Z}}\underbrace{a_{i,k}b_{k,j}}%
_{\substack{=0\\\text{(since }j-i\notin\mathfrak{A}+\mathfrak{B}\text{)}%
}}=\sum\limits_{k\in\mathbb{Z}}0=0.
\]
Thus, for every integer $w\notin\mathfrak{A}+\mathfrak{B}$, and any
$i\in\mathbb{Z}$ and $j\in\mathbb{Z}$ satisfying $j-i=w$, we have $\left(
\left(  i,j\right)  \text{-th entry of the matrix }AB\right)  =0$ (since
$j-i=w\notin\mathfrak{A}+\mathfrak{B}$). In other words, for every integer
$w\notin\mathfrak{A}+\mathfrak{B}$, the $w$-th diagonal of $AB$ is zero. Since
$\mathfrak{A}+\mathfrak{B}$ is a finite set, this yields that all but finitely
many diagonals of $AB$ are zero. In other words, only finitely many diagonals
of $AB$ are nonzero. In other words, $AB\in\overline{\mathfrak{a}_{\infty}}$.
This proves Remark \ref{rmk.ainf.mult} \textbf{(a)}.

\textbf{(b)} We know from Remark \ref{rmk.ainf.mult} \textbf{(a)} that the
matrix $AB$ is well-defined (since $B\in\mathfrak{gl}_{\infty}\subseteq
\overline{\mathfrak{a}_{\infty}}$).

The matrix $B$ lies in $\mathfrak{gl}_{\infty}$ and thus has only finitely
many nonzero entries. Hence, $B$ has only finitely many nonzero rows. In other
words, there exists a finite subset $\mathfrak{R}$ of $\mathbb{Z}$ such that%
\begin{equation}
\text{for every }x\in\mathbb{Z}\diagdown\mathfrak{R}\text{, the }x\text{-th
row of }B\text{ is zero.} \label{pf.ainf.mult.3}%
\end{equation}


Also, $B$ has only finitely many nonzero entries, and thus only finitely many
nonzero columns. In other words, there exists a finite subset $\mathfrak{C}$
of $\mathbb{Z}$ such that%
\begin{equation}
\text{for every }y\in\mathbb{Z}\diagdown\mathfrak{C}\text{, the }y\text{-th
column of }B\text{ is zero.} \label{pf.ainf.mult.4}%
\end{equation}


Define $\mathfrak{A}$ as in the proof of Remark \ref{rmk.ainf.mult}
\textbf{(a)}. Let $\mathfrak{R}-\mathfrak{A}$ denote the set $\left\{
r-a\ \mid\ \left(  r,a\right)  \in\mathfrak{R}\times\mathfrak{A}\right\}  $.
Clearly, $\mathfrak{R}-\mathfrak{A}$ is a finite set (since $\mathfrak{A}$ and
$\mathfrak{R}$ are finite), and thus $\left(  \mathfrak{R}-\mathfrak{A}%
\right)  \times\mathfrak{C}$ is a finite set (since $\mathfrak{C}$, too, is
finite). Now, for any $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$ satisfying
$\left(  i,j\right)  \notin\left(  \mathfrak{R}-\mathfrak{A}\right)
\times\mathfrak{C}$, we have $\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}%
=0$\ \ \ \ \footnote{\textit{Proof.} Let $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$
be such that $\left(  i,j\right)  \notin\left(  \mathfrak{R}-\mathfrak{A}%
\right)  \times\mathfrak{C}$. Assume that $\sum\limits_{k\in\mathbb{Z}}%
a_{i,k}b_{k,j}\neq0$. Then, there exists some $k\in\mathbb{Z}$ such that
$a_{i,k}b_{k,j}\neq0$. Consider this $k$.
\par
Since $a_{i,k}b_{k,j}\neq0$, we have $a_{i,k}\neq0$ and $b_{k,j}\neq0$.
\par
Since $a_{i,k}$ is an entry of the $\left(  k-i\right)  $-th diagonal of $A$,
we see that some entry of the $\left(  k-i\right)  $-th diagonal of $A$ is
nonzero (since $a_{i,k}\neq0$). Hence, the $\left(  k-i\right)  $-th diagonal
of $A$ is nonzero. Thus, $k-i\notin\mathbb{Z}\diagdown\mathfrak{A}$ (because
otherwise, we would have $k-i\in\mathbb{Z}\diagdown\mathfrak{A}$, so that
(\ref{pf.ainf.mult.1}) (applied to $u=k-i$) would yield that the $\left(
k-i\right)  $-th diagonal of $A$ is zero, contradicting the fact that it is
nonzero), so that $k-i\in\mathfrak{A}$.
\par
Since $b_{k,j}$ is an entry of the $k$-th row of $B$, we see that some entry
of the $k$-th row of $B$ is nonzero (since $b_{k,j}\neq0$). Hence, the $k$-th
row of $B$ is nonzero. Thus, $k\notin\mathbb{Z}\diagdown\mathfrak{R}$ (because
otherwise, we would have $k\in\mathbb{Z}\diagdown\mathfrak{R}$, so that
(\ref{pf.ainf.mult.3}) (applied to $x=k$) would yield that the $k$-th row of
$B$ is zero, contradicting the fact that it is nonzero), so that
$k\in\mathfrak{R}$.
\par
Thus, $i=\underbrace{k}_{\in\mathfrak{R}}-\underbrace{\left(  k-i\right)
}_{\in\mathfrak{A}}\in\mathfrak{R}-\mathfrak{A}$.
\par
Since $b_{k,j}$ is an entry of the $j$-th column of $B$, we see that some
entry of the $j$-th column of $B$ is nonzero (since $b_{k,j}\neq0$). Hence,
the $j$-th column of $B$ is nonzero. Thus, $j\notin\mathbb{Z}\diagdown
\mathfrak{C}$ (because otherwise, we would have $j\in\mathbb{Z}\diagdown
\mathfrak{C}$, so that (\ref{pf.ainf.mult.4}) (applied to $y=j$) would yield
that the $j$-th column of $B$ is zero, contradicting the fact that it is
nonzero), so that $j\in\mathfrak{C}$. Combined with $i\in\mathfrak{R}%
-\mathfrak{A}$, this yields $\left(  i,j\right)  \in\left(  \mathfrak{R}%
-\mathfrak{A}\right)  \times\mathfrak{C}$, contradicting $\left(  i,j\right)
\notin\left(  \mathfrak{R}-\mathfrak{A}\right)  \times\mathfrak{C}$. Hence,
the assumption that $\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}\neq0$ must
have been wrong. In other words, $\sum\limits_{k\in\mathbb{Z}}a_{i,k}%
b_{k,j}=0$, qed.}. Hence, for any $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$
satisfying $\left(  i,j\right)  \notin\left(  \mathfrak{R}-\mathfrak{A}%
\right)  \times\mathfrak{C}$, we have%
\[
\left(  \left(  i,j\right)  \text{-th entry of the matrix }AB\right)
=\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}=0.
\]
Since $\left(  \mathfrak{R}-\mathfrak{A}\right)  \times\mathfrak{C}$ is a
finite set, this yields that all but finitely many entries of the matrix $AB$
are zero. In other words, $AB$ has only finitely many nonzero entries. Thus,
$AB\in\mathfrak{gl}_{\infty}$. Remark \ref{rmk.ainf.mult} \textbf{(b)} is proven.

Let us make $\overline{\mathfrak{a}_{\infty}}$ into a graded Lie algebra:

\begin{definition}
\label{def.ainf.grade}For every $i\in\mathbb{Z}$, let $\overline
{\mathfrak{a}_{\infty}^{i}}$ be the subspace of $\overline{\mathfrak{a}%
_{\infty}}$ which consists of matrices which have nonzero entries only on the
$i$-th diagonal. (The $i$\textit{-th diagonal} consists of the entries in the
$\left(  \alpha,\beta\right)  $-th places with $\beta-\alpha=i$.)

Then, $\overline{\mathfrak{a}_{\infty}}=\bigoplus\limits_{i\in\mathbb{Z}%
}\overline{\mathfrak{a}_{\infty}^{i}}$, and this makes $\overline
{\mathfrak{a}_{\infty}}$ into a $\mathbb{Z}$-graded Lie algebra. Note that
$\overline{\mathfrak{a}_{\infty}^{0}}$ is abelian. Let $\overline
{\mathfrak{a}_{\infty}}=\mathfrak{n}_{-}\oplus\mathfrak{h}\oplus
\mathfrak{n}_{+}$ be the triangular decomposition of $\overline{\mathfrak{a}%
_{\infty}}$, so that the subspace $\mathfrak{n}_{-}=\bigoplus\limits_{i<0}%
\overline{\mathfrak{a}_{\infty}^{i}}$ is the space of all strictly
lower-triangular matrices in $\overline{\mathfrak{a}_{\infty}}$, the subspace
$\mathfrak{h}=\overline{\mathfrak{a}_{\infty}^{0}}$ is the space of all
diagonal matrices in $\overline{\mathfrak{a}_{\infty}}$, and the subspace
$\mathfrak{n}_{+}=\bigoplus\limits_{i>0}\overline{\mathfrak{a}_{\infty}^{i}}$
is the space of all strictly upper-triangular matrices in $\overline
{\mathfrak{a}_{\infty}}$.
\end{definition}

Note that this was completely analogous to Definition \ref{def.glinf.grade}.

\subsection{\texorpdfstring{$\mathfrak{a}_{\infty}$}{a-infinity} and its
action on \texorpdfstring{$\wedge^{\dfrac{\infty}{2},m}V$}{the
semi-infinite wedge space}}

\begin{definition}
\label{def.glinf.rho}Let $m\in\mathbb{Z}$. Let $\rho:\mathfrak{gl}_{\infty
}\rightarrow\operatorname*{End}\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  $
be the representation of $\mathfrak{gl}_{\infty}$ on $\wedge^{\dfrac{\infty
}{2},m}V$ defined in Definition \ref{def.glinf.semiinfwedge}.
\end{definition}

The following question poses itself naturally now: Can we extend this
representation $\rho$ to a representation of $\overline{\mathfrak{a}_{\infty}%
}$ in a reasonable way?

This question depends on what we mean by \textquotedblleft
reasonable\textquotedblright. One way to concretize this is by noticing that
$\overline{\mathfrak{a}_{\infty}}=\bigoplus\limits_{i\in\mathbb{Z}}%
\overline{\mathfrak{a}_{\infty}^{i}}$, where $\overline{\mathfrak{a}_{\infty
}^{i}}$ is the space of all matrices with nonzero entries only on the $i$-th
diagonal. For each $i\in\mathbb{Z}$, the vector space $\overline
{\mathfrak{a}_{\infty}^{i}}$ can be given the product topology (i. e., the
topology in which a net $\left(  s_{z}\right)  _{z\in Z}$ of matrices
converges to a matrix $s$ if and only if for any $\left(  m,n\right)
\in\mathbb{Z}^{2}$ satisfying $n-m=i$, the net of the $\left(  m,n\right)
$-th entries of the matrices $s_{z}$ converge to the $\left(  m,n\right)  $-th
entry of $s$ in the discrete topology). Then, $\mathfrak{gl}_{\infty}^{i}$ in
dense in $\overline{\mathfrak{a}_{\infty}^{i}}$ for every $i\in\mathbb{Z}$. We
can also make $\wedge^{\dfrac{\infty}{2},m}V$ into a topological space by
using the discrete topology. Our question can now be stated as follows: Can we
extend $\rho$ by continuity to a representation of $\overline{\mathfrak{a}%
_{\infty}}$ (where \textquotedblleft continuous\textquotedblright\ means
\textquotedblleft continuous on each $\overline{\mathfrak{a}_{\infty}^{i}}%
$\textquotedblright, since we have not defined a topology on the whole space
$\overline{\mathfrak{a}_{\infty}}$) ?

Answer: Almost, but not precisely. We cannot make $\overline{\mathfrak{a}%
_{\infty}}$ act on $\wedge^{\dfrac{\infty}{2},m}V$ in such a way that its
action extends $\rho$ continuously, but we can make a central extension of
$\overline{\mathfrak{a}_{\infty}}$ act on $\wedge^{\dfrac{\infty}{2},m}V$ in a
way that only slightly differs from $\rho$.

Let us first see what goes wrong if we try to find an extension of $\rho$ to
$\overline{\mathfrak{a}_{\infty}}$ by continuity:

For $i\neq0$, a typical element $X\in\overline{\mathfrak{a}_{\infty}^{i}}$ is
of the form $X=\sum\limits_{j\in\mathbb{Z}}z_{j}E_{j,j+i}$ with $z_{j}%
\in\mathbb{C}$. Now we can define $\rho\left(  X\right)  v=\sum\limits_{j\in
\mathbb{Z}}z_{j}\rho\left(  E_{j,j+i}\right)  v$ for every $v\in\wedge
^{\dfrac{\infty}{2},m}V$; this sum has only finitely many nonzero
addends\footnote{\textit{Proof.} We must prove that, for every $v\in
\wedge^{\dfrac{\infty}{2},m}V$, the sum $\sum\limits_{j\in\mathbb{Z}}z_{j}%
\rho\left(  E_{j,j+i}\right)  v$ has only finitely many nonzero addends. It is
clearly enough to prove this in the case when $v$ is an elementary
semiinfinite wedge. So let us WLOG assume that $v$ is an elementary
semiinfinite wedge. In other words, WLOG assume that $v=v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...$ for some $m$-degression $\left(
i_{0},i_{1},i_{2},...\right)  $. Consider this $m$-degression. By the
definition of an $m$-degression, every sufficiently high $k\in\mathbb{N}$
satisfies $i_{k}+k=m$. In other words, there exists a $K\in\mathbb{N}$ such
that every integer $k\geq K$ satisfies $i_{k}+k=m$. Consider this $K$. Then,
every integer $j\leq i_{K}$ appears in the $m$-degression $\left(  i_{0}%
,i_{1},i_{2},...\right)  $.
\par
Now, we have the following two observations:
\par
\begin{itemize}
\item Every integer $j>i_{0}-i$ satisfies $\rho\left(  E_{j,j+i}\right)  v=0$
(because for every integer $j>i_{0}-i$, we have $j+i>i_{0}$, so that the
integer $j+i$ does not appear in the $m$-degression $\left(  i_{0},i_{1}%
,i_{2},...\right)  $).
\par
\item Every integer $j\leq i_{K}$ satisfies $\rho\left(  E_{j,j+i}\right)
v=0$ (because every integer $j\leq i_{K}$ appears in the $m$-degression
$\left(  i_{0},i_{1},i_{2},...\right)  $, and because $i\neq0$).
\end{itemize}
\par
Combining these two observations, we conclude that every sufficiently large
integer $j$ satisfies $\rho\left(  E_{j,j+i}\right)  v=0$ and that every
sufficiently small integer $j$ satisfies $\rho\left(  E_{j,j+i}\right)  v=0$.
Hence, only finitely many integers $j$ satisfy $\rho\left(  E_{j,j+i}\right)
v\neq0$. Thus, the sum $\sum\limits_{j\in\mathbb{Z}}z_{j}\rho\left(
E_{j,j+i}\right)  v$ has only finitely many nonzero addends, qed.} and thus
makes sense.

But when $i=0$, we run into a problem with this approach: $\rho\left(
\sum\limits_{j\in\mathbb{Z}}z_{j}E_{j,j}\right)  v=\sum\limits_{j\in
\mathbb{Z}}z_{j}\rho\left(  E_{j,j}\right)  v$ is an infinite sum which may
very well have infinitely many nonzero addends, and thus makes no sense.

To fix this problem, we define a map $\widehat{\rho}$ which will be a
``small'' modification of $\rho$:

\begin{definition}
\label{def.glinf.rhohat.abar}Define a linear map $\widehat{\rho}%
:\overline{\mathfrak{a}_{\infty}}\rightarrow\operatorname*{End}\left(
\wedge^{\dfrac{\infty}{2},m}V\right)  $ by%
\begin{align}
\widehat{\rho}\left(  \left(  a_{i,j}\right)  _{\left(  i,j\right)
\in\mathbb{Z}^{2}}\right)   &  =\sum\limits_{\left(  i,j\right)  \in
\mathbb{Z}^{2}}a_{i,j}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right. \label{def.glinf.rhohat.generalcase}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for every }\left(  a_{i,j}\right)
_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\in\overline{\mathfrak{a}_{\infty}%
}\right. \nonumber
\end{align}
(where $1$ means the endomorphism $\operatorname*{id}$ of $\wedge
^{\dfrac{\infty}{2},m}V$). Here, the infinite sum $\sum\limits_{\left(
i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  $ is well-defined as an endomorphism of $\wedge^{\dfrac{\infty}{2}%
,m}V$, because for every $v\in\wedge^{\dfrac{\infty}{2},m}V$, the sum
$\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  v$ has only finitely many nonzero addends (as Proposition
\ref{prop.glinf.rhohat.welldef} shows).
\end{definition}

The map $\widehat{\rho}$ just defined does not extend the map $\rho$, but is
the unique continuous (in the sense explained above) extension of the map
$\widehat{\rho}\mid_{\mathfrak{gl}_{\infty}}$ to $\overline{\mathfrak{a}%
_{\infty}}$ as a linear map. The map $\widehat{\rho}\mid_{\mathfrak{gl}%
_{\infty}}$ is, in a certain sense, a ``very close approximation to $\rho$'',
as can be seen from the following remark:

\begin{remark}
\label{rmk.glinf.rhohat.abar}From Definition \ref{def.glinf.rhohat.abar}, it
follows that%
\begin{equation}
\widehat{\rho}\left(  E_{i,j}\right)  =\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }\left(  i,j\right)
\in\mathbb{Z}^{2}. \label{def.glinf.rhohat}%
\end{equation}

\end{remark}

We are not done yet: This map $\widehat{\rho}$ is not a representation of
$\overline{\mathfrak{a}_{\infty}}$. We will circumvent this by defining a
central extension $\mathfrak{a}_{\infty}$ of $\overline{\mathfrak{a}_{\infty}%
}$ for which the map $\widehat{\rho}$ (once suitably extended) will be a
representation. But first, let us show a lemma that we owe for the definition
of $\widehat{\rho}$:

\begin{proposition}
\label{prop.glinf.rhohat.welldef}Let $\left(  a_{i,j}\right)  _{\left(
i,j\right)  \in\mathbb{Z}^{2}}\in\overline{\mathfrak{a}_{\infty}}$ and
$v\in\wedge^{\dfrac{\infty}{2},m}V$. Then, the sum%
\[
\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  v
\]
has only finitely many nonzero addends.
\end{proposition}

\textit{Proof of Proposition \ref{prop.glinf.rhohat.welldef}.} We know that
$v$ is an element of $\wedge^{\dfrac{\infty}{2},m}V$. Hence, $v$ is a
$\mathbb{C}$-linear combination of elements of the form $v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...$ with $\left(  i_{0},i_{1},i_{2}%
,...\right)  $ being an $m$-degression (since $\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  _{\left(  i_{0},i_{1}%
,i_{2},...\right)  \text{ is an }m\text{-degression}}$ is a basis of
$\wedge^{\dfrac{\infty}{2},m}V$). Hence, we can WLOG assume that $v$ is an
element of the form $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ with
$\left(  i_{0},i_{1},i_{2},...\right)  $ being an $m$-degression (because the
claim of Proposition \ref{prop.glinf.rhohat.welldef} is clearly linear in
$v$). Assume this. Then, $v=v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...$ for some $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $.
Consider this $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $. By the
definition of an $m$-degression, every sufficiently high $k\in\mathbb{N}$
satisfies $i_{k}+k=m$. In other words, there exists a $K\in\mathbb{N}$ such
that every integer $k\geq K$ satisfies $i_{k}+k=m$. Consider this $K$. Then,
every integer which is less or equal to $i_{K}$ appears in the $m$-degression
$\left(  i_{0},i_{1},i_{2},...\right)  $.

For every $\left(  i,j\right)  \in\mathbb{Z}^{2}$, let $r_{i,j}$ be the map
$\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  \in\operatorname*{End}\left(  \wedge^{\dfrac{\infty}{2},m}V\right)
$. Then, the sum%
\[
\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  v
\]
clearly rewrites as $\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}%
}a_{i,j}r_{i,j}v$. Hence, in order to prove Proposition
\ref{prop.glinf.rhohat.welldef}, we only need to prove that the sum
$\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}r_{i,j}v$ has only
finitely many nonzero addends.

Since $\left(  a_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}%
\in\overline{\mathfrak{a}_{\infty}}$, only finitely many diagonals of the
matrix $\left(  a_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$ are
nonzero. In other words, there exists an $M\in\mathbb{N}$ such that%
\begin{equation}
\left(  \text{the }m\text{-th diagonal of the matrix }\left(  a_{i,j}\right)
_{\left(  i,j\right)  \in\mathbb{Z}^{2}}\text{ is zero for every }%
m\in\mathbb{Z}\text{ such that }\left\vert m\right\vert \geq M\right)  .
\label{pf.glinf.rhohat.welldef.M}%
\end{equation}
Consider this $M$.

Now, we have the following three observations:

\begin{itemize}
\item Every $\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that $j>\max\left\{
i_{0},0\right\}  $ satisfies $r_{i,j}v=0\ \ \ \ $\footnote{\textit{Proof.} Let
$\left(  i,j\right)  \in\mathbb{Z}^{2}$ be such that $j>\max\left\{
i_{0},0\right\}  $. Then, $j>i_{0}$ and $j>0$.
\par
Since $j>i_{0}$, the integer $j$ does not appear in the $m$-degression
$\left(  i_{0},i_{1},i_{2},...\right)  $. Hence, $\rho\left(  E_{i,j}\right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =0$. Since
$v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...=v$, this rewrites as
$\rho\left(  E_{i,j}\right)  v=0$.
\par
Since $j>0$, we cannot have $i=j$ and $i\leq0$. Now, $r_{i,j}=\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  =\rho\left(  E_{i,j}\right)  $ (since we cannot have $i=j$ and
$i\leq0$), so that $r_{i,j}v=\rho\left(  E_{i,j}\right)  v=0$, qed.} and thus
$a_{i,j}\underbrace{r_{i,j}v}_{=0}=0$.

\item Every $\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that $i\leq
\min\left\{  i_{K},0\right\}  $ satisfies $r_{i,j}v=0$%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\mathbb{Z}^{2}$
be such that $i\leq\min\left\{  i_{K},0\right\}  $. Then, $i\leq i_{K}$ and
$i\leq0$.
\par
Since $i\leq i_{K}$, the integer $i$ appears in the $m$-degression $\left(
i_{0},i_{1},i_{2},...\right)  $ (because every integer which is less or equal
to $i_{K}$ appears in the $m$-degression $\left(  i_{0},i_{1},i_{2}%
,...\right)  $). We now must be in one of the following two cases:
\par
\textit{Case 1:} We have $i\neq j$.
\par
\textit{Case 2:} We have $i=j$.
\par
Let us first consider Case 1. In this case, $i\neq j$. Thus, $\rho\left(
E_{i,j}\right)  v=0$ (because the integer $i$ appears in the $m$-degression
$\left(  i_{0},i_{1},i_{2},...\right)  $, so that after applying $\rho\left(
E_{i,j}\right)  $ to $v=v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$,
we obtain a wedge in which $v_{i}$ appears twice). On the other hand, $i\neq
j$, so that we cannot have $i=j$ and $i\leq0$. Now, $r_{i,j}=\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  =\rho\left(  E_{i,j}\right)  $ (since we cannot have $i=j$ and
$i\leq0$), and thus $r_{i,j}v=\rho\left(  E_{i,j}\right)  v=0$.
\par
Now, let us consider Case 2. In this case, $i=j$. Thus, $r_{i,j}=\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  =\rho\left(  E_{i,j}\right)  -1$ (since $i=j$ and $i\leq0$). Since
$E_{i,j}=E_{i,i}$ (because $j=i$), this rewrites as $r_{i,j}=\rho\left(
E_{i,i}\right)  -1$. On the other hand, the integer $i$ appears in the
$m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $, so that $\rho\left(
E_{i,i}\right)  v=v$. Hence, from $r_{i,j}=\rho\left(  E_{i,i}\right)  -1$, we
get $r_{i,j}v=\left(  \rho\left(  E_{i,i}\right)  -1\right)
v=\underbrace{\rho\left(  E_{i,i}\right)  v}_{=v}-v=v-v=0$.
\par
Thus, in each of the cases 1 and 2, we have proven that $r_{i,j}v=0$. Hence,
$r_{i,j}v=0$ always holds, qed.} and thus $a_{i,j}\underbrace{r_{i,j}v}%
_{=0}=0$.

\item Every $\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that $\left\vert
i-j\right\vert \geq M$ satisfies $a_{i,j}=0$\ \ \ \ \footnote{\textit{Proof.}
Let $\left(  u,v\right)  \in\mathbb{Z}^{2}$ be such that $\left\vert
u-v\right\vert \geq M$. Then, since $\left\vert v-u\right\vert =\left\vert
u-v\right\vert \geq M$, the $\left(  v-u\right)  $-th diagonal of the matrix
$\left(  a_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$ is zero (by
(\ref{pf.glinf.rhohat.welldef.M}), applied to $m=v-u$), and thus $a_{u,v}=0$
(since $a_{u,v}$ is an entry on the $\left(  v-u\right)  $-th diagonal of the
matrix $\left(  a_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$). We
thus have shown that every $\left(  u,v\right)  \in\mathbb{Z}^{2}$ such that
$\left\vert u-v\right\vert \geq M$ satisfies $a_{u,v}=0$. Renaming $\left(
u,v\right)  $ as $\left(  i,j\right)  $ in this fact, we obtain: Every
$\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that $\left\vert i-j\right\vert
\geq M$ satisfies $a_{i,j}=0$, qed.} and thus $\underbrace{a_{i,j}}%
_{=0}r_{i,j}v=0$.
\end{itemize}

Now, for any $\alpha\in\mathbb{Z}$ and $\beta\in\mathbb{Z}$, let $\left[
\alpha,\beta\right]  _{\mathbb{Z}}$ denote the set $\left\{  x\in
\mathbb{Z}\ \mid\ \alpha\leq x\leq\beta\right\}  $ (this set is finite). It is
easy to see that%
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{every }\left(  i,j\right)  \in\mathbb{Z}^{2}\text{ such that }%
a_{i,j}r_{i,j}v\neq0\text{ satisfies}\\
\left(  i,j\right)  \in\left[  \min\left\{  i_{K},0\right\}  +1,\max\left\{
i_{0},0\right\}  +M-1\right]  _{\mathbb{Z}}\times\left[  \min\left\{
i_{K},0\right\}  -M+2,\max\left\{  i_{0},0\right\}  \right]  _{\mathbb{Z}}%
\end{array}
\right)  \label{pf.gli.rhohat.welldef.bnd}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.gli.rhohat.welldef.bnd}):} Let $\left(
i,j\right)  \in\mathbb{Z}^{2}$ be such that $a_{i,j}r_{i,j}v\neq0$. Then, we
cannot have $j>\max\left\{  i_{0},0\right\}  $ (since every $\left(
i,j\right)  \in\mathbb{Z}^{2}$ such that $j>\max\left\{  i_{0},0\right\}  $
satisfies $a_{i,j}r_{i,j}v=0$, whereas we have $a_{i,j}r_{i,j}v\neq0$). In
other words, $j\leq\max\left\{  i_{0},0\right\}  $. Also, we cannot have
$i\leq\min\left\{  i_{K},0\right\}  $ (since every $\left(  i,j\right)
\in\mathbb{Z}^{2}$ such that $i\leq\min\left\{  i_{K},0\right\}  $ satisfies
$a_{i,j}r_{i,j}v=0$, whereas we have $a_{i,j}r_{i,j}v\neq0$). Thus, we have
$i>\min\left\{  i_{K},0\right\}  $, so that $i\geq\min\left\{  i_{K}%
,0\right\}  +1$ (since $i$ and $\min\left\{  i_{K},0\right\}  $ are integers).
Finally, we cannot have $\left\vert i-j\right\vert \geq M$ (since every
$\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that $\left\vert i-j\right\vert
\geq M$ satisfies $a_{i,j}r_{i,j}v=0$, whereas we have $a_{i,j}r_{i,j}v\neq
0$). Thus, we have $\left\vert i-j\right\vert <M$, so that $\left\vert
i-j\right\vert \leq M-1$ (since $\left\vert i-j\right\vert $ and $M$ are
integers). Thus, $i-j\leq\left\vert i-j\right\vert \leq M-1$. Hence,
$i\leq\underbrace{j}_{\leq\max\left\{  i_{0},0\right\}  }+M-1\leq\max\left\{
i_{0},0\right\}  +M-1$. Combined with $i\geq\min\left\{  i_{K},0\right\}  +1$,
this yields $i\in\left[  \min\left\{  i_{K},0\right\}  +1,\max\left\{
i_{0},0\right\}  +M-1\right]  _{\mathbb{Z}}$. From $i-j\leq M-1$, we also
obtain $j\geq\underbrace{i}_{\geq\min\left\{  i_{K},0\right\}  +1}-\left(
M-1\right)  \geq\min\left\{  i_{K},0\right\}  +1-\left(  M-1\right)
=\min\left\{  i_{K},0\right\}  -M+2$. Combined with $j\leq\max\left\{
i_{0},0\right\}  $, this yields $j\in\left[  \min\left\{  i_{K},0\right\}
-M+2,\max\left\{  i_{0},0\right\}  \right]  _{\mathbb{Z}}$. Combined with
$i\in\left[  \min\left\{  i_{K},0\right\}  +1,\max\left\{  i_{0},0\right\}
+M-1\right]  _{\mathbb{Z}}$, this yields $\left(  i,j\right)  \in\left[
\min\left\{  i_{K},0\right\}  +1,\max\left\{  i_{0},0\right\}  +M-1\right]
_{\mathbb{Z}}\times\left[  \min\left\{  i_{K},0\right\}  -M+2,\max\left\{
i_{0},0\right\}  \right]  _{\mathbb{Z}}$. This proves
(\ref{pf.gli.rhohat.welldef.bnd}).}. Since $\left[  \min\left\{
i_{K},0\right\}  +1,\max\left\{  i_{0},0\right\}  +M-1\right]  _{\mathbb{Z}%
}\times\left[  \min\left\{  i_{K},0\right\}  -M+2,\max\left\{  i_{0}%
,0\right\}  \right]  _{\mathbb{Z}}$ is a finite set, this shows that only
finitely many $\left(  i,j\right)  \in\mathbb{Z}^{2}$ satisfy $a_{i,j}%
r_{i,j}v\neq0$. In other words, the sum $\sum\limits_{\left(  i,j\right)
\in\mathbb{Z}^{2}}a_{i,j}r_{i,j}v$ has only finitely many nonzero addends.
This proves Proposition \ref{prop.glinf.rhohat.welldef}.

Our definition of $\widehat{\rho}$ is somewhat unwieldy, since computing
$\widehat{\rho}\left(  a\right)  v$ for a matrix $a\in\overline{\mathfrak{a}%
_{\infty}}$ and a $v\in\wedge^{\dfrac{\infty}{2},m}V$ using it requires
writing $v$ as a linear combination of elementary semiinfinite wedges.
However, since our $\widehat{\rho}$ only slightly differs from $\rho$, there
are many matrices $a$ for which $\widehat{\rho}\left(  a\right)  $ behaves
exactly as $\rho\left(  a\right)  $ would if we could extend $\rho$ to
$\overline{\mathfrak{a}_{\infty}}$:

\begin{proposition}
\label{prop.glinf.ainfact}Let $m\in\mathbb{Z}$. Let $b_{0},b_{1},b_{2},...$ be
vectors in $V$ which satisfy%
\[
b_{i}=v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for sufficiently large }i.
\]
Let $a\in\overline{\mathfrak{a}_{\infty}}$. Assume that, for every integer
$i\leq0$, the $\left(  i,i\right)  $-th entry of $a$ is $0$. Then,%
\[
\left(  \widehat{\rho}\left(  a\right)  \right)  \left(  b_{0}\wedge
b_{1}\wedge b_{2}\wedge...\right)  =\sum\limits_{k\geq0}b_{0}\wedge
b_{1}\wedge...\wedge b_{k-1}\wedge\left(  a\rightharpoonup b_{k}\right)
\wedge b_{k+1}\wedge b_{k+2}\wedge....
\]
In particular, the infinite sum $\sum\limits_{k\geq0}b_{0}\wedge b_{1}%
\wedge...\wedge b_{k-1}\wedge\left(  a\rightharpoonup b_{k}\right)  \wedge
b_{k+1}\wedge b_{k+2}\wedge...$ is well-defined (i. e., all but finitely many
integers $k\geq0$ satisfy $b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}%
\wedge\left(  a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge
b_{k+2}\wedge...=0$).
\end{proposition}

\textit{Proof of Proposition \ref{prop.glinf.ainfact}.} For every $\left(
i,j\right)  \in\mathbb{Z}^{2}$, let $a_{i,j}$ be the $\left(  i,j\right)  $-th
entry of the matrix $a$. Then, $a=\left(  a_{i,j}\right)  _{\left(
i,j\right)  \in\mathbb{Z}^{2}}=\sum\limits_{\left(  i,j\right)  \in
\mathbb{Z}^{2}}a_{i,j}E_{i,j}$. But every $\left(  i,j\right)  \in
\mathbb{Z}^{2}$ such that $i=j$ and $i\leq0$ satisfies $a_{i,j}=a_{i,i}=0$
(because we assumed that, for every integer $i\leq0$, the $\left(  i,i\right)
$-th entry of $a$ is $0$). Thus, $\sum\limits_{\substack{\left(  i,j\right)
\in\mathbb{Z}^{2};\\i=j\text{ and }i\leq0}}\underbrace{a_{i,j}}_{=0}%
E_{i,j}=\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}%
^{2};\\i=j\text{ and }i\leq0}}0E_{i,j}=0$, so that%
\[
a=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}E_{i,j}%
=\underbrace{\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}%
^{2};\\i=j\text{ and }i\leq0}}0E_{i,j}}_{=0}+\sum\limits_{\substack{\left(
i,j\right)  \in\mathbb{Z}^{2};\\\text{not }\left(  i=j\text{ and }%
i\leq0\right)  }}a_{i,j}E_{i,j}=\sum\limits_{\substack{\left(  i,j\right)
\in\mathbb{Z}^{2};\\\text{not }\left(  i=j\text{ and }i\leq0\right)  }%
}a_{i,j}E_{i,j}.
\]


But from $a=\left(  a_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$,
we have%
\begin{align*}
\widehat{\rho}\left(  a\right)   &  =\widehat{\rho}\left(  \left(
a_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}\right)
=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{def.glinf.rhohat.generalcase})}\right) \\
&  =\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2};\\i=j\text{
and }i\leq0}}\underbrace{a_{i,j}}_{=0}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{\left(  i,j\right)
\in\mathbb{Z}^{2};\\\text{not }\left(  i=j\text{ and }i\leq0\right)  }%
}a_{i,j}\underbrace{\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  }_{\substack{=\rho\left(  E_{i,j}\right)  \\\text{(since we do not
have }\left(  i=j\text{ and }i\leq0\right)  \text{)}}}\\
&  =\underbrace{\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}%
^{2};\\i=j\text{ and }i\leq0}}0\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,j}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and
}i\leq0;\\
\rho\left(  E_{i,j}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and
}i\leq0
\end{array}
\right.  }_{=0}+\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}%
^{2};\\\text{not }\left(  i=j\text{ and }i\leq0\right)  }}a_{i,j}\rho\left(
E_{i,j}\right) \\
&  =\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2};\\\text{not
}\left(  i=j\text{ and }i\leq0\right)  }}a_{i,j}\rho\left(  E_{i,j}\right)  ,
\end{align*}
so that%
\begin{align*}
&  \left(  \widehat{\rho}\left(  a\right)  \right)  \left(  b_{0}\wedge
b_{1}\wedge b_{2}\wedge...\right) \\
&  =\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2};\\\text{not
}\left(  i=j\text{ and }i\leq0\right)  }}a_{i,j}\underbrace{\rho\left(
E_{i,j}\right)  \left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right)
}_{\substack{=E_{i,j}\rightharpoonup\left(  b_{0}\wedge b_{1}\wedge
b_{2}\wedge...\right)  \\=\sum\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge
b_{k-1}\wedge\left(  E_{i,j}\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge
b_{k+2}\wedge...\\\text{(by Proposition \ref{prop.glinf.glinfact}, applied to
}E_{i,j}\text{ instead of }a\text{)}}}\\
&  =\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2};\\\text{not
}\left(  i=j\text{ and }i\leq0\right)  }}a_{i,j}\sum\limits_{k\geq0}%
b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(  E_{i,j}\rightharpoonup
b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge...\\
&  =\sum\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}%
\wedge\underbrace{\left(  \sum\limits_{\substack{\left(  i,j\right)
\in\mathbb{Z}^{2};\\\text{not }\left(  i=j\text{ and }i\leq0\right)  }%
}a_{i,j}\left(  E_{i,j}\rightharpoonup b_{k}\right)  \right)  }%
_{\substack{=\left(  \sum\limits_{\substack{\left(  i,j\right)  \in
\mathbb{Z}^{2};\\\text{not }\left(  i=j\text{ and }i\leq0\right)  }%
}a_{i,j}E_{i,j}\right)  \rightharpoonup b_{k}=a\rightharpoonup b_{k}%
\\\text{(since }\sum\limits_{\substack{\left(  i,j\right)  \in\mathbb{Z}%
^{2};\\\text{not }\left(  i=j\text{ and }i\leq0\right)  }}a_{i,j}%
E_{i,j}=a\text{)}}}\wedge b_{k+1}\wedge b_{k+2}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we interchanged the summation signs; this is allowed because (as
the reader}\\
\text{can check) all but finitely many }\left(  \left(  i,j\right)  ,k\right)
\in\mathbb{Z}^{2}\times\mathbb{Z}\text{ satisfying }k\geq0\text{ and not}\\
\left(  i=j\text{ and }i\leq0\right)  \text{ satisfy }a_{i,j}\cdot b_{0}\wedge
b_{1}\wedge...\wedge b_{k-1}\wedge\left(  E_{i,j}\rightharpoonup b_{k}\right)
\wedge b_{k+1}\wedge b_{k+2}\wedge...=0
\end{array}
\right) \\
&  =\sum\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge...
\end{align*}
(and en passant, this argument has shown that the infinite sum $\sum
\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge...$ is
well-defined). This proves Proposition \ref{prop.glinf.ainfact}.

The issue that remains is that $\widehat{\rho}$ is not a representation of
$\overline{\mathfrak{a}_{\infty}}$. To mitigate this, we will define a central
extension of $\overline{\mathfrak{a}_{\infty}}$ by the so-called
\textit{Japanese cocycle}. Let us define this cocycle first:

\begin{theorem}
\label{thm.japan}For any $A\in\overline{\mathfrak{a}_{\infty}}$ and
$B\in\overline{\mathfrak{a}_{\infty}}$, we have $\widehat{\rho}\left(  \left[
A,B\right]  \right)  -\left[  \widehat{\rho}\left(  A\right)  ,\widehat{\rho
}\left(  B\right)  \right]  =\alpha\left(  A,B\right)  $ where $\alpha\left(
A,B\right)  $ is a scalar depending on $A$ and $B$ (and where we identify any
scalar $\lambda\in\mathbb{C}$ with the matrix $\lambda\cdot\operatorname*{id}%
\in\overline{\mathfrak{a}_{\infty}}$). This $\alpha\left(  A,B\right)  $ can
be computed as follows: Write $A$ and $B$ as block matrices $A=\left(
\begin{array}
[c]{cc}%
A_{11} & A_{12}\\
A_{21} & A_{22}%
\end{array}
\right)  $ and $B=\left(
\begin{array}
[c]{cc}%
B_{11} & B_{12}\\
B_{21} & B_{22}%
\end{array}
\right)  $, where the blocks are separated as follows:

- The left blocks contain the $j$-th columns for all $j\leq0$; the right
blocks contain the $j$-th columns for all $j>0$.

- The upper blocks contain the $i$-th rows for all $i\leq0$; the lower blocks
contain the $i$-th rows for all $i>0$.

Then, $\alpha\left(  A,B\right)  =\operatorname*{Tr}\left(  -B_{12}%
A_{21}+A_{12}B_{21}\right)  $. (This trace makes sense because the matrices
$A_{12}$, $B_{21}$, $A_{21}$, $B_{12}$ have only finitely many nonzero entries.)
\end{theorem}

\begin{corollary}
\label{cor.japan}The bilinear map $\alpha:\overline{\mathfrak{a}_{\infty}%
}\times\overline{\mathfrak{a}_{\infty}}\rightarrow\mathbb{C}$ defined in
Theorem \ref{thm.japan} is a $2$-cocycle on $\overline{\mathfrak{a}_{\infty}}$.

We define $\mathfrak{a}_{\infty}$ as the $1$-dimensional central extension
$\widehat{\overline{\mathfrak{a}_{\infty}}}_{\alpha}$ of $\overline
{\mathfrak{a}_{\infty}}$ by $\mathbb{C}$ using this cocycle $\alpha$ (see
Definition \ref{def.centex} for what this means).
\end{corollary}

\begin{definition}
\label{def.japan}The $2$-cocycle $\alpha:\overline{\mathfrak{a}_{\infty}%
}\times\overline{\mathfrak{a}_{\infty}}\rightarrow\mathbb{C}$ introduced in
Corollary \ref{cor.japan} is called the \textit{Japanese cocycle}.
\end{definition}

The proofs of Theorem \ref{thm.japan} and Corollary \ref{cor.japan} are a
homework problem. A few remarks on the Japanese cocycle are in order. It can
be explicitly computed by the formula%
\begin{align*}
&  \alpha\left(  \left(  a_{i,j}\right)  _{\left(  i,j\right)  \in
\mathbb{Z}^{2}},\left(  b_{i,j}\right)  _{\left(  i,j\right)  \in
\mathbb{Z}^{2}}\right) \\
&  =-\sum_{\substack{i\leq0;\\j>0}}b_{i,j}a_{j,i}+\sum_{\substack{i\leq
0;\\j>0}}a_{i,j}b_{j,i}=-\sum_{\substack{i>0;\\j\leq0}}a_{i,j}b_{j,i}%
+\sum_{\substack{i\leq0;\\j>0}}a_{i,j}b_{j,i}\\
&  =\sum_{\left(  i,j\right)  \in\mathbb{Z}^{2}}a_{i,j}b_{j,i}\left(  \left[
j>0\right]  -\left[  i>0\right]  \right)  \ \ \ \ \ \ \ \ \ \ \text{for every
}\left(  a_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}},\left(
b_{i,j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}\in\overline
{\mathfrak{a}_{\infty}}%
\end{align*}
where we are using the \textit{Iverson bracket notation}\footnote{This is the
notation $\left[  \mathcal{S}\right]  $ for the truth value of any logical
statement $\mathcal{S}$ (that is, $\left[  \mathcal{S}\right]  $ denotes the
integer $%
\begin{cases}
1, & \text{if }\mathcal{S}\text{ is true;}\\
0, & \text{if }\mathcal{S}\text{ is false}%
\end{cases}
$).}. The cocycle $\alpha$\ owes its name \textquotedblleft Japanese
cocycle\textquotedblright\ to the fact that it (first?) appeared in the work
of the Tokyo mathematical physicists Date, Jimbo, Kashiwara and
Miwa\footnote{More precisely, it is the skew-symmetric bilinear form $c$ in
the following paper:
\par
\begin{itemize}
\item Etsuro Date, Michio Jimbo, Masaki Kashiwara, Tetuji Miwa,
\textit{Transformation Groups for Soliton Equations -- Euclidean Lie Algebras
and Reduction of the KP Hierarchy}, Publ. RIMS, Kyoto Univ. 18 (1982), pp.
1077--1110.
\end{itemize}
\par
In this paper, the Lie algebras that we are denoting by $\overline
{\mathfrak{a}_{\infty}}$ and $\mathfrak{a}_{\infty}$ are called
$\mathfrak{pgl}\left(  \infty\right)  $ and $\mathfrak{gl}\left(
\infty\right)  $, respectively.}.

We are going to prove soon (Proposition \ref{prop.japan.nontr} and Corollary
\ref{cor.japan.triv}) that $\alpha$ is a nontrivial $2$-cocycle, but its
restriction to $\mathfrak{gl}_{\infty}$ is trivial. This is a strange
situation (given that $\mathfrak{gl}_{\infty}$ is a dense Lie subalgebra of
$\overline{\mathfrak{a}_{\infty}}$ with respect to a reasonably defined
topology), but we will later see the reason for this behavior.

\begin{theorem}
Let us extend the linear map $\widehat{\rho}:\overline{\mathfrak{a}_{\infty}%
}\rightarrow\operatorname*{End}\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  $
(introduced in Definition \ref{def.glinf.rhohat.abar}) to a linear map
$\widehat{\rho}:\mathfrak{a}_{\infty}\rightarrow\operatorname*{End}\left(
\wedge^{\dfrac{\infty}{2},m}V\right)  $ by setting $\widehat{\rho}\left(
K\right)  =\operatorname*{id}$. (This makes sense since $\mathfrak{a}_{\infty
}=\overline{\mathfrak{a}_{\infty}}\oplus\mathbb{C}K$ as vector spaces.) Then,
this map $\widehat{\rho}:\mathfrak{a}_{\infty}\rightarrow\operatorname*{End}%
\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  $ is a representation of
$\mathfrak{a}_{\infty}$.

Thus, $\wedge^{\dfrac{\infty}{2},m}V$ becomes an $\mathfrak{a}_{\infty}$-module.
\end{theorem}

\begin{definition}
\label{def.ainf.grade2}Since $\mathfrak{a}_{\infty}=\overline{\mathfrak{a}%
_{\infty}}\oplus\mathbb{C}K$ as vector space, we can define a grading on
$\mathfrak{a}_{\infty}$ as the direct sum of the grading on $\overline
{\mathfrak{a}_{\infty}}$ (which was defined in Definition \ref{def.ainf.grade}%
) and the trivial grading on $\mathbb{C}K$ (that is the grading which puts $K$
in degree $0$). This is easily seen to make $\mathfrak{a}_{\infty}$ a
$\mathbb{Z}$-graded Lie algebra. We will consider $\mathfrak{a}_{\infty}$ to
be $\mathbb{Z}$-graded in this way.
\end{definition}

\begin{proposition}
Let $m\in\mathbb{Z}$. With the grading defined in Definition
\ref{def.ainf.grade2}, the $\mathfrak{a}_{\infty}$-module $\wedge
^{\dfrac{\infty}{2},m}V$ is graded.
\end{proposition}

\begin{corollary}
\label{cor.japan.triv}The restriction of $\alpha$ to $\mathfrak{gl}_{\infty
}\times\mathfrak{gl}_{\infty}$ is a $2$-coboundary.
\end{corollary}

\textit{Proof of Corollary \ref{cor.japan.triv}.} Let $J$ be the block matrix
$\left(
\begin{array}
[c]{cc}%
0 & 0\\
0 & -I_{\infty}%
\end{array}
\right)  \in\overline{\mathfrak{a}_{\infty}}$, where the blocks are separated
in the same way as in Theorem \ref{thm.japan}. Define a linear map
$f:\mathfrak{gl}_{\infty}\rightarrow\mathbb{C}$ by
\[
\left(  f\left(  A\right)  =\operatorname*{Tr}\left(  JA\right)
\ \ \ \ \ \ \ \ \ \ \text{for any }A\in\mathfrak{gl}_{\infty}\right)
\]
\footnote{Note that $\operatorname*{Tr}\left(  JA\right)  $ is well-defined
for every $A\in\mathfrak{gl}_{\infty}$, since Remark \ref{rmk.ainf.mult}
\textbf{(b)} (applied to $J$ and $A$ instead of $A$ and $B$) yields that
$JA\in\mathfrak{gl}_{\infty}$.}. Then, any $A\in\mathfrak{gl}_{\infty}$ and
$B\in\mathfrak{gl}_{\infty}$ satisfy $\alpha\left(  A,B\right)  =f\left(
\left[  A,B\right]  \right)  $. This is because (for any $A\in\mathfrak{gl}%
_{\infty}$ and $B\in\mathfrak{gl}_{\infty}$) we can write the matrix $\left[
A,B\right]  $ in the form $\left[  A,B\right]  =\left(
\begin{array}
[c]{cc}%
\ast & \ast\\
\ast & \left[  A_{22},B_{22}\right]  +A_{21}B_{12}-B_{21}A_{12}%
\end{array}
\right)  $ (where asterisks mean blocks which we don't care about), so that
$J\left[  A,B\right]  =\left(
\begin{array}
[c]{cc}%
0 & 0\\
\ast & -\left(  \left[  A_{22},B_{22}\right]  +A_{21}B_{12}-B_{21}%
A_{12}\right)
\end{array}
\right)  $ and thus
\begin{align*}
&  \operatorname*{Tr}\left(  J\left[  A,B\right]  \right) \\
&  =-\operatorname*{Tr}\left(  \left[  A_{22},B_{22}\right]  +A_{21}%
B_{12}-B_{21}A_{12}\right)  =-\underbrace{\operatorname*{Tr}\left[
A_{22},B_{22}\right]  }_{=0}-\underbrace{\operatorname*{Tr}\left(
A_{21}B_{12}\right)  }_{=\operatorname*{Tr}\left(  B_{12}A_{21}\right)
}+\underbrace{\operatorname*{Tr}\left(  B_{21}A_{12}\right)  }%
_{=\operatorname*{Tr}\left(  A_{12}B_{21}\right)  }\\
&  =-\operatorname*{Tr}\left(  B_{12}A_{21}\right)  +\operatorname*{Tr}\left(
A_{12}B_{21}\right)  =\operatorname*{Tr}\left(  -B_{12}A_{21}+A_{12}%
B_{21}\right)  =\alpha\left(  A,B\right)  .
\end{align*}
The proof of Corollary \ref{cor.japan.triv} is thus finished.

But note that this proof does not extend to $\overline{\mathfrak{a}_{\infty}}%
$, because $f$ does not continuously extend to $\overline{\mathfrak{a}%
_{\infty}}$ (for any reasonable notion of continuity).

\begin{proposition}
\label{prop.japan.nontr}The $2$-cocycle $\alpha$ itself is not a $2$-coboundary.
\end{proposition}

\textit{Proof of Proposition \ref{prop.japan.nontr}.} Let $T$ be the shift
operator defined above. The span $\left\langle T^{j}\ \mid\ j\in
\mathbb{Z}\right\rangle $ is an abelian Lie subalgebra of $\overline
{\mathfrak{a}_{\infty}}$ (isomorphic to the abelian Lie algebra $\mathbb{C}%
\left[  t,t^{-1}\right]  $, and to the quotient $\overline{\mathcal{A}}$ of
the Heisenberg algebra $\mathcal{A}$ by its central subalgebra $\left\langle
K\right\rangle $). Any $2$-coboundary must become zero when restricted onto an
abelian Lie subalgebra. But the $2$-cocycle $\alpha$, restricted onto the span
$\left\langle T^{j}\ \mid\ j\in\mathbb{Z}\right\rangle $, does not become $0$,
since%
\[
\alpha\left(  T^{i},T^{j}\right)  =\left\{
\begin{array}
[c]{c}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\neq-j;\\
i,\ \ \ \ \ \ \ \ \ \ \text{if }i=-j
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for all }i,j\in\mathbb{Z}.
\]
Proposition \ref{prop.japan.nontr} is thus proven.

In this proof, we have constructed an embedding $\overline{\mathcal{A}%
}\rightarrow\overline{\mathfrak{a}_{\infty}}$ which sends $\overline{a_{j}}$
to $T^{j}$ for every $j\in\mathbb{Z}$. This embedding is crucial to what we
are going to do, so let us give it a formal definition:

\begin{definition}
\label{def.ainf.A}The map%
\[
\overline{\mathcal{A}}\rightarrow\overline{\mathfrak{a}_{\infty}%
},\ \ \ \ \ \ \ \ \ \ a_{j}\mapsto T^{j}%
\]
(where $\overline{\mathcal{A}}$ is the quotient of the Heisenberg algebra
$\mathcal{A}$ by its central subalgebra $\left\langle K\right\rangle $) is an
embedding of Lie algebras. We will regard this embedding as an inclusion, and
thus we will regard $\overline{\mathcal{A}}$ as a Lie subalgebra of
$\overline{\mathfrak{a}_{\infty}}$.

This embedding is easily seen to give rise to an embedding $\mathcal{A}%
\rightarrow\mathfrak{a}_{\infty}$ of Lie algebras which sends $K$ to $K$ and
sends $a_{j}$ to $T^{j}$ for every $j\in\mathbb{Z}$. This embedding will also
be regarded as an inclusion, so that $\mathcal{A}$ will be considered as a Lie
subalgebra of $\mathfrak{a}_{\infty}$.
\end{definition}

It is now easy to see:

\begin{proposition}
Extend our map $\widehat{\rho}:\overline{\mathfrak{a}_{\infty}}\rightarrow
\operatorname*{End}\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  $ to a map
$\mathfrak{a}_{\infty}\rightarrow\operatorname*{End}\left(  \wedge
^{\dfrac{\infty}{2},m}V\right)  $, also denoted by $\widehat{\rho}$, by
setting $\widehat{\rho}\left(  K\right)  =\operatorname*{id}$. Then, this map
$\widehat{\rho}:\mathfrak{a}_{\infty}\rightarrow\operatorname*{End}\left(
\wedge^{\dfrac{\infty}{2},m}V\right)  $ is a Lie algebra homomorphism, i. e.,
it makes $\wedge^{\dfrac{\infty}{2},m}V$ into an $\mathfrak{a}_{\infty}%
$-module. The element $K$ of $\mathfrak{a}_{\infty}$ acts as
$\operatorname*{id}$ on this module.

By means of the embedding $\mathcal{A}\rightarrow\mathfrak{a}_{\infty}$, this
$\mathfrak{a}_{\infty}$-module gives rise to an $\mathcal{A}$-module
$\wedge^{\dfrac{\infty}{2},m}V$, on which $K$ acts as $\operatorname*{id}$.
\end{proposition}

In Proposition \ref{prop.Lomegam}, we identified $\wedge^{\dfrac{\infty}{2}%
,m}V$ as an irreducible highest-weight $\mathfrak{gl}_{\infty}$-module;
similarly, we can identify it as an irreducible highest-weight $\mathfrak{a}%
_{\infty}$-module:

\begin{proposition}
\label{prop.Lomegam.a}Let $m\in\mathbb{Z}$. Let $\overline{\omega}_{m}$ be the
$\mathbb{C}$-linear map $\mathfrak{a}_{\infty}\left[  0\right]  \rightarrow
\mathbb{C}$ which sends every infinite diagonal matrix $\operatorname*{diag}%
\left(  ...,d_{-2},d_{-1},d_{0},d_{1},d_{2},...\right)  \in\overline
{\mathfrak{a}_{\infty}}$ to $\left\{
\begin{array}
[c]{c}%
\sum\limits_{j=1}^{m}d_{j},\ \ \ \ \ \ \ \ \ \ \text{if }m\geq0;\\
-\sum\limits_{j=m+1}^{0}d_{j},\ \ \ \ \ \ \ \ \ \ \text{if }m<0
\end{array}
\right.  $, and sends $K$ to $1$. Then, the graded $\mathfrak{a}_{\infty}%
$-module $\wedge^{\dfrac{\infty}{2},m}V$ is the irreducible highest-weight
representation $L_{\overline{\omega}_{m}}$ of $\mathfrak{a}_{\infty}$ with
highest weight $L_{\overline{\omega}_{m}}$. Moreover, $L_{\overline{\omega
}_{m}}$ is unitary.
\end{proposition}

\begin{remark}
Note the analogy between the weight $\overline{\omega}_{m}$ in Proposition
\ref{prop.Lomegam.a} and the weight $\omega_{m}$ in Proposition
\ref{prop.Lomegam}: The weight $\omega_{m}$ in Proposition \ref{prop.Lomegam}
sends every diagonal matrix $\operatorname*{diag}\left(  ...,d_{-2}%
,d_{-1},d_{0},d_{1},d_{2},...\right)  \in\mathfrak{gl}_{\infty}$ to
$\sum\limits_{j=-\infty}^{m}d_{j}$. Note that this sum $\sum\limits_{j=-\infty
}^{m}d_{j}$ is well-defined (because for a diagonal matrix
$\operatorname*{diag}\left(  ...,d_{-2},d_{-1},d_{0},d_{1},d_{2},...\right)  $
to lie in $\mathfrak{gl}_{\infty}$, it has to satisfy $d_{j}=0$ for all but
finitely many $j\in\mathbb{Z}$).
\end{remark}

In analogy to Corollary \ref{cor.lomegam.unit}, we can also show:

\begin{corollary}
\label{cor.lomegam.unit.a}For every finite sum $\sum\limits_{i\in\mathbb{Z}%
}k_{i}\overline{\omega}_{i}$ with $k_{i}\in\mathbb{N}$, the representation
$L_{\sum\limits_{i\in\mathbb{Z}}k_{i}\overline{\omega}_{i}}$ of $\mathfrak{a}%
_{\infty}$ is unitary.
\end{corollary}

\subsection{Virasoro actions on
\texorpdfstring{$\wedge^{\dfrac{\infty}{2},m}V$} {the semi-infinite wedge
space}}

We can also embed the Virasoro algebra $\operatorname*{Vir}$ into
$\mathfrak{a}_{\infty}$, and not just in one way, but in infinitely many ways
depending on two parameters:

\begin{proposition}
Let $\alpha\in\mathbb{C}$ and $\beta\in\mathbb{C}$. Let the
$\operatorname*{Vir}$-module $V_{\alpha,\beta}$ be defined as in Proposition
\ref{prop.Vab.1}.

For every $k\in\mathbb{Z}$, let $v_{k}=t^{-k+\alpha}\left(  dt\right)
^{\beta}\in V_{\alpha,\beta}$. Here, for any $\ell\in\mathbb{Z}$, the term
$t^{\ell+\alpha}\left(  dt\right)  ^{\beta}$ denotes $t^{\ell}t^{\alpha
}\left(  dt\right)  ^{\beta}$.

According to Proposition \ref{prop.Vab.1} \textbf{(b)}, every $m\in\mathbb{Z}$
satisfies%
\[
L_{m}v_{k}=\left(  k-\alpha-\beta\left(  m+1\right)  \right)  v_{k-m}%
\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{Z}.
\]
Thus, if we write $L_{m}$ as a matrix with respect to the basis $\left(
v_{k}\right)  _{k\in\mathbb{Z}}$ of $V_{\alpha,\beta}$, then this matrix lies
in $\overline{\mathfrak{a}_{\infty}}$ (in fact, its only nonzero diagonal is
the $m$-th one).

This defines an injective map $\overline{\varphi_{\alpha,\beta}}%
:W\rightarrow\overline{\mathfrak{a}_{\infty}}$, which sends every $L_{m}\in W$
to the matrix representing the action of $L_{m}$ on $V_{\alpha,\beta}$. This
map $\overline{\varphi_{\alpha,\beta}}$ is a Lie algebra homomorphism (since
the $\operatorname*{Vir}$-module $V_{\alpha,\beta}$ has central charge $0$, i.
e., is an $W$-module). Hence, this map $\overline{\varphi_{\alpha,\beta}}$
lifts to an injective map $\widehat{W}\rightarrow\mathfrak{a}_{\infty}$, where
$\widehat{W}$ is defined as follows: Let $\widetilde{\alpha}:\overline
{\mathfrak{a}_{\infty}}\times\overline{\mathfrak{a}_{\infty}}\rightarrow
\mathbb{C}$ be the Japanese cocycle (this cocycle has been called $\alpha$ in
Definition \ref{def.japan}, but here we use the letter $\alpha$ for something
different), and let $\widetilde{\alpha}^{\prime}:W\times W\rightarrow
\mathbb{C}$ be the restriction of this Japanese cocycle $\widetilde{\alpha
}:\overline{\mathfrak{a}_{\infty}}\times\overline{\mathfrak{a}_{\infty}%
}\rightarrow\mathbb{C}$ to $W\times W$ via the map $\overline{\varphi
_{\alpha,\beta}}\times\overline{\varphi_{\alpha,\beta}}:W\times W\rightarrow
\overline{\mathfrak{a}_{\infty}}\times\overline{\mathfrak{a}_{\infty}}$. Then,
$\widehat{W}$ denotes the central extension of $W$ defined by the $2$-cocycle
$\widetilde{\alpha}^{\prime}$.

But let us now compute $\widetilde{\alpha}^{\prime}$ and $\widehat{W}$. In
fact, from a straightforward calculation (Homework Set 4 exercise 3) it
follows that%
\[
\widetilde{\alpha}^{\prime}\left(  L_{m},L_{n}\right)  =\delta_{n,-m}\left(
\dfrac{n^{3}-n}{12}c_{\beta}+2nh_{\alpha,\beta}\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }n,m\in\mathbb{Z},
\]
where
\[
c_{\beta}=-12\beta^{2}+12\beta-2\ \ \ \ \ \ \ \ \ \ \text{and}%
\ \ \ \ \ \ \ \ \ \ h_{\alpha,\beta}=\dfrac{1}{2}\alpha\left(  \alpha
+2\beta-1\right)  .
\]
Thus, the $2$-cocycle $\widetilde{\alpha}^{\prime}$ differs from the
$2$-cocycle $\omega$ (defined in Theorem \ref{thm.H^2(W)}) merely by a
multiplicative factor ($\dfrac{c_{\beta}}{2}$) and a $2$-coboundary (which
sends every $\left(  L_{m},L_{n}\right)  $ to $\delta_{n,-m}\cdot
2nh_{\alpha,\beta}$). Thus, the central extension $\widehat{W}$ of $W$ defined
by the $2$-cocycle $\widetilde{\alpha}^{\prime}$ is isomorphic (as a Lie
algebra) to the central extension of $W$ defined by the $2$-cocycle $\omega$,
that is, to the Virasoro algebra $\operatorname*{Vir}$. This turns the Lie
algebra homomorphism $\widehat{W}\rightarrow\mathfrak{a}_{\infty}$ into a
homomorphism $\operatorname*{Vir}\rightarrow\mathfrak{a}_{\infty}$. Let us
describe this homomorphism explicitly:

Let $\widehat{L_{0}}$ be the element $\overline{\varphi_{\alpha,\beta}}\left(
L_{0}\right)  +h_{\alpha,\beta}K\in\mathfrak{a}_{\infty}$. Then, the linear
map%
\begin{align*}
\operatorname*{Vir}  &  \rightarrow\mathfrak{a}_{\infty},\\
L_{n}  &  \mapsto\overline{\varphi_{\alpha,\beta}}\left(  L_{n}\right)
\ \ \ \ \ \ \ \ \ \ \text{for }n\neq0,\\
L_{0}  &  \mapsto\widehat{L_{0}},\\
C  &  \mapsto c_{\beta}K
\end{align*}
is a Lie algebra homomorphism. Denote this map by $\varphi_{\alpha,\beta}$. By
means of this homomorphism, we can restrict the $\mathfrak{a}_{\infty}$-module
$\wedge^{\dfrac{\infty}{2},m}V$ to a $\operatorname*{Vir}$-module. Denote this
$\operatorname*{Vir}$-module by $\wedge^{\dfrac{\infty}{2},m}V_{\alpha,\beta}%
$. Note that $\wedge^{\dfrac{\infty}{2},m}V_{\alpha,\beta}$ is a Virasoro
module with central charge $c=c_{\beta}$. This $\wedge^{\dfrac{\infty}{2}%
,m}V_{\alpha,\beta}$ is called the \textit{module of semiinfinite forms}. The
vector $\psi_{m}=v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...$ (defined in
Definition \ref{def.psim}) has highest degree (namely, $0$).

We have $L_{i}\psi_{m}=0$ for $i>0$, and we have $L_{0}\psi_{m}=\dfrac{1}%
{2}\left(  \alpha-m\right)  \left(  \alpha+2\beta-1-m\right)  \psi_{m}$.
(Proof: Homework exercise.)
\end{proposition}

\begin{corollary}
Let $\alpha,\beta\in\mathbb{C}$. We have a homomorphism%
\begin{align*}
M_{\lambda}  &  \rightarrow\wedge^{\dfrac{\infty}{2},m}V_{\alpha,\beta},\\
v_{\lambda}  &  \mapsto\psi_{m}%
\end{align*}
of Virasoro modules, where%
\[
\lambda=\left(  \dfrac{1}{2}\left(  \alpha-m\right)  \left(  \alpha
+2\beta-1-m\right)  ,-12\beta^{2}+12\beta-2\right)  .
\]

\end{corollary}

We will see that this is an isomorphism for generic $\lambda$. For concrete
$\lambda$ it is not always one, and can have a rather complicated kernel.

\subsection{The dimensions of the homogeneous components of
\texorpdfstring{$\wedge ^{\dfrac{\infty}{2},m}V$}{the semi-infinite wedge
space}}

Fix $m\in\mathbb{Z}$. We already know from Definition
\ref{def.glinf.wedge.grading} that $\wedge^{\dfrac{\infty}{2},m}V$ is a graded
$\mathbb{C}$-vector space. More concretely,%
\[
\wedge^{\dfrac{\infty}{2},m}V=\bigoplus\limits_{d\geq0}\left(  \wedge
^{\dfrac{\infty}{2},m}V\right)  \left[  -d\right]  ,
\]
where every $d\geq0$ satisfies%
\[
\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[  -d\right]  =\left\langle
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\ \mid\ \sum\limits_{k\geq
0}\left(  i_{k}+k-m\right)  =d\right\rangle .
\]


We also know that the $m$-degressions are in a 1-to-1 correspondence with the
partitions. This correspondence maps any $m$-degression $\left(  i_{0}%
,i_{1},i_{2},...\right)  $ to the partition $\left(  i_{k}+k-m\right)
_{k\geq0}$; this is a partition of the integer $\sum\limits_{k\geq0}\left(
i_{k}+k-m\right)  $. As a consequence, for every integer $d\geq0$, the
$m$-degressions $\left(  i_{0},i_{1},i_{2},...\right)  $ satisfying
$\sum\limits_{k\geq0}\left(  i_{k}+k-m\right)  =d$ are in 1-to-1
correspondence with the partitions of $d$. Hence, for every integer $d\geq0$,
the number of all $m$-degressions $\left(  i_{0},i_{1},i_{2},...\right)  $
satisfying $\sum\limits_{k\geq0}\left(  i_{k}+k-m\right)  =d$ equals the
number of the partitions of $d$. Thus, for every integer $d\geq0$, we have%
\begin{align*}
&  \dim\left(  \left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[
-d\right]  \right) \\
&  =\left(  \text{the number of }m\text{-degressions }\left(  i_{0}%
,i_{1},i_{2},...\right)  \text{ satisfying}\sum\limits_{k\geq0}\left(
i_{k}+k-m\right)  =d\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge
...\right)  _{\left(  i_{0},i_{1},i_{2},...\right)  \text{ is an
}m\text{-degression satisfying }\sum\limits_{k\geq0}\left(  i_{k}+k-m\right)
=d}\\
\text{ is a basis of }\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[
-d\right]
\end{array}
\right) \\
&  =\left(  \text{the number of partitions of }d\right)  =p\left(  d\right)  ,
\end{align*}
where $p$ is the partition function. Hence:

\begin{proposition}
\label{prop.wedge.genfun}Let $m\in\mathbb{Z}$. Every integer $d\geq0$
satisfies $\dim\left(  \left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[
-d\right]  \right)  =p\left(  d\right)  $, where $p$ is the partition
function. As a consequence, in the ring of formal power series $\mathbb{C}%
\left[  \left[  q\right]  \right]  $, we have%
\[
\sum\limits_{d\geq0}\dim\left(  \left(  \wedge^{\dfrac{\infty}{2},m}V\right)
\left[  -d\right]  \right)  q^{d}=\sum\limits_{d\geq0}p\left(  d\right)
q^{d}=\dfrac{1}{\left(  1-q\right)  \left(  1-q^{2}\right)  \left(
1-q^{3}\right)  \cdots}.
\]

\end{proposition}

\subsection{The Boson-Fermion correspondence}

\begin{proposition}
\label{prop.wedge.fock}Let $m\in\mathbb{Z}$. Recall the vector $\psi_{m}$
defined in Definition \ref{def.psim}.

\textbf{(a)} As an $\mathcal{A}$-module, $\wedge^{\dfrac{\infty}{2},m}V$ is
isomorphic to the Fock module $F_{m}$. More precisely, there exists a graded
$\mathcal{A}$-module isomorphism $\widetilde{\sigma}_{m}:F_{m}\rightarrow
\wedge^{\dfrac{\infty}{2},m}V$ of $\mathcal{A}$-modules such that
$\widetilde{\sigma}_{m}\left(  1\right)  =\psi_{m}$.

\textbf{(b)} As an $\mathcal{A}$-module, $\wedge^{\dfrac{\infty}{2},m}V$ is
isomorphic to the Fock module $\widetilde{F}_{m}$. More precisely, there
exists a graded $\mathcal{A}$-module isomorphism $\sigma_{m}:\widetilde{F}%
_{m}\rightarrow\wedge^{\dfrac{\infty}{2},m}V$ of $\mathcal{A}$-modules such
that $\sigma_{m}\left(  1\right)  =\psi_{m}$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.wedge.fock}.} \textbf{(a)} Let us first
notice that in the ring $\mathbb{C}\left[  \left[  q\right]  \right]  $, we
have
\begin{align*}
\sum\limits_{d\geq0}\dim\left(  \left(  \wedge^{\dfrac{\infty}{2},m}V\right)
\left[  -d\right]  \right)  q^{d}  &  =\dfrac{1}{\left(  1-q\right)  \left(
1-q^{2}\right)  \left(  1-q^{3}\right)  \cdots}\ \ \ \ \ \ \ \ \ \ \left(
\text{by Proposition \ref{prop.wedge.genfun}}\right) \\
&  =\sum\limits_{n\geq0}\dim\left(  \underbrace{F}_{\substack{=F_{m}%
\\\text{(as vector spaces)}}}\left[  -n\right]  \right)  q^{n}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Definition \ref{def.fock.grad}}\right) \\
&  =\sum\limits_{n\geq0}\dim\left(  F_{m}\left[  -n\right]  \right)
q^{n}=\sum\limits_{d\geq0}\dim\left(  F_{m}\left[  -d\right]  \right)  q^{d}.
\end{align*}
By comparing coefficients, this yields that every integer $d\geq0$ satisfies
\begin{equation}
\dim\left(  \left(  \wedge^{\dfrac{\infty}{2},m}V\right)  \left[  -d\right]
\right)  =\dim\left(  F_{m}\left[  -d\right]  \right)  .
\label{pf.wedge.fock.dim}%
\end{equation}


We have $a_{i}\psi_{m}=0$ for all $i>0$ (by degree considerations), and we
also have $K\psi_{m}=\psi_{m}$. Besides, it is easy to see that $a_{0}\psi
_{m}=m\psi_{m}$\ \ \ \ \footnote{\textit{Proof.} The embedding $\mathcal{A}%
\rightarrow\mathfrak{a}_{\infty}$ sends $a_{0}$ to $T^{0}=\mathbf{1}$, where
$\mathbf{1}$ denotes the identity matrix in $\mathfrak{a}_{\infty}$. Thus,
$a_{0}\psi_{m}=\mathbf{1}\psi_{m}$. (Note that $\mathbf{1}\psi_{m}$ needs not
equal $\psi_{m}$ in general, since the action of $\mathfrak{a}_{\infty}$ on
$\wedge^{\dfrac{\infty}{2},m}V$ is not an associative algebra action, but just
a Lie algebra action.) Recall that $\wedge^{\dfrac{\infty}{2},m}V$ became an
$\mathfrak{a}_{\infty}$-module via the map $\widehat{\rho}$, so that
$U\psi_{m}=\widehat{\rho}\left(  U\right)  \psi_{m}$ for every $U\in
\mathfrak{a}_{\infty}$. Now,%
\begin{align*}
a_{0}\psi_{m}  &  =\mathbf{1}\psi_{m}=\sum\limits_{i\in\mathbb{Z}}E_{i,i}%
\psi_{m}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathbf{1}=\sum
\limits_{i\in\mathbb{Z}}E_{i,i}\right) \\
&  =\sum\limits_{i\in\mathbb{Z}}\underbrace{\widehat{\rho}\left(
E_{i,i}\right)  }_{\substack{=\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,i}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=i\text{ and
}i\leq0;\\
\rho\left(  E_{i,i}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=i\text{ and
}i\leq0
\end{array}
\right.  \\\text{(by the definition of }\widehat{\rho}\text{)}}%
}\underbrace{\psi_{m}}_{=v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }U\psi_{m}=\widehat{\rho}\left(
U\right)  \psi_{m}\text{ for every }U\in\mathfrak{a}_{\infty}\right) \\
&  =\sum\limits_{i\in\mathbb{Z}}\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,i}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=i\text{ and
}i\leq0;\\
\rho\left(  E_{i,i}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=i\text{ and
}i\leq0
\end{array}
\right.  \cdot v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\\
&  =\sum\limits_{\substack{i\in\mathbb{Z};\\i>0}}\underbrace{\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,i}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=i\text{ and
}i\leq0;\\
\rho\left(  E_{i,i}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=i\text{ and
}i\leq0
\end{array}
\right.  }_{=\rho\left(  E_{i,i}\right)  }\cdot v_{m}\wedge v_{m-1}\wedge
v_{m-2}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{i\in\mathbb{Z};\\i\leq
0}}\underbrace{\left\{
\begin{array}
[c]{c}%
\rho\left(  E_{i,i}\right)  ,\ \ \ \ \ \ \ \ \ \ \text{unless }i=i\text{ and
}i\leq0;\\
\rho\left(  E_{i,i}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }i=i\text{ and
}i\leq0
\end{array}
\right.  }_{=\rho\left(  E_{i,i}\right)  -1}\cdot v_{m}\wedge v_{m-1}\wedge
v_{m-2}\wedge...\\
&  =\sum\limits_{\substack{i\in\mathbb{Z};\\i>0}}\rho\left(  E_{i,i}\right)
\cdot v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...+\sum\limits_{\substack{i\in
\mathbb{Z};\\i\leq0}}\left(  \rho\left(  E_{i,i}\right)  -1\right)  \cdot
v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge....
\end{align*}
\par
Now, we distinguish between two cases:
\par
\textit{Case 1:} We have $m\geq0$.
\par
\textit{Case 2:} We have $m<0$.
\par
In Case 1, we have%
\begin{align*}
a_{0}\psi_{m}  &  =\sum\limits_{\substack{i\in\mathbb{Z};\\i>0}}\rho\left(
E_{i,i}\right)  \cdot v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...+\sum
\limits_{\substack{i\in\mathbb{Z};\\i\leq0}}\left(  \rho\left(  E_{i,i}%
\right)  -1\right)  \cdot v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...\\
&  =\sum\limits_{\substack{i\in\mathbb{Z};\\i>0;\ i>m}}\underbrace{\rho\left(
E_{i,i}\right)  \cdot v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...}%
_{\substack{=0\\\text{(since }i\text{ does not appear in the }%
m\text{-degression }\left(  m,m-1,m-2,...\right)  \text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{i\in\mathbb{Z};\\i>0;\ i\leq
m}}\underbrace{\rho\left(  E_{i,i}\right)  \cdot v_{m}\wedge v_{m-1}\wedge
v_{m-2}\wedge...}_{\substack{=v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge
...\\\text{(since }i\text{ appears in the }m\text{-degression }\left(
m,m-1,m-2,...\right)  \text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{i\in\mathbb{Z};\\i\leq
0}}\underbrace{\left(  \rho\left(  E_{i,i}\right)  -1\right)  \cdot
v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...}_{\substack{=0\\\text{(since
}i\text{ appears in the }m\text{-degression }\left(  m,m-1,m-2,...\right)
\\\text{and thus we have }\rho\left(  E_{i,i}\right)  \cdot v_{m}\wedge
v_{m-1}\wedge v_{m-2}\wedge...=v_{m}\wedge v_{m-1}\wedge v_{m-2}%
\wedge...\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since we are in Case 1, so that }%
m\geq0\right) \\
&  =\underbrace{\sum\limits_{\substack{i\in\mathbb{Z};\\i>0;\ i>m}}0}%
_{=0}+\sum\limits_{\substack{i\in\mathbb{Z};\\i>0;\ i\leq m}}\underbrace{v_{m}%
\wedge v_{m-1}\wedge v_{m-2}\wedge...}_{=\psi_{m}}+\underbrace{\sum
\limits_{\substack{i\in\mathbb{Z};\\i\leq0}}0}_{=0}=\sum
\limits_{\substack{i\in\mathbb{Z};\\i>0;\ i\leq m}}\psi_{m}=m\psi_{m}.
\end{align*}
Hence, $a_{0}\psi_{m}=m\psi_{m}$ is proven in Case 1. In Case 2, the proof of
$a_{0}\psi_{m}=m\psi_{m}$ is similar (but instead of splitting the
$\sum\limits_{\substack{i\in\mathbb{Z};\\i>0}}$ sum into a $\sum
\limits_{\substack{i\in\mathbb{Z};\\i>0;\ i>m}}$ and a $\sum
\limits_{\substack{i\in\mathbb{Z};\\i>0;\ i\leq m}}$ sum, we must now split
the $\sum\limits_{\substack{i\in\mathbb{Z};\\i\leq0}}$ sum into a
$\sum\limits_{\substack{i\in\mathbb{Z};\\i\leq0;\ i>m}}$ and a $\sum
\limits_{\substack{i\in\mathbb{Z};\\i\leq0;\ i\leq m}}$ sum). Thus, $a_{0}%
\psi_{m}=m\psi_{m}$ holds in both cases 1 and 2. In other words, the proof of
$a_{0}\psi_{m}=m\psi_{m}$ is complete.}.

Hence, Lemma \ref{lem.V=F.A.gr} (applied to $m$ and $\wedge^{\dfrac{\infty}%
{2},m}V$ instead of $\mu$ and $V$) yields that there exists a $\mathbb{Z}%
$-graded homomorphism $\widetilde{\sigma}_{m}:F_{m}\rightarrow\wedge
^{\dfrac{\infty}{2},m}V$ of $\mathcal{A}$-modules such that $\widetilde{\sigma
}_{m}\left(  1\right)  =\psi_{m}$. (An alternative way to prove the existence
of this $\widetilde{\sigma}_{m}$ would be to apply Lemma \ref{lem.singvec},
making use of the fact (Proposition \ref{prop.fockverma.A}) that $F_{m}$ is a
Verma module for $\mathcal{A}$.)

This $\widetilde{\sigma}_{m}$ is injective (since $F_{m}$ is irreducible) and
$\mathbb{Z}$-graded. Hence, for every integer $d\geq0$, it induces a
homomorphism from $F_{m}\left[  -d\right]  $ to $\left(  \wedge^{\dfrac
{\infty}{2},m}V\right)  \left[  -d\right]  $. This induced homomorphism must
be injective (since $\widetilde{\sigma}_{m}$ was injective), and thus is an
isomorphism (since the vector spaces $F_{m}\left[  -d\right]  $ and $\left(
\wedge^{\dfrac{\infty}{2},m}V\right)  \left[  -d\right]  $ have the same
dimension (by (\ref{pf.wedge.fock.dim})) and are both finite-dimensional).
Since this holds for every integer $d\geq0$, this yields that
$\widetilde{\sigma}_{m}$ itself must be an isomorphism. This proves
Proposition \ref{prop.wedge.fock} \textbf{(a)}.

Proposition \ref{prop.wedge.fock} \textbf{(b)} follows from Proposition
\ref{prop.wedge.fock} \textbf{(a)} due to Proposition \ref{prop.resc}
\textbf{(b)}.

Note that Proposition \ref{prop.wedge.fock} is surprising: It gives an
isomorphism between a space of polynomials (the Fock space $F_{m}$, also
called a \textit{bosonic space}) and a space of wedge products (the space
$\wedge^{\dfrac{\infty}{2},m}V$, also called a \textit{fermionic space});
isomorphisms like this are unheard of in finite-dimensional contexts.

\begin{definition}
We write $\mathcal{B}^{\left(  m\right)  }$ for the $\mathcal{A}$-module
$\widetilde{F}_{m}$. We write $\mathcal{B}$ for the $\mathcal{A}$-module
$\bigoplus\limits_{m}\mathcal{B}^{\left(  m\right)  }=\bigoplus\limits_{m}%
\widetilde{F}_{m}$. We write $\mathcal{F}^{\left(  m\right)  }$ for the
$\mathcal{A}$-module $\wedge^{\dfrac{\infty}{2},m}V$. We write $\mathcal{F}$
for the $\mathcal{A}$-module $\bigoplus\limits_{m}\mathcal{F}^{\left(
m\right)  }$.

The isomorphism $\sigma_{m}$ (constructed in Proposition \ref{prop.wedge.fock}
\textbf{(b)}) is thus an isomorphism $\mathcal{B}^{\left(  m\right)
}\rightarrow\mathcal{F}^{\left(  m\right)  }$. We write $\sigma$ for the
$\mathcal{A}$-module isomorphism $\bigoplus\limits_{m}\sigma_{m}%
:\mathcal{B}\rightarrow\mathcal{F}$. This $\sigma$ is called the
\textit{Boson-Fermion Correspondence}.
\end{definition}

Note that we can do the same for the Virasoro algebra: If $M_{\lambda}$ is
irreducible, then the homomorphism $M_{\lambda}\rightarrow\wedge
^{\dfrac{\infty}{2},m}V_{\alpha,\beta}$ is an isomorphism. And we know that
$\operatorname*{Vir}$ is nondegenerate, so $M_{\lambda}$ is irreducible for
Weil-generic $\lambda$.

\begin{corollary}
For generic $\alpha$ and $\beta$, the $\operatorname*{Vir}$-module
$\wedge^{\dfrac{\infty}{2},m}V_{\alpha,\beta}$ is irreducible.
\end{corollary}

But now, back to the Boson-Fermion Correspondence:

Both $\mathcal{B}$ and $\mathcal{F}$ are $\mathcal{A}$-modules, and
Proposition \ref{prop.wedge.fock} \textbf{(b)} showed us that they are
isomorphic as such through the isomorphism $\sigma:\mathcal{B}\rightarrow
\mathcal{F}$. However, $\mathcal{F}$ is also an $\mathfrak{a}_{\infty}%
$-module, whereas $\mathcal{B}$ is not. But of course, with the isomorphism
$\sigma$ being given, we can transfer the $\mathfrak{a}_{\infty}$-module
structure from $\mathcal{F}$ to $\mathcal{B}$. The same can be done with the
$\mathfrak{gl}_{\infty}$-module structure. Let us explicitly define these:

\begin{definition}
\textbf{(a)} We make $\mathcal{B}$ into an $\mathfrak{a}_{\infty}$-module by
transferring the $\mathfrak{a}_{\infty}$-module structure on $\mathcal{F}$
(given by the map $\widehat{\rho}:\mathfrak{a}_{\infty}\rightarrow
\operatorname*{End}\mathcal{F}$) to $\mathcal{B}$ via the isomorphism
$\sigma:\mathcal{B}\rightarrow\mathcal{F}$. Note that the $\mathcal{A}$-module
$\mathcal{B}$ is a restriction of the $\mathfrak{a}_{\infty}$-module
$\mathcal{B}$ (since the $\mathcal{A}$-module $\mathcal{F}$ is the restriction
of the $\mathfrak{a}_{\infty}$-module $\mathcal{F}$). We denote the
$\mathfrak{a}_{\infty}$-module structure on $\mathcal{B}$ by $\widehat{\rho
}:\mathfrak{a}_{\infty}\rightarrow\operatorname*{End}\mathcal{B}$.

\textbf{(b)} We make $\mathcal{B}$ into a $\mathfrak{gl}_{\infty}$-module by
transferring the $\mathfrak{gl}_{\infty}$-module structure on $\mathcal{F}$
(given by the map $\rho:\mathfrak{gl}_{\infty}\rightarrow\operatorname*{End}%
\mathcal{F}$) to $\mathcal{B}$ via the isomorphism $\sigma:\mathcal{B}%
\rightarrow\mathcal{F}$. We denote the $\mathfrak{gl}_{\infty}$-module
structure on $\mathcal{B}$ by $\rho:\mathfrak{gl}_{\infty}\rightarrow
\operatorname*{End}\mathcal{B}$.
\end{definition}

How do we describe these module structures on $\mathcal{B}$ explicitly (i. e.,
in formulas?) This question is answered using the so-called \textit{vertex
operator construction}.

But first, some easier things:

\begin{definition}
\label{def.createdestroy}Let $m\in\mathbb{Z}$. Let $i\in\mathbb{Z}$.

\textbf{(a)} We define the so-called $i$\textit{-th wedging operator}
$\widehat{v_{i}}:\mathcal{F}^{\left(  m\right)  }\rightarrow\mathcal{F}%
^{\left(  m+1\right)  }$ by%
\[
\widehat{v_{i}}\cdot\psi=v_{i}\wedge\psi\ \ \ \ \ \ \ \ \ \ \text{for all
}\psi\in\mathcal{F}^{\left(  m\right)  }.
\]
Here, $v_{i}\wedge\psi$ is formally defined as follows: Write $\psi$ as a
$\mathbb{C}$-linear combination of (well-defined) semiinfinite wedge products
$b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ (for instance, elementary
semiinfinite wedges); then, $v_{i}\wedge\psi$ is obtained by replacing each
such product $b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ by $v_{i}\wedge
b_{0}\wedge b_{1}\wedge b_{2}\wedge...$.

\textbf{(b)} We define the so-called $i$\textit{-th contraction operator}
$\overset{\vee}{v_{i}}:\mathcal{F}^{\left(  m\right)  }\rightarrow
\mathcal{F}^{\left(  m-1\right)  }$ as follows:

For every $m$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $, we let
$\overset{\vee}{v_{i}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  $ be%
\[
\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  i_{0},i_{1},i_{2},...\right\}
;\\
\left(  -1\right)  ^{j}v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\wedge v_{i_{j-1}}\wedge v_{i_{j+1}}\wedge v_{i_{j+2}}\wedge
...,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  i_{0},i_{1},i_{2},...\right\}
\end{array}
\right.  ,
\]
where, in the case $i\in\left\{  i_{0},i_{1},i_{2},...\right\}  $, we denote
by $j$ the integer $k$ satisfying $i_{k}=i$. Thus, the map $\overset{\vee
}{v_{i}}$ is defined on all elementary semiinfinite wedges; we extend this to
a map $\mathcal{F}^{\left(  m\right)  }\rightarrow\mathcal{F}^{\left(
m-1\right)  }$ by linearity.
\end{definition}

Note that the somewhat unwieldy definition of $\overset{\vee}{v_{i}}$ can be
slightly improved: While it only gave a formula for $m$-degressions, it is
easy to see that the same formula holds for straying $m$-degressions:

\begin{proposition}
Let $m\in\mathbb{Z}$ and $i\in\mathbb{Z}$. Let $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ be a straying $m$-degression which has no two equal
elements. Then,%
\begin{align*}
&  \overset{\vee}{v_{i}}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  i_{0},i_{1},i_{2},...\right\}
;\\
\left(  -1\right)  ^{j}v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\wedge v_{i_{j-1}}\wedge v_{i_{j+1}}\wedge v_{i_{j+2}}\wedge
...,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  i_{0},i_{1},i_{2},...\right\}
\end{array}
\right.  ,
\end{align*}
where, in the case $i\in\left\{  i_{0},i_{1},i_{2},...\right\}  $, we denote
by $j$ the integer $k$ satisfying $i_{k}=i$.
\end{proposition}

These operators satisfy the relations%
\begin{align*}
\widehat{v_{i}}\widehat{v_{j}}+\widehat{v_{j}}\widehat{v_{i}}  &
=0,\ \ \ \ \ \ \ \ \ \ \overset{\vee}{v_{i}}\overset{\vee}{v_{j}%
}+\overset{\vee}{v_{j}}\overset{\vee}{v_{i}}=0,\\
\overset{\vee}{v_{i}}\widehat{v_{j}}+\widehat{v_{j}}\overset{\vee}{v_{i}}  &
=\delta_{i,j}%
\end{align*}
for all $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$.

\begin{definition}
For every $i\in\mathbb{Z}$, define $\xi_{i}=\widehat{v_{i}}$ and $\xi
_{i}^{\ast}=\overset{\vee}{v_{i}}$.
\end{definition}

Then, all $i\in\mathbb{Z}$ and $j\in\mathbb{Z}$ satisfy $\rho\left(
E_{i,j}\right)  =\xi_{i}\xi_{j}^{\ast}$ and
\[
\widehat{\rho}\left(  E_{i,j}\right)  =\left\{
\begin{array}
[c]{c}%
\xi_{i}\xi_{j}^{\ast}-1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and }i\leq0,\\
\xi_{i}\xi_{j}^{\ast},\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and }i\leq0
\end{array}
\right.  .
\]


The $\xi_{i}$ and $\xi_{i}^{\ast}$ are called \textit{fermionic operators}.

So what are the $\xi_{i}$ in terms of $a_{j}$ ?

\subsection{The vertex operator construction}

We identify the space $\mathbb{C}\left[  z,z^{-1},x_{1},x_{2},...\right]
=\bigoplus\limits_{m}z^{m}\mathbb{C}\left[  x_{1},x_{2},...\right]  $ with
$\mathcal{B}=\bigoplus\limits_{m}\mathcal{B}^{\left(  m\right)  }$ by means of
identifying $z^{m}\mathbb{C}\left[  x_{1},x_{2},...\right]  $ with
$\mathcal{B}^{\left(  m\right)  }$ for every $m\in\mathbb{Z}$ (the
identification being made through the map%
\begin{align*}
\mathcal{B}^{\left(  m\right)  }  &  \rightarrow z^{m}\mathbb{C}\left[
x_{1},x_{2},...\right]  ,\\
p  &  \mapsto z^{m}\cdot p
\end{align*}
).

Note also that $z$ (that is, multiplication by $z$) is an isomorphism of
$\mathcal{A}_{0}$-modules, but not of $\mathcal{A}$-modules.

The Boson-Fermion correspondence goes like this:%
\[
\mathcal{F}=\bigoplus\limits_{m}\mathcal{F}^{\left(  m\right)  }%
\overset{\sigma=\bigoplus\limits_{m}\sigma_{m}}{\leftarrow}\mathcal{B}%
=\bigoplus\limits_{m}\mathcal{B}^{\left(  m\right)  }.
\]
On $\mathcal{F}$ there are operators $\widehat{v_{i}}=\xi_{i}$, $\overset{\vee
}{v_{i}}=\xi_{i}^{\ast}$, $\rho\left(  E_{i,j}\right)  =\xi_{i}\xi_{j}^{\ast}%
$, \newline$\widehat{\rho}\left(  E_{i,j}\right)  =\left\{
\begin{array}
[c]{c}%
\xi_{i}\xi_{j}^{\ast}-1,\ \ \ \ \ \ \ \ \ \ \text{if }i=j\text{ and }i\leq0,\\
\xi_{i}\xi_{j}^{\ast},\ \ \ \ \ \ \ \ \ \ \text{unless }i=j\text{ and }i\leq0
\end{array}
\right.  $. By conjugating with the Boson-Fermion correspondence $\sigma$,
these operators give rise to operators on $\mathcal{B}$. How do the latter
operators look like?

\begin{definition}
\label{def.euler.XGamma}Introduce the quantum fields%
\begin{align*}
X\left(  u\right)   &  =\sum\limits_{n\in\mathbb{Z}}\xi_{n}u^{n}\in\left(
\operatorname*{End}\mathcal{F}\right)  \left[  \left[  u,u^{-1}\right]
\right]  ,\\
X^{\ast}\left(  u\right)   &  =\sum\limits_{n\in\mathbb{Z}}\xi_{n}^{\ast
}u^{-n}\in\left(  \operatorname*{End}\mathcal{F}\right)  \left[  \left[
u,u^{-1}\right]  \right]  ,\\
\Gamma\left(  u\right)   &  =\sigma^{-1}\circ X\left(  u\right)  \circ
\sigma\in\left(  \operatorname*{End}\mathcal{B}\right)  \left[  \left[
u,u^{-1}\right]  \right]  ,\\
\Gamma^{\ast}\left(  u\right)   &  =\sigma^{-1}\circ X^{\ast}\left(  u\right)
\circ\sigma\in\left(  \operatorname*{End}\mathcal{B}\right)  \left[  \left[
u,u^{-1}\right]  \right]  .
\end{align*}
Note that $\sigma^{-1}\circ X\left(  u\right)  \circ\sigma$ is to be read as
``conjugate every term of the power series $X\left(  u\right)  $ by $\sigma
$''; in other words, $\sigma^{-1}\circ X\left(  u\right)  \circ\sigma$ means
$\sum\limits_{n\in\mathbb{Z}}\left(  \sigma^{-1}\circ\xi_{n}\circ
\sigma\right)  u^{n}$.
\end{definition}

Recall that $\xi_{n}=\widehat{v_{n}}$ sends $\mathcal{F}^{\left(  m\right)  }$
to $\mathcal{F}^{\left(  m+1\right)  }$ for any $m\in\mathbb{Z}$ and
$n\in\mathbb{Z}$. Thus, every term of the power series $X\left(  u\right)
=\sum\limits_{n\in\mathbb{Z}}\xi_{n}u^{n}$ sends $\mathcal{F}^{\left(
m\right)  }$ to $\mathcal{F}^{\left(  m+1\right)  }$ for any $m\in\mathbb{Z}$.
Abusing notation, we will abbreviate this fact by saying that $X\left(
u\right)  :\mathcal{F}^{\left(  m\right)  }\rightarrow\mathcal{F}^{\left(
m+1\right)  }$ for any $m\in\mathbb{Z}$. Similarly, $X^{\ast}\left(  u\right)
:\mathcal{F}^{\left(  m\right)  }\rightarrow\mathcal{F}^{\left(  m-1\right)
}$ for any $m\in\mathbb{Z}$ (since $\xi_{n}^{\ast}=\overset{\vee}{v_{n}}$
sends $\mathcal{F}^{\left(  m\right)  }$ to $\mathcal{F}^{\left(  m-1\right)
}$ for any $m\in\mathbb{Z}$ and $n\in\mathbb{Z}$). As a consequence,
$\Gamma\left(  u\right)  :\mathcal{B}^{\left(  m\right)  }\rightarrow
\mathcal{B}^{\left(  m+1\right)  }$ and $\Gamma^{\ast}\left(  u\right)
:\mathcal{B}^{\left(  m\right)  }\rightarrow\mathcal{B}^{\left(  m-1\right)
}$ for any $m\in\mathbb{Z}$.

Now, here is how we can describe $\Gamma\left(  u\right)  $ and $\Gamma^{\ast
}\left(  u\right)  $ (and therefore the operators $\sigma^{-1}\circ\xi
_{n}\circ\sigma$ and $\sigma^{-1}\circ\xi_{n}^{\ast}\circ\sigma$) in terms of
$\mathcal{B}$:

\begin{theorem}
\label{thm.euler}Let $m\in\mathbb{Z}$. On $\mathcal{B}^{\left(  m\right)  }$,
we have%
\begin{align*}
\Gamma\left(  u\right)   &  =u^{m+1}z\exp\left(  \sum\limits_{j>0}%
\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac
{a_{j}}{j}u^{-j}\right)  ;\\
\Gamma^{\ast}\left(  u\right)   &  =u^{-m}z^{-1}\exp\left(  -\sum
\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(  \sum
\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  .
\end{align*}
Here, $\exp A$ means $1+A+\dfrac{A^{2}}{2!}+\dfrac{A^{3}}{3!}+...$ for any $A$
for which this series makes any sense.
\end{theorem}

Let us explain what we mean by the products $\exp\left(  \sum\limits_{j>0}%
\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac
{a_{j}}{j}u^{-j}\right)  $ and $\exp\left(  -\sum\limits_{j>0}\dfrac{a_{-j}%
}{j}u^{j}\right)  \cdot\exp\left(  \sum\limits_{j>0}\dfrac{a_{j}}{j}%
u^{-j}\right)  $ in Theorem \ref{thm.euler}. Why do these products (which are
products of exponentials of infinite sums) make any sense? This is easily answered:

\begin{itemize}
\item For any $v\in\mathcal{B}^{\left(  m\right)  }$, the term $\exp\left(
-\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \left(  v\right)  $ is
well-defined and is valued in $\mathcal{B}^{\left(  m\right)  }\left[
u^{-1}\right]  $. (In fact, if we blindly expand%
\begin{align*}
\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)   &
=\sum\limits_{\ell=0}^{\infty}\dfrac{1}{\ell!}\left(  -\sum\limits_{j>0}%
\dfrac{a_{j}}{j}u^{-j}\right)  ^{\ell}\\
&  =\sum\limits_{\ell=0}^{\infty}\dfrac{1}{\ell!}\left(  -1\right)  ^{\ell
}\sum\limits_{j_{1},j_{2},...,j_{\ell}\text{ positive integers}}%
\dfrac{a_{j_{1}}a_{j_{2}}...a_{j_{\ell}}}{j_{1}j_{2}...j_{\ell}}u^{-\left(
j_{1}+j_{2}+...+j_{\ell}\right)  },
\end{align*}
and apply every term of the resulting power series to $v$, then (for fixed
$v$) only finitely many of these terms yield a nonzero result, since $v$ is a
polynomial and thus has finite degree, whereas each $a_{j}$ lowers degree by
$j$.)

\item For any $v\in\mathcal{B}^{\left(  m\right)  }$, the term $\exp\left(
\sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  v$ is well-defined and is
valued in $\mathcal{B}^{\left(  m\right)  }\left(  \left(  u\right)  \right)
$. (In fact, we have just shown that $\exp\left(  -\sum\limits_{j>0}%
\dfrac{a_{j}}{j}u^{-j}\right)  \left(  v\right)  \in\mathcal{B}^{\left(
m\right)  }\left[  u^{-1}\right]  $; therefore, applying $\exp\left(
\sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \in\left(  \operatorname*{End}%
\left(  \mathcal{B}^{\left(  m\right)  }\right)  \right)  \left[  \left[
u\right]  \right]  $ to this gives a well-defined power series in
$\mathcal{B}^{\left(  m\right)  }\left(  \left(  u\right)  \right)  $ (because
if $\mathfrak{A}$ is an algebra and $\mathfrak{M}$ is an $\mathfrak{A}%
$-module, then the application of a power series in $\mathfrak{A}\left[
\left[  u\right]  \right]  $ to an element of $\mathfrak{M}\left[
u^{-1}\right]  $ gives a well-defined element of $\mathfrak{M}\left(  \left(
u\right)  \right)  $).)

\item For any $v\in\mathcal{B}^{\left(  m\right)  }$, the term $\exp\left(
-\sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(
\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  v$ is well-defined and is
valued in $\mathcal{B}^{\left(  m\right)  }\left(  \left(  u\right)  \right)
$. (This is proven similarly.)
\end{itemize}

Thus, the formulas of Theorem \ref{thm.euler} make sense.

\begin{remark}
Here is some of physicists' intuition for the right hand sides of the
equations in Theorem \ref{thm.euler}. [Note: I (=Darij) don't fully understand
it, so don't expect me to explain it well.]

Consider the quantum field $a\left(  u\right)  =\sum\limits_{j\in\mathbb{Z}%
}a_{j}u^{-j-1}\in U\left(  \mathcal{A}\right)  \left[  \left[  u,u^{-1}%
\right]  \right]  $ defined in Section \ref{subsect.quantumfields}. Let us
work on an informal level, and pretend that integration of series in $U\left(
\mathcal{A}\right)  \left[  \left[  u,u^{-1}\right]  \right]  $ is
well-defined and behaves similar to that of functions on $\mathbb{R}$. Then,
$\int a\left(  u\right)  du=-\sum\limits_{j\neq0}\dfrac{a_{j}}{j}u^{-j}%
+a_{0}\log u$. Exponentiating this \textbf{``in the normal ordering''} (this
means we expand the series $\exp\left(  -\sum\limits_{j\neq0}\dfrac{a_{j}}%
{j}u^{-j}+a_{0}\log u\right)  $ and replace all products by their normal
ordered versions, i. e., shovel all $a_{m}$ with $m<0$ to the left and all
$a_{m}$ with $m>0$ to the right), we obtain%
\begin{align*}
&  \left.  :\exp\left(  \int a\left(  u\right)  du\right)  :\right.  =\left.
:\exp\left(  -\sum\limits_{j\neq0}\dfrac{a_{j}}{j}u^{-j}+a_{0}\log u\right)
:\right. \\
&  =\exp\left(  \underbrace{-\sum\limits_{j<0}\dfrac{a_{j}}{j}u^{-j}}%
_{=\sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}}\right)  \cdot\exp\left(  a_{0}\log
u\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right) \\
&  =\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot
\exp\left(  a_{0}\log u\right)  \cdot\exp\left(  -\sum\limits_{j>0}%
\dfrac{a_{j}}{j}u^{-j}\right)  .
\end{align*}
But for every $m\in\mathbb{Z}$, we have%
\begin{align*}
&  \Gamma\left(  u\right) \\
&  =u^{m+1}z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)
\cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.euler}}\right) \\
&  =uz\cdot\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)
\cdot\underbrace{u^{m}}_{\substack{=\exp\left(  m\log u\right)  =\exp\left(
a_{0}\log u\right)  \\\text{(since }a_{0}\text{ acts by }m\text{ on
}\mathcal{B}^{\left(  m\right)  }\text{,}\\\text{and thus }\exp\left(
a_{0}\log u\right)  =\exp\left(  m\log u\right)  \text{ on }\mathcal{B}%
^{\left(  m\right)  }\text{)}}}\cdot\exp\left(  -\sum\limits_{j>0}\dfrac
{a_{j}}{j}u^{-j}\right) \\
&  =uz\cdot\underbrace{\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}%
u^{j}\right)  \cdot\exp\left(  a_{0}\log u\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  }_{=\left.  :\exp\left(  \int
a\left(  u\right)  du\right)  :\right.  }\\
&  =uz\cdot\left.  :\exp\left(  \int a\left(  u\right)  du\right)  :\right.  .
\end{align*}
Since the right hand side of this equality does not depend on $m$, we thus
have $\Gamma\left(  u\right)  =uz\left.  :\exp\left(  \int a\left(  u\right)
du\right)  :\right.  $.

Hence, we have rewritten half of the statement of Theorem \ref{thm.euler} as
the identity $\Gamma\left(  u\right)  =uz\left.  :\exp\left(  \int a\left(
u\right)  du\right)  :\right.  $ (which holds on all of $\mathcal{B}$).
Similarly, the other half of Theorem \ref{thm.euler} rewrites as the identity
$\Gamma^{\ast}\left(  u\right)  =z^{-1}\left.  :\exp\left(  -\int a\left(
u\right)  du\right)  :\right.  $.

This is reminiscent of Euler's formula $y=c\exp\left(  \int a\left(  u\right)
du\right)  $ for the solution $y$ of the differential equation $y^{\prime}=ay$.
\end{remark}

Before we can show Theorem \ref{thm.euler}, we state a lemma about the action
of $\mathcal{A}$ on $\mathcal{B}$:

\begin{lemma}
\label{lem.euler.aGamma}For every $j\in\mathbb{Z}$, we have $\left[
a_{j},\Gamma\left(  u\right)  \right]  =u^{j}\Gamma\left(  u\right)  $ and
$\left[  a_{j},\Gamma^{\ast}\left(  u\right)  \right]  =-u^{j}\Gamma^{\ast
}\left(  u\right)  $.
\end{lemma}

\textit{Proof of Lemma \ref{lem.euler.aGamma}.} Let us prove the first
formula. Let $j\in\mathbb{Z}$.

On the fermionic space $\mathcal{F}$, the element $a_{j}\in\mathcal{A}$ acts
as%
\begin{align*}
\widehat{\rho}\left(  T^{j}\right)   &  =\sum\limits_{i}\widehat{\rho}\left(
E_{i,i+j}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }T^{j}%
=\sum\limits_{i\in\mathbb{Z}}E_{i,i+j}\right) \\
&  =\sum\limits_{i}\left\{
\begin{array}
[c]{c}%
\xi_{i}\xi_{i+j}^{\ast}-1,\ \ \ \ \ \ \ \ \ \ \text{if }i=i+j\text{ and }%
i\leq0,\\
\xi_{i}\xi_{i+j}^{\ast},\ \ \ \ \ \ \ \ \ \ \text{unless }i=i+j\text{ and
}i\leq0
\end{array}
\right.
\end{align*}
(since $\widehat{\rho}\left(  E_{i,i+j}\right)  =\left\{
\begin{array}
[c]{c}%
\xi_{i}\xi_{i+j}^{\ast}-1,\ \ \ \ \ \ \ \ \ \ \text{if }i=i+j\text{ and }%
i\leq0,\\
\xi_{i}\xi_{i+j}^{\ast},\ \ \ \ \ \ \ \ \ \ \text{unless }i=i+j\text{ and
}i\leq0
\end{array}
\right.  $ for every $i\in\mathbb{Z}$). Hence, on $\mathcal{F}$, we have%
\begin{align*}
\left[  a_{j},X\left(  u\right)  \right]   &  =\left[  \sum\limits_{i}\left\{
%
\begin{array}
[c]{c}%
\xi_{i}\xi_{i+j}^{\ast}-1,\ \ \ \ \ \ \ \ \ \ \text{if }i=i+j\text{ and }%
i\leq0,\\
\xi_{i}\xi_{i+j}^{\ast},\ \ \ \ \ \ \ \ \ \ \text{unless }i=i+j\text{ and
}i\leq0
\end{array}
\right.  ,X\left(  u\right)  \right] \\
&  =\sum\limits_{i}\left\{
\begin{array}
[c]{c}%
\left[  \xi_{i}\xi_{i+j}^{\ast}-1,X\left(  u\right)  \right]
,\ \ \ \ \ \ \ \ \ \ \text{if }i=i+j\text{ and }i\leq0,\\
\left[  \xi_{i}\xi_{i+j}^{\ast},X\left(  u\right)  \right]
,\ \ \ \ \ \ \ \ \ \ \text{unless }i=i+j\text{ and }i\leq0
\end{array}
\right. \\
&  =\sum\limits_{i}\left\{
\begin{array}
[c]{c}%
\left[  \xi_{i}\xi_{i+j}^{\ast},X\left(  u\right)  \right]
,\ \ \ \ \ \ \ \ \ \ \text{if }i=i+j\text{ and }i\leq0,\\
\left[  \xi_{i}\xi_{i+j}^{\ast},X\left(  u\right)  \right]
,\ \ \ \ \ \ \ \ \ \ \text{unless }i=i+j\text{ and }i\leq0
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left[  \xi_{i}\xi_{i+j}^{\ast
}-1,X\left(  u\right)  \right]  =\left[  \xi_{i}\xi_{i+j}^{\ast},X\left(
u\right)  \right]  \right) \\
&  =\sum\limits_{i}\left[  \xi_{i}\xi_{i+j}^{\ast},X\left(  u\right)  \right]
=\sum\limits_{i}\left[  \xi_{i}\xi_{i+j}^{\ast},\sum\limits_{m}\xi_{m}%
u^{m}\right]  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }X\left(  u\right)
=\sum\limits_{m}\xi_{m}u^{m}\right) \\
&  =\sum\limits_{i}\sum\limits_{m}\underbrace{\left[  \xi_{i}\xi_{i+j}^{\ast
},\xi_{m}\right]  }_{\substack{=\delta_{m,i+j}\xi_{i}\\\text{(this is easy to
check)}}}u^{m}=\sum\limits_{i}\sum\limits_{m}\delta_{m,i+j}\xi_{i}u^{m}\\
&  =\sum\limits_{m}\xi_{m-j}u^{m}=u^{j}\underbrace{\sum\limits_{m}\xi
_{m-j}u^{m-j}}_{=X\left(  u\right)  }=u^{j}X\left(  u\right)  .
\end{align*}
Conjugating this equation by $\sigma$, we obtain $\left[  a_{j},\Gamma\left(
u\right)  \right]  =u^{j}\Gamma\left(  u\right)  $. Similarly, we can prove
$\left[  a_{j},\Gamma^{\ast}\left(  u\right)  \right]  =-u^{j}\Gamma^{\ast
}\left(  u\right)  $. Lemma \ref{lem.euler.aGamma} is proven.

\textit{Proof of Theorem \ref{thm.euler}.} Define an element $\Gamma
_{+}\left(  u\right)  $ of the $\mathbb{C}$-algebra $\left(
\operatorname*{End}\mathcal{B}\right)  \left[  \left[  u^{-1}\right]  \right]
$ by $\Gamma_{+}\left(  u\right)  =\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}%
}{j}u^{-j}\right)  $. Then,%
\begin{align}
\left[  a_{i},\Gamma_{+}\left(  u\right)  \right]   &
=0\ \ \ \ \ \ \ \ \ \ \text{if }i\geq0;\label{pf.euler.1}\\
\left[  a_{i},\Gamma_{+}\left(  u\right)  \right]   &  =u^{i}\Gamma_{+}\left(
u\right)  \ \ \ \ \ \ \ \ \ \ \text{if }i<0. \label{pf.euler.2}%
\end{align}
In fact, (\ref{pf.euler.1}) is trivial (because when $i\geq0$, the element
$a_{i}$ commutes with $a_{j}$ for every $j>0$, and thus also commutes with
$\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  $). To prove
(\ref{pf.euler.2}), it is enough to show that $\left[  a_{i},\exp\left(
-\dfrac{a_{-i}}{-i}u^{i}\right)  \right]  =u^{i}\exp\left(  -\dfrac{a_{-i}%
}{-i}u^{i}\right)  $ (since we can write $\Gamma_{+}\left(  u\right)  $ in the
form
\[
\Gamma_{+}\left(  u\right)  =\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}%
{j}u^{-j}\right)  =\prod\limits_{j>0}\exp\left(  -\dfrac{a_{j}}{j}%
u^{-j}\right)  ,
\]
and it is clear that $a_{i}$ commutes with all terms $-\dfrac{a_{j}}{j}u^{-j}$
for $j\neq-i$). But this is easily checked using the fact that $\left[
a_{i},a_{-i}\right]  =i$ and Lemma \ref{lem.powerseries1} (applied to
$K=\mathbb{Q}$, $R=\left(  \operatorname*{End}\mathcal{B}\right)  \left[
\left[  u^{-1}\right]  \right]  $, $\alpha=a_{i}$, $\beta=a_{-i}$ and
$P=\exp\left(  -\dfrac{X}{-i}u^{i}\right)  $). This completes the proof of
(\ref{pf.euler.2}).

Since $\Gamma_{+}\left(  u\right)  $ is an invertible power series in $\left(
\operatorname*{End}\mathcal{B}\right)  \left[  \left[  u^{-1}\right]  \right]
$ (because the constant term of $\Gamma_{+}\left(  u\right)  $ is $1$), it
makes sense to speak of the power series $\Gamma_{+}\left(  u\right)  ^{-1}%
\in\left(  \operatorname*{End}\mathcal{B}\right)  \left[  \left[
u^{-1}\right]  \right]  $. From (\ref{pf.euler.1}) and (\ref{pf.euler.2}), we
can derive the formulas%
\begin{align}
\left[  a_{i},\Gamma_{+}\left(  u\right)  ^{-1}\right]   &
=0\ \ \ \ \ \ \ \ \ \ \text{if }i\geq0;\label{pf.euler.1inv}\\
\left[  a_{i},\Gamma_{+}\left(  u\right)  ^{-1}\right]   &  =-u^{i}\Gamma
_{+}\left(  u\right)  ^{-1}\ \ \ \ \ \ \ \ \ \ \text{if }i<0
\label{pf.euler.2inv}%
\end{align}
(using the standard fact that $\left[  \alpha,\beta^{-1}\right]  =-\beta
^{-1}\left[  \alpha,\beta\right]  \beta^{-1}$ for any two elements $\alpha$
and $\beta$ of a ring such that $\beta$ is invertible).

Now define a map $\Delta\left(  u\right)  :\mathcal{B}^{\left(  m\right)
}\rightarrow\mathcal{B}^{\left(  m\right)  }\left(  \left(  u\right)  \right)
$ by $\Delta\left(  u\right)  =\Gamma\left(  u\right)  \Gamma_{+}\left(
u\right)  ^{-1}z^{-1}$. Let us check why this definition makes sense:

\begin{itemize}
\item For any $v\in\mathcal{B}^{\left(  m\right)  }$, we have $z^{-1}%
v\in\mathcal{B}^{\left(  m-1\right)  }$, and the term $\Gamma_{+}\left(
u\right)  ^{-1}z^{-1}v$ is well-defined and is valued in $\mathcal{B}^{\left(
m-1\right)  }\left[  u^{-1}\right]  $.\ \ \ \ \footnote{\textit{Proof.} Recall
that $\mathcal{A}$ is a $\mathbb{Z}$-graded Lie algebra, and that
$\mathcal{B}$ is a $\mathbb{Z}$-graded $\mathcal{A}$-module concentrated in
nonpositive degrees. Let us (for this single proof!) change the $\mathbb{Z}%
$-gradings on both $\mathcal{A}$ and $\mathcal{B}$ to their inverses (i. e.,
switch $\mathcal{A}\left[  N\right]  $ with $\mathcal{A}\left[  -N\right]  $
for every $N\in\mathbb{Z}$, and switch $\mathcal{B}\left[  N\right]  $ with
$\mathcal{B}\left[  -N\right]  $ for every $N\in\mathbb{Z}$); then,
$\mathcal{A}$ remains still a $\mathbb{Z}$-graded Lie algebra, but
$\mathcal{B}$ is now a $\mathbb{Z}$-graded $\mathcal{A}$-module concentrated
in nonnegative degrees. Moreover, $\mathcal{B}$ is actually a $\mathbb{Z}%
$-graded $\operatorname*{End}\nolimits_{\operatorname{hg}}\mathcal{B}$-module
concentrated in nonnegative degrees.
\par
The power series $\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\in\left(
\operatorname*{End}\nolimits_{\operatorname{hg}}\mathcal{B}\right)  \left[
\left[  u^{-1}\right]  \right]  $ is now equigraded (since our modified
grading on $\mathcal{A}$ has the property that $\deg\left(  a_{j}\right)
=-j$), so that the power series $\exp\left(  \sum\limits_{j>0}\dfrac{a_{j}}%
{j}u^{-j}\right)  \in\left(  \operatorname*{End}\nolimits_{\operatorname{hg}%
}\mathcal{B}\right)  \left[  \left[  u^{-1}\right]  \right]  $ is equigraded
as well (because a consequence of Proposition \ref{prop.equigraded.basics}
\textbf{(b)} is that whenever the exponential of an equigraded power series is
well-defined, this exponential is also equigraded). Since%
\[
\Gamma_{+}\left(  u\right)  ^{-1}=\left(  \exp\left(  -\sum\limits_{j>0}%
\dfrac{a_{j}}{j}u^{-j}\right)  \right)  ^{-1}=\exp\left(  \sum\limits_{j>0}%
\dfrac{a_{j}}{j}u^{-j}\right)
\]
(since Corollary \ref{cor.exp(-w)} (applied to $R=\left(  \operatorname*{End}%
\mathcal{B}\right)  \left[  \left[  u^{-1}\right]  \right]  $, $I=\left(
\text{the ideal of }R\text{ consisting of all power series with constant term
}1\right)  $, and $\gamma=-\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}$) yields
$\left(  \exp\left(  \sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \right)
\cdot\left(  \exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)
\right)  =1$), this rewrites as follows: The power series $\Gamma_{+}\left(
u\right)  ^{-1}\in\left(  \operatorname*{End}\nolimits_{\operatorname{hg}%
}\mathcal{B}\right)  \left[  \left[  u^{-1}\right]  \right]  $ is equigraded.
\par
Therefore, Proposition \ref{prop.equigraded.fx} \textbf{(c)} (applied to
$\operatorname*{End}\nolimits_{\operatorname{hg}}\mathcal{B}$, $\mathcal{B}$,
$\Gamma_{+}\left(  u\right)  ^{-1}$ and $z^{-1}v$ instead of $A$, $M$, $f$ and
$x$) yields that $\Gamma_{+}\left(  u\right)  ^{-1}z^{-1}v$ is a well-defined
element of $\mathcal{B}^{\left(  m-1\right)  }\left[  u^{-1}\right]  $, qed.}

\item For any $v\in\mathcal{B}^{\left(  m\right)  }$, the term $\Gamma\left(
u\right)  \Gamma_{+}\left(  u\right)  ^{-1}z^{-1}$ is well-defined and is
valued in $\mathcal{B}^{\left(  m\right)  }\left(  \left(  u\right)  \right)
$.\ \ \ \ \footnote{\textit{Proof.} We have just shown that $\Gamma_{+}\left(
u\right)  ^{-1}z^{-1}v\in\mathcal{B}^{\left(  m-1\right)  }\left[
u^{-1}\right]  $. Thus, $\Gamma_{+}\left(  u\right)  ^{-1}z^{-1}%
v\in\mathcal{B}^{\left(  m-1\right)  }\left[  u^{-1}\right]  \subseteq
\mathcal{B}\left[  u^{-1}\right]  \subseteq\mathcal{B}\left[  u,u^{-1}\right]
$.
\par
Recall that $\mathcal{A}$ is a $\mathbb{Z}$-graded Lie algebra, and that
$\mathcal{B}$ and $\mathcal{F}$ are $\mathbb{Z}$-graded $\mathcal{A}$-modules
concentrated in nonpositive degrees. Let us (for this single proof!) change
the $\mathbb{Z}$-gradings on all of $\mathcal{A}$, $\mathcal{B}$ and
$\mathcal{F}$ to their inverses (i. e., switch $\mathcal{A}\left[  N\right]  $
with $\mathcal{A}\left[  -N\right]  $ for every $N\in\mathbb{Z}$, and switch
$\mathcal{B}\left[  N\right]  $ with $\mathcal{B}\left[  -N\right]  $ for
every $N\in\mathbb{Z}$, and switch $\mathcal{F}\left[  N\right]  $ with
$\mathcal{F}\left[  -N\right]  $ for every $N\in\mathbb{Z}$); then,
$\mathcal{A}$ remains still a $\mathbb{Z}$-graded Lie algebra, but
$\mathcal{B}$ and $\mathcal{F}$ now are $\mathbb{Z}$-graded $\mathcal{A}%
$-modules concentrated in nonnegative degrees. Moreover, $\mathcal{B}$ is
actually a $\mathbb{Z}$-graded $\operatorname*{End}%
\nolimits_{\operatorname{hg}}\mathcal{B}$-module concentrated in nonnegative
degrees, and $\mathcal{F}$ is a $\mathbb{Z}$-graded $\operatorname*{End}%
\nolimits_{\operatorname{hg}}\mathcal{F}$-module concentrated in nonnegative
degrees.
\par
It is easy to see (from the definition of $X\left(  u\right)  $) that
$X\left(  u\right)  \in\left(  \operatorname*{End}\nolimits_{\operatorname{hg}%
}\mathcal{F}\right)  \left[  \left[  u,u^{-1}\right]  \right]  $ is
equigraded. As a consequence, $\Gamma\left(  u\right)  \in\left(
\operatorname*{End}\nolimits_{\operatorname{hg}}\mathcal{B}\right)  \left[
\left[  u,u^{-1}\right]  \right]  $ is equigraded (since $\Gamma\left(
u\right)  =\sigma^{-1}\circ X\left(  u\right)  \circ\sigma$). Therefore,
Proposition \ref{prop.equigraded.fx} \textbf{(b)} (applied to
$\operatorname*{End}\nolimits_{\operatorname{hg}}\mathcal{B}$, $\mathcal{B}$,
$\Gamma\left(  u\right)  $ and $\Gamma_{+}\left(  u\right)  ^{-1}z^{-1}v$
instead of $A$, $M$, $f$ and $x$) yields that $\Gamma\left(  u\right)
\Gamma_{+}\left(  u\right)  ^{-1}z^{-1}$ is a well-defined element of
$\mathcal{B}\left(  \left(  u\right)  \right)  $. This element actually lies
in $\mathcal{B}^{\left(  m\right)  }\left(  \left(  u\right)  \right)  $
(since $\Gamma\left(  u\right)  :\mathcal{B}^{\left(  m-1\right)  }%
\rightarrow\mathcal{B}^{\left(  m\right)  }$), qed.}
\end{itemize}

Since $\left[  a_{0},z\right]  =z$ and $\left[  a_{i},z\right]  =0$ for all
$i\neq0$, we have%
\begin{equation}
\left[  a_{i},\Delta\left(  u\right)  \right]  =\left\{
\begin{array}
[c]{c}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\leq0;\\
u^{i}\Delta\left(  u\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }i>0
\end{array}
\right.  \label{pf.euler.Del}%
\end{equation}
(due to (\ref{pf.euler.1inv}), (\ref{pf.euler.2inv}) and Lemma
\ref{lem.euler.aGamma}). In particular, $\left[  a_{i},\Delta\left(  u\right)
\right]  =0$ if $i\leq0$. Thus, $\Delta\left(  u\right)  $ is a homomorphism
of $\mathcal{A}_{-}$-modules, where $\mathcal{A}_{-}$ is the Lie subalgebra
$\left\langle a_{-1},a_{-2},a_{-3},...\right\rangle $ of $\mathcal{A}$. (Of
course, this formulation means that every term of the formal power series
$\Delta\left(  u\right)  $ is a homomorphism of $\mathcal{A}_{-}$-modules.)

Consider now the element $z^{m}$ of $z^{m}\mathbb{C}\left[  x_{1}%
,x_{2},...\right]  =\mathcal{B}^{\left(  m\right)  }=\widetilde{F}_{m}$. Also,
consider the element $\psi_{m}=v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...$ of
$\wedge^{\dfrac{\infty}{2},m}V=\mathcal{F}^{\left(  m\right)  }$ defined in
Definition \ref{def.psim}. By the definition of $\sigma_{m}$, we have
$\sigma_{m}\left(  z^{m}\right)  =\psi_{m}$. (In fact, $z^{m}$ is what was
denoted by $1$ in Proposition \ref{prop.wedge.fock}.)

From Lemma \ref{lem.F.P1=P}, it is clear that the Fock module $F$ is generated
by $1$ as an $\mathcal{A}_{-}$-module (since $\mathcal{A}_{-}=\left\langle
a_{-1},a_{-2},a_{-3},...\right\rangle $). Since there exists an $\mathcal{A}%
_{-}$-module isomorphism $F\rightarrow\widetilde{F}$ which sends $1$ to $1$
(in fact, the map $\operatorname*{resc}$ of Proposition \ref{prop.resc} is
such an isomorphism), this yields that $\widetilde{F}$ is generated by $1$ as
an $\mathcal{A}_{-}$-module. Since there exists an $\mathcal{A}_{-}$-module
isomorphism $\widetilde{F}\rightarrow\widetilde{F}_{m}$ which sends $1$ to
$z^{m}$ (in fact, multiplication by $z^{m}$ is such an isomorphism), this
yields that $\widetilde{F}_{m}$ is generated by $z^{m}$ as an $\mathcal{A}%
_{-}$-module. Consequently, the $m$-th term of the power series $\Delta\left(
u\right)  $ is completely determined by $\left(  \Delta\left(  u\right)
\right)  \left(  z^{m}\right)  $ (because we know that $\Delta\left(
u\right)  $ is a homomorphism of $\mathcal{A}_{-}$-modules). So let us compute
$\left(  \Delta\left(  u\right)  \right)  \left(  z^{m}\right)  $. Since
$\Delta\left(  u\right)  :\mathcal{B}^{\left(  m\right)  }\rightarrow
\mathcal{B}^{\left(  m\right)  }\left(  \left(  u\right)  \right)  $, we know
that $\left(  \Delta\left(  u\right)  \right)  \left(  z^{m}\right)  $ is an
element of $\underbrace{\mathcal{B}^{\left(  m\right)  }}_{=z^{m}%
\widetilde{F}}\left(  \left(  u\right)  \right)  =z^{m}\widetilde{F}\left(
\left(  u\right)  \right)  $. In other words, $\left(  \Delta\left(  u\right)
\right)  \left(  z^{m}\right)  $ is $z^{m}$ times a Laurent series in $u$
whose coefficients are polynomials in $x_{1},x_{2},x_{3},...$. Denote this
Laurent series by $Q$. Thus, $\left(  \Delta\left(  u\right)  \right)  \left(
z^{m}\right)  =z^{m}Q$.

For every $i>0$, we have
\[
a_{i}\Delta\left(  u\right)  =\Delta\left(  u\right)  a_{i}%
+\underbrace{\left[  a_{i},\Delta\left(  u\right)  \right]  }%
_{\substack{=u^{i}\Delta\left(  u\right)  \\\text{(by (\ref{pf.euler.Del}))}%
}}=\Delta\left(  u\right)  a_{i}+u^{i}\Delta\left(  u\right)  ,
\]
so that%
\begin{align*}
\left(  a_{i}\Delta\left(  u\right)  \right)  \left(  z^{m}\right)   &
=\left(  \Delta\left(  u\right)  a_{i}+u^{i}\Delta\left(  u\right)  \right)
\left(  z^{m}\right)  =\Delta\left(  u\right)  \underbrace{a_{i}z^{m}%
}_{\substack{=0\\\text{(since }a_{i}=\dfrac{\partial}{\partial x_{i}}\text{)}%
}}+u^{i}\underbrace{\left(  \Delta\left(  u\right)  \right)  \left(
z^{m}\right)  }_{=z^{m}Q}\\
&  =u^{i}z^{m}Q=z^{m}u^{i}Q.
\end{align*}
Since $\left(  a_{i}\Delta\left(  u\right)  \right)  \left(  z^{m}\right)
=a_{i}\underbrace{\left(  \left(  \Delta\left(  u\right)  \right)  \left(
z^{m}\right)  \right)  }_{=z^{m}Q}=z^{m}\underbrace{a_{i}}_{=\dfrac{\partial
}{\partial x_{i}}}Q=z^{m}\dfrac{\partial Q}{\partial x_{i}}$, this rewrites as
$z^{m}\dfrac{\partial Q}{\partial x_{i}}=z^{m}u^{i}Q$. Hence, for every $i>0$,
we have $\dfrac{\partial Q}{\partial x_{i}}=u^{i}Q$. Thus, we can write the
formal Laurent series $Q$ in the form $Q=f\left(  u\right)  \exp\left(
\sum\limits_{j>0}x_{j}u^{j}\right)  $ for some Laurent series $f\left(
u\right)  \in\mathbb{C}\left(  \left(  u\right)  \right)  \ \ \ \ $%
.\footnote{This follows from Proposition \ref{prop.euler.recognizing-exp},
applied to $R=\mathbb{C}\left[  u\right]  $, $U=\mathbb{C}\left(  \left(
u\right)  \right)  $, $\left(  \alpha_{1},\alpha_{2},\alpha_{3},...\right)
=\left(  u^{1},u^{2},u^{3},...\right)  $ and $P=Q$.} Thus,
\begin{align*}
&  \left(  \Delta\left(  u\right)  \right)  \left(  z^{m}\right) \\
&  =z^{m}Q=z^{m}f\left(  u\right)  \exp\left(  \sum\limits_{j>0}x_{j}%
u^{j}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }Q=f\left(  u\right)
\exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  \right) \\
&  =f\left(  u\right)  \exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}%
u^{j}\right)  \left(  z^{m}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
each }\dfrac{a_{-j}}{j}\text{ acts as multiplication by }x_{j}\text{ on
}\widetilde{F}\right)  .
\end{align*}
In other words, the two maps $\Delta\left(  u\right)  $ and $f\left(
u\right)  \exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  $ are
equal on $z^{m}$. Since each of these two maps is an $\mathcal{A}_{-}$-module
homomorphism\footnote{In fact, we know that $\Delta\left(  u\right)  $ is an
$\mathcal{A}_{-}$-module homomorphism, and it is clear that $f\left(
u\right)  \exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  $ is an
$\mathcal{A}_{-}$-module homomorphism because $\mathcal{A}_{-}$ is an abelian
Lie algebra.}, this yields that these two maps must be identical (because
$\widetilde{F}_{m}$ is generated by $z^{m}$ as an $\mathcal{A}_{-}$-module).
In other words, $\Delta\left(  u\right)  =f\left(  u\right)  \exp\left(
\sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  $. Since $\Delta\left(
u\right)  =\Gamma\left(  u\right)  \Gamma_{+}\left(  u\right)  ^{-1}z^{-1}$,
this becomes $\Gamma\left(  u\right)  \Gamma_{+}\left(  u\right)  ^{-1}%
z^{-1}=f\left(  u\right)  \exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}%
u^{j}\right)  $, so that%
\begin{align}
\Gamma\left(  u\right)   &  =f\left(  u\right)  \exp\left(  \sum
\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot z\cdot\Gamma_{+}\left(
u\right)  =f\left(  u\right)  \exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}%
{j}u^{j}\right)  \cdot z\cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}%
{j}u^{-j}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\Gamma_{+}\left(  u\right)
=\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \right)
\nonumber\\
&  =f\left(  u\right)  z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}%
u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}%
u^{-j}\right)  \label{pf.euler.Gamma-through-f}%
\end{align}
on $\mathcal{B}^{\left(  m\right)  }$. It remains to show that $f\left(
u\right)  =u^{m+1}$.

In order to do this, we recall that
\begin{align*}
\left(  \Gamma\left(  u\right)  \right)  \left(  z^{m}\right)   &  =f\left(
u\right)  z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)
\cdot\underbrace{\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)
\left(  z^{m}\right)  }_{\substack{=z^{m}\\\text{(because }a_{j}\left(
z^{m}\right)  =0\text{ for every }j>0\text{)}}}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.euler.Gamma-through-f})}\right) \\
&  =f\left(  u\right)  z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}%
u^{j}\right)  \left(  z^{m}\right)  =f\left(  u\right)  z\exp\left(
\sum\limits_{j>0}x_{j}u^{j}\right)  z^{m}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since each }\dfrac{a_{-j}}{j}\text{ acts
as multiplication by }x_{j}\text{ on }\widetilde{F}\right) \\
&  =f\left(  u\right)  \exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)
z^{m+1}.
\end{align*}
On the other hand, back on the fermionic side, for the vector $\psi_{m}%
=v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...$, we have%
\begin{align*}
\left(  X\left(  u\right)  \right)  \psi_{m}  &  =\sum\limits_{n\in\mathbb{Z}%
}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }X\left(  u\right)  =\sum\limits_{n\in\mathbb{Z}}\underbrace{\xi
_{n}}_{=\widehat{v_{n}}}u^{n}=\sum\limits_{n\in\mathbb{Z}}\widehat{v_{n}}%
u^{n}\right) \\
&  =\sum\limits_{\substack{n\in\mathbb{Z};\\n\leq m}%
}\underbrace{\widehat{v_{n}}\left(  \psi_{m}\right)  }%
_{\substack{=0\\\text{(since }n\leq m\text{, so that }v_{n}\\\text{appears in
}v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...=\psi_{m}\text{)}}}u^{n}%
+\sum\limits_{\substack{n\in\mathbb{Z};\\n\geq m+1}}\widehat{v_{n}}\left(
\psi_{m}\right)  u^{n}=\sum\limits_{\substack{n\in\mathbb{Z};\\n\geq
m+1}}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}.
\end{align*}
Thus, $\sigma^{-1}\left(  \left(  X\left(  u\right)  \right)  \psi_{m}\right)
=\sigma^{-1}\left(  \sum\limits_{\substack{n\in\mathbb{Z};\\n\geq
m+1}}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}\right)  $. Compared with%
\begin{align*}
\sigma^{-1}\left(  \left(  X\left(  u\right)  \right)  \underbrace{\psi_{m}%
}_{=\sigma\left(  z^{m}\right)  }\right)   &  =\sigma^{-1}\left(  \left(
X\left(  u\right)  \right)  \left(  \sigma\left(  z^{m}\right)  \right)
\right)  =\underbrace{\left(  \sigma^{-1}\circ X\left(  u\right)  \circ
\sigma\right)  }_{=\Gamma\left(  u\right)  }\left(  z^{m}\right)  =\left(
\Gamma\left(  u\right)  \right)  \left(  z^{m}\right) \\
&  =f\left(  u\right)  \exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)
z^{m+1},
\end{align*}
this yields $\sigma^{-1}\left(  \sum\limits_{\substack{n\in\mathbb{Z};\\n\geq
m+1}}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}\right)  =f\left(  u\right)
\exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  z^{m+1}$, so that%
\begin{equation}
\sigma\left(  f\left(  u\right)  \exp\left(  \sum\limits_{j>0}x_{j}%
u^{j}\right)  z^{m+1}\right)  =\sum\limits_{\substack{n\in\mathbb{Z};\\n\geq
m+1}}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}. \label{pf.euler.compare}%
\end{equation}
We want to find $f\left(  u\right)  $ by comparing the sides of this equation.
In order to do this, we recall that each space $\mathcal{B}^{\left(  i\right)
}$ is graded; hence, $\mathcal{B}$ (being the direct sum of the $\mathcal{B}%
^{\left(  i\right)  }$) is also graded (by taking the direct sum of all the
gradings). Also, each space $\mathcal{F}^{\left(  i\right)  }$ is graded;
hence, $\mathcal{F}$ (being the direct sum of the $\mathcal{F}^{\left(
i\right)  }$) is also graded (by taking the direct sum of all the gradings).
Since each $\sigma_{m}$ is a graded map, the direct sum $\sigma=\bigoplus
\limits_{m\in\mathbb{Z}}\sigma_{m}$ is also graded. Therefore,%
\begin{align}
&  \sigma\left(  0\text{-th homogeneous component of }f\left(  u\right)
\exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  z^{m+1}\right) \nonumber\\
&  =\left(  0\text{-th homogeneous component of }\sigma\left(  f\left(
u\right)  \exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  z^{m+1}\right)
\right) \nonumber\\
&  =\left(  0\text{-th homogeneous component of }\sum\limits_{\substack{n\in
\mathbb{Z};\\n\geq m+1}}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}\right)
\label{pf.euler.compare2}%
\end{align}
(by (\ref{pf.euler.compare})). Now, for every $n\in\mathbb{Z}$ satisfying
$n\geq m+1$, the element $\widehat{v_{n}}\left(  \psi_{m}\right)  $ equals
$v_{n}\wedge v_{m}\wedge v_{m-1}\wedge v_{m-2}\wedge...$, and thus has degree
$-\left(  n-m-1\right)  $. Hence, for every nonpositive $i\in\mathbb{Z}$, the
$i$-th homogeneous component of the sum $\sum\limits_{\substack{n\in
\mathbb{Z};\\n\geq m+1}}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}%
\in\mathcal{F}$ is $\widehat{v_{m+1-i}}\left(  \psi_{m}\right)  u^{m+1-i}$. In
particular, the $0$-th homogeneous component of $\sum\limits_{\substack{n\in
\mathbb{Z};\\n\geq m+1}}\widehat{v_{n}}\left(  \psi_{m}\right)  u^{n}$ is
$\widehat{v_{m+1}}\left(  \psi_{m}\right)  u^{m+1}=\psi_{m+1}u^{m+1}$ (since
$\widehat{v_{m+1}}\left(  \psi_{m}\right)  =v_{m+1}\wedge v_{m}\wedge
v_{m-1}\wedge v_{m-2}\wedge...=\psi_{m+1}$). Therefore,
(\ref{pf.euler.compare2}) becomes%
\begin{equation}
\sigma\left(  0\text{-th homogeneous component of }f\left(  u\right)
\exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  z^{m+1}\right)  =\psi
_{m+1}u^{m+1}. \label{pf.euler.compare3}%
\end{equation}
On the other hand, the $0$-th homogeneous component of the element $f\left(
u\right)  \exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  z^{m+1}%
\in\mathcal{B}$ is clearly $f\left(  u\right)  z^{m+1}$ (because $\exp\left(
\sum\limits_{j>0}x_{j}u^{j}\right)  =1+\left(  \text{terms involving at least
one }x_{j}\right)  $, and every $x_{j}$ lowers the degree). Thus,
(\ref{pf.euler.compare3}) becomes $\sigma\left(  f\left(  u\right)
z^{m+1}\right)  =\psi_{m+1}u^{m+1}$. Since $\sigma\left(  f\left(  u\right)
z^{m+1}\right)  =f\left(  u\right)  \underbrace{\sigma\left(  z^{m+1}\right)
}_{=\psi_{m+1}}=f\left(  u\right)  \psi_{m+1}$, this rewrites as $f\left(
u\right)  \psi_{m+1}=\psi_{m+1}u^{m+1}$, so that $f\left(  u\right)  =u^{m+1}%
$. Hence, (\ref{pf.euler.Gamma-through-f}) becomes
\begin{align*}
\Gamma\left(  u\right)   &  =\underbrace{f\left(  u\right)  }_{=u^{m+1}}%
z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right) \\
&  =u^{m+1}z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)
\cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)
\ \ \ \ \ \ \ \ \ \ \text{on }\mathcal{B}^{\left(  m\right)  }.
\end{align*}
This proves one of the equalities of Theorem \ref{thm.euler}. The other is
proven similarly.

Theorem \ref{thm.euler} is proven.

\begin{corollary}
\label{cor.euler}Let $m\in\mathbb{Z}$. On $\mathcal{B}^{\left(  m\right)  }$,
we have%
\[
\rho\left(  \sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}u^{i}%
v^{-j}E_{i,j}\right)  =\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}%
}u^{i}v^{-j}\xi_{i}\xi_{j}^{\ast}=X\left(  u\right)  X^{\ast}\left(  v\right)
,
\]
thus%
\begin{align*}
&  \sigma^{-1}\circ\rho\left(  \sum\limits_{\left(  i,j\right)  \in
\mathbb{Z}^{2}}u^{i}v^{-j}E_{i,j}\right)  \circ\sigma\\
&  =\sigma^{-1}\circ X\left(  u\right)  X^{\ast}\left(  v\right)  \circ
\sigma=\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right) \\
&  =\dfrac{1}{1-\dfrac{v}{u}}\cdot\left(  \dfrac{u}{v}\right)  ^{m}\exp\left(
\sum\limits_{j>0}\dfrac{u^{j}-v^{j}}{j}a_{-j}\right)  \exp\left(
-\sum\limits_{j>0}\dfrac{u^{-j}-v^{-j}}{j}a_{j}\right)
\end{align*}
as linear maps from $\mathcal{B}^{\left(  m\right)  }$ to $\mathcal{B}%
^{\left(  m\right)  }\left(  \left(  u,v\right)  \right)  $.
\end{corollary}

\begin{remark}
It must be pointed out that the term%
\[
\dfrac{1}{1-\dfrac{v}{u}}\cdot\left(  \dfrac{u}{v}\right)  ^{m}\exp\left(
\sum\limits_{j>0}\dfrac{u^{j}-v^{j}}{j}a_{-j}\right)  \exp\left(
-\sum\limits_{j>0}\dfrac{u^{-j}-v^{-j}}{j}a_{j}\right)
\]
only makes sense as a map from $\mathcal{B}^{\left(  m\right)  }$ to
$\mathcal{B}^{\left(  m\right)  }\left(  \left(  u,v\right)  \right)  $, but
not (for example) as a map from $\mathcal{B}^{\left(  m\right)  }$ to
$\mathcal{B}^{\left(  m\right)  }\left[  \left[  u,u^{-1},v,v^{-1}\right]
\right]  $ or as an element of $\left(  \operatorname*{End}\left(
\mathcal{B}^{\left(  m\right)  }\right)  \right)  \left[  \left[
u,u^{-1},v,v^{-1}\right]  \right]  $. Indeed, $1-\dfrac{v}{u}$ is a
zero-divisor in $\mathbb{C}\left[  \left[  u,u^{-1},v,v^{-1}\right]  \right]
$ (since $\left(  1-\dfrac{v}{u}\right)  \sum\limits_{k\in\mathbb{Z}}\left(
\dfrac{v}{u}\right)  ^{k}=0$), so it does not make sense, for example, to
multiply a generic element of $\mathcal{B}^{\left(  m\right)  }\left[  \left[
u,u^{-1},v,v^{-1}\right]  \right]  $ by $\dfrac{1}{1-\dfrac{v}{u}}$. An
element of $\mathcal{B}^{\left(  m\right)  }\left(  \left(  u,v\right)
\right)  $ needs not always be a multiple of $1-\dfrac{v}{u}$, but at least
when it is, the quotient is unique.
\end{remark}

The importance of Corollary \ref{cor.euler} lies in the fact that it gives an
easy way to compute the $\rho$-action of $\mathfrak{gl}_{\infty}$ on
$\mathcal{B}^{\left(  m\right)  }$: In fact, for any $p\in\mathbb{Z}$ and
$q\in\mathbb{Z}$, the coefficient of $\sigma^{-1}\circ\rho\left(
\sum\limits_{i,j}u^{i}v^{-j}E_{i,j}\right)  \circ\sigma\in\left(
\operatorname*{End}\left(  \mathcal{B}^{\left(  m\right)  }\right)  \right)
\left[  \left[  u,u^{-1},v,v^{-1}\right]  \right]  $ before $u^{p}v^{-q}$ is
$\sigma^{-1}\circ\rho\left(  E_{p,q}\right)  \circ\sigma$, and this is exactly
the action of $E_{p,q}$ on $\mathcal{B}^{\left(  m\right)  }$ obtained by
transferring the action $\rho$ of $\mathfrak{gl}_{\infty}$ on $\mathcal{F}%
^{\left(  m\right)  }$ to $\mathcal{B}^{\left(  m\right)  }$.

\textit{Proof of Corollary \ref{cor.euler}.} First of all, we clearly have%
\begin{align*}
\rho\left(  \sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}u^{i}%
v^{-j}E_{i,j}\right)   &  =\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}%
}u^{i}v^{-j}\underbrace{\rho\left(  E_{i,j}\right)  }_{=\xi_{i}\xi_{j}^{\ast}%
}=\sum\limits_{\left(  i,j\right)  \in\mathbb{Z}^{2}}u^{i}v^{-j}\xi_{i}\xi
_{j}^{\ast}\\
&  =\underbrace{\left(  \sum\limits_{i\in\mathbb{Z}}\xi_{i}u^{i}\right)
}_{=\sum\limits_{n\in\mathbb{Z}}\xi_{n}u^{n}=X\left(  u\right)  }%
\underbrace{\sum\limits_{j\in\mathbb{Z}}\xi_{j}^{\ast}v^{-j}}_{=\sum
\limits_{n\in\mathbb{Z}}\xi_{n}^{\ast}v^{-n}=X^{\ast}\left(  v\right)
}=X\left(  u\right)  X^{\ast}\left(  v\right)  ,
\end{align*}
so that%
\begin{align*}
&  \sigma^{-1}\circ\rho\left(  \sum\limits_{\left(  i,j\right)  \in
\mathbb{Z}^{2}}u^{i}v^{-j}E_{i,j}\right)  \circ\sigma\\
&  =\sigma^{-1}\circ X\left(  u\right)  X^{\ast}\left(  v\right)  \circ
\sigma=\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right)  .
\end{align*}
It thus only remains to prove that%
\[
\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right)  =\dfrac{1}{1-\dfrac
{v}{u}}\cdot\left(  \dfrac{u}{v}\right)  ^{m}\exp\left(  \sum\limits_{j>0}%
\dfrac{u^{j}-v^{j}}{j}a_{-j}\right)  \exp\left(  -\sum\limits_{j>0}%
\dfrac{u^{-j}-v^{-j}}{j}a_{j}\right)  .
\]


By Theorem \ref{thm.euler} (applied to $m-1$ instead of $m$), we have%
\[
\Gamma\left(  u\right)  =u^{m}z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}%
{j}u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}%
u^{-j}\right)  \ \ \ \ \ \ \ \ \ \ \text{on }\mathcal{B}^{\left(  m-1\right)
}.
\]
By Theorem \ref{thm.euler}, we have%
\[
\Gamma^{\ast}\left(  v\right)  =v^{-m}z^{-1}\exp\left(  -\sum\limits_{j>0}%
\dfrac{a_{-j}}{j}v^{j}\right)  \cdot\exp\left(  \sum\limits_{j>0}\dfrac{a_{j}%
}{j}v^{-j}\right)  \ \ \ \ \ \ \ \ \ \ \text{on }\mathcal{B}^{\left(
m\right)  }.
\]
Multiplying these two equalities, we obtain%
\begin{align*}
\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right)   &  =u^{m}v^{-m}%
\cdot\exp\left(  \sum\limits_{j>0}\dfrac{u^{j}}{j}a_{-j}\right)  \exp\left(
-\sum\limits_{j>0}\dfrac{u^{-j}}{j}a_{j}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{v^{j}}%
{j}a_{-j}\right)  \exp\left(  \sum\limits_{j>0}\dfrac{v^{-j}}{j}a_{j}\right)
\ \ \ \ \ \ \ \ \ \ \text{on }\mathcal{B}^{\left(  m\right)  }%
\end{align*}
(since multiplication by $z$ commutes with each of $\exp\left(  \sum
\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  $ and $\exp\left(  -\sum
\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  $). We wish to ``switch'' the
second and the third exponential on the right hand side of this equation
(although they don't commute). To do so, we notice that each of $-\sum
\limits_{j>0}\dfrac{u^{-j}}{j}a_{j}$ and $-\sum\limits_{j>0}\dfrac{v^{j}}%
{j}a_{-j}$ lies in the ring $\left(  \operatorname*{End}\left(  \mathcal{B}%
^{\left(  m\right)  }\right)  \right)  \left[  \left[  u^{-1},v\right]
\right]  $\ \ \ \ \footnote{This is the ring of formal power series in the
indeterminates $u^{-1}$ and $v$ over the ring $\operatorname*{End}\left(
\mathcal{B}^{\left(  m\right)  }\right)  $. Note that $\operatorname*{End}%
\left(  \mathcal{B}^{\left(  m\right)  }\right)  $ is non-commutative, but the
ring of formal power series is still defined in the same way as over
commutative rings. The indeterminates $u^{-1}$ and $v$ themselves commute with
each other and with each element of $\operatorname*{End}\left(  \mathcal{B}%
^{\left(  m\right)  }\right)  $.}. Let $I$ be the ideal of the ring $\left(
\operatorname*{End}\left(  \mathcal{B}^{\left(  m\right)  }\right)  \right)
\left[  \left[  u^{-1},v\right]  \right]  $ consisting of all power series
with constant term $0$. This ring $\left(  \operatorname*{End}\left(
\mathcal{B}^{\left(  m\right)  }\right)  \right)  \left[  \left[
u^{-1},v\right]  \right]  $ is a $\mathbb{Q}$-algebra and is complete and
Hausdorff with respect to the $I$-adic topology. Let $\alpha=-\sum
\limits_{j>0}\dfrac{u^{-j}}{j}a_{j}$ and $\beta=-\sum\limits_{j>0}\dfrac
{v^{j}}{j}a_{-j}$. Clearly, both $\alpha$ and $\beta$ lie in $I$. Also,%
\begin{align*}
\left[  \alpha,\beta\right]   &  =\left[  -\sum\limits_{j>0}\dfrac{u^{-j}}%
{j}a_{j},-\sum\limits_{j>0}\dfrac{v^{j}}{j}a_{-j}\right]  =\sum\limits_{j>0}%
\sum\limits_{k>0}\dfrac{u^{-j}v^{k}}{jk}\underbrace{\left[  a_{j}%
,a_{-k}\right]  }_{=\delta_{j,k}j}\\
&  =\sum\limits_{j>0}\sum\limits_{k>0}\dfrac{u^{-j}v^{k}}{jk}\delta
_{j,k}j=\sum\limits_{j>0}\dfrac{u^{-j}v^{j}}{jj}j=\sum\limits_{j>0}\dfrac
{1}{j}\left(  \dfrac{v}{u}\right)  ^{j}=-\log\left(  1-\dfrac{v}{u}\right)
\end{align*}
is a power series with coefficients in $\mathbb{Q}$, and thus lies in the
center of $\left(  \operatorname*{End}\left(  \mathcal{B}^{\left(  m\right)
}\right)  \right)  \left[  \left[  u^{-1},v\right]  \right]  $, and hence
commutes with each of $\alpha$ and $\beta$. Thus, we can apply Lemma
\ref{lem.powerseries3} to $K=\mathbb{Q}$ and $R=\left(  \operatorname*{End}%
\left(  \mathcal{B}^{\left(  m\right)  }\right)  \right)  \left[  \left[
u^{-1},v\right]  \right]  $, and obtain $\left(  \exp\alpha\right)
\cdot\left(  \exp\beta\right)  =\left(  \exp\beta\right)  \cdot\left(
\exp\alpha\right)  \cdot\left(  \exp\left[  \alpha,\beta\right]  \right)  $.
Hence,%
\begin{align*}
&  \Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right) \\
&  =u^{m}v^{-m}\cdot\exp\left(  \sum\limits_{j>0}\dfrac{u^{j}}{j}%
a_{-j}\right)  \exp\underbrace{\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}%
{j}a_{j}\right)  }_{=\alpha}\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\underbrace{\left(  -\sum\limits_{j>0}%
\dfrac{v^{j}}{j}a_{-j}\right)  }_{=\beta}\exp\left(  \sum\limits_{j>0}%
\dfrac{v^{-j}}{j}a_{j}\right) \\
&  =u^{m}v^{-m}\cdot\exp\left(  \sum\limits_{j>0}\dfrac{u^{j}}{j}%
a_{-j}\right)  \cdot\underbrace{\left(  \exp\alpha\right)  \cdot\left(
\exp\beta\right)  }_{=\left(  \exp\beta\right)  \cdot\left(  \exp
\alpha\right)  \cdot\left(  \exp\left[  \alpha,\beta\right]  \right)  }%
\cdot\exp\left(  \sum\limits_{j>0}\dfrac{v^{-j}}{j}a_{j}\right) \\
&  =\underbrace{u^{m}v^{-m}}_{=\left(  \dfrac{u}{v}\right)  ^{m}}\cdot
\exp\left(  \sum\limits_{j>0}\dfrac{u^{j}}{j}a_{-j}\right)  \cdot
\exp\underbrace{\beta}_{=-\sum\limits_{j>0}\dfrac{v^{j}}{j}a_{-j}}\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\underbrace{\alpha}_{=-\sum\limits_{j>0}%
\dfrac{u^{-j}}{j}a_{j}}\cdot\underbrace{\exp\left[  \alpha,\beta\right]
}_{\substack{=\dfrac{1}{1-\dfrac{v}{u}}\\\text{(since }\left[  \alpha
,\beta\right]  =-\log\left(  1-\dfrac{v}{u}\right)  \text{)}}}\cdot\exp\left(
\sum\limits_{j>0}\dfrac{v^{-j}}{j}a_{j}\right) \\
&  =\left(  \dfrac{u}{v}\right)  ^{m}\cdot\exp\left(  \sum\limits_{j>0}%
\dfrac{u^{j}}{j}a_{-j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac
{v^{j}}{j}a_{-j}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}%
{j}a_{j}\right)  \cdot\dfrac{1}{1-\dfrac{v}{u}}\cdot\exp\left(  \sum
\limits_{j>0}\dfrac{v^{-j}}{j}a_{j}\right)
\end{align*}%
\begin{align*}
&  =\dfrac{1}{1-\dfrac{v}{u}}\cdot\left(  \dfrac{u}{v}\right)  ^{m}%
\cdot\underbrace{\exp\left(  \sum\limits_{j>0}\dfrac{u^{j}}{j}a_{-j}\right)
\cdot\exp\left(  -\sum\limits_{j>0}\dfrac{v^{j}}{j}a_{-j}\right)
}_{\substack{=\exp\left(  \sum\limits_{j>0}\dfrac{u^{j}-v^{j}}{j}%
a_{-j}\right)  \\\text{(by Theorem \ref{thm.exp(u+v)}, applied to
}R=\operatorname*{End}\left(  \mathcal{B}^{\left(  m\right)  }\right)  \left[
\left[  u,v\right]  \right]  \text{,}\\I=\left(  \text{the ideal of }R\text{
consisting of all power series with constant term }0\right)  \text{,}%
\\\alpha=\sum\limits_{j>0}\dfrac{u^{j}}{j}a_{-j}\text{ and }\beta
=-\sum\limits_{j>0}\dfrac{v^{j}}{j}a_{-j}\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\underbrace{\exp\left(  -\sum\limits_{j>0}%
\dfrac{u^{-j}}{j}a_{j}\right)  \cdot\exp\left(  \sum\limits_{j>0}\dfrac
{v^{-j}}{j}a_{j}\right)  }_{\substack{=\exp\left(  -\sum\limits_{j>0}%
\dfrac{u^{-j}-v^{-j}}{j}a_{j}\right)  \\\text{(by Theorem \ref{thm.exp(u+v)},
applied to }R=\operatorname*{End}\left(  \mathcal{B}^{\left(  m\right)
}\right)  \left[  \left[  u^{-1},v^{-1}\right]  \right]  \text{,}\\I=\left(
\text{the ideal of }R\text{ consisting of all power series with constant term
}0\right)  \text{,}\\\alpha=-\sum\limits_{j>0}\dfrac{u^{-j}}{j}a_{j}\text{ and
}\beta=\sum\limits_{j>0}\dfrac{v^{-j}}{j}a_{j}\text{)}}}\\
&  =\dfrac{1}{1-\dfrac{v}{u}}\cdot\left(  \dfrac{u}{v}\right)  ^{m}\exp\left(
\sum\limits_{j>0}\dfrac{u^{j}-v^{j}}{j}a_{-j}\right)  \exp\left(
-\sum\limits_{j>0}\dfrac{u^{-j}-v^{-j}}{j}a_{j}\right)  .
\end{align*}
This proves Corollary \ref{cor.euler}.

\subsection{Expliciting \texorpdfstring{$\sigma^{-1}$}{the inverse of the
Boson-Fermion correspondence} using Schur polynomials}

Next we are going to give an explicit (in as far as one can do) formula for
$\sigma^{-1}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
$ for an elementary semiinfinite wedge $v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...$. Before we do so, we need to introduce the notion of
\textit{Schur polynomials}. We first define \textit{elementary Schur
polynomials}:

\subsubsection{Schur polynomials}

\begin{Convention}
\label{conv.schur.x}In the following, we let $x$ denote the countable family
of indeterminates $\left(  x_{1},x_{2},x_{3},...\right)  $. Thus, for any
polynomial $P$ in countably many indeterminates, we write $P\left(  x\right)
$ for $P\left(  x_{1},x_{2},x_{3},...\right)  $.
\end{Convention}

\begin{definition}
\label{def.schur.Sk}For every $k\in\mathbb{N}$, let $S_{k}\in\mathbb{Q}\left[
x_{1},x_{2},x_{3},...\right]  $ be the coefficient of the power series
$\exp\left(  \sum\limits_{i\geq1}x_{i}z^{i}\right)  \in\mathbb{Q}\left[
x_{1},x_{2},x_{3},...\right]  \left[  \left[  z\right]  \right]  $ before
$z^{k}$. Then, obviously,%
\begin{equation}
\sum\limits_{k\geq0}S_{k}\left(  x\right)  z^{k}=\exp\left(  \sum
\limits_{i\geq1}x_{i}z^{i}\right)  . \label{def.schur.sk.genfun}%
\end{equation}

\end{definition}

For example, $S_{0}\left(  x\right)  =1$, $S_{1}\left(  x\right)  =x_{1}$,
$S_{2}\left(  x\right)  =\dfrac{x_{1}^{2}}{2}+x_{2}$, $S_{3}\left(  x\right)
=\dfrac{x_{1}^{3}}{6}+x_{1}x_{2}+x_{3}$.

Note that the polynomials $S_{k}$ that we just defined are \textbf{not}
symmetric polynomials. Instead, they ``represent'' the complete symmetric
functions in terms of the $\dfrac{p_{i}}{i}$ (where $p_{i}$ are the power
sums). Here is what exactly we mean by this:

\begin{definition}
\label{def.schur.y}Let $N\in\mathbb{N}$, and let $y$ denote a family of $N$
indeterminates $\left(  y_{1},y_{2},...,y_{N}\right)  $. Thus, for any
polynomial $P$ in $N$ indeterminates, we write $P\left(  y\right)  $ for
$P\left(  y_{1},y_{2},...,y_{N}\right)  $.
\end{definition}

\begin{definition}
\label{def.schur.hk}For every $k\in\mathbb{N}$, define the $k$\textit{-th
complete symmetric function} $h_{k}$ in the variables $y_{1},y_{2},...,y_{N}$
by $h_{k}\left(  y_{1},y_{2},...,y_{N}\right)  =\sum\limits_{\substack{p_{1}%
,p_{2},...,p_{N}\in\mathbb{N};\\p_{1}+p_{2}+...+p_{N}=k}}y_{1}^{p_{1}}%
y_{2}^{p_{2}}...y_{N}^{p_{N}}$.
\end{definition}

\begin{proposition}
\label{prop.schur.hk}In the ring $\mathbb{Q}\left[  y_{1},y_{2},...,y_{N}%
\right]  \left[  \left[  z\right]  \right]  $, we have%
\[
\sum\limits_{k\geq0}z^{k}h_{k}\left(  y\right)  =\prod\limits_{j=1}^{N}%
\dfrac{1}{1-zy_{j}}.
\]

\end{proposition}

\textit{Proof of Proposition \ref{prop.schur.hk}.} For every $j\in\left\{
1,2,...,N\right\}  $, the sum formula for the geometric series yields
$\dfrac{1}{1-zy_{j}}=\sum\limits_{p\in\mathbb{N}}\left(  zy_{j}\right)
^{p}=\sum\limits_{p\in\mathbb{N}}y_{j}^{p}z^{p}$. Hence,%
\begin{align*}
\prod\limits_{j=1}^{N}\dfrac{1}{1-zy_{j}}  &  =\prod\limits_{j=1}^{N}\left(
\sum\limits_{p\in\mathbb{N}}y_{j}^{p}z^{p}\right)  =\sum\limits_{p_{1}%
,p_{2},...,p_{N}\in\mathbb{N}}\underbrace{\left(  y_{1}^{p_{1}}z^{p_{1}%
}\right)  \left(  y_{2}^{p_{2}}z^{p_{2}}\right)  ...\left(  y_{N}^{p_{N}%
}z^{p_{N}}\right)  }_{=y_{1}^{p_{1}}y_{2}^{p_{2}}...y_{N}^{p_{N}}%
z^{p_{1}+p_{2}+...+p_{N}}}\\
&  =\sum\limits_{p_{1},p_{2},...,p_{N}\in\mathbb{N}}y_{1}^{p_{1}}y_{2}^{p_{2}%
}...y_{N}^{p_{N}}z^{p_{1}+p_{2}+...+p_{N}}=\sum\limits_{k\geq0}%
\underbrace{\sum\limits_{\substack{p_{1},p_{2},...,p_{N}\in\mathbb{N}%
;\\p_{1}+p_{2}+...+p_{N}=k}}y_{1}^{p_{1}}y_{2}^{p_{2}}...y_{N}^{p_{N}}%
}_{=h_{k}\left(  y_{1},y_{2},...,y_{N}\right)  =h_{k}\left(  y\right)  }%
z^{k}\\
&  =\sum\limits_{k\geq0}h_{k}\left(  y\right)  z^{k}=\sum\limits_{k\geq0}%
z^{k}h_{k}\left(  y\right)  .
\end{align*}
This proves Proposition \ref{prop.schur.hk}.

\begin{definition}
Let $N\in\mathbb{N}$. We define a map $\operatorname*{PSE}\nolimits_{N}%
:\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  \rightarrow\mathbb{C}\left[
y_{1},y_{2},...,y_{N}\right]  $ as follows: For every polynomial $P\in\left[
x_{1},x_{2},x_{3},...\right]  $, let $\operatorname*{PSE}\nolimits_{N}\left(
P\right)  $ be the result of substituting $x_{j}=\dfrac{y_{1}^{j}+y_{2}%
^{j}+...+y_{N}^{j}}{j}$ for all positive integers $j$ into the polynomial $P$.

Clearly, this map $\operatorname*{PSE}\nolimits_{N}$ is a $\mathbb{C}$-algebra homomorphism.
\end{definition}

(The notation $\operatorname*{PSE}\nolimits_{N}$ is mine and has been chosen
as an abbreviation for ``Power Sum Evaluation in $N$ variables''.)

\begin{proposition}
\label{prop.schur.h_k.as.schur}For every $N\in\mathbb{N}$, we have
$h_{k}\left(  y\right)  =\operatorname*{PSE}\nolimits_{N}\left(  S_{k}\left(
x\right)  \right)  $ for each $k\in\mathbb{N}$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.schur.h_k.as.schur}.} Fix
$N\in\mathbb{N}$. We know that $\sum\limits_{k\geq0}S_{k}\left(  x\right)
z^{k}=\exp\left(  \sum\limits_{i\geq1}x_{i}z^{i}\right)  $. Since
$\operatorname*{PSE}\nolimits_{N}$ is a $\mathbb{C}$-algebra homomorphism,
this yields%
\begin{align*}
\sum\limits_{k\geq0}\operatorname*{PSE}\nolimits_{N}\left(  S_{k}\left(
x\right)  \right)  z^{k}  &  =\exp\left(  \sum\limits_{i\geq1}%
\operatorname*{PSE}\nolimits_{N}\left(  x_{i}\right)  z^{i}\right)
=\exp\left(  \sum\limits_{i\geq1}\sum\limits_{j=1}^{N}\dfrac{y_{j}^{i}}%
{i}z^{i}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{PSE}\nolimits_{N}%
\left(  x_{i}\right)  =\dfrac{y_{1}^{i}+y_{2}^{i}+...+y_{N}^{i}}{i}%
=\sum\limits_{j=1}^{N}\dfrac{y_{j}^{i}}{i}\right) \\
&  =\exp\left(  \sum\limits_{j=1}^{N}\sum\limits_{i\geq1}\dfrac{y_{j}^{i}}%
{i}z^{i}\right)  =\prod\limits_{j=1}^{N}\exp\left(  \sum\limits_{i\geq1}%
\dfrac{y_{j}^{i}}{i}z^{i}\right) \\
&  =\prod\limits_{j=1}^{N}\exp\underbrace{\left(  \sum\limits_{i\geq1}%
\dfrac{y_{j}^{i}z^{i}}{i}\right)  }_{=-\log\left(  1-y_{j}z\right)  }%
=\prod\limits_{j=1}^{N}\underbrace{\exp\left(  -\log\left(  1-y_{j}z\right)
\right)  }_{=\dfrac{1}{1-y_{j}z}=\dfrac{1}{1-zy_{j}}}\\
&  =\prod\limits_{j=1}^{N}\dfrac{1}{1-zy_{j}}=\sum\limits_{k\geq0}z^{k}%
h_{k}\left(  y\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition
\ref{prop.schur.hk}}\right)  .
\end{align*}
By comparing coefficients in this equality, we conclude that
$\operatorname*{PSE}\nolimits_{N}\left(  S_{k}\left(  x\right)  \right)
=h_{k}\left(  y\right)  $ for each $k\in\mathbb{N}$. Proposition
\ref{prop.schur.h_k.as.schur} is proven.

\begin{definition}
\label{def.schur.Slambda}Let $\lambda=\left(  \lambda_{1},\lambda
_{2},...,\lambda_{m}\right)  $ be a partition, so that $\lambda_{1}\geq
\lambda_{2}\geq...\geq\lambda_{m}\geq0$ are integers.

We define $S_{\lambda}\left(  x\right)  \in\mathbb{Q}\left[  x_{1},x_{2}%
,x_{3},...\right]  $ to be the polynomial%
\begin{align*}
&  \det\left(
\begin{array}
[c]{ccccc}%
S_{\lambda_{1}}\left(  x\right)  & S_{\lambda_{1}+1}\left(  x\right)  &
S_{\lambda_{1}+2}\left(  x\right)  & ... & S_{\lambda_{1}+m-1}\left(  x\right)
\\
S_{\lambda_{2}-1}\left(  x\right)  & S_{\lambda_{2}}\left(  x\right)  &
S_{\lambda_{2}+1}\left(  x\right)  & ... & S_{\lambda_{2}+m-2}\left(  x\right)
\\
S_{\lambda_{3}-2}\left(  x\right)  & S_{\lambda_{3}-1}\left(  x\right)  &
S_{\lambda_{3}}\left(  x\right)  & ... & S_{\lambda_{3}+m-3}\left(  x\right)
\\
... & ... & ... & ... & ...\\
S_{\lambda_{m}-m+1}\left(  x\right)  & S_{\lambda_{m}-m+2}\left(  x\right)  &
S_{\lambda_{m}-m+3}\left(  x\right)  & ... & S_{\lambda_{m}}\left(  x\right)
\end{array}
\right) \\
&  =\det\left(  \left(  S_{\lambda_{i}+j-i}\left(  x\right)  \right)  _{1\leq
i\leq m,\ 1\leq j\leq m}\right)  ,
\end{align*}
where $S_{j}$ denotes $0$ if $j<0$. (Note that this does not depend on
trailing zeroes in the partition; in other words, $S_{\left(  \lambda
_{1},\lambda_{2},...,\lambda_{m}\right)  }\left(  x\right)  =S_{\left(
\lambda_{1},\lambda_{2},...,\lambda_{m},0,0,...,0\right)  }\left(  x\right)  $
for any number of zeroes. This is because any nonnegative integers $m$ and
$\ell$, any $m\times m$-matrix $A$, any $m\times\ell$-matrix $B$ and any upper
unitriangular $\ell\times\ell$-matrix $C$ satisfy $\det\left(
\begin{array}
[c]{cc}%
A & B\\
0 & C
\end{array}
\right)  =\det A$.)

We refer to $S_{\lambda}\left(  x\right)  $ as the \textit{bosonic Schur
polynomial corresponding to the partition }$\lambda$.
\end{definition}

To a reader acquainted with the Schur polynomials of combinatorics (and
representation theory of symmetric groups), this definition may look familiar,
but it should be reminded that our polynomial $S_{\lambda}\left(  x\right)  $
is \textbf{not a symmetric function per se} (this is why we call it ``bosonic
Schur polynomial'' and not just simply ``Schur polynomial''); instead, it can
be made into a symmetric function -- and this will, indeed, be the $\lambda
$-Schur polynomial known from combinatorics -- by substituting for each
$x_{j}$ the term $\dfrac{\left(  j\text{-th power sum symmetric function}%
\right)  }{j}$. We will prove this in Proposition \ref{prop.schur.Schur=schur}
(albeit only for finitely many variables). Let us first formulate one of the
many definitions of Schur polynomials from combinatorics:

\begin{definition}
\label{def.schur.schurpoly}Let $\lambda=\left(  \lambda_{1},\lambda
_{2},...,\lambda_{m}\right)  $ be a partition, so that $\lambda_{1}\geq
\lambda_{2}\geq...\geq\lambda_{m}\geq0$ are integers. We define $\lambda
_{\ell}$ to mean $0$ for all integers $\ell>m$; thus, we obtain a
nonincreasing sequence $\left(  \lambda_{1},\lambda_{2},\lambda_{3}%
,...\right)  $ of nonnegative integers.

Let $N\in\mathbb{N}$.

The so-called $\lambda$\textit{-Schur module} $V_{\lambda}$ \textit{over
}$\operatorname*{GL}\left(  N\right)  $ is defined to be the
$\operatorname*{GL}\left(  N\right)  $-module $\operatorname*{Hom}%
\nolimits_{S_{n}}\left(  S^{\lambda},\left(  \mathbb{C}^{N}\right)  ^{\otimes
n}\right)  $, where $n$ denotes the number $\lambda_{1}+\lambda_{2}%
+...+\lambda_{m}$ and $S^{\lambda}$ denotes the Specht module over the
symmetric group $S_{n}$ corresponding to the partition $\lambda$. (The
$\operatorname*{GL}\left(  N\right)  $-module structure on
$\operatorname*{Hom}\nolimits_{S_{n}}\left(  S^{\lambda},\left(
\mathbb{C}^{N}\right)  ^{\otimes n}\right)  $ is obtained from the
$\operatorname*{GL}\left(  N\right)  $-module structure on $\mathbb{C}^{N}$.)
This $\lambda$-Schur module $V_{\lambda}$ is not only a $\operatorname*{GL}%
\left(  N\right)  $-module, but also a $\mathfrak{gl}\left(  N\right)
$-module. If $\lambda_{N+1}=0$, then $V_{\lambda}$ is irreducible both as a
representation of $\operatorname*{GL}\left(  N\right)  $ and as a
representation of $\mathfrak{gl}\left(  N\right)  $. If $\lambda_{N+1}\neq0$,
then $V_{\lambda}=0$.

It is known that there exists a unique polynomial $\chi_{\lambda}\in
\mathbb{Q}\left[  y_{1},y_{2},...,y_{N}\right]  $ (depending both on $\lambda$
and on $N$) such that every diagonal matrix $A=\operatorname*{diag}\left(
a_{1},a_{2},...,a_{N}\right)  \in\operatorname*{GL}\left(  N\right)  $
satisfies $\chi_{\lambda}\left(  a_{1},a_{2},...,a_{N}\right)  =\left(
\operatorname*{Tr}\mid_{V_{\lambda}}\right)  \left(  A\right)  $ (where
$\left(  \operatorname*{Tr}\mid_{V_{\lambda}}\right)  \left(  A\right)  $
means the trace of the action of $A\in\operatorname*{GL}\left(  N\right)  $ on
$V_{\lambda}$ by means of the $\operatorname*{GL}\left(  N\right)  $-module
structure on $V_{\lambda}$). In the language of representation theory,
$\chi_{\lambda}$ is thus the character of the $\operatorname*{GL}\left(
N\right)  $-module $V_{\lambda}$. This polynomial $\chi_{\lambda}$ is called
the $\lambda$\textit{-th Schur polynomial in }$N$ \textit{variables}.
\end{definition}

Now, the relation between the $S_{\lambda}$ and the Schur polynomials looks
like this:

\begin{proposition}
\label{prop.schur.Schur=schur}Let $\lambda=\left(  \lambda_{1},\lambda
_{2},...,\lambda_{m}\right)  $ be a partition. Then, $\chi_{\lambda}\left(
y_{1},y_{2},...,y_{N}\right)  =\operatorname*{PSE}\nolimits_{N}\left(
S_{\lambda}\left(  x\right)  \right)  $.
\end{proposition}

This generalizes Proposition \ref{prop.schur.h_k.as.schur} (in fact, set
$\lambda=\left(  k\right)  $ and notice that $V_{\lambda}=S^{k}\mathbb{C}^{N}$).

\textit{Proof of Proposition \ref{prop.schur.Schur=schur}.} Define $h_{k}$ to
mean $0$ for every $k<0$.

Proposition \ref{prop.schur.h_k.as.schur} yields $h_{k}\left(  y\right)
=\operatorname*{PSE}\nolimits_{N}\left(  S_{k}\left(  x\right)  \right)  $ for
each $k\in\mathbb{N}$. Since $h_{k}\left(  y\right)  =\operatorname*{PSE}%
\nolimits_{N}\left(  S_{k}\left(  x\right)  \right)  $ also holds for every
negative integer $k$ (since every negative integer $k$ satisfies $h_{k}=0$ and
$S_{k}=0$), we thus conclude that%
\begin{equation}
h_{k}\left(  y\right)  =\operatorname*{PSE}\nolimits_{N}\left(  S_{k}\left(
x\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{Z}.
\label{pf.schur.Schur=schur.1}%
\end{equation}


We know that $\chi_{\lambda}$ is the $\lambda$-th Schur polynomial in $N$
variables. By the first Giambelli formula, this yields that%
\begin{align*}
&  \chi_{\lambda}\left(  y_{1},y_{2},...,y_{N}\right) \\
&  =\det\underbrace{\left(
\begin{array}
[c]{ccccc}%
h_{\lambda_{1}}\left(  y\right)  & h_{\lambda_{1}+1}\left(  y\right)  &
h_{\lambda_{1}+2}\left(  y\right)  & ... & h_{\lambda_{1}+m-1}\left(  y\right)
\\
h_{\lambda_{2}-1}\left(  y\right)  & h_{\lambda_{2}}\left(  y\right)  &
h_{\lambda_{2}+1}\left(  y\right)  & ... & h_{\lambda_{2}+m-2}\left(  y\right)
\\
h_{\lambda_{3}-2}\left(  y\right)  & h_{\lambda_{3}-1}\left(  y\right)  &
h_{\lambda_{3}}\left(  y\right)  & ... & h_{\lambda_{3}+m-3}\left(  y\right)
\\
... & ... & ... & ... & ...\\
h_{\lambda_{m}-m+1}\left(  y\right)  & h_{\lambda_{m}-m+2}\left(  y\right)  &
h_{\lambda_{m}-m+3}\left(  y\right)  & ... & h_{\lambda_{m}}\left(  y\right)
\end{array}
\right)  }_{=\left(  h_{\lambda_{i}+j-i}\left(  y\right)  \right)  _{1\leq
i\leq m,\ 1\leq j\leq m}}\\
&  =\det\left(  \left(  h_{\lambda_{i}+j-i}\left(  y\right)  \right)  _{1\leq
i\leq m,\ 1\leq j\leq m}\right)  =\det\left(  \left(  \operatorname*{PSE}%
\nolimits_{N}\left(  S_{\lambda_{i}+j-i}\left(  x\right)  \right)  \right)
_{1\leq i\leq m,\ 1\leq j\leq m}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.schur.Schur=schur.1})}\right)
\\
&  =\operatorname*{PSE}\nolimits_{N}\underbrace{\left(  \det\left(  \left(
S_{\lambda_{i}+j-i}\left(  x\right)  \right)  _{1\leq i\leq m,\ 1\leq j\leq
m}\right)  \right)  }_{=S_{\lambda}\left(  x\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\operatorname*{PSE}\nolimits_{N}\text{ is a }\mathbb{C}%
\text{-algebra homomorphism, whereas }\det\text{ is a polynomial}\\
\text{(and any }\mathbb{C}\text{-algebra homomorphism commutes with any
polynomial)}%
\end{array}
\right) \\
&  =\operatorname*{PSE}\nolimits_{N}\left(  S_{\lambda}\left(  x\right)
\right)  .
\end{align*}
Proposition \ref{prop.schur.Schur=schur} is proven.

\subsubsection{The statement of the fact}

\begin{theorem}
\label{thm.schur}Whenever $\left(  i_{0},i_{1},i_{2},...\right)  $ is a
$0$-degression (see Definition \ref{def.glinf.m-deg} for what this means), we
have $\sigma^{-1}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =S_{\lambda}\left(  x\right)  $ where $\lambda=\left(
i_{0}+0,i_{1}+1,i_{2}+2,...\right)  $. (Note that this $\lambda$ is indeed a
partition since $\left(  i_{0},i_{1},i_{2},...\right)  $ is a $0$-degression.)
\end{theorem}

We are going to give two proofs of this theorem. The first proof will be
covered in Section \ref{subsect.schur.pf1}, whereas the second proof will
encompass Section \ref{subsect.schur.pf2}.

\subsection{\label{subsect.schur.pf1}Expliciting
\texorpdfstring{$\sigma^{-1}$}{the inverse of the Boson-Fermion
correspondence} using Schur polynomials: first proof}

\subsubsection{\label{subsubsect.powersums}The power sums are algebraically
independent}

Our first proof of Theorem \ref{thm.schur} will require some lemmata from
algebraic combinatorics. First of all:

\begin{lemma}
\label{lem.schur.algind}Let $N\in\mathbb{N}$. For every positive integer $j$,
let $p_{j}$ denote the polynomial $y_{1}^{j}+y_{2}^{j}+...+y_{N}^{j}%
\in\mathbb{C}\left[  y_{1},y_{2},...,y_{N}\right]  $. Then, the polynomials
$p_{1}$, $p_{2}$, $...$, $p_{N}$ are algebraically independent.
\end{lemma}

In order to prove this fact, we need the following known facts (which we won't prove):

\begin{lemma}
\label{lem.schur.elsym}Let $N\in\mathbb{N}$. For every $j\in\mathbb{N}$, let
$e_{j}$ denote the $j$-th elementary symmetric polynomial $\sum\limits_{1\leq
i_{1}<i_{2}<...<i_{j}\leq N}y_{i_{1}}y_{i_{2}}...y_{i_{j}}$ in $\mathbb{C}%
\left[  y_{1},y_{2},...,y_{N}\right]  $. Then, the elements $e_{1}$, $e_{2}$,
$...$, $e_{N}$ are algebraically independent.
\end{lemma}

Lemma \ref{lem.schur.elsym} is one half of a known theorem. The other half
says that the elements $e_{1}$, $e_{2}$, $...$, $e_{N}$ generate the
$\mathbb{C}$-algebra of symmetric polynomials in $\mathbb{C}\left[
y_{1},y_{2},...,y_{N}\right]  $. We will prove neither of these halves; they
are both classical and well-known (under the name ``fundamental theorem of
symmetric polynomials'', which is usually formulated in a more general setting
when $\mathbb{C}$ is replaced by any commutative ring).

\begin{lemma}
\label{lem.schur.newtonid}Let $N\in\mathbb{N}$. For every positive integer
$j$, define $p_{j}$ as in Lemma \ref{lem.schur.algind}. For every
$j\in\mathbb{N}$, define $e_{j}$ as in Lemma \ref{lem.schur.elsym}. Then,
every $k\in\mathbb{N}$ satisfies $ke_{k}=\sum\limits_{i=1}^{k}\left(
-1\right)  ^{i-1}e_{k-i}p_{i}$.
\end{lemma}

This lemma is known as the \textit{Newton identity} (or identities), and won't
be proven due to being well-known. But we will use it to derive the following corollary:

\begin{corollary}
\label{cor.schur.newton}Let $N\in\mathbb{N}$. For every positive integer $j$,
define $p_{j}$ as in Lemma \ref{lem.schur.algind}. For every $j\in\mathbb{N}$,
define $e_{j}$ as in Lemma \ref{lem.schur.elsym}. Then, for every positive
$k\in\mathbb{N}$, there exists a polynomial $P_{k}\in\mathbb{Q}\left[
T_{1},T_{2},...,T_{k}\right]  $ such that $p_{k}=P_{k}\left(  e_{1}%
,e_{2},...,e_{k}\right)  $ and $P_{k}-\left(  -1\right)  ^{k-1}kT_{k}%
\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{k-1}\right]  $. (Here, of course,
$\mathbb{Q}\left[  T_{1},T_{2},...,T_{k-1}\right]  $ is identified with a
subalgebra of $\mathbb{Q}\left[  T_{1},T_{2},...,T_{k}\right]  $.)
\end{corollary}

\textit{Proof of Corollary \ref{cor.schur.newton}.} We will prove Corollary
\ref{cor.schur.newton} by strong induction over $k$:

\textit{Induction step:} Let $\ell$ be a positive integer. Assume that
Corollary \ref{cor.schur.newton} holds for every positive integer $k<\ell$. We
must then prove that Corollary \ref{cor.schur.newton} holds for $k=\ell$.

Corollary \ref{cor.schur.newton} holds for every positive integer $k<\ell$ (by
the induction hypothesis). In other words, for every $k<\ell$, there exists a
polynomial $P_{k}\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{k}\right]  $ such
that $p_{k}=P_{k}\left(  e_{1},e_{2},...,e_{k}\right)  $ and $P_{k}-\left(
-1\right)  ^{k-1}kT_{k}\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{k-1}\right]  $.
Consider these polynomials $P_{1}$, $P_{2}$, $...$, $P_{\ell-1}$.

Applying Lemma \ref{lem.schur.newtonid} to $k=\ell$, we obtain%
\begin{align*}
\ell e_{\ell}  &  =\sum\limits_{i=1}^{\ell}\left(  -1\right)  ^{i-1}e_{\ell
-i}p_{i}=\sum\limits_{k=1}^{\ell}\left(  -1\right)  ^{k-1}e_{\ell-k}%
p_{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed }i\text{ as }k\text{
in the sum}\right) \\
&  =\sum\limits_{k=1}^{\ell-1}\left(  -1\right)  ^{k-1}e_{\ell-k}p_{k}+\left(
-1\right)  ^{\ell-1}\underbrace{e_{\ell-\ell}}_{=e_{0}=1}p_{\ell}%
=\sum\limits_{k=1}^{\ell-1}\left(  -1\right)  ^{k-1}e_{\ell-k}p_{k}+\left(
-1\right)  ^{\ell-1}p_{\ell},
\end{align*}
so that $\left(  -1\right)  ^{\ell-1}p_{\ell}=\ell e_{\ell}-\sum
\limits_{k=1}^{\ell-1}\left(  -1\right)  ^{k-1}e_{\ell-k}p_{k}$ and thus%
\[
p_{\ell}=\left(  -1\right)  ^{\ell-1}\left(  \ell e_{\ell}-\sum\limits_{k=1}%
^{\ell-1}\left(  -1\right)  ^{k-1}e_{\ell-k}p_{k}\right)  =\left(  -1\right)
^{\ell-1}\ell e_{\ell}-\left(  -1\right)  ^{\ell-1}\sum\limits_{k=1}^{\ell
-1}\left(  -1\right)  ^{k-1}e_{\ell-k}p_{k}.
\]


Now, define a polynomial $P_{\ell}\in\mathbb{Q}\left[  T_{1},T_{2}%
,...,T_{\ell}\right]  $ by%
\[
P_{\ell}=\left(  -1\right)  ^{\ell-1}\ell T_{\ell}-\left(  -1\right)
^{\ell-1}\sum\limits_{k=1}^{\ell-1}\left(  -1\right)  ^{k-1}T_{\ell-k}%
P_{k}\left(  T_{1},T_{2},...,T_{k}\right)  .
\]
Then,%
\[
P_{\ell}-\left(  -1\right)  ^{\ell-1}\ell T_{\ell}=-\left(  -1\right)
^{\ell-1}\sum\limits_{k=1}^{\ell-1}\left(  -1\right)  ^{k-1}%
\underbrace{T_{\ell-k}}_{\substack{\in\mathbb{Q}\left[  T_{1},T_{2}%
,...,T_{\ell-1}\right]  \\\text{(since }\ell-k\leq\ell-1\text{)}%
}}\underbrace{P_{k}\left(  T_{1},T_{2},...,T_{k}\right)  }_{\substack{\in
\mathbb{Q}\left[  T_{1},T_{2},...,T_{\ell-1}\right]  \\\text{(since }k\leq
\ell-1\text{)}}}\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{\ell-1}\right]  .
\]
Moreover, $P_{\ell}=\left(  -1\right)  ^{\ell-1}\ell T_{\ell}-\left(
-1\right)  ^{\ell-1}\sum\limits_{k=1}^{\ell-1}\left(  -1\right)  ^{k-1}%
T_{\ell-k}P_{k}\left(  T_{1},T_{2},...,T_{k}\right)  $ yields%
\begin{align*}
P_{\ell}\left(  e_{1},e_{2},...,e_{\ell}\right)   &  =\left(  -1\right)
^{\ell-1}\ell e_{\ell}-\left(  -1\right)  ^{\ell-1}\sum\limits_{k=1}^{\ell
-1}\left(  -1\right)  ^{k-1}e_{\ell-k}\underbrace{P_{k}\left(  e_{1}%
,e_{2},...,e_{k}\right)  }_{\substack{=p_{k}\\\text{(by the definition of
}P_{k}\text{)}}}\\
&  =\left(  -1\right)  ^{\ell-1}\ell e_{\ell}-\left(  -1\right)  ^{\ell-1}%
\sum\limits_{k=1}^{\ell-1}\left(  -1\right)  ^{k-1}e_{\ell-k}p_{k}=p_{\ell}.
\end{align*}


We thus have shown that $p_{\ell}=P_{\ell}\left(  e_{1},e_{2},...,e_{\ell
}\right)  $ and $P_{\ell}-\left(  -1\right)  ^{\ell-1}\ell T_{\ell}%
\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{\ell-1}\right]  $. Thus, there exists
a polynomial $P_{\ell}\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{\ell}\right]  $
such that $p_{\ell}=P_{\ell}\left(  e_{1},e_{2},...,e_{\ell}\right)  $ and
$P_{\ell}-\left(  -1\right)  ^{\ell-1}\ell T_{\ell}\in\mathbb{Q}\left[
T_{1},T_{2},...,T_{\ell-1}\right]  $. In other words, Corollary
\ref{cor.schur.newton} holds for $k=\ell$. This completes the induction step.
The induction proof of Corollary \ref{cor.schur.newton} is thus complete.

\textit{Proof of Lemma \ref{lem.schur.algind}.} Assume the contrary. Thus, the
polynomials $p_{1}$, $p_{2}$, $...$, $p_{N}$ are algebraically dependent.
Hence, there exists a nonzero polynomial $Q\in\mathbb{C}\left[  U_{1}%
,U_{2},...,U_{N}\right]  $ such that $Q\left(  p_{1},p_{2},...,p_{N}\right)
=0$. Consider this $Q$.

Consider the lexicographic order on the monomials in $\mathbb{C}\left[
T_{1},T_{2},...,T_{N}\right]  $ given by $T_{1}<T_{2}<...<T_{N}$.

For every $j\in\mathbb{N}$, define $e_{j}$ as in Lemma \ref{lem.schur.elsym}.
For every positive $k\in\mathbb{N}$, Corollary \ref{cor.schur.newton}
guarantees the existence of a polynomial $P_{k}\in\mathbb{Q}\left[
T_{1},T_{2},...,T_{k}\right]  $ such that $p_{k}=P_{k}\left(  e_{1}%
,e_{2},...,e_{k}\right)  $ and $P_{k}-\left(  -1\right)  ^{k-1}kT_{k}%
\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{k-1}\right]  $. Consider such a
polynomial $P_{k}$.

For every $k\in\left\{  1,2,...,N\right\}  $, there exists a polynomial
$Q_{k}\in\mathbb{Q}\left[  T_{1},T_{2},...,T_{k-1}\right]  $ such that
$P_{k}-\left(  -1\right)  ^{k-1}kT_{k}=Q_{k}\left(  T_{1},T_{2},...,T_{k-1}%
\right)  $ (since $P_{k}-\left(  -1\right)  ^{k-1}kT_{k}\in\mathbb{Q}\left[
T_{1},T_{2},...,T_{k-1}\right]  $). Consider such a polynomial $Q_{k}$.

For every $k\in\left\{  1,2,...,N\right\}  $, let $\widetilde{P}_{k}$ be the
polynomial $P_{k}\left(  T_{1},T_{2},...,T_{k}\right)  \in\mathbb{C}\left[
T_{1},T_{2},...,T_{N}\right]  $. (This is the same polynomial as $P_{k}$, but
now considered as a polynomial in $N$ variables over $\mathbb{C}$ rather than
in $k$ variables over $\mathbb{Q}$.)

Then, for every $k\in\left\{  1,2,...,N\right\}  $, we have%
\begin{align*}
\widetilde{P}_{k}\left(  e_{1},e_{2},...,e_{N}\right)   &  =P_{k}\left(
e_{1},e_{2},...,e_{k}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\widetilde{P}_{k}=P_{k}\left(  T_{1},T_{2},...,T_{k}\right)  \right) \\
&  =p_{k}.
\end{align*}


Also, for every $k\in\left\{  1,2,...,N\right\}  $, the leading
monomial\footnote{Here, ``monomial'' means ``monomial without coefficient'',
and the ``leading monomial'' of a polynomial means the highest monomial (with
nonzero coefficient) of the polynomial.} of $\widetilde{P}_{k}$ (with respect
to the lexicographic order defined above) is $T_{k}$%
\ \ \ \ \footnote{\textit{Proof.} Let $k\in\left\{  1,2,...,N\right\}  $.
Then,
\begin{align*}
\underbrace{\widetilde{P}_{k}}_{=P_{k}\left(  T_{1},T_{2},...,T_{k}\right)
}-\left(  -1\right)  ^{k-1}kT_{k}  &  =P_{k}\left(  T_{1},T_{2},...,T_{k}%
\right)  -\left(  -1\right)  ^{k-1}kT_{k}\\
&  =\underbrace{\left(  P_{k}-\left(  -1\right)  ^{k-1}kT_{k}\right)
}_{=Q_{k}\left(  T_{1},T_{2},...,T_{k-1}\right)  }\left(  T_{1},T_{2}%
,...,T_{k}\right) \\
&  =\left(  Q_{k}\left(  T_{1},T_{2},...,T_{k-1}\right)  \right)  \left(
T_{1},T_{2},...,T_{k}\right)  =Q_{k}\left(  T_{1},T_{2},...,T_{k-1}\right)  ,
\end{align*}
so that $\widetilde{P}_{k}=\left(  -1\right)  ^{k-1}kT_{k}+Q_{k}\left(
T_{1},T_{2},...,T_{k-1}\right)  $. Hence, the only monomials which occur with
nonzero coefficient in the polynomial $\widetilde{P}_{k}$ are the monomial
$T_{k}$ (occurring with coefficient $\left(  -1\right)  ^{k-1}k$) and the
monomials of the polynomial $Q_{k}\left(  T_{1},T_{2},...,T_{k-1}\right)  $.
But the latter monomials don't contain any variable other than $T_{1}$,
$T_{2}$, $...$, $T_{k-1}$ (because they are monomials of the polynomial
$Q_{k}\left(  T_{1},T_{2},...,T_{k-1}\right)  $), and thus are smaller than
the monomial $T_{k}$ (because any monomial which doesn't contain any variable
other than $T_{1}$, $T_{2}$, $...$, $T_{k-1}$ is smaller than any monomial
which contains $T_{k}$ (since we have a lexicographic order given by
$T_{1}<T_{2}<...<T_{N}$)). Hence, the leading monomial of $\widetilde{P}_{k}$
must be $T_{k}$, qed.}. Since the leading monomial of a product of polynomials
equals the product of their leading monomials, this yields that for every
$\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)  \in\mathbb{N}^{N}$,%
\begin{equation}
\text{the leading monomial of }\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}%
_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}\text{ is }T_{1}^{\alpha
_{1}}T_{2}^{\alpha_{2}}...T_{N}^{\alpha_{N}}. \label{pf.schur.algind.3}%
\end{equation}


Since every $k\in\left\{  1,2,...,N\right\}  $ satisfies $p_{k}=\widetilde{P}%
_{k}\left(  e_{1},e_{2},...,e_{N}\right)  $, we have%
\begin{align*}
Q\left(  p_{1},p_{2},...,p_{N}\right)   &  =Q\left(  \widetilde{P}_{1}\left(
e_{1},e_{2},...,e_{N}\right)  ,\widetilde{P}_{2}\left(  e_{1},e_{2}%
,...,e_{N}\right)  ,...,\widetilde{P}_{N}\left(  e_{1},e_{2},...,e_{N}\right)
\right) \\
&  =\left(  Q\left(  \widetilde{P}_{1},\widetilde{P}_{2},...,\widetilde{P}%
_{N}\right)  \right)  \left(  e_{1},e_{2},...,e_{N}\right)  .
\end{align*}
Hence, $Q\left(  p_{1},p_{2},...,p_{N}\right)  =0$ rewrites as $\left(
Q\left(  \widetilde{P}_{1},\widetilde{P}_{2},...,\widetilde{P}_{N}\right)
\right)  \left(  e_{1},e_{2},...,e_{N}\right)  =0$. Since $e_{1}$, $e_{2}$,
$...$, $e_{N}$ are algebraically independent (by Lemma \ref{lem.schur.elsym}),
this yields $Q\left(  \widetilde{P}_{1},\widetilde{P}_{2},...,\widetilde{P}%
_{N}\right)  =0$. Since $Q\neq0$, this shows that the elements $\widetilde{P}%
_{1}$, $\widetilde{P}_{2}$, $...$, $\widetilde{P}_{N}$ are algebraically
dependent. In other words, the family $\left(  \widetilde{P}_{1}^{\alpha_{1}%
}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}\right)
_{\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)  \in\mathbb{N}^{N}}$ is
linearly dependent. Thus, there exists a family $\left(  \lambda_{\alpha
_{1},\alpha_{2},...,\alpha_{N}}\right)  _{\left(  \alpha_{1},\alpha
_{2},...,\alpha_{N}\right)  \in\mathbb{N}^{N}}$ of elements of $\mathbb{C}$
such that:

\begin{itemize}
\item all but finitely many $\left(  \alpha_{1},\alpha_{2},...,\alpha
_{N}\right)  \in\mathbb{N}^{N}$ satisfy $\lambda_{\alpha_{1},\alpha
_{2},...,\alpha_{N}}=0$;

\item not all $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\in\mathbb{N}^{N}$ satisfy $\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}}=0$;

\item we have $\sum\limits_{\left(  \alpha_{1},\alpha_{2},...,\alpha
_{N}\right)  \in\mathbb{N}^{N}}\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}%
}\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}%
_{N}^{\alpha_{N}}=0$.
\end{itemize}

Consider this family. By identifying every $N$-tuple $\left(  \alpha
_{1},\alpha_{2},...,\alpha_{N}\right)  \in\mathbb{N}^{N}$ with the monomial
$T_{1}^{\alpha_{1}}T_{2}^{\alpha_{2}}...T_{N}^{\alpha_{N}}\in\mathbb{C}\left[
T_{1},T_{2},...,T_{N}\right]  $, we obtain a lexicographic order on the
$N$-tuples $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)  $ (from the
lexicographic order on the monomials in $\mathbb{C}\left[  T_{1}%
,T_{2},...,T_{N}\right]  $).

Since all but finitely many $\left(  \alpha_{1},\alpha_{2},...,\alpha
_{N}\right)  \in\mathbb{N}^{N}$ satisfy $\lambda_{\alpha_{1},\alpha
_{2},...,\alpha_{N}}=0$, but not all $\left(  \alpha_{1},\alpha_{2}%
,...,\alpha_{N}\right)  \in\mathbb{N}^{N}$ satisfy $\lambda_{\alpha_{1}%
,\alpha_{2},...,\alpha_{N}}=0$, there exists a highest (with respect to the
above-defined order) $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\in\mathbb{N}^{N}$ satisfying $\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}%
}\neq0$. Let this $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)  $ be
called $\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)  $. Then, $\left(
\beta_{1},\beta_{2},...,\beta_{N}\right)  $ is the highest $\left(  \alpha
_{1},\alpha_{2},...,\alpha_{N}\right)  \in\mathbb{N}^{N}$ satisfying
$\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}}\neq0$. Thus, $\lambda
_{\beta_{1},\beta_{2},...,\beta_{N}}\neq0$, but
\begin{equation}
\text{every }\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\in\mathbb{N}^{N}\text{ higher than }\left(  \beta_{1},\beta_{2},...,\beta
_{N}\right)  \text{ satisfies }\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}%
}=0. \label{pf.schur.algind.4}%
\end{equation}


Now it is easy to see that for every $\left(  \alpha_{1},\alpha_{2}%
,...,\alpha_{N}\right)  \in\mathbb{N}^{N}$ satisfying $\left(  \alpha
_{1},\alpha_{2},...,\alpha_{N}\right)  \neq\left(  \beta_{1},\beta
_{2},...,\beta_{N}\right)  $, the term%
\begin{equation}
\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}}\widetilde{P}_{1}^{\alpha_{1}%
}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}\text{ is a
}\mathbb{C}\text{-linear combination of monomials smaller than }T_{1}%
^{\beta_{1}}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}. \label{pf.schur.algind.5}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.schur.algind.5}).} Let $\left(  \alpha
_{1},\alpha_{2},...,\alpha_{N}\right)  \in\mathbb{N}^{N}$ satisfy $\left(
\alpha_{1},\alpha_{2},...,\alpha_{N}\right)  \neq\left(  \beta_{1},\beta
_{2},...,\beta_{N}\right)  $. Since the lexicographic order is a total order,
we must be in one of the following two cases:
\par
\textit{Case 1:} We have $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\geq\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)  $.
\par
\textit{Case 2:} We have $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
<\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)  $.
\par
First, consider Case 1. In this case, $\left(  \alpha_{1},\alpha
_{2},...,\alpha_{N}\right)  \geq\left(  \beta_{1},\beta_{2},...,\beta
_{N}\right)  $, so that $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
>\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)  $ (since $\left(
\alpha_{1},\alpha_{2},...,\alpha_{N}\right)  \neq\left(  \beta_{1},\beta
_{2},...,\beta_{N}\right)  $). Thus, $\left(  \alpha_{1},\alpha_{2}%
,...,\alpha_{N}\right)  \in\mathbb{N}^{N}$ is higher than $\left(  \beta
_{1},\beta_{2},...,\beta_{N}\right)  $. Hence, $\lambda_{\alpha_{1},\alpha
_{2},...,\alpha_{N}}=0$ (by (\ref{pf.schur.algind.4})), so that $\lambda
_{\alpha_{1},\alpha_{2},...,\alpha_{N}}\widetilde{P}_{1}^{\alpha_{1}%
}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}=0$ is clearly
a $\mathbb{C}$-linear combination of monomials smaller than $T_{1}^{\alpha
_{1}}T_{2}^{\alpha_{2}}...T_{N}^{\alpha_{N}}$. Thus, (\ref{pf.schur.algind.5})
holds in Case 1.
\par
Now, let us consider Case 2. In this case, $\left(  \alpha_{1},\alpha
_{2},...,\alpha_{N}\right)  <\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)
$, so that $T_{1}^{\alpha_{1}}T_{2}^{\alpha_{2}}...T_{N}^{\alpha_{N}}%
<T_{1}^{\beta_{1}}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$ (because the order on
$N$-tuples is obtained from the order on monomials by identifying every
$N$-tuple $\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)  \in
\mathbb{N}^{N}$ with the monomial $T_{1}^{\alpha_{1}}T_{2}^{\alpha_{2}%
}...T_{N}^{\alpha_{N}}\in\mathbb{C}\left[  T_{1},T_{2},...,T_{N}\right]  $).
\par
Due to (\ref{pf.schur.algind.3}), every monomial which occurs with nonzero
coefficient in $\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}_{2}^{\alpha_{2}%
}...\widetilde{P}_{N}^{\alpha_{N}}$ is smaller or equal to $T_{1}^{\alpha_{1}%
}T_{2}^{\alpha_{2}}...T_{N}^{\alpha_{N}}$. Combined with $T_{1}^{\alpha_{1}%
}T_{2}^{\alpha_{2}}...T_{N}^{\alpha_{N}}<T_{1}^{\beta_{1}}T_{2}^{\beta_{2}%
}...T_{N}^{\beta_{N}}$, this yields that every monomial which occurs with
nonzero coefficient in $\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}%
_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}$ is smaller than
$T_{1}^{\beta_{1}}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$. Hence,
$\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}%
_{N}^{\alpha_{N}}$ is a $\mathbb{C}$-linear combination of monomials smaller
than $T_{1}^{\beta_{1}}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$. Thus,
$\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}}\widetilde{P}_{1}^{\alpha_{1}%
}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}$ is a
$\mathbb{C}$-linear combination of monomials smaller than $T_{1}^{\beta_{1}%
}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$. We have thus proven that
(\ref{pf.schur.algind.5}) holds in Case 2.
\par
Hence, (\ref{pf.schur.algind.5}) holds in each of cases 1 and 2. Since no
other cases are possible, this yields that (\ref{pf.schur.algind.5}) always
holds.} As a consequence,%
\[
\sum\limits_{\substack{\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\in\mathbb{N}^{N};\\\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\neq\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)  }}\lambda_{\alpha
_{1},\alpha_{2},...,\alpha_{N}}\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}%
_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}%
\]
is a sum of $\mathbb{C}$-linear combinations of monomials smaller than
$T_{1}^{\beta_{1}}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$, and thus itself a
$\mathbb{C}$-linear combination of monomials smaller than $T_{1}^{\beta_{1}%
}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$.

Now,%
\begin{align*}
0  &  =\sum\limits_{\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\in\mathbb{N}^{N}}\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}}%
\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}%
_{N}^{\alpha_{N}}\\
&  =\lambda_{\beta_{1},\beta_{2},...,\beta_{N}}\widetilde{P}_{1}^{\beta_{1}%
}\widetilde{P}_{2}^{\beta_{2}}...\widetilde{P}_{N}^{\beta_{N}}+\sum
\limits_{\substack{\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\in\mathbb{N}^{N};\\\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\neq\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)  }}\lambda_{\alpha
_{1},\alpha_{2},...,\alpha_{N}}\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}%
_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}},
\end{align*}
so that%
\[
\sum\limits_{\substack{\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\in\mathbb{N}^{N};\\\left(  \alpha_{1},\alpha_{2},...,\alpha_{N}\right)
\neq\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)  }}\lambda_{\alpha
_{1},\alpha_{2},...,\alpha_{N}}\widetilde{P}_{1}^{\alpha_{1}}\widetilde{P}%
_{2}^{\alpha_{2}}...\widetilde{P}_{N}^{\alpha_{N}}=-\lambda_{\beta_{1}%
,\beta_{2},...,\beta_{N}}\widetilde{P}_{1}^{\beta_{1}}\widetilde{P}_{2}%
^{\beta_{2}}...\widetilde{P}_{N}^{\beta_{N}}.
\]
Since we know that $\sum\limits_{\substack{\left(  \alpha_{1},\alpha
_{2},...,\alpha_{N}\right)  \in\mathbb{N}^{N};\\\left(  \alpha_{1},\alpha
_{2},...,\alpha_{N}\right)  \neq\left(  \beta_{1},\beta_{2},...,\beta
_{N}\right)  }}\lambda_{\alpha_{1},\alpha_{2},...,\alpha_{N}}\widetilde{P}%
_{1}^{\alpha_{1}}\widetilde{P}_{2}^{\alpha_{2}}...\widetilde{P}_{N}%
^{\alpha_{N}}$ is a $\mathbb{C}$-linear combination of monomials smaller than
$T_{1}^{\beta_{1}}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$, we thus conclude
that $-\lambda_{\beta_{1},\beta_{2},...,\beta_{N}}\widetilde{P}_{1}^{\beta
_{1}}\widetilde{P}_{2}^{\beta_{2}}...\widetilde{P}_{N}^{\beta_{N}}$ is a
$\mathbb{C}$-linear combination of monomials smaller than $T_{1}^{\beta_{1}%
}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$. Since $-\lambda_{\beta_{1},\beta
_{2},...,\beta_{N}}$ is invertible (because $\lambda_{\beta_{1},\beta
_{2},...,\beta_{N}}\neq0$), this yields that $\widetilde{P}_{1}^{\beta_{1}%
}\widetilde{P}_{2}^{\beta_{2}}...\widetilde{P}_{N}^{\beta_{N}}$ is a
$\mathbb{C}$-linear combination of monomials smaller than $T_{1}^{\beta_{1}%
}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$. In other words, every monomial which
occurs with nonzero coefficient in $\widetilde{P}_{1}^{\beta_{1}}%
\widetilde{P}_{2}^{\beta_{2}}...\widetilde{P}_{N}^{\beta_{N}}$ is less than
$T_{1}^{\beta_{1}}T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$. In particular, the
leading monomial of $\widetilde{P}_{1}^{\beta_{1}}\widetilde{P}_{2}^{\beta
_{2}}...\widetilde{P}_{N}^{\beta_{N}}$ is less than $T_{1}^{\beta_{1}}%
T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$. But this contradicts the fact that
(due to (\ref{pf.schur.algind.3}), applied to $\left(  \alpha_{1},\alpha
_{2},...,\alpha_{N}\right)  =\left(  \beta_{1},\beta_{2},...,\beta_{N}\right)
$) the leading monomial of $\widetilde{P}_{1}^{\beta_{1}}\widetilde{P}%
_{2}^{\beta_{2}}...\widetilde{P}_{N}^{\beta_{N}}$ is $T_{1}^{\beta_{1}}%
T_{2}^{\beta_{2}}...T_{N}^{\beta_{N}}$.

This contradiction shows that our assumption was wrong. Hence, Lemma
\ref{lem.schur.algind} is proven.

(I have learned the above proof from:

Julia Pevtsova and Nate Bottman, \textit{504A Fall 2009 Homework Set
3},\newline%
\texttt{\url{http://www.math.washington.edu/~julia/teaching/504_Fall2009/HW7_sol.pdf}}
.)

We will apply Lemma \ref{lem.schur.algind} not directly, but through the
following corollary:

\begin{corollary}
\label{cor.schur.PSEinj}Let $P$ and $Q$ be polynomials in $\mathbb{C}\left[
x_{1},x_{2},x_{3},...\right]  $. Assume that $\operatorname*{PSE}%
\nolimits_{N}\left(  P\right)  =\operatorname*{PSE}\nolimits_{N}\left(
Q\right)  $ for every sufficiently high $N\in\mathbb{N}$. Then, $P=Q$.
\end{corollary}

\textit{Proof of Corollary \ref{cor.schur.PSEinj}.} Any polynomial (even if it
is a polynomial in infinitely many indeterminates) has only finitely many
indeterminates actually appear in it. Hence, only finitely many indeterminates
appear in $P-Q$. Thus, there exists an $M\in\mathbb{N}$ such that no
indeterminates other than $x_{1}$, $x_{2}$, $...$, $x_{M}$ appear in $P-Q$.
Consider this $M$.

Recall that $\operatorname*{PSE}\nolimits_{N}\left(  P\right)
=\operatorname*{PSE}\nolimits_{N}\left(  Q\right)  $ for every sufficiently
high $N\in\mathbb{N}$. Thus, there exists an $N\in\mathbb{N}$ such that $N\geq
M$ and $\operatorname*{PSE}\nolimits_{N}\left(  P\right)  =\operatorname*{PSE}%
\nolimits_{N}\left(  Q\right)  $. Pick such an $N$.

No indeterminates other than $x_{1}$, $x_{2}$, $...$, $x_{M}$ appear in $P-Q$.
Since $N\geq M$, this clearly yields that no indeterminates other than $x_{1}%
$, $x_{2}$, $...$, $x_{N}$ appear in $P-Q$. Hence, there exists a polynomial
$R\in\mathbb{C}\left[  x_{1},x_{2},...,x_{N}\right]  $ such that $P-Q=R\left(
x_{1},x_{2},...,x_{N}\right)  $. Consider this $R$.

Now, let us use the notations of Lemma \ref{lem.schur.algind}.

We defined $\operatorname*{PSE}\nolimits_{N}\left(  P-Q\right)  $ as the
result of substituting $x_{j}=\dfrac{y_{1}^{j}+y_{2}^{j}+...+y_{N}^{j}}{j}$
for all positive integers $j$ into the polynomial $P-Q$. Since $y_{1}%
^{j}+y_{2}^{j}+...+y_{N}^{j}=p_{j}$ for all positive integers $j$, this
rewrites as follows: $\operatorname*{PSE}\nolimits_{N}\left(  P-Q\right)  $ is
the result of substituting $x_{j}=\dfrac{p_{j}}{j}$ for all positive integers
$j$ into the polynomial $P-Q$. In other words,
\begin{align*}
\operatorname*{PSE}\nolimits_{N}\left(  P-Q\right)   &  =\underbrace{\left(
P-Q\right)  }_{=R\left(  x_{1},x_{2},...,x_{N}\right)  }\left(  \dfrac{p_{1}%
}{1},\dfrac{p_{2}}{2},\dfrac{p_{3}}{3},...\right)  =\left(  R\left(
x_{1},x_{2},...,x_{N}\right)  \right)  \left(  \dfrac{p_{1}}{1},\dfrac{p_{2}%
}{2},\dfrac{p_{3}}{3},...\right) \\
&  =R\left(  \dfrac{p_{1}}{1},\dfrac{p_{2}}{2},...,\dfrac{p_{N}}{N}\right)  .
\end{align*}
But since $\operatorname*{PSE}\nolimits_{N}$ is a $\mathbb{C}$-algebra
homomorphism, we have $\operatorname*{PSE}\nolimits_{N}\left(  P-Q\right)
=\operatorname*{PSE}\nolimits_{N}\left(  P\right)  -\operatorname*{PSE}%
\nolimits_{N}\left(  Q\right)  =0$ (since $\operatorname*{PSE}\nolimits_{N}%
\left(  P\right)  =\operatorname*{PSE}\nolimits_{N}\left(  Q\right)  $). Thus,%
\[
R\left(  \dfrac{p_{1}}{1},\dfrac{p_{2}}{2},...,\dfrac{p_{N}}{N}\right)
=\operatorname*{PSE}\nolimits_{N}\left(  P-Q\right)  =0.
\]
Since $\dfrac{p_{1}}{1}$, $\dfrac{p_{2}}{2}$, $...$, $\dfrac{p_{N}}{N}$ are
algebraically independent (because Lemma \ref{lem.schur.algind} yields that
$p_{1}$, $p_{2}$, $...$, $p_{N}$ are algebraically independent), this yields
$R=0$, so that $P-Q=\underbrace{R}_{=0}\left(  x_{1},x_{2},...,x_{N}\right)
=0$, thus $P=Q$. Corollary \ref{cor.schur.PSEinj} is proven.

Corollary \ref{cor.schur.PSEinj} allows us to prove equality of polynomials in
$\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $ by means of evaluating them
at power sums. Now, let us show what such evaluations look like for the Schur functions:

\subsubsection{\label{subsubsect.schur1}First proof of Theorem \ref{thm.schur}%
}

\begin{theorem}
\label{thm.schur.altern}Let $\lambda=\left(  \lambda_{1},\lambda_{2}%
,\lambda_{3},...\right)  $ be a partition, so that $\lambda_{1}\geq\lambda
_{2}\geq...$ are nonnegative integers.

Let $N$ be a nonnegative integer such that $\lambda_{N+1}=0$. Then,%
\[
\operatorname*{PSE}\nolimits_{N}\left(  S_{\lambda}\left(  x\right)  \right)
=\dfrac{\det\left(  \left(  y_{i}^{\lambda_{j}+N-j}\right)  _{1\leq i\leq
N,\ 1\leq j\leq N}\right)  }{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq
i\leq N,\ 1\leq j\leq N}\right)  }.
\]

\end{theorem}

\textit{Proof of Theorem \ref{thm.schur.altern}.} We will not really prove
this theorem; we will just reduce it to a known fact about Schur functions.

In fact, let $m$ be an integer such that $\lambda_{m+1}=0$ (such an integer
clearly exists). Then, the partition $\lambda$ can also be written in the form
$\left(  \lambda_{1},\lambda_{2},...,\lambda_{m}\right)  $. Hence, by the
first Giambelli formula, the $\lambda$-th Schur polynomial evaluated at
$\left(  y_{1},y_{2},...,y_{N}\right)  $ equals%
\begin{align*}
&  \det\left(
\begin{array}
[c]{ccccc}%
h_{\lambda_{1}}\left(  y\right)  & h_{\lambda_{1}+1}\left(  y\right)  &
h_{\lambda_{1}+2}\left(  y\right)  & ... & h_{\lambda_{1}+m-1}\left(  y\right)
\\
h_{\lambda_{2}-1}\left(  y\right)  & h_{\lambda_{2}}\left(  y\right)  &
h_{\lambda_{2}+1}\left(  y\right)  & ... & h_{\lambda_{2}+m-2}\left(  y\right)
\\
h_{\lambda_{3}-2}\left(  y\right)  & h_{\lambda_{3}-1}\left(  y\right)  &
h_{\lambda_{3}}\left(  y\right)  & ... & h_{\lambda_{3}+m-3}\left(  y\right)
\\
... & ... & ... & ... & ...\\
h_{\lambda_{m}-m+1}\left(  y\right)  & h_{\lambda_{m}-m+2}\left(  y\right)  &
h_{\lambda_{m}-m+3}\left(  y\right)  & ... & h_{\lambda_{m}}\left(  y\right)
\end{array}
\right) \\
&  =\det\left(  \left(  h_{\lambda_{i}+j-i}\left(  y\right)  \right)  _{1\leq
i\leq m,\ 1\leq j\leq m}\right)  .
\end{align*}
But since the $\lambda$-th Schur polynomial evaluated at $\left(  y_{1}%
,y_{2},...,y_{N}\right)  $ also equals \newline$\dfrac{\det\left(  \left(
y_{i}^{\lambda_{j}+N-j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)
}{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }$ (by the ``Vandermonde-determinant'' definition of Schur
polynomials), this yields that
\[
\det\left(  \left(  h_{\lambda_{i}+j-i}\left(  y\right)  \right)  _{1\leq
i\leq m,\ 1\leq j\leq m}\right)  =\dfrac{\det\left(  \left(  y_{i}%
^{\lambda_{j}+N-j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  }%
{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }.
\]
Comparing this with the equality $\det\left(  \left(  h_{\lambda_{i}%
+j-i}\left(  y\right)  \right)  _{1\leq i\leq m,\ 1\leq j\leq m}\right)
=\operatorname*{PSE}\nolimits_{N}\left(  S_{\lambda}\left(  x\right)  \right)
$ (which was verified during the proof of Proposition
\ref{prop.schur.Schur=schur}), we obtain%
\[
\dfrac{\det\left(  \left(  y_{i}^{\lambda_{j-1}+N-j}\right)  _{1\leq i\leq
N,\ 1\leq j\leq N}\right)  }{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq
i\leq N,\ 1\leq j\leq N}\right)  }=\operatorname*{PSE}\nolimits_{N}\left(
S_{\lambda}\left(  x\right)  \right)  .
\]
Theorem \ref{thm.schur.altern} is thus proven.

We will now use a harmless-looking result about determinants:

\begin{proposition}
\label{prop.schur.det}Let $N\in\mathbb{N}$. Let $\left(  a_{i,j}\right)
_{1\leq i\leq N,\ 1\leq j\leq N}$ be an $N\times N$-matrix of elements of a
commutative ring $R$. Let $b_{1}$, $b_{2}$, $...$, $b_{N}$ be $N$ elements of
$R$. Then,%
\begin{equation}
\sum\limits_{k=1}^{N}\det\left(  \left(  a_{i,j}b_{i}^{\delta_{j,k}}\right)
_{1\leq i\leq N,\ 1\leq j\leq N}\right)  =\left(  b_{1}+b_{2}+...+b_{N}%
\right)  \det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  . \label{prop.schur.det.1}%
\end{equation}
Equivalently (in more reader-friendly terms):%
\begin{align}
&  \det\left(
\begin{array}
[c]{cccc}%
b_{1}a_{1,1} & a_{1,2} & ... & a_{1,N}\\
b_{2}a_{2,1} & a_{2,2} & ... & a_{2,N}\\
... & ... & ... & ...\\
b_{N}a_{N,1} & a_{N,2} & ... & a_{N,N}%
\end{array}
\right)  +\det\left(
\begin{array}
[c]{cccc}%
a_{1,1} & b_{1}a_{1,2} & ... & a_{1,N}\\
a_{2,1} & b_{2}a_{2,2} & ... & a_{2,N}\\
... & ... & ... & ...\\
a_{N,1} & b_{N}a_{N,2} & ... & a_{N,N}%
\end{array}
\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +...+\det\left(
\begin{array}
[c]{cccc}%
a_{1,1} & a_{1,2} & ... & b_{1}a_{1,N}\\
a_{2,1} & a_{2,2} & ... & b_{2}a_{2,N}\\
... & ... & ... & ...\\
a_{N,1} & a_{N,2} & ... & b_{N}a_{N,N}%
\end{array}
\right) \nonumber\\
&  =\left(  b_{1}+b_{2}+...+b_{N}\right)  \det\left(
\begin{array}
[c]{cccc}%
a_{1,1} & a_{1,2} & ... & a_{1,N}\\
a_{2,1} & a_{2,2} & ... & a_{2,N}\\
... & ... & ... & ...\\
a_{N,1} & a_{N,2} & ... & a_{N,N}%
\end{array}
\right)  . \label{prop.schur.det.2}%
\end{align}

\end{proposition}

\textit{Proof of Proposition \ref{prop.schur.det}.} Recall the explicit
formula for a determinant of a matrix as a sum over permutations: For every
$N\times N$-matrix $\left(  c_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}$,
we have%
\begin{equation}
\det\left(  \left(  c_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)
=\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}\prod\limits_{j=1}%
^{N}c_{\sigma\left(  j\right)  ,j}. \label{pf.schur.det.1}%
\end{equation}
Applied to $\left(  c_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}=\left(
a_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}$, this yields%
\begin{equation}
\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)
=\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}\prod\limits_{j=1}%
^{N}a_{\sigma\left(  j\right)  ,j}. \label{pf.schur.det.2}%
\end{equation}


For every $k\in\left\{  1,2,...,N\right\}  $, we can apply
(\ref{pf.schur.det.1}) to $\left(  c_{i,j}\right)  _{1\leq i\leq N,\ 1\leq
j\leq N}=\left(  a_{i,j}b_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq N,\ 1\leq
j\leq N}$, and obtain%
\begin{align*}
\det\left(  \left(  a_{i,j}b_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq
N,\ 1\leq j\leq N}\right)   &  =\sum\limits_{\sigma\in S_{N}}\left(
-1\right)  ^{\sigma}\underbrace{\prod\limits_{j=1}^{N}\left(  a_{\sigma\left(
j\right)  ,j}b_{\sigma\left(  j\right)  }^{\delta_{j,k}}\right)  }%
_{=\prod\limits_{j=1}^{N}a_{\sigma\left(  j\right)  ,j}\prod\limits_{j=1}%
^{N}b_{\sigma\left(  j\right)  }^{\delta_{j,k}}}\\
&  =\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}\prod
\limits_{j=1}^{N}a_{\sigma\left(  j\right)  ,j}\underbrace{\prod
\limits_{j=1}^{N}b_{\sigma\left(  j\right)  }^{\delta_{j,k}}}_{=b_{\sigma
\left(  k\right)  }^{\delta_{k,k}}\prod\limits_{\substack{j\in\left\{
1,2,...,N\right\}  ;\\j\neq k}}b_{\sigma\left(  j\right)  }^{\delta_{j,k}}}\\
&  =\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}\prod
\limits_{j=1}^{N}a_{\sigma\left(  j\right)  ,j}\underbrace{b_{\sigma\left(
k\right)  }^{\delta_{k,k}}}_{\substack{=b_{\sigma\left(  k\right)
}\\\text{(since }\delta_{k,k}=1\text{)}}}\prod\limits_{\substack{j\in\left\{
1,2,...,N\right\}  ;\\j\neq k}}\underbrace{b_{\sigma\left(  j\right)
}^{\delta_{j,k}}}_{\substack{=1\\\text{(since }j\neq k\text{ and thus }%
\delta_{j,k}=0\text{)}}}\\
&  =\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}\prod
\limits_{j=1}^{N}a_{\sigma\left(  j\right)  ,j}b_{\sigma\left(  k\right)
}\underbrace{\prod\limits_{\substack{j\in\left\{  1,2,...,N\right\}  ;\\j\neq
k}}1}_{=1}=\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}%
\prod\limits_{j=1}^{N}a_{\sigma\left(  j\right)  ,j}b_{\sigma\left(  k\right)
}.
\end{align*}
Hence,%
\begin{align*}
&  \sum\limits_{k=1}^{N}\det\left(  \left(  a_{i,j}b_{i}^{\delta_{j,k}%
}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right) \\
&  =\sum\limits_{k=1}^{N}\sum\limits_{\sigma\in S_{N}}\left(  -1\right)
^{\sigma}\prod\limits_{j=1}^{N}a_{\sigma\left(  j\right)  ,j}b_{\sigma\left(
k\right)  }=\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}%
\prod\limits_{j=1}^{N}a_{\sigma\left(  j\right)  ,j}\underbrace{\sum
\limits_{k=1}^{N}b_{\sigma\left(  k\right)  }}_{\substack{=\sum\limits_{k=1}%
^{N}b_{k}\\\text{(since }\sigma\text{ is a permutation)}}}\\
&  =\sum\limits_{\sigma\in S_{N}}\left(  -1\right)  ^{\sigma}\prod
\limits_{j=1}^{N}a_{\sigma\left(  j\right)  ,j}\sum\limits_{k=1}^{N}%
b_{k}=\underbrace{\left(  \sum\limits_{k=1}^{N}b_{k}\right)  }_{=b_{1}%
+b_{2}+...+b_{N}}\underbrace{\sum\limits_{\sigma\in S_{N}}\left(  -1\right)
^{\sigma}\prod\limits_{j=1}^{N}a_{\sigma\left(  j\right)  ,j}}%
_{\substack{=\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  \\\text{(by (\ref{pf.schur.det.2}))}}}\\
&  =\left(  b_{1}+b_{2}+...+b_{N}\right)  \det\left(  \left(  a_{i,j}\right)
_{1\leq i\leq N,\ 1\leq j\leq N}\right)  .
\end{align*}
This proves Proposition \ref{prop.schur.det}.

\begin{corollary}
\label{cor.schur.det}Let $N\in\mathbb{N}$. Let $\left(  i_{0},i_{1}%
,...,i_{N-1}\right)  \in\mathbb{Z}^{N}$ be such that $i_{j-1}+N>0$ for every
$j\in\left\{  1,2,...,N\right\}  $. Let $m\in\mathbb{N}$. Then,%
\begin{align*}
&  \sum\limits_{k=1}^{N}\det\left(  \left(  y_{i}^{i_{j-1}+\delta_{j,k}%
m+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right) \\
&  =\left(  y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}\right)  \det\left(  \left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  .
\end{align*}

\end{corollary}

\textit{Proof of Corollary \ref{cor.schur.det}.} Applying Proposition
\ref{prop.schur.det} to $R=\mathbb{C}\left[  y_{1},y_{2},...,y_{N}\right]  $,
$\left(  a_{i,j}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}=\left(
y_{i}^{i_{j-1}+N}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}$ and $b_{i}%
=y_{i}^{m}$, we obtain%
\begin{align*}
&  \sum\limits_{k=1}^{N}\det\left(  \left(  y_{i}^{i_{j-1}+N-1}\left(
y_{i}^{m}\right)  ^{\delta_{j,k}}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right) \\
&  =\left(  y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}\right)  \det\left(  \left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  .
\end{align*}
Since any $i\in\left\{  1,2,...,N\right\}  $, $j\in\left\{  1,2,...,N\right\}
$ and $k\in\left\{  1,2,...,N\right\}  $ satisfy $y_{i}^{i_{j-1}+N-1}\left(
y_{i}^{m}\right)  ^{\delta_{j,k}}=y_{i}^{i_{j-1}+N+\delta_{j,k}m}%
=y_{i}^{i_{j-1}+\delta_{j,k}m+N-1}$, this rewrites as%
\begin{align*}
&  \sum\limits_{k=1}^{N}\det\left(  \left(  y_{i}^{i_{j-1}+\delta_{j,k}%
m+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right) \\
&  =\left(  y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}\right)  \det\left(  \left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  .
\end{align*}
Corollary \ref{cor.schur.det} is proven.

Now, to the main proof.

\textit{Proof of Theorem \ref{thm.schur}.} Define a $\mathbb{C}$-linear map
$\tau:\mathcal{F}^{\left(  0\right)  }\rightarrow\mathbb{C}\left[  x_{1}%
,x_{2},x_{3},...\right]  $ by%
\[
\tau\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=S_{\left(  i_{0}+0,i_{1}+1,i_{2}+2,...\right)  }\left(  x\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }0\text{-degression }\left(  i_{0}%
,i_{1},i_{2},...\right)  .
\]
(This definition makes sense, because we know that $\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  _{\left(  i_{0},i_{1}%
,i_{2},...\right)  \text{ is a }0\text{-degression}}$ is a basis of
$\wedge^{\dfrac{\infty}{2},0}V=\mathcal{F}^{\left(  0\right)  }$.)

Our aim is to prove that $\tau=\sigma^{-1}$.

\textit{1st step:} First of all, the definition of $\tau$ (applied to the
$0$-degression $\left(  0,-1,-2,...\right)  $) yields%
\[
\tau\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)  =S_{\left(
0+0,-1+1,-2+2,...\right)  }\left(  x\right)  =S_{\left(  0,0,0,...\right)
}\left(  x\right)  =1.
\]


\textit{2nd step:} If $N\in\mathbb{N}$, and $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ is a straying $0$-degression, then we say that $\left(
i_{0},i_{1},i_{2},...\right)  $ is $N$\textit{-finished} if the following two
conditions (\ref{pf.schur.step2.fin1}) and (\ref{pf.schur.step2.fin2}) hold:%
\begin{align}
&  \left(  \text{every integer }k\geq N\text{ satisfies }i_{k}+k=0\right)
;\label{pf.schur.step2.fin1}\\
&  \left(  \text{each of the integers }i_{0}\text{, }i_{1}\text{, }...\text{,
}i_{N-1}\text{ is }>-N\right)  . \label{pf.schur.step2.fin2}%
\end{align}


Now, we claim the following:

For any $N\in\mathbb{N}$, and any $N$-finished straying $0$-degression
$\left(  i_{0},i_{1},i_{2},...\right)  $, we have%
\begin{equation}
\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  \right)  =\dfrac{\det\left(  \left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  }%
{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }. \label{pf.schur.step2}%
\end{equation}


\textit{Proof of (\ref{pf.schur.step2}):} Let $N\in\mathbb{N}$, and let
$\left(  i_{0},i_{1},i_{2},...\right)  $ be an $N$-finished straying $0$-degression.

Since $\left(  i_{0},i_{1},i_{2},...\right)  $ is $N$-finished, we conclude
(by the definition of ``$N$-finished'') that it satisfies the conditions
(\ref{pf.schur.step2.fin1}) and (\ref{pf.schur.step2.fin2}).

If some two of the integers $i_{0}$, $i_{1}$, $...$, $i_{N-1}$ are equal, then
(\ref{pf.schur.step2}) is true.\footnote{\textit{Proof.} Assume that some two
of the integers $i_{0}$, $i_{1}$, $...$, $i_{N-1}$ are equal. Then, some two
elements of the sequence $\left(  i_{0},i_{1},i_{2},...\right)  $ are equal,
so that $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...=0$ (by the
definition of $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$) and thus
$\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  \right)  =\operatorname*{PSE}%
\nolimits_{N}\left(  0\right)  =0$. Thus, the left hand side of
(\ref{pf.schur.step2}) is $0$. On the other hand, the matrix $\left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}$ has two equal
columns (since two of the integers $i_{0}$, $i_{1}$, $...$, $i_{N-1}$ are
equal) and thus its determinant vanishes, i. e., we have $\det\left(  \left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  =0$, so
that the right hand side of (\ref{pf.schur.step2}) is $0$.
\par
Thus, both the left hand side and the right hand side of (\ref{pf.schur.step2}%
) are $0$. Hence, (\ref{pf.schur.step2}) is true, qed.} Hence, for the rest of
this proof, we assume that no two of the integers $i_{0}$, $i_{1}$, $...$,
$i_{N-1}$ are equal. Then, there exists a permutation $\phi$ of the set
$\left\{  0,1,...,N-1\right\}  $ such that $i_{\phi^{-1}\left(  0\right)
}>i_{\phi^{-1}\left(  1\right)  }>...>i_{\phi^{-1}\left(  N-1\right)  }$.
Consider this $\phi$.

It is easy to see that $i_{\phi^{-1}\left(  0\right)  }>i_{\phi^{-1}\left(
1\right)  }>...>i_{\phi^{-1}\left(  N-1\right)  }>-N$%
.\ \ \ \ \footnote{\textit{Proof.} Every $j\in\left\{  0,1,...,N-1\right\}  $
satisfies $\phi^{-1}\left(  j\right)  \in\phi^{-1}\left(  \left\{
0,1,...,N-1\right\}  \right)  =\left\{  0,1,...,N-1\right\}  $. Hence, for
every $j\in\left\{  0,1,...,N-1\right\}  $, the integer $i_{\phi^{-1}\left(
j\right)  }$ is one of the integers $i_{0}$, $i_{1}$, $...$, $i_{N-1}$, and
therefore $>-N$ (due to (\ref{pf.schur.step2.fin2})). That is, $i_{\phi
^{-1}\left(  j\right)  }>-N$ for every $j\in\left\{  0,1,...,N-1\right\}  $.
Combining this with $i_{\phi^{-1}\left(  0\right)  }>i_{\phi^{-1}\left(
1\right)  }>...>i_{\phi^{-1}\left(  N-1\right)  }$, we get $i_{\phi
^{-1}\left(  0\right)  }>i_{\phi^{-1}\left(  1\right)  }>...>i_{\phi
^{-1}\left(  N-1\right)  }>-N$.}

Let $\pi$ be the finitary permutation of $\mathbb{N}$ which sends every
$k\in\mathbb{N}$ to \newline$\left\{
\begin{array}
[c]{l}%
\phi\left(  k\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }k\in\left\{
0,1,...,N-1\right\}  ;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k\notin\left\{  0,1,...,N-1\right\}
\end{array}
\right.  $. Then, $\left(  -1\right)  ^{\pi}=\left(  -1\right)  ^{\phi}$;
moreover, every $k\in\mathbb{N}$ satisfies%
\begin{equation}
\pi^{-1}\left(  k\right)  =\left\{
\begin{array}
[c]{l}%
\phi^{-1}\left(  k\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }k\in\left\{
0,1,...,N-1\right\}  ;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k\notin\left\{  0,1,...,N-1\right\}
\end{array}
\right.  . \label{pf.schur.step2.pf.2}%
\end{equation}
In particular, every integer $k\geq N$ satisfies $\pi^{-1}\left(  k\right)
=k$.

From (\ref{pf.schur.step2.pf.2}), it is clear that%
\begin{equation}
\text{every }k\in\left\{  0,1,...,N-1\right\}  \text{ satisfies }\pi
^{-1}\left(  k\right)  =\phi^{-1}\left(  k\right)  .
\label{pf.schur.step2.pf.4}%
\end{equation}
Hence, $i_{\pi^{-1}\left(  0\right)  }>i_{\pi^{-1}\left(  1\right)
}>...>i_{\pi^{-1}\left(  N-1\right)  }>-N$ (since $i_{\phi^{-1}\left(
0\right)  }>i_{\phi^{-1}\left(  1\right)  }>...>i_{\phi^{-1}\left(
N-1\right)  }>-N$).

Now, every integer $k\geq N$ satisfies $\pi^{-1}\left(  k\right)  =k$, thus
$i_{\pi^{-1}\left(  k\right)  }=i_{k}=-k$ (since (\ref{pf.schur.step2.fin1})
yields $i_{k}+k=0$). Hence, $-N=i_{\pi^{-1}\left(  N\right)  }>i_{\pi
^{-1}\left(  N+1\right)  }>i_{\pi^{-1}\left(  N+2\right)  }>...$ (because
$-N=-N>-\left(  N+1\right)  >-\left(  N+2\right)  >...$). Combined with
$i_{\pi^{-1}\left(  0\right)  }>i_{\pi^{-1}\left(  1\right)  }>...>i_{\pi
^{-1}\left(  N-1\right)  }>-N$, this becomes%
\[
i_{\pi^{-1}\left(  0\right)  }>i_{\pi^{-1}\left(  1\right)  }>...>i_{\pi
^{-1}\left(  N-1\right)  }>-N=i_{\pi^{-1}\left(  N\right)  }>i_{\pi
^{-1}\left(  N+1\right)  }>i_{\pi^{-1}\left(  N+2\right)  }>....
\]
Thus,%
\[
i_{\pi^{-1}\left(  0\right)  }>i_{\pi^{-1}\left(  1\right)  }>...>i_{\pi
^{-1}\left(  N-1\right)  }>i_{\pi^{-1}\left(  N\right)  }>i_{\pi^{-1}\left(
N+1\right)  }>i_{\pi^{-1}\left(  N+2\right)  }>....
\]
In other words, the sequence $\left(  i_{\pi^{-1}\left(  0\right)  }%
,i_{\pi^{-1}\left(  1\right)  },i_{\pi^{-1}\left(  2\right)  },...\right)  $
is strictly decreasing. Since every sufficiently high $k\in\mathbb{N}$
satisfies $i_{\pi^{-1}\left(  k\right)  }+k=0$ (in fact, every $k\geq N$
satisfies $i_{\pi^{-1}\left(  k\right)  }=-k$ and thus $i_{\pi^{-1}\left(
k\right)  }+k=0$), this sequence $\left(  i_{\pi^{-1}\left(  0\right)
},i_{\pi^{-1}\left(  1\right)  },i_{\pi^{-1}\left(  2\right)  },...\right)  $
must thus be a $0$-degression. Hence, by the definition of $\tau$, we have%
\[
\tau\left(  v_{i_{\pi^{-1}\left(  0\right)  }}\wedge v_{i_{\pi^{-1}\left(
1\right)  }}\wedge v_{i_{\pi^{-1}\left(  2\right)  }}\wedge...\right)
=S_{\left(  i_{\pi^{-1}\left(  0\right)  }+0,i_{\pi^{-1}\left(  1\right)
}+1,i_{\pi^{-1}\left(  2\right)  }+2,...\right)  }\left(  x\right)  .
\]
Since $\pi$ is a finitary permutation of $\mathbb{N}$ such that $\left(
i_{\pi^{-1}\left(  0\right)  },i_{\pi^{-1}\left(  1\right)  },i_{\pi
^{-1}\left(  2\right)  },...\right)  $ is a $0$-degression, it is clear that
$\pi$ is the straightening permutation of $\left(  i_{0},i_{1},i_{2}%
,...\right)  $. Thus, by the definition of $v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...$, we have%
\begin{align*}
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...  &  =\underbrace{\left(
-1\right)  ^{\pi}}_{=\left(  -1\right)  ^{\phi}}v_{i_{\pi^{-1}\left(
0\right)  }}\wedge v_{i_{\pi^{-1}\left(  1\right)  }}\wedge v_{i_{\pi
^{-1}\left(  2\right)  }}\wedge...\\
&  =\left(  -1\right)  ^{\phi}v_{i_{\pi^{-1}\left(  0\right)  }}\wedge
v_{i_{\pi^{-1}\left(  1\right)  }}\wedge v_{i_{\pi^{-1}\left(  2\right)  }%
}\wedge...,
\end{align*}
so that%
\begin{align}
&  \operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right) \nonumber\\
&  =\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  \left(  -1\right)
^{\phi}v_{i_{\pi^{-1}\left(  0\right)  }}\wedge v_{i_{\pi^{-1}\left(
1\right)  }}\wedge v_{i_{\pi^{-1}\left(  2\right)  }}\wedge...\right)  \right)
\nonumber\\
&  =\left(  -1\right)  ^{\phi}\operatorname*{PSE}\nolimits_{N}%
\underbrace{\left(  \tau\left(  v_{i_{\pi^{-1}\left(  0\right)  }}\wedge
v_{i_{\pi^{-1}\left(  1\right)  }}\wedge v_{i_{\pi^{-1}\left(  2\right)  }%
}\wedge...\right)  \right)  }_{\substack{=S_{\left(  i_{\pi^{-1}\left(
0\right)  }+0,i_{\pi^{-1}\left(  1\right)  }+1,i_{\pi^{-1}\left(  2\right)
}+2,...\right)  }\left(  x\right)  \\\text{(by the definition of }\tau\text{,
since }\left(  i_{\pi^{-1}\left(  0\right)  },i_{\pi^{-1}\left(  1\right)
},i_{\pi^{-1}\left(  2\right)  },...\right)  \text{ is a }0\text{-degression)}%
}}\nonumber\\
&  =\left(  -1\right)  ^{\phi}\operatorname*{PSE}\nolimits_{N}\left(
S_{\left(  i_{\pi^{-1}\left(  0\right)  }+0,i_{\pi^{-1}\left(  1\right)
}+1,i_{\pi^{-1}\left(  2\right)  }+2,...\right)  }\left(  x\right)  \right)  .
\label{pf.schur.step2.pf.4a}%
\end{align}


Let $\mu$ be the partition $\left(  i_{\pi^{-1}\left(  0\right)  }%
+0,i_{\pi^{-1}\left(  1\right)  }+1,i_{\pi^{-1}\left(  2\right)
}+2,...\right)  $. For every positive integer $\alpha$, let $\mu_{\alpha}$
denote the $\alpha$-th part of the partition $\mu$, so that $\mu=\left(
\mu_{1},\mu_{2},\mu_{3},...\right)  $. Then, every $j\in\left\{
1,2,...,N\right\}  $ satisfies%
\begin{align*}
\mu_{j}  &  =i_{\pi^{-1}\left(  j-1\right)  }+\left(  j-1\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mu\right) \\
&  =i_{\phi^{-1}\left(  j-1\right)  }+\left(  j-1\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since (\ref{pf.schur.step2.pf.4}) (applied
to }k=j-1\text{) yields }\pi^{-1}\left(  j-1\right)  =\phi^{-1}\left(
j-1\right)  \right)  ,
\end{align*}
so that $\mu_{j}+N-j=i_{\phi^{-1}\left(  j-1\right)  }+\left(  j-1\right)
+N-j=i_{\phi^{-1}\left(  j-1\right)  }+N-1$. Hence,%
\begin{equation}
\det\left(  \left(  y_{i}^{\mu_{j}+N-j}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  =\det\left(  \left(  y_{i}^{i_{\phi^{-1}\left(  j-1\right)  }%
+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  .
\label{pf.schur.step2.pf.5}%
\end{equation}
But the matrix $\left(  y_{i}^{i_{\phi^{-1}\left(  j-1\right)  }+N-1}\right)
_{1\leq i\leq N,\ 1\leq j\leq N}$ is obtained from the matrix $\left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}$ by permuting the
columns using the permutation $\phi$. Hence,%
\[
\det\left(  \left(  y_{i}^{i_{\phi^{-1}\left(  j-1\right)  }+N-1}\right)
_{1\leq i\leq N,\ 1\leq j\leq N}\right)  =\left(  -1\right)  ^{\phi}%
\det\left(  \left(  y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)
\]
(since permuting the columns of a matrix changes the determinant by the sign
of the permutation). Combining this with (\ref{pf.schur.step2.pf.5}), we
obtain%
\begin{equation}
\det\left(  \left(  y_{i}^{\mu_{j}+N-j}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  =\left(  -1\right)  ^{\phi}\det\left(  \left(  y_{i}^{i_{j-1}%
+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  .
\label{pf.schur.step2.pf.7}%
\end{equation}


Also, by the definition of $\mu$, we have $\mu_{N+1}=i_{\pi^{-1}\left(
N\right)  }+N=0$ (because $-N=i_{\pi^{-1}\left(  N\right)  }$), and thus we
can apply Theorem \ref{thm.schur.altern} to $\mu$ instead of $\lambda$. This
results in
\begin{align}
\operatorname*{PSE}\nolimits_{N}\left(  S_{\mu}\left(  x\right)  \right)   &
=\dfrac{\det\left(  \left(  y_{i}^{\mu_{j}+N-j}\right)  _{1\leq i\leq
N,\ 1\leq j\leq N}\right)  }{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq
i\leq N,\ 1\leq j\leq N}\right)  }\nonumber\\
&  =\dfrac{\left(  -1\right)  ^{\phi}\det\left(  \left(  y_{i}^{i_{j-1}%
+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  }{\det\left(  \left(
y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  }
\label{pf.schur.step2.pf.8}%
\end{align}
(by (\ref{pf.schur.step2.pf.7})). But (\ref{pf.schur.step2.pf.4a}) becomes%
\begin{align*}
&  \operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right) \\
&  =\left(  -1\right)  ^{\phi}\operatorname*{PSE}\nolimits_{N}\left(
S_{\left(  i_{\pi^{-1}\left(  0\right)  }+0,i_{\pi^{-1}\left(  1\right)
}+1,i_{\pi^{-1}\left(  2\right)  }+2,...\right)  }\left(  x\right)  \right) \\
&  =\left(  -1\right)  ^{\phi}\operatorname*{PSE}\nolimits_{N}\left(  S_{\mu
}\left(  x\right)  \right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
i_{\pi^{-1}\left(  0\right)  }+0,i_{\pi^{-1}\left(  1\right)  }+1,i_{\pi
^{-1}\left(  2\right)  }+2,...\right)  =\mu\right) \\
&  =\left(  -1\right)  ^{\phi}\dfrac{\left(  -1\right)  ^{\phi}\det\left(
\left(  y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)
}{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.schur.step2.pf.8}%
)}\right) \\
&  =\dfrac{\det\left(  \left(  y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq
N,\ 1\leq j\leq N}\right)  }{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq
i\leq N,\ 1\leq j\leq N}\right)  }.
\end{align*}
This proves (\ref{pf.schur.step2}). The proof of the 2nd step is thus complete.

\textit{3rd step:} Consider the action of the Heisenberg algebra $\mathcal{A}$
on $\widetilde{F}=\mathcal{B}^{\left(  0\right)  }$ and $\wedge^{\dfrac
{\infty}{2},0}V=\mathcal{F}^{\left(  0\right)  }$. We will now prove that the
map $\tau:\wedge^{\dfrac{\infty}{2},0}V\rightarrow\widetilde{F}$ satisfies%
\begin{equation}
\tau\circ a_{-m}=a_{-m}\circ\tau\ \ \ \ \ \ \ \ \ \ \text{for every positive
integer }m. \label{pf.schur.step3}%
\end{equation}


\textit{Proof of (\ref{pf.schur.step3}):} Let $m$ be a positive integer.

Let $\left(  i_{0},i_{1},i_{2},...\right)  $ be a $0$-degression. By the
definition of a $0$-degression, $\left(  i_{0},i_{1},i_{2},...\right)  $ is a
strictly decreasing sequence of integers such that every sufficiently high
$k\in\mathbb{N}$ satisfies $i_{k}+k=0$. In other words, there exists an
$\ell\in\mathbb{N}$ such that every integer $k\geq\ell$ satisfies $i_{k}+k=0$.
Consider this $\ell$.

Let $N$ be any integer satisfying $N\geq\ell+m$. Then, it is easy to see that,
for every integer $k\geq N$, we have $i_{k}+m=i_{k-m}$.

By the definition of the $\mathcal{A}$-module structure on $\wedge
^{\dfrac{\infty}{2},0}V$, the action of $a_{-m}$ on $\wedge^{\dfrac{\infty}%
{2},0}V$ is $\widehat{\rho}\left(  T^{-m}\right)  $, where $T$ is the shift
operator. Thus,%
\begin{equation}
a_{-m}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=\left(  \widehat{\rho}\left(  T^{-m}\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  . \label{pf.schur.step3.2}%
\end{equation}


Since $m\neq0$, the matrix $T^{-m}$ has the property that, for every integer
$i$, the $\left(  i,i\right)  $-th entry of $T^{-m}$ is $0$. Hence,
Proposition \ref{prop.glinf.ainfact} (applied to $0$, $T^{-m}$ and $v_{i_{k}}$
instead of $m$, $a$ and $b_{k}$) yields%
\begin{align*}
&  \left(  \widehat{\rho}\left(  T^{-m}\right)  \right)  \left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\underbrace{\left(  T^{-m}\rightharpoonup v_{i_{k}}\right)
}_{=v_{i_{k}+m}}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge v_{i_{k}+m}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  =\underbrace{\sum\limits_{\substack{k\geq0;\\k<N}}}_{=\sum\limits_{k=0}%
^{N-1}}\underbrace{v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge
v_{i_{k}+m}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...}_{\substack{=v_{i_{0}%
+\delta_{0,k}m}\wedge v_{i_{1}+\delta_{1,k}m}\wedge...\wedge v_{i_{k-1}%
+\delta_{k-1,k}m}\wedge v_{i_{k}+\delta_{k,k}m}\wedge v_{i_{k+1}%
+\delta_{k+1,k}m}\wedge v_{i_{k+2}+\delta_{k+2,k}m}\wedge...\\\text{(here we
are simply making use of the fact that every }j\in\mathbb{N}\text{ such that
}j\neq k\text{ satisfies}\\i_{j}=i_{j}+\delta_{j,k}m\text{ (since }%
\delta_{j,k}=0\text{), whereas }i_{k}+m=i_{k}+\delta_{k,k}m\text{ (since
}\delta_{k,k}=1\text{))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{k\geq N}\underbrace{v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}+m}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...}_{\substack{=0\text{ (because the sequence }\left(
i_{0},i_{1},...,i_{k-1},i_{k}+m,i_{k+1},i_{k+2},...\right)  \\\text{has two
equal elements (since }i_{k}+m=i_{k-m}\text{))}}}\\
&  =\sum\limits_{k=0}^{N-1}v_{i_{0}+\delta_{0,k}m}\wedge v_{i_{1}+\delta
_{1,k}m}\wedge...\wedge v_{i_{k-1}+\delta_{k-1,k}m}\wedge v_{i_{k}%
+\delta_{k,k}m}\wedge v_{i_{k+1}+\delta_{k+1,k}m}\wedge v_{i_{k+2}%
+\delta_{k+2,k}m}\wedge...\\
&  =\sum\limits_{k=0}^{N-1}v_{i_{0}+\delta_{0,k}m}\wedge v_{i_{1}+\delta
_{1,k}m}\wedge v_{i_{2}+\delta_{2,k}m}\wedge....
\end{align*}
Combined with (\ref{pf.schur.step3.2}), this yields%
\[
a_{-m}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=\sum\limits_{k=0}^{N-1}v_{i_{0}+\delta_{0,k}m}\wedge v_{i_{1}+\delta_{1,k}%
m}\wedge v_{i_{2}+\delta_{2,k}m}\wedge...,
\]
so that%
\begin{align}
&  \operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  a_{-m}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  \right)
\nonumber\\
&  =\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  \sum\limits_{k=0}%
^{N-1}v_{i_{0}+\delta_{0,k}m}\wedge v_{i_{1}+\delta_{1,k}m}\wedge
v_{i_{2}+\delta_{2,k}m}\wedge...\right)  \right) \nonumber\\
&  =\sum\limits_{k=0}^{N-1}\underbrace{\operatorname*{PSE}\nolimits_{N}\left(
\tau\left(  v_{i_{0}+\delta_{0,k}m}\wedge v_{i_{1}+\delta_{1,k}m}\wedge
v_{i_{2}+\delta_{2,k}m}\wedge...\right)  \right)  }_{\substack{=\dfrac
{\det\left(  \left(  y_{i}^{i_{j-1}+\delta_{j-1,k}m+N-1}\right)  _{1\leq i\leq
N,\ 1\leq j\leq N}\right)  }{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq
i\leq N,\ 1\leq j\leq N}\right)  }\\\text{(by (\ref{pf.schur.step2}), applied
to }\left(  i_{0}+\delta_{0,k}m,i_{1}+\delta_{1,k}m,i_{2}+\delta
_{2,k}m,...\right)  \\\text{instead of }\left(  i_{0},i_{1},i_{2},...\right)
\text{ (since }\left(  i_{0}+\delta_{0,k}m,i_{1}+\delta_{1,k}m,i_{2}%
+\delta_{2,k}m,...\right)  \\\text{is easily seen to be an }N\text{-finished
straying }0\text{-degression))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{PSE}\nolimits_{N}%
\text{ and }\tau\text{ are both linear}\right) \nonumber\\
&  =\sum\limits_{k=0}^{N-1}\dfrac{\det\left(  \left(  y_{i}^{i_{j-1}%
+\delta_{j-1,k}m+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  }%
{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }\nonumber\\
&  =\sum\limits_{k=1}^{N}\dfrac{\det\left(  \left(  y_{i}^{i_{j-1}%
+\delta_{j-1,k-1}m+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)
}{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }k-1\text{ for
}k\text{ in the sum}\right) \nonumber\\
&  =\sum\limits_{k=1}^{N}\dfrac{\det\left(  \left(  y_{i}^{i_{j-1}%
+\delta_{j,k}m+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  }%
{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\delta_{j-1,k-1}%
=\delta_{j,k}\text{ for all }j\text{ and }k\right) \nonumber\\
&  =\dfrac{1}{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq
j\leq N}\right)  }\underbrace{\sum\limits_{k=1}^{N}\det\left(  \left(
y_{i}^{i_{j-1}+\delta_{j,k}m+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }_{\substack{=\left(  y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}\right)
\det\left(  \left(  y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  \\\text{(by Corollary \ref{cor.schur.det})}}}\nonumber\\
&  =\dfrac{1}{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq
j\leq N}\right)  }\left(  y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}\right)
\det\left(  \left(  y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right) \nonumber\\
&  =\left(  y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}\right)  \cdot\dfrac{\det\left(
\left(  y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)
}{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }. \label{pf.schur.step3.8}%
\end{align}
On the other hand, since $\left(  i_{0},i_{1},i_{2},...\right)  $ is strictly
decreasing, $\left(  i_{0},i_{1},i_{2},...\right)  $ is $N$-finished. Thus,
(\ref{pf.schur.step2}) yields%
\begin{equation}
\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  \right)  =\dfrac{\det\left(  \left(
y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq N}\right)  }%
{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }. \label{pf.schur.step3.9}%
\end{equation}
Now, (\ref{pf.schur.step3.8}) becomes%
\begin{align*}
&  \operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  a_{-m}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  \right) \\
&  =\underbrace{\left(  y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}\right)
}_{\substack{=m\operatorname*{PSE}\nolimits_{N}\left(  x_{m}\right)
\\\text{(since the definition of }\operatorname*{PSE}\nolimits_{N}\text{
yields}\\\operatorname*{PSE}\nolimits_{N}\left(  x_{m}\right)  =\dfrac
{y_{1}^{m}+y_{2}^{m}+...+y_{N}^{m}}{m}\text{)}}}\cdot\underbrace{\dfrac
{\det\left(  \left(  y_{i}^{i_{j-1}+N-1}\right)  _{1\leq i\leq N,\ 1\leq j\leq
N}\right)  }{\det\left(  \left(  y_{i}^{j-1}\right)  _{1\leq i\leq N,\ 1\leq
j\leq N}\right)  }}_{\substack{=\operatorname*{PSE}\nolimits_{N}\left(
\tau\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
\right)  \\\text{(by (\ref{pf.schur.step3.9}))}}}\\
&  =m\operatorname*{PSE}\nolimits_{N}\left(  x_{m}\right)  \cdot
\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  \right) \\
&  =\operatorname*{PSE}\nolimits_{N}\underbrace{\left(  mx_{m}\cdot\tau\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)
}_{\substack{=a_{-m}\left(  \tau\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  \right)  \\\text{(since }a_{-m}\text{ acts on
}\widetilde{F}\text{ as multiplication by }mx_{m}\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{PSE}\nolimits_{N}%
\text{ is a }\mathbb{C}\text{-algebra homomorphism}\right) \\
&  =\operatorname*{PSE}\nolimits_{N}\left(  a_{-m}\left(  \tau\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  \right)  .
\end{align*}
Now forget that we fixed $N$. We thus have shown that every integer $N\geq
\ell+m$ satisfies%
\[
\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  a_{-m}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  \right)
=\operatorname*{PSE}\nolimits_{N}\left(  a_{-m}\left(  \tau\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  \right)  .
\]
Hence,%
\[
\operatorname*{PSE}\nolimits_{N}\left(  \tau\left(  a_{-m}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  \right)
=\operatorname*{PSE}\nolimits_{N}\left(  a_{-m}\left(  \tau\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  \right)
\]
for every sufficiently high $N\in\mathbb{N}$. Thus, Corollary
\ref{cor.schur.PSEinj} (applied to $P=\tau\left(  a_{-m}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  $ and
$Q=a_{-m}\left(  \tau\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \right)  $) yields that
\[
\tau\left(  a_{-m}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  \right)  =a_{-m}\left(  \tau\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  \right)  .
\]
In other words,%
\[
\left(  \tau\circ a_{-m}\right)  \left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  =\left(  a_{-m}\circ\tau\right)  \left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  .
\]
Now forget that we fixed $\left(  i_{0},i_{1},i_{2},...\right)  $. We have
thus shown that $\left(  \tau\circ a_{-m}\right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =\left(  a_{-m}\circ\tau\right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $ for every
$0$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $. Hence, the maps
$\tau\circ a_{-m}$ and $a_{-m}\circ\tau$ are equal to each other on a basis of
$\wedge^{\dfrac{\infty}{2},0}V$ (namely, on the basis $\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  _{\left(  i_{0},i_{1}%
,i_{2},...\right)  \text{ is a }0\text{-degression}}$). Since these two maps
are linear, this yields that these two maps must be identical, i. e., we have
$\tau\circ a_{-m}=a_{-m}\circ\tau$. This proves (\ref{pf.schur.step3}). The
proof of the 3rd step is thus complete.

\textit{4th step:} We can now easily conclude Theorem \ref{thm.schur}.

Let $\mathcal{A}_{-}$ be the Lie subalgebra $\left\langle a_{-1},a_{-2}%
,a_{-3},...\right\rangle $ of $\mathcal{A}$. Then, $\tau$ is an $\mathcal{A}%
_{-}$-module homomorphism $\wedge^{\dfrac{\infty}{2},0}V\rightarrow
\widetilde{F}$ (according to (\ref{pf.schur.step3})).

Consider the element $\psi_{0}=v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...$ of
$\wedge^{\dfrac{\infty}{2},0}V=\mathcal{F}^{\left(  0\right)  }$. By the
definition of $\sigma_{0}$, we have $\sigma_{0}\left(  1\right)  =\psi_{0}$,
so that $\sigma_{0}^{-1}\left(  \psi_{0}\right)  =1$. Compared with%
\begin{align*}
\tau\left(  \psi_{0}\right)   &  =\tau\left(  v_{0}\wedge v_{-1}\wedge
v_{-2}\wedge...\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\psi
_{0}=v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right) \\
&  =1,
\end{align*}
this yields $\tau\left(  \psi_{0}\right)  =\sigma_{0}^{-1}\left(  \psi
_{0}\right)  $.

From Lemma \ref{lem.F.P1=P}, it is clear that the Fock module $F$ is generated
by $1$ as an $\mathcal{A}_{-}$-module (since $\mathcal{A}_{-}=\left\langle
a_{-1},a_{-2},a_{-3},...\right\rangle $). Since there exists an $\mathcal{A}%
_{-}$-module isomorphism $F\rightarrow\widetilde{F}$ which sends $1$ to $1$
(in fact, the map $\operatorname*{resc}$ of Proposition \ref{prop.resc} is
such an isomorphism), this yields that $\widetilde{F}$ is generated by $1$ as
an $\mathcal{A}_{-}$-module. Since there exists an $\mathcal{A}_{-}$-module
isomorphism $\widetilde{F}\rightarrow\wedge^{\dfrac{\infty}{2},0}V$ which
sends $1$ to $\psi_{0}$ (in fact, the map $\sigma_{0}$ is such an isomorphism,
since $\sigma_{0}\left(  1\right)  =\psi_{0}$), this yields that
$\wedge^{\dfrac{\infty}{2},0}V$ is generated by $\psi_{0}$ as an
$\mathcal{A}_{-}$-module. Hence, if two $\mathcal{A}_{-}$-module homomorphisms
from $\wedge^{\dfrac{\infty}{2},0}V$ to another $\mathcal{A}_{-}$-module are
equal to each other on $\psi_{0}$, then they must be identical. We can apply
this observation to the two $\mathcal{A}_{-}$-module homomorphisms
$\tau:\wedge^{\dfrac{\infty}{2},0}V\rightarrow\widetilde{F}$ and $\sigma
_{0}^{-1}:\wedge^{\dfrac{\infty}{2},0}V\rightarrow\widetilde{F}$ (which are
equal to each other on $\psi_{0}$, since $\tau\left(  \psi_{0}\right)
=\sigma_{0}^{-1}\left(  \psi_{0}\right)  $), and conclude that these
homomorphisms are identical, i. e., we have $\tau=\sigma_{0}^{-1}$. Now, every
$0$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $ satisfies%
\begin{align*}
\sigma^{-1}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
&  =\underbrace{\sigma_{0}^{-1}}_{=\tau}\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  =\tau\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right) \\
&  =S_{\left(  i_{0}+0,i_{1}+1,i_{2}+2,...\right)  }\left(  x\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\tau\right) \\
&  =S_{\lambda}\left(  x\right)  ,
\end{align*}
where $\lambda=\left(  i_{0}+0,i_{1}+1,i_{2}+2,...\right)  $. This proves
Theorem \ref{thm.schur}.

\subsection{\label{subsect.schur.pf2}Expliciting
\texorpdfstring{$\sigma^{-1}$}{the inverse of the Boson-Fermion
correspondence} using Schur polynomials: second proof}

We are next going to give a second proof of Theorem \ref{thm.schur}. We will
give this proof in two versions: The first version (Subsection
\ref{subsubsect.schur2}) will proceed by manipulations with infinite matrices,
using various properties of infinite matrices acting on $\wedge^{\dfrac
{\infty}{2},m}V$. Since we are not going to prove all these properties, this
first version is not completely self-contained (although the missing proofs
are easy to fill in). The second version (Subsection
\ref{subsubsect.schur2.finitary}) will be a rewriting of the first version
without the use of all these properties of infinite matrices; it is
self-contained. Both versions of the proof require lengthy preparations, some
of which (like the definition of $\operatorname*{GL}\left(  \infty\right)  $)
will also turn out useful to us later.

\subsubsection{\label{subsubsect.newton}The multivariate Taylor formula}

Before we step to the second proof of Theorem \ref{thm.schur}, we show a lemma
about polynomials over $\mathbb{Q}$-algebras:

\begin{lemma}
\label{lem.hirota.newton}Let $K$ be a commutative $\mathbb{Q}$-algebra, let
$\left(  y_{1},y_{2},y_{3},...\right)  $ be a sequence of elements of $K$, and
let $\left(  z_{1},z_{2},z_{3},...\right)  $ be a sequence of new symbols.
Denote the sequence $\left(  y_{1},y_{2},y_{3},...\right)  $ by $y$. Denote
the sequence $\left(  z_{1},z_{2},z_{3},...\right)  $ by $z$. Then, every
$P\in K\left[  z_{1},z_{2},z_{3},...\right]  $ satisfies%
\[
\exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)
P\left(  z\right)  =P\left(  y+z\right)  .
\]
Here, $y+z$ means the componentwise sum of the sequences $y$ and $z$ (so that
$y+z=\left(  y_{1}+z_{1},y_{2}+z_{2},y_{3}+z_{3},...\right)  $).
\end{lemma}

Lemma \ref{lem.hirota.newton} is actually a multivariate generalization of the
famous Taylor formula%
\[
\exp\left(  \alpha\dfrac{\partial}{\partial\xi}\right)  P\left(  \xi\right)
=P\left(  \alpha+\xi\right)
\]
which holds for any polynomial $P\in K\left[  \xi\right]  $ and any $\alpha\in
K$.

\textit{Proof of Lemma \ref{lem.hirota.newton}.} Let $A$ be the map%
\[
\exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)
:K\left[  z_{1},z_{2},z_{3},...\right]  \rightarrow K\left[  z_{1},z_{2}%
,z_{3},...\right]
\]
(this is easily seen to be well-defined). Let $B$ be the map%
\[
K\left[  z_{1},z_{2},z_{3},...\right]  \rightarrow K\left[  z_{1},z_{2}%
,z_{3},...\right]  ,\ \ \ \ \ \ \ \ \ \ P\mapsto P\left(  y+z\right)  .
\]


We have $A=\exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}%
}\right)  $, so that $A$ is the exponential of a derivation (since
$\sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}$ is a derivation).
Thus, $A$ is a $K$-algebra homomorphism (since there is a known fact that the
exponential of a derivation is a $K$-algebra homomorphism). Combined with the
fact that $B$ is a $K$-algebra homomorphism (in fact, $B$ is an evaluation
homomorphism), this yields that both $A$ and $B$ are $K$-algebra homomorphisms.

Now, let $k$ be a positive integer. We will prove that $Az_{k}=Bz_{k}$.

We have
\[
\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)
z_{k}=\sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}z_{k}%
=y_{k}\underbrace{\dfrac{\partial}{\partial z_{k}}z_{k}}_{=1}+\sum
\limits_{\substack{s>0;\\s\neq k}}y_{s}\underbrace{\dfrac{\partial}{\partial
z_{s}}z_{k}}_{\substack{=0\\\text{(since }s\neq k\text{)}}}=y_{k}%
+\underbrace{\sum\limits_{\substack{s>0;\\s\neq k}}y_{s}0}_{=0}=y_{k},
\]
so that%
\[
\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)
^{2}z_{k}=\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}%
}\right)  \underbrace{\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial
z_{s}}\right)  z_{k}}_{=y_{k}}=\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial
}{\partial z_{s}}\right)  y_{k}=\sum\limits_{s>0}y_{s}\underbrace{\dfrac
{\partial}{\partial z_{s}}y_{k}}_{=0}=0.
\]
As a consequence,
\begin{equation}
\text{every integer }i\geq2\text{ satisfies }\left(  \sum\limits_{s>0}%
y_{s}\dfrac{\partial}{\partial z_{s}}\right)  ^{i}z_{k}=0.
\label{pf.hirota.newton.1}%
\end{equation}
Now, since $A=\exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial
z_{s}}\right)  =\sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}\left(
\sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)  ^{i}$, we have%
\begin{align*}
Az_{k}  &  =\sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}\left(  \sum
\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)  ^{i}z_{k}\\
&  =\underbrace{\dfrac{1}{0!}}_{=1}\underbrace{\left(  \sum\limits_{s>0}%
y_{s}\dfrac{\partial}{\partial z_{s}}\right)  ^{0}}_{=\operatorname*{id}}%
z_{k}+\underbrace{\dfrac{1}{1!}}_{=1}\underbrace{\left(  \sum\limits_{s>0}%
y_{s}\dfrac{\partial}{\partial z_{s}}\right)  ^{1}}_{=\sum\limits_{s>0}%
y_{s}\dfrac{\partial}{\partial z_{s}}}z_{k}+\sum\limits_{i\geq2}\dfrac{1}%
{i!}\underbrace{\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}%
}\right)  ^{i}z_{k}}_{\substack{=0\\\text{(by (\ref{pf.hirota.newton.1}))}}}\\
&  =\underbrace{\operatorname*{id}z_{k}}_{=z_{k}}+\underbrace{\left(
\sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)  z_{k}}_{=y_{k}%
}+\underbrace{\sum\limits_{i\geq2}\dfrac{1}{i!}0}_{=0}=z_{k}+y_{k}=y_{k}%
+z_{k}.
\end{align*}
Compared to
\begin{align*}
Bz_{k}  &  =z_{k}\left(  y+z\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the
definition of }B\right) \\
&  =y_{k}+z_{k},
\end{align*}
this yields $Az_{k}=Bz_{k}$.

Now, forget that we fixed $k$. We thus have shown that $Az_{k}=Bz_{k}$ for
every positive integer $k$. In other words, the maps $A$ and $B$ coincide on
the set $\left\{  z_{1},z_{2},z_{3},...\right\}  $. Since the set $\left\{
z_{1},z_{2},z_{3},...\right\}  $ generates $K\left[  z_{1},z_{2}%
,z_{3},...\right]  $ as a $K$-algebra, this yields that the maps $A$ and $B$
coincide on a generating set of the $K$-algebra $K\left[  z_{1},z_{2}%
,z_{3},...\right]  $. Since $A$ and $B$ are $K$-algebra homomorphisms, this
yields that $A=B$ (because if two $K$-algebra homomorphisms coincide on a
$K$-algebra generating set of their domain, then they must be equal). Hence,
every $P\in K\left[  z_{1},z_{2},z_{3},...\right]  $ satisfies%
\[
\underbrace{\exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}%
}\right)  }_{=A=B}\underbrace{P\left(  z\right)  }_{=P}=BP=P\left(
y+z\right)
\]
(by the definition of $B$). This proves Lemma \ref{lem.hirota.newton}.

\subsubsection{\label{subsubsect.GLinfty}
\texorpdfstring{$\operatorname*{GL}\left( \infty\right)  $}{GL-infinity} and
\texorpdfstring{$\operatorname*{M}\left(  \infty\right)  $}{M-infinity}}

We now introduce the groups $\operatorname*{GL}\left(  \infty\right)  $ and
$\operatorname*{M}\left(  \infty\right)  $ and their actions on $\wedge
^{\dfrac{\infty}{2},m}V$. On the one hand, this will prepare us to the second
proof of Theorem \ref{thm.schur}; on the other hand, these group actions are
of autonomous interest, and we will meet them again in Subsection
\ref{subsubsect.infgrass}.

\begin{definition}
\label{def.Minf}We let $\operatorname*{M}\left(  \infty\right)  $ denote the
set $\operatorname*{id}+\mathfrak{gl}_{\infty}$. In other words, we let
$\operatorname*{M}\left(  \infty\right)  $ denote the set of all infinite
matrices (infinite in both directions) which are equal to the infinite
identity matrix $\operatorname*{id}$ in all but finitely many entries.
\end{definition}

Clearly, $\operatorname*{M}\left(  \infty\right)  \subseteq\overline
{\mathfrak{a}_{\infty}}$ as sets. We notice that:

\begin{proposition}
\label{prop.Minf.monoid}\textbf{(a)} For every $A\in\operatorname*{M}\left(
\infty\right)  $ and $B\in\operatorname*{M}\left(  \infty\right)  $, the
matrix $AB$ is well-defined and lies in $\operatorname*{M}\left(
\infty\right)  $.

\textbf{(b)} We have $\operatorname*{id}\in\operatorname*{M}\left(
\infty\right)  $ (where $\operatorname*{id}$ denotes the infinite identity matrix).

\textbf{(c)} The set $\operatorname*{M}\left(  \infty\right)  $ becomes a
monoid under multiplication of matrices.

\textbf{(d)} If a matrix $A\in\operatorname*{M}\left(  \infty\right)  $ is
invertible, then its inverse also lies in $\operatorname*{M}\left(
\infty\right)  $.

\textbf{(e)} Denote by $\operatorname*{GL}\left(  \infty\right)  $ the subset
$\left\{  A\in\operatorname*{M}\left(  \infty\right)  \ \mid\ A\text{ is
invertible}\right\}  $ of $\operatorname*{M}\left(  \infty\right)  $. Then,
$\operatorname*{GL}\left(  \infty\right)  $ becomes a group under
multiplication of matrices.
\end{proposition}

\begin{remark}
\label{rmk.GLinf}In Proposition \ref{prop.Minf.monoid}, a matrix
$A\in\operatorname*{M}\left(  \infty\right)  $ is said to be
\textit{invertible} if there exists an infinite matrix $B$ (with rows and
columns indexed by integers) satisfying $AB=BA=\operatorname*{id}$. The matrix
$B$ is then called the \textit{inverse} of $A$. Note that we don't a-priori
require that $B$ lie in $\operatorname*{M}\left(  \infty\right)  $, or any
other ``finiteness conditions'' for $B$; Proposition \ref{prop.Minf.monoid}
\textbf{(d)} shows that these conditions are automatically satisfied.
\end{remark}

\begin{definition}
\label{def.GLinf}Let $\operatorname*{GL}\left(  \infty\right)  $ denote the
group $\operatorname*{GL}\left(  \infty\right)  $ defined in Proposition
\ref{prop.Minf.monoid} \textbf{(e)}.
\end{definition}

\textit{Proof of Proposition \ref{prop.Minf.monoid}.} \textbf{(a)} Let
$A\in\operatorname*{M}\left(  \infty\right)  $ and $B\in\operatorname*{M}%
\left(  \infty\right)  $. Since $A\in\operatorname*{M}\left(  \infty\right)
=\operatorname*{id}+\mathfrak{gl}_{\infty}$, there exists an $a\in
\mathfrak{gl}_{\infty}$ such that $A=\operatorname*{id}+a$. Consider this $a$.

Since $B\in\operatorname*{M}\left(  \infty\right)  =\operatorname*{id}%
+\mathfrak{gl}_{\infty}$, there exists a $b\in\mathfrak{gl}_{\infty}$ such
that $B=\operatorname*{id}+b$. Consider this $b$.

Since $A=\operatorname*{id}+a$ and $B=\operatorname*{id}+b$, we have
$AB=\left(  \operatorname*{id}+a\right)  \left(  \operatorname*{id}+b\right)
=\operatorname*{id}+a+b+ab$, which is clearly well-defined (because
$a\in\mathfrak{gl}_{\infty}$ and $b\in\mathfrak{gl}_{\infty}$ lead to $ab$
being well-defined) and lies in $\operatorname*{M}\left(  \infty\right)  $
(since $\underbrace{a}_{\in\mathfrak{gl}_{\infty}}+\underbrace{b}%
_{\in\mathfrak{gl}_{\infty}}+\underbrace{ab}_{\substack{\in\mathfrak{gl}%
_{\infty}\\\text{(since }a\in\mathfrak{gl}_{\infty}\text{ and }b\in
\mathfrak{gl}_{\infty}\text{)}}}\in\mathfrak{gl}_{\infty}+\mathfrak{gl}%
_{\infty}+\mathfrak{gl}_{\infty}\subseteq\mathfrak{gl}_{\infty}$ and thus
$\operatorname*{id}+a+b+ab\in\operatorname*{id}+\mathfrak{gl}_{\infty
}=\operatorname*{M}\left(  \infty\right)  $). This proves Proposition
\ref{prop.Minf.monoid} \textbf{(a)}.

\textbf{(b)} Trivial.

\textbf{(c)} Follows from \textbf{(a)} and \textbf{(b)}.

\textbf{(d)} Let $A\in\operatorname*{M}\left(  \infty\right)  $ be invertible.

Since $A\in\operatorname*{M}\left(  \infty\right)  =\operatorname*{id}%
+\mathfrak{gl}_{\infty}$, there exists an $a\in\mathfrak{gl}_{\infty}$ such
that $A=\operatorname*{id}+a$. Consider this $a$.

Since $A$ is invertible, there exists an infinite matrix $B$ (with rows and
columns indexed by integers) satisfying $AB=BA=\operatorname*{id}$ (according
to how we defined ``invertible'' in Remark \ref{rmk.GLinf}). Consider this
$B$. This $B$ is the inverse of $A$. Let $b=B-\operatorname*{id}$. Then,
$B=\operatorname*{id}+b$. Since $A=\operatorname*{id}+a$ and
$B=\operatorname*{id}+b$, we have $AB=\left(  \operatorname*{id}+a\right)
\left(  \operatorname*{id}+b\right)  =\operatorname*{id}+a+b+ab$, which is
clearly well-defined (because $a\in\mathfrak{gl}_{\infty}$ leads to $ab$ being
well-defined). Since $\operatorname*{id}=AB=\operatorname*{id}+ab+a+b$, we
have $0=ab+a+b$.

Let us introduce two notations that we will use during this proof:

\begin{itemize}
\item For any infinite matrix $M$ and any pair $\left(  i,j\right)  $ of
integers, let us denote by $M_{i,j}$ the $\left(  i,j\right)  $-th entry of
the matrix $M$. (In particular, for any pair $\left(  i,j\right)  $ of
integers, we denote by $a_{i,j}$ the $\left(  i,j\right)  $-th entry of the
matrix $a$ (not of the matrix $A$ !), and we denote by $b_{i,j}$ the $\left(
i,j\right)  $-th entry of the matrix $b$ (not of the matrix $B$ !).)

\item For any assertion $\mathcal{A}$, let $\left[  \mathcal{A}\right]  $
denote the integer $\left\{
\begin{array}
[c]{l}%
1,\text{ if }\mathcal{A}\text{ is true;}\\
0,\text{ if }\mathcal{A}\text{ is wrong}%
\end{array}
\right.  $.
\end{itemize}

Since $a\in\mathfrak{gl}_{\infty}$, only finitely many entries of the matrix
$a$ are nonzero. In particular, this yields that only finitely many columns of
the matrix $a$ are nonzero. Hence, there exists a nonnegative integer $N$ such
that
\begin{equation}
\left(  \text{for every integer }j\text{ with }\left\vert j\right\vert
>N\text{, the }j\text{-th column of }a\text{ is zero}\right)  .
\label{pf.Minf.monoid.1}%
\end{equation}
Consider this $N$. Clearly,%
\begin{equation}
\left(  \text{for every }\left(  i,j\right)  \in\mathbb{Z}^{2}\text{ such that
}\left\vert j\right\vert >N\text{, we have }a_{i,j}=0\right)
\label{pf.Minf.monoid.1a}%
\end{equation}
(because for every $\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that
$\left\vert j\right\vert >N$, the $j$-th column of $a$ is zero (by
(\ref{pf.Minf.monoid.1})), so that every entry on the $j$-th column of $a$ is
zero, so that $a_{i,j}$ is zero (because the element $a_{i,j}$ is the $\left(
i,j\right)  $-th entry of $a$, hence an entry on the $j$-th column of $a$)).

Recall that only finitely many entries of the matrix $a$ are nonzero. In
particular, this yields that only finitely many rows of the matrix $a$ are
nonzero. Hence, there exists a nonnegative integer $M$ such that
\begin{equation}
\left(  \text{for every integer }i\text{ with }\left\vert i\right\vert
>M\text{, the }i\text{-th row of }a\text{ is zero}\right)  .
\label{pf.Minf.monoid.2}%
\end{equation}
Consider this $M$. Clearly,%
\begin{equation}
\left(  \text{for every }\left(  i,j\right)  \in\mathbb{Z}^{2}\text{ such that
}\left\vert i\right\vert >M\text{, we have }a_{i,j}=0\right)
\label{pf.Minf.monoid.2a}%
\end{equation}
(because for every $\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that
$\left\vert i\right\vert >M$, the $i$-th row of $a$ is zero (by
(\ref{pf.Minf.monoid.2})), so that every entry on the $i$-th row of $a$ is
zero, so that $a_{i,j}$ is zero (because the element $a_{i,j}$ is the $\left(
i,j\right)  $-th entry of $a$, hence an entry on the $i$-th row of $a$)).

Let $P=\max\left\{  M,N\right\}  $. Clearly, $P\geq M$ and $P\geq N$. It is
now easy to see that
\begin{equation}
\text{any }\left(  i,j\right)  \in\mathbb{Z}^{2}\text{ satisfies }%
a_{i,j}=\left[  \left\vert i\right\vert \leq P\right]  \cdot a_{i,j}.
\label{pf.Minf.monoid.a}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.Minf.monoid.a}):} Let $\left(  i,j\right)
\in\mathbb{Z}^{2}$. Then, we must be in one of the following three cases:
\par
\textit{Case 1:} We don't have $\left\vert i\right\vert \leq P$.
\par
\textit{Case 2:} We have $\left\vert i\right\vert \leq P$.
\par
Let us consider Case 1 first. In this case, we don't have $\left\vert
i\right\vert \leq P$. Thus, $\left[  \left\vert i\right\vert \leq P\right]
=0$ and $\left\vert i\right\vert >P$. From $\left\vert i\right\vert >P\geq M$,
we conclude that $a_{i,j}=0$ (by (\ref{pf.Minf.monoid.2a})). Compared with
$\underbrace{\left[  \left\vert i\right\vert \leq P\right]  }_{=0}\cdot
a_{i,j}=0$, this yields $a_{i,j}=\left[  \left\vert i\right\vert \leq
P\right]  \cdot a_{i,j}$. Hence, (\ref{pf.Minf.monoid.a}) is proven in Case 1.
\par
Finally, let us consider Case 2. In this case, we have $\left\vert
i\right\vert \leq P$. Hence, $\left[  \left\vert i\right\vert \leq P\right]
=1$. Thus, $\underbrace{\left[  \left\vert i\right\vert \leq P\right]  }%
_{=1}\cdot a_{i,j}=a_{i,j}$. Hence, (\ref{pf.Minf.monoid.a}) is proven in Case
2.
\par
Altogether, we have thus proven (\ref{pf.Minf.monoid.a}) in each of the two
cases 1 and 2. Since these two cases cover all possibilities, this shows that
(\ref{pf.Minf.monoid.a}) always holds. Thus, (\ref{pf.Minf.monoid.a}) is
proven.} Similarly,%
\begin{equation}
\text{any }\left(  i,j\right)  \in\mathbb{Z}^{2}\text{ satisfies }%
a_{i,j}=\left[  \left\vert j\right\vert \leq P\right]  \cdot a_{i,j}.
\label{pf.Minf.monoid.a'}%
\end{equation}


Let $b^{\prime}$ be the infinite matrix (with rows and columns indexed by
integers) defined by%
\begin{equation}
\left(  b_{i,j}^{\prime}=\left[  \left\vert i\right\vert \leq P\right]
\cdot\left[  \left\vert j\right\vert \leq P\right]  \cdot b_{i,j}%
\ \ \ \ \ \ \ \ \ \ \text{for all }\left(  i,j\right)  \in\mathbb{Z}%
^{2}\right)  . \label{pf.Minf.monoid.bprime}%
\end{equation}
It is clear that only finitely many entries of $b^{\prime}$ are
nonzero\footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\mathbb{Z}^{2}$
such that $b_{i,j}^{\prime}\neq0$. Then, $\left\vert i\right\vert \leq P$
(because otherwise, we would have $\left[  \left\vert i\right\vert \leq
P\right]  =0$, so that $b_{i,j}^{\prime}=\underbrace{\left[  \left\vert
i\right\vert \leq P\right]  }_{=0}\cdot\left[  \left\vert j\right\vert \leq
P\right]  \cdot b_{i,j}=0$, contradicting to $b_{i,j}^{\prime}\neq0$), so that
$i\in\left\{  -P,-P+1,...,P\right\}  $, and similarly $j\in\left\{
-P,-P+1,...,P\right\}  $. Hence, $\left(  i,j\right)  \in\left\{
-P,-P+1,...,P\right\}  ^{2}$ (since $i\in\left\{  -P,-P+1,...,P\right\}  $ and
$j\in\left\{  -P,-P+1,...,P\right\}  $).
\par
Now forget that we fixed $\left(  i,j\right)  $. We thus have showed that
every $\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that $b_{i,j}^{\prime}%
\neq0$ satisfies $\left(  i,j\right)  \in\left\{  -P,-P+1,...,P\right\}  ^{2}%
$. Since there are only finitely many $\left(  i,j\right)  \in\left\{
-P,-P+1,...,P\right\}  ^{2}$, this yields that there are only finitely many
$\left(  i,j\right)  \in\mathbb{Z}^{2}$ such that $b_{i,j}^{\prime}\neq0$. In
other words, there are only finitely many $\left(  i,j\right)  \in
\mathbb{Z}^{2}$ such that the $\left(  i,j\right)  $-th entry of $b^{\prime}$
is nonzero. In other words, only finitely many entries of $b^{\prime}$ are
nonzero, qed.}. In other words, $b^{\prime}\in\mathfrak{gl}_{\infty}$, so that
$\operatorname*{id}+b^{\prime}\in\operatorname*{id}+\mathfrak{gl}_{\infty
}=\operatorname*{M}\left(  \infty\right)  $.

We will now prove that $A\left(  \operatorname*{id}+b^{\prime}\right)
=\operatorname*{id}$.

For every $\left(  i,j\right)  \in\mathbb{Z}^{2}$, we have%
\begin{align*}
&  \left(  ab^{\prime}+a+b^{\prime}\right)  _{i,j}\\
&  =\underbrace{\left(  ab^{\prime}\right)  _{i,j}}_{\substack{=\sum
\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}^{\prime}\\\text{(by the definition of
the}\\\text{product of two matrices)}}}+a_{i,j}+\underbrace{b_{i,j}^{\prime}%
}_{\substack{=\left[  \left\vert i\right\vert \leq P\right]  \cdot\left[
\left\vert j\right\vert \leq P\right]  \cdot b_{i,j}\\\text{(by
(\ref{pf.Minf.monoid.bprime}))}}}\\
&  =\sum\limits_{k\in\mathbb{Z}}a_{i,k}\underbrace{b_{k,j}^{\prime}%
}_{\substack{=\left[  \left\vert k\right\vert \leq P\right]  \cdot\left[
\left\vert j\right\vert \leq P\right]  \cdot b_{k,j}\\\text{(by
(\ref{pf.Minf.monoid.bprime}), applied to}\\k\text{ instead of }i\text{)}%
}}+a_{i,j}+\left[  \left\vert i\right\vert \leq P\right]  \cdot\left[
\left\vert j\right\vert \leq P\right]  \cdot b_{i,j}\\
&  =\sum\limits_{k\in\mathbb{Z}}\underbrace{a_{i,k}\left[  \left\vert
k\right\vert \leq P\right]  }_{\substack{=\left[  \left\vert k\right\vert \leq
P\right]  \cdot a_{i,k}=a_{i,k}\\\text{(since (\ref{pf.Minf.monoid.a'})
(applied to}\\k\text{ instead of }j\text{) yields }a_{i,k}=\left[  \left\vert
k\right\vert \leq P\right]  \cdot a_{i,k}\text{)}}}\cdot\left[  \left\vert
j\right\vert \leq P\right]  \cdot b_{k,j}+\underbrace{a_{i,j}}%
_{\substack{=\left[  \left\vert i\right\vert \leq P\right]  \cdot
a_{i,j}\\\text{(by (\ref{pf.Minf.monoid.a}))}}}+\left[  \left\vert
i\right\vert \leq P\right]  \cdot\left[  \left\vert j\right\vert \leq
P\right]  \cdot b_{i,j}\\
&  =\sum\limits_{k\in\mathbb{Z}}\underbrace{a_{i,k}}_{\substack{=\left[
\left\vert i\right\vert \leq P\right]  \cdot a_{i,k}\\\text{(by
(\ref{pf.Minf.monoid.a}), applied to}\\k\text{ instead of }j\text{)}}%
}\cdot\left[  \left\vert j\right\vert \leq P\right]  \cdot b_{k,j}+\left[
\left\vert i\right\vert \leq P\right]  \cdot\underbrace{a_{i,j}}%
_{\substack{=\left[  \left\vert j\right\vert \leq P\right]  \cdot
a_{i,j}\\\text{(by (\ref{pf.Minf.monoid.a'}))}}}+\left[  \left\vert
i\right\vert \leq P\right]  \cdot\left[  \left\vert j\right\vert \leq
P\right]  \cdot b_{i,j}\\
&  =\sum\limits_{k\in\mathbb{Z}}\left[  \left\vert i\right\vert \leq P\right]
\cdot a_{i,k}\cdot\left[  \left\vert j\right\vert \leq P\right]  \cdot
b_{k,j}+\left[  \left\vert i\right\vert \leq P\right]  \cdot\left[  \left\vert
j\right\vert \leq P\right]  \cdot a_{i,j}+\left[  \left\vert i\right\vert \leq
P\right]  \cdot\left[  \left\vert j\right\vert \leq P\right]  \cdot b_{i,j}\\
&  =\left[  \left\vert i\right\vert \leq P\right]  \cdot\left[  \left\vert
j\right\vert \leq P\right]  \cdot\left(  \sum\limits_{k\in\mathbb{Z}}%
a_{i,k}b_{k,j}+a_{i,j}+b_{i,j}\right)  =\left[  \left\vert i\right\vert \leq
P\right]  \cdot\left[  \left\vert j\right\vert \leq P\right]  \cdot
\underbrace{\left(  \left(  ab\right)  _{i,j}+a_{i,j}+b_{i,j}\right)
}_{\substack{=\left(  ab+a+b\right)  _{i,j}=0\\\text{(since }ab+a+b=0\text{)}%
}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\left(  ab\right)  _{i,j}=\sum\limits_{k\in\mathbb{Z}}%
a_{i,k}b_{k,j}\text{ (by the definition of the product of two matrices),}\\
\text{so that }\sum\limits_{k\in\mathbb{Z}}a_{i,k}b_{k,j}=\left(  ab\right)
_{i,j}%
\end{array}
\right) \\
&  =0.
\end{align*}
Thus, $ab^{\prime}+a+b^{\prime}=0$. Since $A=\operatorname*{id}+a$, we have
$A\left(  \operatorname*{id}+b^{\prime}\right)  =\left(  \operatorname*{id}%
+a\right)  \left(  \operatorname*{id}+b^{\prime}\right)  =\operatorname*{id}%
+\underbrace{ab^{\prime}+a+b^{\prime}}_{=0}=\operatorname*{id}$.

We thus have shown that $A\left(  \operatorname*{id}+b^{\prime}\right)
=\operatorname*{id}$.

Now, it is easy to see that the products $B\left(  A\left(  \operatorname*{id}%
+b^{\prime}\right)  \right)  $ and $\left(  BA\right)  \left(
\operatorname*{id}+b^{\prime}\right)  $ are well-defined and satisfy
associativity, i. e., we have $B\left(  A\left(  \operatorname*{id}+b^{\prime
}\right)  \right)  =\left(  BA\right)  \left(  \operatorname*{id}+b^{\prime
}\right)  $. Now,%
\[
B=B\cdot\underbrace{\operatorname*{id}}_{=A\left(  \operatorname*{id}%
+b^{\prime}\right)  }=B\left(  A\left(  \operatorname*{id}+b^{\prime}\right)
\right)  =\underbrace{\left(  BA\right)  }_{=\operatorname*{id}}\left(
\operatorname*{id}+b^{\prime}\right)  =\operatorname*{id}+b^{\prime}%
\in\operatorname*{M}\left(  \infty\right)  .
\]
Since $B$ is the inverse of $A$, this yields that the inverse of $A$ lies in
$\operatorname*{M}\left(  \infty\right)  $. This proves Proposition
\ref{prop.Minf.monoid} \textbf{(d)}.

\textbf{(e)} Follows from \textbf{(c)} and \textbf{(d)}.

The proof of Proposition \ref{prop.Minf.monoid} is complete.

We now construct a group action of $\operatorname*{GL}\left(  \infty\right)  $
on $\mathcal{F}^{\left(  m\right)  }$ that is related to the Lie algebra
action $\rho$ of $\mathfrak{gl}_{\infty}$ on $\mathcal{F}^{\left(  m\right)
}$ in the same way as the action of a Lie group on a representation is usually
related to its ``derivative'' action of the corresponding Lie algebra:

\begin{definition}
\label{def.GLinf.act}Let $m\in\mathbb{Z}$. We define an action $\varrho
:\operatorname*{M}\left(  \infty\right)  \rightarrow\operatorname*{End}\left(
\mathcal{F}^{\left(  m\right)  }\right)  $ of the monoid $\operatorname*{M}%
\left(  \infty\right)  $ on the vector space $\mathcal{F}^{\left(  m\right)
}=\wedge^{\dfrac{\infty}{2},m}V$ as follows: For every $A\in\operatorname*{M}%
\left(  \infty\right)  $ and every $m$-degression $\left(  i_{0},i_{1}%
,i_{2},...\right)  $, we set%
\[
\left(  \varrho\left(  A\right)  \right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  =Av_{i_{0}}\wedge Av_{i_{1}}\wedge
Av_{i_{2}}\wedge....
\]
(This is then extended to the whole $\mathcal{F}^{\left(  m\right)  }$ by
linearity.) It is very easy to see that this is well-defined (because
$Av_{k}=v_{k}$ for all sufficiently small $k$) and indeed gives a monoid action.

The restriction $\varrho\mid_{\operatorname*{GL}\left(  \infty\right)
}:\operatorname*{GL}\left(  \infty\right)  \rightarrow\operatorname*{End}%
\left(  \mathcal{F}^{\left(  m\right)  }\right)  $ to $\operatorname*{GL}%
\left(  \infty\right)  $ is thus a group action of $\operatorname*{GL}\left(
\infty\right)  $ on $\mathcal{F}^{\left(  m\right)  }$.

Since we have defined an action of $\operatorname*{M}\left(  \infty\right)  $
on $\mathcal{F}^{\left(  m\right)  }$ for every $m\in\mathbb{Z}$, we thus
obtain an action of $\operatorname*{M}\left(  \infty\right)  $ on
$\mathcal{F}=\bigoplus\limits_{m\in\mathbb{Z}}\mathcal{F}^{\left(  m\right)
}$ (namely, the direct sum of the previous actions). This latter action will
also be denoted by $\varrho$.
\end{definition}

Note that the letter $\varrho$ is a capital rho, as opposed to $\rho$ which is
the lowercase rho.

When $A$ is a matrix in $\operatorname*{M}\left(  \infty\right)  $, the
endomorphism $\varrho\left(  A\right)  $ of $\mathcal{F}^{\left(  m\right)  }$
can be seen as an infinite analogue of the endomorphisms $\wedge^{\ell}A$ of
$\wedge^{\ell}V$ defined for all $\ell\in\mathbb{N}$.

We are next going to give an explicit formula for the action of $\varrho
\left(  A\right)  $ on $\mathcal{F}^{\left(  m\right)  }$ in terms of
(infinite) minors of $A$. The formula will be an infinite analogue of the
following well-known formula:

\begin{proposition}
\label{prop.GLinf.det.fin}Let $P$ be a finite-dimensional $\mathbb{C}$-vector
space with basis $\left(  e_{1},e_{2},...,e_{n}\right)  $, and let $Q$ be a
finite-dimensional $\mathbb{C}$-vector space with basis $\left(  f_{1}%
,f_{2},...,f_{m}\right)  $. Let $\ell\in\mathbb{N}$.

Let $f:P\rightarrow Q$ be a linear map, and let $A$ be the $m\times n$-matrix
which represents this map $f$ with respect to the bases $\left(  e_{1}%
,e_{2},...,e_{n}\right)  $ and $\left(  f_{1},f_{2},...,f_{m}\right)  $ of $P$
and $Q$.

Let $i_{1}$, $i_{2}$, $...$, $i_{\ell}$ be integers such that $1\leq
i_{1}<i_{2}<...<i_{\ell}\leq n$. For any $\ell$ integers $j_{1}$, $j_{2}$,
$...$, $j_{\ell}$ satisfying $1\leq j_{1}<j_{2}<...<j_{\ell}\leq m$, let
$A_{j_{1},j_{2},...,j_{\ell}}^{i_{1},i_{2},...,i_{\ell}}$ denote the matrix
which is obtained from $A$ by removing all columns except for the $i_{1}$-th,
the $i_{2}$-th, $...$, the $i_{\ell}$-th ones and removing all rows except for
the $j_{1}$-th, the $j_{2}$-th, $...$, the $j_{\ell}$-th ones. Then,%
\[
\left(  \wedge^{\ell}f\right)  \left(  e_{i_{1}}\wedge e_{i_{2}}%
\wedge...\wedge e_{i_{\ell}}\right)  =\sum\limits_{\substack{j_{1}\text{,
}j_{2}\text{, }...\text{, }j_{\ell}\text{ are }\ell\text{ integers;}\\1\leq
j_{1}<j_{2}<...<j_{\ell}\leq m}}\det\left(  A_{j_{1},j_{2},...,j_{\ell}%
}^{i_{1},i_{2},...,i_{\ell}}\right)  e_{j_{1}}\wedge e_{j_{2}}\wedge...\wedge
e_{j_{\ell}}.
\]

\end{proposition}

Note that Proposition \ref{prop.GLinf.det.fin} is the main link between
exterior powers and minors of matrices. It is commonly used both to prove
results involving exterior powers and to give slick proofs of identities
involving minors.

In order to obtain an infinite analogue of this result, we need to first
define determinants of infinite matrices. This cannot be done for arbitrary
infinite matrices, but there exist classes of infinite matrices for which a
notion of determinant can be made sense of. Let us define it for so-called
``upper almost-unitriangular'' matrices:

\begin{definition}
\label{def.infdet}\textbf{(a)} In the following, when $S$ and $T$ are two sets
of integers (not necessarily finite), an $S\times T$\textit{-matrix} will mean
a matrix whose rows are indexed by the elements of $S$ and whose columns are
indexed by the elements of $T$. (Hence, the elements of $\mathfrak{gl}%
_{\infty}$, as well as those of $\overline{\mathfrak{a}_{\infty}}$ and those
of $\operatorname*{M}\left(  \infty\right)  $, are $\mathbb{Z}\times
\mathbb{Z}$-matrices.)

\textbf{(b)} If $S$ is a set of integer, then an $S\times S$-matrix $B$ over
$\mathbb{C}$ is said to be\textit{ upper unitriangular} if it satisfies the
following two assertions:

-- All entries on the main diagonal of $B$ are $=1$.

-- All entries of $B$ below the main diagonal are $=0$.

\textbf{(c)} An $\mathbb{N}\times\mathbb{N}$-matrix $B$ over $\mathbb{C}$ is
said to be\textit{ upper almost-unitriangular} if it satisfies the following
two assertions:

-- All but finitely many of the entries on the main diagonal of $B$ are $=1$.

-- All but finitely many of the entries of $B$ below the main diagonal are
$=0$.

\textbf{(d)} Let $B$ be an upper almost-unitriangular $\mathbb{N}%
\times\mathbb{N}$-matrix over $\mathbb{C}$. Then, we can write the matrix $B$
in the form $\left(
\begin{array}
[c]{cc}%
C & D\\
0 & E
\end{array}
\right)  $ for some $n\in\mathbb{N}$, some $\left\{  0,1,...,n-1\right\}
\times\left\{  0,1,...,n-1\right\}  $-matrix $C$, some $\left\{
0,1,...,n-1\right\}  \times\left\{  n,n+1,n+2,...\right\}  $-matrix $D$, and
some upper unitriangular $\left\{  n,n+1,n+2,...\right\}  \times\left\{
n,n+1,n+2,...\right\}  $-matrix $E$. The matrix $C$ in such a representation
of $B$ will be called a \textit{faithful block-triangular truncation} of $B$.

\textbf{(e)} Let $B$ be an upper almost-unitriangular $\mathbb{N}%
\times\mathbb{N}$-matrix over $\mathbb{C}$. We define the \textit{determinant}
$\det B$ of the matrix $B$ to be $\det C$, where $C$ is a faithful
block-triangular truncation of $B$. This is well-defined, because a faithful
block-triangular truncation of $B$ exists and because the determinant $\det C$
does not depend on the choice of the faithful block-triangular truncation $C$.
(The latter assertion follows from the fact that $\det\left(
\begin{array}
[c]{cc}%
F & G\\
0 & H
\end{array}
\right)  =\det F$ for any $n\in\mathbb{N}$, any $k\in\mathbb{N}$, any $n\times
n$-matrix $F$, any $n\times k$-matrix $G$, and any upper unitriangular
$k\times k$-matrix $H$.)
\end{definition}

Now, the following fact (an analogue of Proposition \ref{prop.GLinf.det.fin})
gives an explicit formula for the action of $\varrho\left(  A\right)  $:

\begin{remark}
\label{rmk.GLinf.det}Let $\left(  i_{0},i_{1},i_{2},...\right)  $ be an
$m$-degression. Let $A\in\operatorname*{M}\left(  \infty\right)  $. For any
$m$-degression $\left(  j_{0},j_{1},j_{2},...\right)  $, let $A_{j_{0}%
,j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}$ denote the $\mathbb{N}%
\times\mathbb{N}$-matrix defined by%
\[
\left(  \left(  \text{the }\left(  u,v\right)  \text{-th entry of }%
A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}\right)  =\left(  \text{the
}\left(  j_{u},i_{v}\right)  \text{-th entry of }A\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  u,v\right)  \in\mathbb{N}%
^{2}\right)  .
\]
(In other words, let $A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}$
denote the matrix which is obtained from $A$ by removing all columns except
for the $i_{0}$-th, the $i_{1}$-th, the $i_{2}$-th, etc. ones and removing all
rows except for the $j_{0}$-th, the $j_{1}$-th, the $j_{2}$-th, etc. ones, and
then inverting the order of the rows, and inverting the order of the columns.)
Then, for any $m$-degression $\left(  j_{0},j_{1},j_{2},...\right)  $, the
matrix $A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}$ is upper
almost-unitriangular (in fact, one can easily check that more is true: all but
finitely many entries of $A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}$
are equal to the corresponding entries of the identity $\mathbb{N}%
\times\mathbb{N}$ matrix), and thus the determinant $\det\left(
A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}\right)  $ makes sense
(according to Definition \ref{def.infdet} \textbf{(e)}). We have%
\[
\left(  \varrho\left(  A\right)  \right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  =\sum\limits_{\left(  j_{0},j_{1}%
,j_{2},...\right)  \text{ is an }m\text{-degression}}\det\left(
A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}\right)  v_{j_{0}}\wedge
v_{j_{1}}\wedge v_{j_{2}}\wedge....
\]

\end{remark}

The analogy between Remark \ref{rmk.GLinf.det} and Proposition
\ref{prop.GLinf.det.fin} is slightly obscured by technicalities (such as the
fact that Remark \ref{rmk.GLinf.det} only concerns itself with certain
endomorphisms of $V$ and not with homomorphisms between different vector
spaces, and the fact that the $m$-degressions in Remark \ref{rmk.GLinf.det}
are decreasing, while the $\ell$-tuples $\left(  i_{1},i_{2},...,i_{\ell
}\right)  $ and $\left(  j_{1},j_{2},...,j_{\ell}\right)  $ in Proposition
\ref{prop.GLinf.det.fin} are increasing). Still, it should be rather evident
why Remark \ref{rmk.GLinf.det} is (informally speaking) a consequence of ``the
$\ell=\infty$ case'' of Proposition \ref{prop.GLinf.det.fin}.

\subsubsection{\label{subsubsect.Uinf}Semiinfinite vectors and actions of
\texorpdfstring{$\mathfrak{u}_{\infty}$}{u-infinity} and
\texorpdfstring{$\operatorname*{U}\left(  \infty\right)  $}{U-infinity} on
\texorpdfstring{$\wedge^{\dfrac{\infty}{2},m}V$}{the semi-infinite wedge
space}}

The actions of $\mathfrak{gl}_{\infty}$, $\mathfrak{a}_{\infty}$,
$\operatorname*{M}\left(  \infty\right)  $ and $\operatorname*{GL}\left(
\infty\right)  $ on $\wedge^{\dfrac{\infty}{2},m}V$ have many good properties,
but for what we want to do with them, they are in some sense ``too small''
(even $\mathfrak{a}_{\infty}$). Of course, we cannot let the space of
\textbf{all} infinite matrices act on $\wedge^{\dfrac{\infty}{2},m}V$ (this
space is not even a Lie algebra), but it turns out that we can get away with
restricting ourselves to strictly upper-triangular infinite matrices. First,
let us define a kind of completion of $V$:

\begin{definition}
\label{def.uinf.Vhat}\textbf{(a)} A family $\left(  x_{i}\right)
_{i\in\mathbb{Z}}$ of elements of some additive group indexed by integers is
said to be \textit{semiinfinite} if every sufficiently high $i\in\mathbb{Z}$
satisfies $x_{i}=0$.

\textbf{(b)} Let $\widehat{V}$ be the vector subspace $\left\{  v\in
\mathbb{C}^{\mathbb{Z}}\text{\ }\mid\ v\text{ is semiinfinite}\right\}  $ of
$\mathbb{C}^{\mathbb{Z}}$. Let $\mathfrak{u}_{\infty}$ denote the Lie algebra
of all \textbf{strictly} upper-triangular infinite matrices (with rows and
columns indexed by integers). It is easy to see that the Lie algebra
$\mathfrak{u}_{\infty}$ acts on the vector space $\widehat{V}$ in the obvious
way: namely, for any $a\in\mathfrak{u}_{\infty}$ and $v\in\widehat{V}$, we let
$a\rightharpoonup v$ be the product of the matrix $a$ with the column vector
$v$. Here, every element $\left(  x_{i}\right)  _{i\in\mathbb{Z}}$ of
$\widehat{V}$ is identified with the column vector $\left(
\begin{array}
[c]{c}%
...\\
x_{-2}\\
x_{-1}\\
x_{0}\\
x_{1}\\
x_{2}\\
...
\end{array}
\right)  $.

The vector space $V$ defined in Definition \ref{def.glinf.V} clearly is a
subspace of $\widehat{V}$. Restricting the $\mathfrak{u}_{\infty}$-action on
$\widehat{V}$ to an $\left(  \mathfrak{u}_{\infty}\cap\mathfrak{gl}_{\infty
}\right)  $-action on $V$ yields the same $\left(  \mathfrak{u}_{\infty}%
\cap\mathfrak{gl}_{\infty}\right)  $-module as restricting the $\mathfrak{gl}%
_{\infty}$-action on $V$ to an $\left(  \mathfrak{u}_{\infty}\cap
\mathfrak{gl}_{\infty}\right)  $-action on $V$.
\end{definition}

We thus have obtained an $\mathfrak{u}_{\infty}$-module $\widehat{V}$, which
is a kind of completion of $V$. One could now hope that this allows us to
construct an $\mathfrak{u}_{\infty}$-module structure on some kind of
completion of $\wedge^{\dfrac{\infty}{2},m}V$. A quick observation shows that
this works better than one would expect, because we don't have to take any
completion of $\wedge^{\dfrac{\infty}{2},m}V$ (although we can if we want to).
We can make $\wedge^{\dfrac{\infty}{2},m}V$ itself an $\mathfrak{u}_{\infty}$-module:

\begin{definition}
\label{def.uinf.Vhatproj}Let $\ell\in\mathbb{Z}$. Let $\pi_{\ell}%
:\widehat{V}\rightarrow V$ be the linear map which sends every $\left(
x_{i}\right)  _{i\in\mathbb{Z}}\in\widehat{V}$ to $\left(  \left\{
\begin{array}
[c]{c}%
x_{i}\text{, if }i\geq\ell;\\
0\text{, if }i<\ell
\end{array}
\right.  \right)  _{i\in\mathbb{Z}}\in V$. (It is very easy to see that this
map $\pi_{\ell}$ is well-defined.)
\end{definition}

\begin{definition}
\label{def.uinf.Vhatwedge}Let $m\in\mathbb{Z}$. Let $b_{0},b_{1},b_{2},...$ be
vectors in $\widehat{V}$ which satisfy%
\[
\pi_{m-i}\left(  b_{i}\right)  =v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all
sufficiently large }i.
\]
Define an element $b_{0}\wedge b_{1}\wedge b_{2}\wedge...$ of $\wedge
^{\dfrac{\infty}{2},m}V$ as follows: Pick some $N\in\mathbb{N}$ such that
every $i>N$ satisfies $\pi_{m-i}\left(  b_{i}\right)  =v_{m-i}$. (Such an $N$
exists, since we know that $\pi_{m-i}\left(  b_{i}\right)  =v_{m-i}$ for all
sufficiently large $i$.) Then, we define $b_{0}\wedge b_{1}\wedge b_{2}%
\wedge...$ to be the element
\[
\pi_{m-N}\left(  b_{0}\right)  \wedge\pi_{m-N}\left(  b_{1}\right)
\wedge...\wedge\pi_{m-N}\left(  b_{N}\right)  \wedge v_{m-N-1}\wedge
v_{m-N-2}\wedge v_{m-N-3}\wedge...\in\wedge^{\dfrac{\infty}{2},m}V.
\]
This element does not depend on the choice of $N$ (according to Proposition
\ref{prop.uinf.Vhatwedge.welldef} below). Hence, $b_{0}\wedge b_{1}\wedge
b_{2}\wedge...$ is well-defined.
\end{definition}

The next few propositions state some properties of wedge products of elements
of $\widehat{V}$ similar to some properties of wedge products of elements of
$V$ stated above. We will not prove them; neither of them is actually
difficult to verify.

\begin{proposition}
\label{prop.uinf.Vhatwedge.welldef}Let $m\in\mathbb{Z}$. Let $b_{0}%
,b_{1},b_{2},...$ be vectors in $\widehat{V}$ which satisfy%
\[
\pi_{m-i}\left(  b_{i}\right)  =v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all
sufficiently large }i.
\]


If we pick some $N\in\mathbb{N}$ such that every $i>N$ satisfies $\pi
_{m-i}\left(  b_{i}\right)  =v_{m-i}$, then the element%
\[
\pi_{m-N}\left(  b_{0}\right)  \wedge\pi_{m-N}\left(  b_{1}\right)
\wedge...\wedge\pi_{m-N}\left(  b_{N}\right)  \wedge v_{m-N-1}\wedge
v_{m-N-2}\wedge v_{m-N-3}\wedge...\in\wedge^{\dfrac{\infty}{2},m}V
\]
does not depend on the choice of $N$.
\end{proposition}

\begin{proposition}
The wedge product defined in Definition \ref{def.uinf.Vhatwedge} is
antisymmetric and multilinear (in the appropriate sense).
\end{proposition}

\begin{definition}
\label{def.uinf.semiinfwedge}Let $m\in\mathbb{Z}$. Define an action of the Lie
algebra $\mathfrak{u}_{\infty}$ on the vector space $\wedge^{\dfrac{\infty}%
{2},m}V$ by the equation%
\[
a\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left(  a\rightharpoonup v_{i_{k}}\right)
\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...
\]
for all $a\in\mathfrak{u}_{\infty}$ and all elementary semiinfinite wedges
$v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...$ (and by linear extension).
\end{definition}

\begin{proposition}
Let $m\in\mathbb{Z}$. Then, Definition \ref{def.uinf.semiinfwedge} really
defines a representation of the Lie algebra $\mathfrak{u}_{\infty}$ on the
vector space $\wedge^{\dfrac{\infty}{2},m}V$.
\end{proposition}

\begin{proposition}
Let $m\in\mathbb{Z}$. Let $b_{0},b_{1},b_{2},...$ be vectors in $\widehat{V}$
which satisfy%
\[
\pi_{m-i}\left(  b_{i}\right)  =v_{m-i}\ \ \ \ \ \ \ \ \ \ \text{for all
sufficiently large }i.
\]
Let $a\in\mathfrak{u}_{\infty}$. Then,%
\[
a\rightharpoonup\left(  b_{0}\wedge b_{1}\wedge b_{2}\wedge...\right)
=\sum\limits_{k\geq0}b_{0}\wedge b_{1}\wedge...\wedge b_{k-1}\wedge\left(
a\rightharpoonup b_{k}\right)  \wedge b_{k+1}\wedge b_{k+2}\wedge....
\]

\end{proposition}

\begin{definition}
\label{def.Uinf.rho}Let $m\in\mathbb{Z}$. Let $\rho:\mathfrak{u}_{\infty
}\rightarrow\operatorname*{End}\left(  \wedge^{\dfrac{\infty}{2},m}V\right)  $
be the representation of $\mathfrak{u}_{\infty}$ on $\wedge^{\dfrac{\infty}%
{2},m}V$ defined in Definition \ref{def.uinf.semiinfwedge}. (We denote this
representation by the same letter $\rho$ as the representation $\mathfrak{gl}%
_{\infty}\rightarrow\operatorname*{End}\left(  \wedge^{\dfrac{\infty}{2}%
,m}V\right)  $ from Definition \ref{def.glinf.rho}. This is intentional and
unproblematic, because both of these representations have the same restriction
onto $\mathfrak{u}_{\infty}\cap\mathfrak{gl}_{\infty}$.)
\end{definition}

\begin{remark}
\label{rmk.Uinf.rhorhohat}Let $m\in\mathbb{Z}$. Let $a\in\mathfrak{u}_{\infty
}\cap\overline{\mathfrak{a}_{\infty}}$. Then, $\rho\left(  a\right)
=\widehat{\rho}\left(  a\right)  $ (where $\rho\left(  a\right)  $ is defined
according to Definition \ref{def.Uinf.rho}, and $\widehat{\rho}\left(
a\right)  $ is defined according to Definition \ref{def.glinf.rhohat.abar}).
\end{remark}

\begin{definition}
\label{def.Uinf}We let $\operatorname*{U}\left(  \infty\right)  $ denote the
set $\operatorname*{id}+\mathfrak{u}_{\infty}$. In other words,
$\operatorname*{U}\left(  \infty\right)  $ is the set of all upper-triangular
infinite matrices (with rows and columns indexed by integers) whose all
diagonal entries are $=1$. This set $\operatorname*{U}\left(  \infty\right)  $
is easily seen to be a group (with respect to matrix multiplication). Inverses
in this group can be computed by means of the formula $\left(  I_{\infty
}+a\right)  ^{-1}=\sum\limits_{k=0}^{\infty}a^{k}$ for all $a\in
\mathfrak{u}_{\infty}$\ \ \ \ \footnotemark
\end{definition}

\footnotetext{Here, we are using the fact that, for every $a\in\mathfrak{u}%
_{\infty}$, the sum $\sum\limits_{k=0}^{\infty}a^{k}$ converges entrywise (i.
e., for every $\left(  i,j\right)  \in\mathbb{Z}^{2}$, the sum $\sum
\limits_{k=0}^{\infty}\left(  \text{the }\left(  i,j\right)  \text{-th entry
of }a^{k}\right)  $ converges in the discrete topology). Here is why this
holds:
\par
Since $a\in\mathfrak{u}_{\infty}$, we know that the $\left(  i,j\right)  $-th
entry of $a$ is $0$ for all $\left(  i,j\right)  \in\mathbb{Z}^{2}$ satisfying
$i>j-1$. From this, it is easy to conclude (by induction over $k$) that for
every $k\in\mathbb{N}$, the $\left(  i,j\right)  $-th entry of $a^{k}$ is $0$
for all $\left(  i,j\right)  \in\mathbb{Z}^{2}$ satisfying $i>j-k$. Hence, for
every $\left(  i,j\right)  \in\mathbb{Z}^{2}$, the $\left(  i,j\right)  $-th
entry of $a^{k}$ is $0$ for all nonnegative integers $k$ satisfying $k>j-i$.
As a consequence, for every $\left(  i,j\right)  \in\mathbb{Z}^{2}$, all but
finitely many addends of the sum%
\[
\sum\limits_{k=0}^{\infty}\left(  \text{the }\left(  i,j\right)  \text{-th
entry of }a^{k}\right)
\]
are $0$. In other words, for every $\left(  i,j\right)  \in\mathbb{Z}^{2}$,
the sum $\sum\limits_{k=0}^{\infty}\left(  \text{the }\left(  i,j\right)
\text{-th entry of }a^{k}\right)  $ converges in the discrete topology. Hence,
the sum $\sum\limits_{k=0}^{\infty}a^{k}$ converges entrywise, qed.}

\begin{definition}
\label{def.Uinf.act}Let $m\in\mathbb{Z}$. We define an action $\varrho
:\operatorname*{U}\left(  \infty\right)  \rightarrow\operatorname*{End}\left(
\mathcal{F}^{\left(  m\right)  }\right)  $ of the group $\operatorname*{U}%
\left(  \infty\right)  $ on the vector space $\mathcal{F}^{\left(  m\right)
}=\wedge^{\dfrac{\infty}{2},m}V$ as follows: For every $A\in\operatorname*{U}%
\left(  \infty\right)  $ and every $m$-degression $\left(  i_{0},i_{1}%
,i_{2},...\right)  $, we set%
\[
\left(  \varrho\left(  A\right)  \right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  =Av_{i_{0}}\wedge Av_{i_{1}}\wedge
Av_{i_{2}}\wedge....
\]
(This is then extended to the whole $\mathcal{F}^{\left(  m\right)  }$ by
linearity.) It is very easy to see that this is well-defined (because
$\pi_{v_{k}}\left(  Av_{k}\right)  =v_{k}$ for all sufficiently small $k$) and
indeed gives a group action. (We denote this action by the same letter
$\varrho$ as the action $\operatorname*{M}\left(  \infty\right)
\rightarrow\operatorname*{End}\left(  \mathcal{F}^{\left(  m\right)  }\right)
$ from Definition \ref{def.GLinf.act}. This is intentional and unproblematic,
because both of these actions have the same restriction onto
$\operatorname*{U}\left(  \infty\right)  \cap\operatorname*{M}\left(
\infty\right)  $.)
\end{definition}

In analogy to Remark \ref{rmk.GLinf.det}, we have:

\begin{remark}
\label{rmk.Uinf.det}Let $\left(  i_{0},i_{1},i_{2},...\right)  $ be an
$m$-degression. Let $A\in\operatorname*{U}\left(  \infty\right)  $. For any
$m$-degression $\left(  j_{0},j_{1},j_{2},...\right)  $, let $A_{j_{0}%
,j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}$ denote the $\mathbb{N}%
\times\mathbb{N}$-matrix defined by%
\[
\left(  \left(  \text{the }\left(  u,v\right)  \text{-th entry of }%
A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}\right)  =\left(  \text{the
}\left(  j_{u},i_{v}\right)  \text{-th entry of }A\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  u,v\right)  \in\mathbb{N}%
^{2}\right)  .
\]
(In other words, let $A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}$
denote the matrix which is obtained from $A$ by removing all columns except
for the $i_{0}$-th, the $i_{1}$-th, the $i_{2}$-th, etc. ones and removing all
rows except for the $j_{0}$-th, the $j_{1}$-th, the $j_{2}$-th, etc. ones, and
then inverting the order of the rows, and inverting the order of the columns.)
Then, for any $m$-degression $\left(  j_{0},j_{1},j_{2},...\right)  $, the
matrix $\left(  A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}\right)
^{T}$ is upper almost-unitriangular, and thus the determinant $\det\left(
\left(  A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}\right)  ^{T}\right)
$ makes sense (according to Definition \ref{def.infdet} \textbf{(e)}). We have%
\[
\left(  \varrho\left(  A\right)  \right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...\right)  =\sum\limits_{\left(  j_{0},j_{1}%
,j_{2},...\right)  \text{ is an }m\text{-degression}}\det\left(  \left(
A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}\right)  ^{T}\right)
v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge....
\]

\end{remark}

The analogy between Remark \ref{rmk.GLinf.det} and Remark \ref{rmk.Uinf.det}
is somewhat marred by the fact that the transposed matrix $\left(
A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}\right)  ^{T}$ is used in
Remark \ref{rmk.Uinf.det} instead of the matrix $A_{j_{0},j_{1},j_{2}%
,...}^{i_{0},i_{1},i_{2},...}$. This is merely a technical difference, and if
we would have defined the determinant of a \textbf{lower} almost-unitriangular
matrix, we could have avoided using the transpose in Remark \ref{rmk.Uinf.det}.

\begin{remark}
There is a way to ``merge'' $\operatorname*{GL}\left(  \infty\right)  $ and
$\operatorname*{U}\left(  \infty\right)  $ into a bigger group of infinite
matrices. Indeed, let $\operatorname*{M}\nolimits^{\operatorname*{U}}\left(
\infty\right)  $ the set of all matrices $A\in\operatorname*{U}\left(
\infty\right)  $ such that all but finitely many among the $\left(
i,j\right)  \in\mathbb{Z}^{2}$ satisfying $i\geq j$ satisfy $\left(  \text{the
}\left(  i,j\right)  \text{-th entry of }A\right)  =\delta_{i,j}$. (Note that
this condition does not restrict the $\left(  i,j\right)  $-th entry of $A$
for any $\left(  i,j\right)  \in\mathbb{Z}^{2}$ satisfying $i<j$. That is, the
entries of $A$ above the main diagonal can be arbitrary, but the entries of
$A$ below and on the main diagonal have to coincide with the respective
entries of the identity matrix save for finitely many exceptions, if $A$ is to
lie in $\operatorname*{M}\nolimits^{\operatorname*{U}}\left(  \infty\right)
$.) Then, it is easy to see that $\operatorname*{M}%
\nolimits^{\operatorname*{U}}\left(  \infty\right)  $ is a monoid. The group
of all invertible elements of this monoid (where ``invertible'' means ``having
an inverse in the monoid $\operatorname*{M}\nolimits^{\operatorname*{U}%
}\left(  \infty\right)  $'') is a group which has both $\operatorname*{GL}%
\left(  \infty\right)  $ and $\operatorname*{U}\left(  \infty\right)  $ as
subgroups. Actually, this group is $\operatorname*{GL}\left(  \infty\right)
\cdot\operatorname*{U}\left(  \infty\right)  $, as the reader can easily check.

We will need neither the monoid $\operatorname*{M}\nolimits^{\operatorname*{U}%
}\left(  \infty\right)  $ nor this group in the following.
\end{remark}

\subsubsection{The exponential relation between \texorpdfstring{$\rho$}{rho}
and \texorpdfstring{$\varrho$}{Rho}}

We now come to a relation which connects the actions $\rho$ and $\varrho$. It
comes in a $\operatorname*{GL}\left(  \infty\right)  $ version, a
$\operatorname*{U}\left(  \infty\right)  $ version, and a finitary version; we
will formulate all three, but only prove the latter. First, the
$\operatorname*{GL}\left(  \infty\right)  $ version:

\begin{theorem}
\label{thm.GLinf.rhoRho}Let $a\in\mathfrak{gl}_{\infty}$. Let $m\in\mathbb{Z}%
$. Then, the exponential $\exp a$ is a well-defined element of
$\operatorname*{GL}\left(  \infty\right)  $ and satisfies $\varrho\left(  \exp
a\right)  =\exp\left(  \rho\left(  a\right)  \right)  $ in
$\operatorname*{End}\left(  \mathcal{F}^{\left(  m\right)  }\right)  $.
\end{theorem}

It should be noticed that Theorem \ref{thm.GLinf.rhoRho}, unlike most of the
other results we have been stating, does rely on the ground field being
$\mathbb{C}$; otherwise, there would be no guarantee that $\exp a$ is
well-defined. However, if we assume, for example, that $a$ is strictly
upper-triangular, or that the entries of $a$ belong to some ideal $I$ of the
ground ring such that the ground ring is complete and Hausdorff in the
$I$-adic topology, then the statement of Theorem \ref{thm.GLinf.rhoRho} would
be guaranteed over any ground ring which is a commutative $\mathbb{Q}$-algebra.

The $\operatorname*{U}\left(  \infty\right)  $ version does not depend on the
ground ring at all (as long as the ground ring is a $\mathbb{Q}$-algebra):

\begin{theorem}
\label{thm.Uinf.rhoRho}Let $a\in\mathfrak{u}_{\infty}$. Let $m\in\mathbb{Z}$.
Then, the exponential $\exp a$ is a well-defined element of $\operatorname*{U}%
\left(  \infty\right)  $ and satisfies $\varrho\left(  \exp a\right)
=\exp\left(  \rho\left(  a\right)  \right)  $ in $\operatorname*{End}\left(
\mathcal{F}^{\left(  m\right)  }\right)  $.
\end{theorem}

We have now stated the $\operatorname*{GL}\left(  \infty\right)  $ and the
$\operatorname*{U}\left(  \infty\right)  $ versions of the relation between
$\rho$ and $\varrho$. Before we state the finitary version, we define a finite
analogue of the map $\rho$:

\begin{definition}
Let $P$ be a vector space, and let $\ell\in\mathbb{N}$. Let $\rho_{P,\ell
}:\mathfrak{gl}\left(  P\right)  \rightarrow\operatorname*{End}\left(
\wedge^{\ell}P\right)  $ denote the representation of the Lie algebra
$\mathfrak{gl}\left(  P\right)  $ on the $\ell$-th exterior power of the
defining representation $P$ of $\mathfrak{gl}\left(  P\right)  $. By the
definition of the $\ell$-th exterior power of a representation of a Lie
algebra, this representation $\rho_{P,\ell}$ satisfies%
\begin{equation}
\left(  \rho_{P,\ell}\left(  a\right)  \right)  \left(  p_{1}\wedge
p_{2}\wedge...\wedge p_{\ell}\right)  =\sum\limits_{k=1}^{\ell}p_{1}\wedge
p_{2}\wedge...\wedge p_{k-1}\wedge\left(  a\rightharpoonup p_{k}\right)
\wedge p_{k+1}\wedge p_{k+2}\wedge...\wedge p_{\ell} \label{def.finitary.rho}%
\end{equation}
for every $a\in\mathfrak{gl}\left(  P\right)  $ and any $p_{1},p_{2}%
,...,p_{\ell}\in P$. (Recall that $a\rightharpoonup p=ap$ for every
$a\in\mathfrak{gl}\left(  P\right)  $ and $p\in P$.)
\end{definition}

Finally, let us state the finitary version of Theorem \ref{thm.GLinf.rhoRho}
and Theorem \ref{thm.Uinf.rhoRho}. To see why it is analogous to the two
aforementioned theorems, one should keep in mind that $\rho_{P,\ell}$ is an
analogue of $\rho$ in the finite case, while $\wedge^{\ell}A$ is an analogue
of $\varrho\left(  A\right)  $.

\begin{theorem}
\label{thm.finitary.rhoRho}Let $P$ be a vector space. Let $a\in\mathfrak{gl}%
\left(  P\right)  $ be a nilpotent linear map. Then, the exponential $\exp a$
is a well-defined element of $\operatorname*{GL}\left(  P\right)  $ and
satisfies $\wedge^{\ell}\left(  \exp a\right)  =\exp\left(  \rho_{P,\ell
}\left(  a\right)  \right)  $ in $\operatorname*{End}\left(  \wedge^{\ell
}P\right)  $ for every $\ell\in\mathbb{N}$.
\end{theorem}

Note that we have formulated Theorem \ref{thm.finitary.rhoRho} only for
nilpotent $a\in\mathfrak{gl}\left(  P\right)  $. We could have also formulated
it for arbitrary $a\in\mathfrak{gl}\left(  P\right)  $ under some mild
conditions on $P$ (such as $P$ being finite-dimensional), but then it would
depend on the ground field being $\mathbb{C}$, which is something we would
like to avoid (as we are going to apply this theorem to a different ground field).

\begin{vershort}
\textit{First proof of Theorem \ref{thm.finitary.rhoRho} (sketched).} Since
$a$ is nilpotent, it is known that the exponential $\exp a$ is a well-defined
element of $\operatorname*{GL}\left(  P\right)  $.
\end{vershort}

\begin{verlong}
\textit{First proof of Theorem \ref{thm.finitary.rhoRho}.} Since $a$ is
nilpotent, it is known that the exponential $\exp a$ is a well-defined element
of $\operatorname*{GL}\left(  P\right)  $.
\end{verlong}

Let $\ell\in\mathbb{N}$. Now define an endomorphism $\rho_{P,\ell}^{\prime
}\left(  a\right)  :P^{\otimes\ell}\rightarrow P^{\otimes\ell}$ by%
\[
\rho_{P,\ell}^{\prime}\left(  a\right)  =\sum\limits_{k=1}^{\ell
}\operatorname*{id}\nolimits_{P}^{\otimes\left(  k-1\right)  }\otimes
a\otimes\operatorname*{id}\nolimits_{P}^{\otimes\left(  \ell-k\right)  }.
\]


Let also $\pi:P^{\otimes\ell}\rightarrow\wedge^{\ell}P$ be the canonical
projection (since $\wedge^{\ell}P$ is defined as a quotient vector space of
$P^{\otimes\ell}$). Clearly, $\pi$ is surjective.

\begin{vershort}
It is easy to see that $\pi\circ\left(  \rho_{P,\ell}^{\prime}\left(
a\right)  \right)  =\left(  \rho_{P,\ell}\left(  a\right)  \right)  \circ\pi$.
From this, one can conclude that%
\begin{equation}
\pi\circ\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{m}=\left(
\rho_{P,\ell}\left(  a\right)  \right)  ^{m}\circ\pi
\ \ \ \ \ \ \ \ \ \ \text{for every }m\in\mathbb{N}.
\label{pf.finitary.rhoRho.projm.short}%
\end{equation}

\end{vershort}

\begin{verlong}
It is easy to see that%
\begin{equation}
\pi\circ\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  =\left(
\rho_{P,\ell}\left(  a\right)  \right)  \circ\pi.
\label{pf.finitary.rhoRho.proj}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.finitary.rhoRho.proj}):} Let $p_{1}%
,p_{2},...,p_{\ell}\in P$ be arbitrary. Then, $p_{1}\wedge p_{2}%
\wedge...\wedge p_{\ell}=\pi\left(  p_{1}\otimes p_{2}\otimes...\otimes
p_{\ell}\right)  $ (by the definition of $p_{1}\wedge p_{2}\wedge...\wedge
p_{\ell}$), and%
\begin{align*}
&  \left(  \left(  \rho_{P,\ell}\left(  a\right)  \right)  \circ\pi\right)
\left(  p_{1}\otimes p_{2}\otimes...\otimes p_{\ell}\right) \\
&  =\left(  \rho_{P,\ell}\left(  a\right)  \right)  \underbrace{\left(
\pi\left(  p_{1}\otimes p_{2}\otimes...\otimes p_{\ell}\right)  \right)
}_{=p_{1}\wedge p_{2}\wedge...\wedge p_{\ell}}=\left(  \rho_{P,\ell}\left(
a\right)  \right)  \left(  p_{1}\wedge p_{2}\wedge...\wedge p_{\ell}\right) \\
&  =\sum\limits_{k=1}^{\ell}\underbrace{p_{1}\wedge p_{2}\wedge...\wedge
p_{k-1}\wedge\left(  a\rightharpoonup p_{k}\right)  \wedge p_{k+1}\wedge
p_{k+2}\wedge...\wedge p_{\ell}}_{\substack{=\pi\left(  p_{1}\otimes
p_{2}\otimes...\otimes p_{k-1}\otimes\left(  a\rightharpoonup p_{k}\right)
\otimes p_{k+1}\otimes p_{k+2}\otimes...\otimes p_{\ell}\right)  \\\text{(by
the definition of }p_{1}\wedge p_{2}\wedge...\wedge p_{k-1}\wedge\left(
a\rightharpoonup p_{k}\right)  \wedge p_{k+1}\wedge p_{k+2}\wedge...\wedge
p_{\ell\ }\text{)}}}\\
&  =\sum\limits_{k=1}^{\ell}\pi\left(  p_{1}\otimes p_{2}\otimes...\otimes
p_{k-1}\otimes\left(  a\rightharpoonup p_{k}\right)  \otimes p_{k+1}\otimes
p_{k+2}\otimes...\otimes p_{\ell}\right) \\
&  =\pi\left(  \sum\limits_{k=1}^{\ell}\underbrace{p_{1}\otimes p_{2}%
\otimes...\otimes p_{k-1}}_{=\operatorname*{id}\nolimits_{P}^{\otimes\left(
k-1\right)  }\left(  p_{1}\otimes p_{2}\otimes...\otimes p_{k-1}\right)
}\otimes\underbrace{\left(  a\rightharpoonup p_{k}\right)  }%
_{\substack{=ap_{k}\\\text{(since }a\rightharpoonup p=ap\\\text{for every
}p\in P\text{)}}}\otimes\underbrace{p_{k+1}\otimes p_{k+2}\otimes...\otimes
p_{\ell}}_{=\operatorname*{id}\nolimits_{P}^{\otimes\left(  \ell-k\right)
}\left(  p_{k+1}\otimes p_{k+2}\otimes...\otimes p_{\ell}\right)  }\right) \\
&  =\pi\left(  \sum\limits_{k=1}^{\ell}\underbrace{\operatorname*{id}%
\nolimits_{P}^{\otimes\left(  k-1\right)  }\left(  p_{1}\otimes p_{2}%
\otimes...\otimes p_{k-1}\right)  \otimes ap_{k}\otimes\operatorname*{id}%
\nolimits_{P}^{\otimes\left(  \ell-k\right)  }\left(  p_{k+1}\otimes
p_{k+2}\otimes...\otimes p_{\ell}\right)  }_{=\left(  \operatorname*{id}%
\nolimits_{P}^{\otimes\left(  k-1\right)  }\otimes a\otimes\operatorname*{id}%
\nolimits_{P}^{\otimes\left(  \ell-k\right)  }\right)  \left(  \left(
p_{1}\otimes p_{2}\otimes...\otimes p_{k-1}\right)  \otimes p_{k}%
\otimes\left(  p_{k+1}\otimes p_{k+2}\otimes...\otimes p_{\ell}\right)
\right)  }\right) \\
&  =\pi\left(  \sum\limits_{k=1}^{\ell}\left(  \operatorname*{id}%
\nolimits_{P}^{\otimes\left(  k-1\right)  }\otimes a\otimes\operatorname*{id}%
\nolimits_{P}^{\otimes\left(  \ell-k\right)  }\right)  \underbrace{\left(
\left(  p_{1}\otimes p_{2}\otimes...\otimes p_{k-1}\right)  \otimes
p_{k}\otimes\left(  p_{k+1}\otimes p_{k+2}\otimes...\otimes p_{\ell}\right)
\right)  }_{=p_{1}\otimes p_{2}\otimes...\otimes p_{\ell}}\right) \\
&  =\pi\left(  \sum\limits_{k=1}^{\ell}\left(  \operatorname*{id}%
\nolimits_{P}^{\otimes\left(  k-1\right)  }\otimes a\otimes\operatorname*{id}%
\nolimits_{P}^{\otimes\left(  \ell-k\right)  }\right)  \left(  p_{1}\otimes
p_{2}\otimes...\otimes p_{\ell}\right)  \right)  =\left(  \pi\circ
\underbrace{\left(  \sum\limits_{k=1}^{\ell}\operatorname*{id}\nolimits_{P}%
^{\otimes\left(  k-1\right)  }\otimes a\otimes\operatorname*{id}%
\nolimits_{P}^{\otimes\left(  \ell-k\right)  }\right)  }_{=\rho_{P,\ell
}^{\prime}\left(  a\right)  }\right)  \left(  p_{1}\otimes p_{2}%
\otimes...\otimes p_{\ell}\right) \\
&  =\left(  \pi\circ\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)
\right)  \left(  p_{1}\otimes p_{2}\otimes...\otimes p_{\ell}\right)  .
\end{align*}
Now, forget that we fixed $p_{1},p_{2},...,p_{\ell}\in P$. We thus have proven
that any $p_{1},p_{2},...,p_{\ell}\in P$ satisfy%
\[
\left(  \left(  \rho_{P,\ell}\left(  a\right)  \right)  \circ\pi\right)
\left(  p_{1}\otimes p_{2}\otimes...\otimes p_{\ell}\right)  =\left(  \pi
\circ\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  \right)  \left(
p_{1}\otimes p_{2}\otimes...\otimes p_{\ell}\right)  .
\]
In other words, the two linear maps $\left(  \rho_{P,\ell}\left(  a\right)
\right)  \circ\pi$ and $\pi\circ\left(  \rho_{P,\ell}^{\prime}\left(
a\right)  \right)  $ are equal to each other on every pure tensor. Since any
two linear maps from a tensor product which are equal to each other on every
pure tensor must be identical, this yields that the maps $\left(  \rho
_{P,\ell}\left(  a\right)  \right)  \circ\pi$ and $\pi\circ\left(
\rho_{P,\ell}^{\prime}\left(  a\right)  \right)  $ are identical. In other
words, $\pi\circ\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)
=\left(  \rho_{P,\ell}\left(  a\right)  \right)  \circ\pi$, and
(\ref{pf.finitary.rhoRho.proj}) is proven.} Using this, we obtain%
\begin{equation}
\pi\circ\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{m}=\left(
\rho_{P,\ell}\left(  a\right)  \right)  ^{m}\circ\pi
\ \ \ \ \ \ \ \ \ \ \text{for every }m\in\mathbb{N}.
\label{pf.finitary.rhoRho.projm}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.finitary.rhoRho.projm}):} We will prove
(\ref{pf.finitary.rhoRho.projm}) by induction over $m$:
\par
\textit{Induction base:} Comparing $\pi\circ\underbrace{\left(  \rho_{P,\ell
}^{\prime}\left(  a\right)  \right)  ^{0}}_{=\operatorname*{id}}=\pi$ with
$\underbrace{\left(  \rho_{P,\ell}\left(  a\right)  \right)  ^{0}%
}_{=\operatorname*{id}}\circ\pi=\pi$, we obtain $\pi\circ\left(  \rho_{P,\ell
}^{\prime}\left(  a\right)  \right)  ^{0}=\left(  \rho_{P,\ell}\left(
a\right)  \right)  ^{0}\circ\pi$. In other words,
(\ref{pf.finitary.rhoRho.projm}) holds for $m=0$. This completes the induction
base.
\par
\textit{Induction step:} Let $\mu\in\mathbb{N}$. Assume that
(\ref{pf.finitary.rhoRho.projm}) holds for $m=\mu$. We must now prove that
(\ref{pf.finitary.rhoRho.projm}) holds for $m=\mu+1$ as well.
\par
Since (\ref{pf.finitary.rhoRho.projm}) holds for $m=\mu$, we have $\pi
\circ\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{\mu}=\left(
\rho_{P,\ell}\left(  a\right)  \right)  ^{\mu}\circ\pi$. Now,%
\begin{align*}
\pi\circ\underbrace{\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)
^{\mu+1}}_{=\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{\mu
}\circ\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  }  &
=\underbrace{\pi\circ\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)
^{\mu}}_{=\left(  \rho_{P,\ell}\left(  a\right)  \right)  ^{\mu}\circ\pi}%
\circ\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  =\left(
\rho_{P,\ell}\left(  a\right)  \right)  ^{\mu}\circ\underbrace{\pi\circ\left(
\rho_{P,\ell}^{\prime}\left(  a\right)  \right)  }_{\substack{=\left(
\rho_{P,\ell}\left(  a\right)  \right)  \circ\pi\\\text{(according to
(\ref{pf.finitary.rhoRho.proj}))}}}\\
&  =\underbrace{\left(  \rho_{P,\ell}\left(  a\right)  \right)  ^{\mu}%
\circ\left(  \rho_{P,\ell}\left(  a\right)  \right)  }_{=\left(  \rho_{P,\ell
}^{\prime}\left(  a\right)  \right)  ^{\mu+1}}\circ\pi=\left(  \rho_{P,\ell
}^{\prime}\left(  a\right)  \right)  ^{\mu+1}\circ\pi.
\end{align*}
In other words, (\ref{pf.finitary.rhoRho.projm}) holds for $m=\mu+1$. Thus,
the induction step is finished. The induction proof of
(\ref{pf.finitary.rhoRho.projm}) is therefore complete.}
\end{verlong}

\begin{vershort}
On the other hand, a routine induction proves that every $m\in\mathbb{N}$
satisfies%
\begin{equation}
\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{m}=\sum
\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}%
^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=m}}\dfrac{m!}{i_{1}!i_{2}!...i_{\ell}%
!}a^{i_{1}}\otimes a^{i_{2}}\otimes...\otimes a^{i_{\ell}}.
\label{pf.finitary.rhoRho.mpower.short}%
\end{equation}

\end{vershort}

\begin{verlong}
On the other hand, let us show that every $m\in\mathbb{N}$ satisfies%
\begin{equation}
\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{m}=\sum
\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}%
^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=m}}\dfrac{m!}{i_{1}!i_{2}!...i_{\ell}%
!}a^{i_{1}}\otimes a^{i_{2}}\otimes...\otimes a^{i_{\ell}}.
\label{pf.finitary.rhoRho.mpower}%
\end{equation}


\textit{Proof of (\ref{pf.finitary.rhoRho.mpower}):} We will prove
(\ref{pf.finitary.rhoRho.mpower}) by induction over $m$:

\textit{Induction base:} There exists only one $\ell$-tuple $\left(
i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell}$ satisfying $i_{1}%
+i_{2}+...+i_{\ell}=0$, namely $\left(  i_{1},i_{2},...,i_{\ell}\right)
=\left(  \underbrace{0,0,...,0}_{\ell\text{ zeroes}}\right)  $. Thus,%
\begin{align*}
\sum\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in
\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=0}}\dfrac{0!}{i_{1}%
!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes a^{i_{2}}\otimes...\otimes a^{i_{\ell}}
&  =\underbrace{\dfrac{0!}{0!0!...0!}}_{=1}\underbrace{a^{0}\otimes
a^{0}\otimes...\otimes a^{0}}_{\ell\text{ tensorands}}=\underbrace{a^{0}%
\otimes a^{0}\otimes...\otimes a^{0}}_{\ell\text{ tensorands}}\\
&  =\underbrace{\operatorname*{id}\otimes\operatorname*{id}\otimes
...\otimes\operatorname*{id}}_{\ell\text{ tensorands}}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }a^{0}=\operatorname*{id}\right) \\
&  =\operatorname*{id}\nolimits_{P^{\otimes\ell}}.
\end{align*}
Thus, $\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{0}%
=\operatorname*{id}\nolimits_{P^{\otimes\ell}}=\sum\limits_{\substack{\left(
i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell};\\i_{1}+i_{2}%
+...+i_{\ell}=0}}\dfrac{0!}{i_{1}!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes
a^{i_{2}}\otimes...\otimes a^{i_{\ell}}$. Hence,
(\ref{pf.finitary.rhoRho.mpower}) holds for $m=0$. This completes the
induction base.

\textit{Induction step:} Let $\mu\in\mathbb{N}$. Assume that
(\ref{pf.finitary.rhoRho.mpower}) holds for $m=\mu$. We must prove that
(\ref{pf.finitary.rhoRho.mpower}) also holds for $m=\mu+1$.

Since (\ref{pf.finitary.rhoRho.mpower}) holds for $m=\mu$, we have%
\begin{equation}
\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{\mu}=\sum
\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}%
^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=\mu}}\dfrac{\mu!}{i_{1}!i_{2}!...i_{\ell}%
!}a^{i_{1}}\otimes a^{i_{2}}\otimes...\otimes a^{i_{\ell}}.
\label{pf.finitary.rhoRho.mpower.pf.indhyp}%
\end{equation}


Now, for every $\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell}%
$, let $C_{\left(  i_{1},i_{2},...,i_{\ell}\right)  }$ denote the endomorphism%
\[
\dfrac{\mu!}{i_{1}!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes a^{i_{2}}%
\otimes...\otimes a^{i_{\ell}}\in\operatorname*{End}\left(  P^{\otimes\ell
}\right)  .
\]
Then, every $\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell}$
satisfies%
\begin{align}
&  \left(  \underbrace{\operatorname*{id}\nolimits_{P}^{\otimes\left(
k-1\right)  }\otimes a\otimes\operatorname*{id}\nolimits_{P}^{\otimes\left(
\ell-k\right)  }}_{=\underbrace{\operatorname*{id}\nolimits_{P}\otimes
\operatorname*{id}\nolimits_{P}\otimes...\otimes\operatorname*{id}%
\nolimits_{P}}_{k-1\text{ tensorands}}\otimes a\otimes
\underbrace{\operatorname*{id}\nolimits_{P}\otimes\operatorname*{id}%
\nolimits_{P}\otimes...\otimes\operatorname*{id}\nolimits_{P}}_{\ell-k\text{
tensorands}}}\right)  \circ\underbrace{C_{\left(  i_{1},i_{2},...,i_{\ell
}\right)  }}_{=\dfrac{\mu!}{i_{1}!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes
a^{i_{2}}\otimes...\otimes a^{i_{\ell}}}\nonumber\\
&  =\left(  \underbrace{\operatorname*{id}\nolimits_{P}\otimes
\operatorname*{id}\nolimits_{P}\otimes...\otimes\operatorname*{id}%
\nolimits_{P}}_{k-1\text{ tensorands}}\otimes a\otimes
\underbrace{\operatorname*{id}\nolimits_{P}\otimes\operatorname*{id}%
\nolimits_{P}\otimes...\otimes\operatorname*{id}\nolimits_{P}}_{\ell-k\text{
tensorands}}\right)  \circ\left(  \dfrac{\mu!}{i_{1}!i_{2}!...i_{\ell}%
!}a^{i_{1}}\otimes a^{i_{2}}\otimes...\otimes a^{i_{\ell}}\right) \nonumber\\
&  =\dfrac{\mu!}{i_{1}!i_{2}!...i_{\ell}!}\underbrace{\left(
\underbrace{\operatorname*{id}\nolimits_{P}\otimes\operatorname*{id}%
\nolimits_{P}\otimes...\otimes\operatorname*{id}\nolimits_{P}}_{k-1\text{
tensorands}}\otimes a\otimes\underbrace{\operatorname*{id}\nolimits_{P}%
\otimes\operatorname*{id}\nolimits_{P}\otimes...\otimes\operatorname*{id}%
\nolimits_{P}}_{\ell-k\text{ tensorands}}\right)  \circ\left(  a^{i_{1}%
}\otimes a^{i_{2}}\otimes...\otimes a^{i_{\ell}}\right)  }_{=\left(
\operatorname*{id}\nolimits_{P}\circ a^{i_{1}}\right)  \otimes\left(
\operatorname*{id}\nolimits_{P}\circ a^{i_{2}}\right)  \otimes...\otimes
\left(  \operatorname*{id}\nolimits_{P}\circ a^{i_{k-1}}\right)
\otimes\left(  a\circ a^{i_{k}}\right)  \otimes\left(  \operatorname*{id}%
\nolimits_{P}\circ a^{i_{k+1}}\right)  \otimes\left(  \operatorname*{id}%
\nolimits_{P}\circ a^{i_{k+2}}\right)  \otimes...\otimes\left(
\operatorname*{id}\nolimits_{P}\circ a^{i_{\ell}}\right)  }\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since composition of linear maps is
bilinear}\right) \nonumber\\
&  =\underbrace{\dfrac{\mu!}{i_{1}!i_{2}!...i_{\ell}!}}_{\substack{=\dfrac
{\left(  i_{k}+1\right)  !}{i_{k}!}\cdot\dfrac{\mu!}{i_{1}!i_{2}%
!...i_{k-1}!\left(  i_{k}+1\right)  !i_{k+1}!i_{k+2}!...i_{\ell}!}\\=\left(
i_{k}+1\right)  \cdot\dfrac{\mu!}{i_{1}!i_{2}!...i_{k-1}!\left(
i_{k}+1\right)  !i_{k+1}!i_{k+2}!...i_{\ell}!}\\\text{(since }\dfrac{\left(
i_{k}+1\right)  !}{i_{k}!}=i_{k}+1\text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \cdot\underbrace{\left(
\operatorname*{id}\nolimits_{P}\circ a^{i_{1}}\right)  \otimes\left(
\operatorname*{id}\nolimits_{P}\circ a^{i_{2}}\right)  \otimes...\otimes
\left(  \operatorname*{id}\nolimits_{P}\circ a^{i_{k-1}}\right)  }_{=a^{i_{1}%
}\otimes a^{i_{2}}\otimes...\otimes a^{i_{k-1}}}\otimes\underbrace{\left(
a\circ a^{i_{k}}\right)  }_{=a^{i_{k}+1}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \otimes
\underbrace{\left(  \operatorname*{id}\nolimits_{P}\circ a^{i_{k+1}}\right)
\otimes\left(  \operatorname*{id}\nolimits_{P}\circ a^{i_{k+2}}\right)
\otimes...\otimes\left(  \operatorname*{id}\nolimits_{P}\circ a^{i_{\ell}%
}\right)  }_{=a^{i_{k+1}}\otimes a^{i_{k+2}}\otimes...\otimes a^{i_{\ell}}%
}\nonumber\\
&  =\left(  i_{k}+1\right)  \cdot\underbrace{\dfrac{\mu!}{i_{1}!i_{2}%
!...i_{k-1}!\left(  i_{k}+1\right)  !i_{k+1}!i_{k+2}!...i_{\ell}!}\cdot
a^{i_{1}}\otimes a^{i_{2}}\otimes...\otimes a^{i_{k-1}}\otimes a^{i_{k}%
+1}\otimes a^{i_{k+1}}\otimes a^{i_{k+2}}\otimes...\otimes a^{i_{\ell}}%
}_{\substack{=C_{\left(  i_{1},i_{2},...,i_{k-1},i_{k}+1,i_{k+1}%
,i_{k+2},...,i_{\ell}\right)  }\\\text{(since the definition of }C_{\left(
i_{1},i_{2},...,i_{k-1},i_{k}+1,i_{k+1},i_{k+2},...,i_{\ell}\right)  }\text{
yields}\\C_{\left(  i_{1},i_{2},...,i_{k-1},i_{k}+1,i_{k+1},i_{k+2}%
,...,i_{\ell}\right)  }\\=\dfrac{\mu!}{i_{1}!i_{2}!...i_{k-1}!\left(
i_{k}+1\right)  !i_{k+1}!i_{k+2}!...i_{\ell}!}\cdot a^{i_{1}}\otimes a^{i_{2}%
}\otimes...\otimes a^{i_{k-1}}\otimes a^{i_{k}+1}\otimes a^{i_{k+1}}\otimes
a^{i_{k+2}}\otimes...\otimes a^{i_{\ell}}\text{)}}}\nonumber\\
&  =\left(  i_{k}+1\right)  \cdot C_{\left(  i_{1},i_{2},...,i_{k-1}%
,i_{k}+1,i_{k+1},i_{k+2},...,i_{\ell}\right)  }
\label{pf.finitary.rhoRho.bigcalc}%
\end{align}
and%
\begin{align}
i_{1}+i_{2}+...+i_{\ell}  &  =\sum\limits_{k\in\left\{  1,2,...,\ell\right\}
}i_{k}=\sum\limits_{\substack{k\in\left\{  1,2,...,\ell\right\}  ;\\i_{k}%
\geq1}}i_{k}+\sum\limits_{\substack{k\in\left\{  1,2,...,\ell\right\}
;\\i_{k}<1}}\underbrace{i_{k}}_{\substack{=0\\\text{(since }i_{k}<1\text{ and
}i_{k}\in\mathbb{N}\text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since every }k\in\left\{  1,2,...,\ell
\right\}  \text{ satisfies either }i_{k}\geq1\text{ or }i_{k}<1\right)
\nonumber\\
&  =\sum\limits_{\substack{k\in\left\{  1,2,...,\ell\right\}  ;\\i_{k}\geq
1}}i_{k}+\underbrace{\sum\limits_{\substack{k\in\left\{  1,2,...,\ell\right\}
;\\i_{k}<1}}0}_{=0}=\sum\limits_{\substack{k\in\left\{  1,2,...,\ell\right\}
;\\i_{k}\geq1}}i_{k}. \label{pf.finitary.rhoRho.smallcalc}%
\end{align}


But now,%
\begin{align*}
&  \left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{\mu+1}\\
&  =\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  \circ\left(
\rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{\mu}\\
&  =\left(  \sum\limits_{k=1}^{\ell}\operatorname*{id}\nolimits_{P}%
^{\otimes\left(  k-1\right)  }\otimes a\otimes\operatorname*{id}%
\nolimits_{P}^{\otimes\left(  \ell-k\right)  }\right)  \circ\left(
\sum\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in
\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=\mu}}\underbrace{\dfrac{\mu
!}{i_{1}!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes a^{i_{2}}\otimes...\otimes
a^{i_{\ell}}}_{=C_{\left(  i_{1},i_{2},...,i_{\ell}\right)  }}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{due to }\rho_{P,\ell}^{\prime}\left(
a\right)  =\sum\limits_{k=1}^{\ell}\operatorname*{id}\nolimits_{P}%
^{\otimes\left(  k-1\right)  }\otimes a\otimes\operatorname*{id}%
\nolimits_{P}^{\otimes\left(  \ell-k\right)  }\text{ and due to
(\ref{pf.finitary.rhoRho.mpower.pf.indhyp})}\right) \\
&  =\left(  \sum\limits_{k=1}^{\ell}\operatorname*{id}\nolimits_{P}%
^{\otimes\left(  k-1\right)  }\otimes a\otimes\operatorname*{id}%
\nolimits_{P}^{\otimes\left(  \ell-k\right)  }\right)  \circ\left(
\sum\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in
\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=\mu}}C_{\left(  i_{1}%
,i_{2},...,i_{\ell}\right)  }\right) \\
&  =\underbrace{\sum\limits_{k=1}^{\ell}}_{=\sum\limits_{k\in\left\{
1,2,...,\ell\right\}  }}\sum\limits_{\substack{\left(  i_{1},i_{2}%
,...,i_{\ell}\right)  \in\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=\mu
}}\underbrace{\left(  \operatorname*{id}\nolimits_{P}^{\otimes\left(
k-1\right)  }\otimes a\otimes\operatorname*{id}\nolimits_{P}^{\otimes\left(
\ell-k\right)  }\right)  \circ C_{\left(  i_{1},i_{2},...,i_{\ell}\right)  }%
}_{\substack{=\left(  i_{k}+1\right)  \cdot C_{\left(  i_{1},i_{2}%
,...,i_{k-1},i_{k}+1,i_{k+1},i_{k+2},...,i_{\ell}\right)  }\\\text{(by
(\ref{pf.finitary.rhoRho.bigcalc}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since composition of linear maps is
bilinear}\right)
\end{align*}%
\begin{align*}
&  =\sum\limits_{k\in\left\{  1,2,...,\ell\right\}  }\underbrace{\sum
\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}%
^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=\mu}}\left(  i_{k}+1\right)  \cdot
C_{\left(  i_{1},i_{2},...,i_{k-1},i_{k}+1,i_{k+1},i_{k+2},...,i_{\ell
}\right)  }}_{\substack{=\sum\limits_{\substack{\left(  i_{1},i_{2}%
,...,i_{\ell}\right)  \in\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}%
=\mu+1;\\i_{k}\geq1}}i_{k}\cdot C_{\left(  i_{1},i_{2},...,i_{k-1}%
,i_{k},i_{k+1},i_{k+2},...,i_{\ell}\right)  }\\\text{(here we substituted
}\left(  i_{1},i_{2},...,i_{\ell}\right)  \text{ for }\left(  i_{1}%
,i_{2},...,i_{k-1},i_{k}+1,i_{k+1},i_{k+2},...,i_{\ell}\right)  \text{ in the
sum)}}}\\
&  =\underbrace{\sum\limits_{k=1}^{\ell}\sum\limits_{\substack{\left(
i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell};\\i_{1}+i_{2}%
+...+i_{\ell}=\mu+1;\\i_{k}\geq1}}}_{=\sum\limits_{\substack{\left(
i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell};\\i_{1}+i_{2}%
+...+i_{\ell}=\mu+1}}\sum\limits_{\substack{k\in\left\{  1,2,...,\ell\right\}
;\\i_{k}\geq1}}}i_{k}\cdot\underbrace{C_{\left(  i_{1},i_{2},...,i_{k-1}%
,i_{k},i_{k+1},i_{k+2},...,i_{\ell}\right)  }}_{=C_{\left(  i_{1}%
,i_{2},...,i_{\ell}\right)  }}=\sum\limits_{\substack{\left(  i_{1}%
,i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell
}=\mu+1}}\sum\limits_{\substack{k\in\left\{  1,2,...,\ell\right\}
;\\i_{k}\geq1}}i_{k}\cdot C_{\left(  i_{1},i_{2},...,i_{\ell}\right)  }\\
&  =\sum\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)
\in\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=\mu+1}}\underbrace{\left(
\sum\limits_{\substack{k\in\left\{  1,2,...,\ell\right\}  ;\\i_{k}\geq1}%
}i_{k}\right)  }_{\substack{=i_{1}+i_{2}+...+i_{\ell}\\\text{(by
(\ref{pf.finitary.rhoRho.smallcalc}))}}}\cdot C_{\left(  i_{1},i_{2}%
,...,i_{\ell}\right)  }\\
&  =\sum\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)
\in\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=\mu+1}}\underbrace{\left(
i_{1}+i_{2}+...+i_{\ell}\right)  }_{=\mu+1}\cdot C_{\left(  i_{1}%
,i_{2},...,i_{\ell}\right)  }\\
&  =\sum\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)
\in\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=\mu+1}}\left(  \mu+1\right)
\cdot\underbrace{C_{\left(  i_{1},i_{2},...,i_{\ell}\right)  }}_{=\dfrac{\mu
!}{i_{1}!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes a^{i_{2}}\otimes...\otimes
a^{i_{\ell}}}\\
&  =\sum\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)
\in\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=\mu+1}}\dfrac{\left(
\mu+1\right)  \cdot\mu!}{i_{1}!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes a^{i_{2}%
}\otimes...\otimes a^{i_{\ell}}\\
&  =\sum\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)
\in\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=\mu+1}}\dfrac{\left(
\mu+1\right)  !}{i_{1}!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes a^{i_{2}}%
\otimes...\otimes a^{i_{\ell}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
\mu+1\right)  \cdot\mu!=\left(  \mu+1\right)  !\right)  .
\end{align*}
In other words, (\ref{pf.finitary.rhoRho.mpower}) also holds for $m=\mu+1$.
This completes the induction step. Thus, the induction proof of
(\ref{pf.finitary.rhoRho.mpower}) is complete.
\end{verlong}

\begin{vershort}
Now, $\exp a=\sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}a^{i}$, whence%
\begin{align*}
\left(  \exp a\right)  ^{\otimes\ell}  &  =\left(  \sum\limits_{i\in
\mathbb{N}}\dfrac{1}{i!}a^{i}\right)  ^{\otimes\ell}=\sum\limits_{\left(
i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell}}\left(  \dfrac{1}%
{i_{1}!}a^{i_{1}}\right)  \otimes\left(  \dfrac{1}{i_{2}!}a^{i_{2}}\right)
\otimes...\otimes\left(  \dfrac{1}{i_{\ell}!}a^{i_{\ell}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the product rule}\right) \\
&  =\sum\limits_{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell
}}\dfrac{1}{i_{1}!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes a^{i_{2}}\otimes
...\otimes a^{i_{\ell}}\\
&  =\sum\limits_{m\in\mathbb{N}}\sum\limits_{\substack{\left(  i_{1}%
,i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}%
=m}}\dfrac{1}{i_{1}!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes a^{i_{2}}%
\otimes...\otimes a^{i_{\ell}}\\
&  =\sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\underbrace{\sum
\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}%
^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=m}}\dfrac{m!}{i_{1}!i_{2}!...i_{\ell}%
!}a^{i_{1}}\otimes a^{i_{2}}\otimes...\otimes a^{i_{\ell}}}%
_{\substack{=\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)
^{m}\\\text{(by (\ref{pf.finitary.rhoRho.mpower.short}))}}}=\sum
\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\left(  \rho_{P,\ell}^{\prime}\left(
a\right)  \right)  ^{m}\\
&  =\exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  .
\end{align*}
Note that this shows that $\exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)
\right)  $ is well-defined. But since $\exp\left(  \rho_{P,\ell}^{\prime
}\left(  a\right)  \right)  =\sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\left(
\rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{m}$, we have%
\begin{align*}
\pi\circ\left(  \exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)
\right)   &  =\pi\circ\left(  \sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\left(
\rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{m}\right)  =\sum
\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\underbrace{\pi\circ\left(  \rho_{P,\ell
}^{\prime}\left(  a\right)  \right)  ^{m}}_{\substack{=\left(  \rho_{P,\ell
}\left(  a\right)  \right)  ^{m}\circ\pi\\\text{(by
(\ref{pf.finitary.rhoRho.projm.short}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since composition of linear maps is
bilinear}\right) \\
&  =\sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\left(  \rho_{P,\ell}\left(
a\right)  \right)  ^{m}\circ\pi=\underbrace{\left(  \sum\limits_{m\in
\mathbb{N}}\dfrac{1}{m!}\left(  \rho_{P,\ell}\left(  a\right)  \right)
^{m}\right)  }_{=\exp\left(  \rho_{P,\ell}\left(  a\right)  \right)  }\circ
\pi=\left(  \exp\left(  \rho_{P,\ell}\left(  a\right)  \right)  \right)
\circ\pi,
\end{align*}
and this also shows that $\exp\left(  \rho_{P,\ell}\left(  a\right)  \right)
$ is well-defined (since $\pi$ is surjective).

Since we have proven earlier that $\left(  \exp a\right)  ^{\otimes\ell}%
=\exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  $, the equality
$\pi\circ\left(  \exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)
\right)  =\left(  \exp\left(  \rho_{P,\ell}\left(  a\right)  \right)  \right)
\circ\pi$ rewrites as $\pi\circ\left(  \exp a\right)  ^{\otimes\ell}=\left(
\exp\left(  \rho_{P,\ell}\left(  a\right)  \right)  \right)  \circ\pi$.
\end{vershort}

\begin{verlong}
Now, $\exp a=\sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}a^{i}$, so that $\left(
\exp a\right)  ^{\otimes\ell}=\left(  \sum\limits_{i\in\mathbb{N}}\dfrac
{1}{i!}a^{i}\right)  ^{\otimes\ell}$. But by the product rule, we have%
\begin{equation}
\left(  \sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}a^{i}\right)  ^{\otimes\ell
}=\sum\limits_{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell}%
}\left(  \dfrac{1}{i_{1}!}a^{i_{1}}\right)  \otimes\left(  \dfrac{1}{i_{2}%
!}a^{i_{2}}\right)  \otimes...\otimes\left(  \dfrac{1}{i_{\ell}!}a^{i_{\ell}%
}\right)  \label{pf.finitary.rhoRho.prodrule}%
\end{equation}
(since the tensor product of linear maps is bilinear)\footnote{Here is a more
formal proof of (\ref{pf.finitary.rhoRho.prodrule}):
\par
Let $\operatorname*{Prod}$ be the canonical linear map $\left(
\operatorname*{End}P\right)  ^{\otimes\ell}\rightarrow\operatorname*{End}%
\left(  P^{\otimes\ell}\right)  $ which sends the (abstract) tensor product
$f_{1}\otimes f_{2}\otimes...\otimes f_{\ell}$ of any $\ell$ endomorphisms
$f_{1},f_{2},...,f_{\ell}\in\operatorname*{End}P$ to the endomorphism
$f_{1}\otimes f_{2}\otimes...\otimes f_{\ell}$ of $P^{\otimes\ell}$. (We need
to carefully distinguish the former $f_{1}\otimes f_{2}\otimes...\otimes
f_{\ell}$ from the latter $f_{1}\otimes f_{2}\otimes...\otimes f_{\ell}$, even
if the same notation is used for both terms.) Note that $\operatorname*{Prod}:
\left(  \operatorname*{End}P\right)  ^{\otimes\ell}\rightarrow
\operatorname*{End}\left(  P^{\otimes\ell}\right)  $ is a linear map (and even
an algebra homomorphism if both $\operatorname*{End}P$ and
$\operatorname*{End}\left(  P^{\otimes\ell}\right)  $ are regarded as algebras
with respect to composition of endomorphisms).
\par
We denote by $u^{\otimes\ell}$ the $\ell$-th power of an element $u\in
\otimes\left(  \operatorname*{End}P\right)  $ in the tensor algebra
$\otimes\left(  \operatorname*{End}P\right)  $. Since the multiplication in
the tensor algebra $\otimes\left(  \operatorname*{End}P\right)  $ is the
tensor product, we have $u^{\otimes\ell}=\underbrace{u\otimes u\otimes
...\otimes u}_{\ell\text{ times}}$.
\par
In the tensor algebra $\otimes\left(  \operatorname*{End}P\right)  $, we have%
\begin{equation}
\underbrace{\left(  \sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}a^{i}\right)
\otimes\left(  \sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}a^{i}\right)
\otimes...\otimes\left(  \sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}%
a^{i}\right)  }_{\ell\text{ times}}=\sum\limits_{\left(  i_{1},i_{2}%
,...,i_{\ell}\right)  \in\mathbb{N}^{\ell}}\left(  \dfrac{1}{i_{1}!}a^{i_{1}%
}\right)  \otimes\left(  \dfrac{1}{i_{2}!}a^{i_{2}}\right)  \otimes
...\otimes\left(  \dfrac{1}{i_{\ell}!}a^{i_{\ell}}\right)
\label{pf.finitary.rhoRho.prodrule.pf.1}%
\end{equation}
(by the product rule), since the multiplication in the tensor algebra
$\otimes\left(  \operatorname*{End}P\right)  $ is the tensor product. Applying
the map $\operatorname*{Prod}:\left(  \operatorname*{End}P\right)
^{\otimes\ell}\rightarrow\operatorname*{End}\left(  P^{\otimes\ell}\right)  $
to the equality (\ref{pf.finitary.rhoRho.prodrule.pf.1}), we obtain%
\begin{align*}
&  \operatorname*{Prod}\left(  \underbrace{\left(  \sum\limits_{i\in
\mathbb{N}}\dfrac{1}{i!}a^{i}\right)  \otimes\left(  \sum\limits_{i\in
\mathbb{N}}\dfrac{1}{i!}a^{i}\right)  \otimes...\otimes\left(  \sum
\limits_{i\in\mathbb{N}}\dfrac{1}{i!}a^{i}\right)  }_{\ell\text{ times}%
}\right) \\
&  =\operatorname*{Prod}\left(  \sum\limits_{\left(  i_{1},i_{2},...,i_{\ell
}\right)  \in\mathbb{N}^{\ell}}\left(  \dfrac{1}{i_{1}!}a^{i_{1}}\right)
\otimes\left(  \dfrac{1}{i_{2}!}a^{i_{2}}\right)  \otimes...\otimes\left(
\dfrac{1}{i_{\ell}!}a^{i_{\ell}}\right)  \right) \\
&  =\sum\limits_{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell
}}\underbrace{\operatorname*{Prod}\left(  \left(  \dfrac{1}{i_{1}!}a^{i_{1}%
}\right)  \otimes\left(  \dfrac{1}{i_{2}!}a^{i_{2}}\right)  \otimes
...\otimes\left(  \dfrac{1}{i_{\ell}!}a^{i_{\ell}}\right)  \right)
}_{\substack{=\left(  \dfrac{1}{i_{1}!}a^{i_{1}}\right)  \otimes\left(
\dfrac{1}{i_{2}!}a^{i_{2}}\right)  \otimes...\otimes\left(  \dfrac{1}{i_{\ell
}!}a^{i_{\ell}}\right)  \\\text{(by the definition of }\operatorname*{Prod}%
\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{Prod}\text{ is a
linear map}\right) \\
&  =\sum\limits_{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell
}}\left(  \dfrac{1}{i_{1}!}a^{i_{1}}\right)  \otimes\left(  \dfrac{1}{i_{2}%
!}a^{i_{2}}\right)  \otimes...\otimes\left(  \dfrac{1}{i_{\ell}!}a^{i_{\ell}%
}\right)
\end{align*}
in $\operatorname*{End}\left(  P^{\otimes\ell}\right)  $. Since
\begin{align*}
&  \operatorname*{Prod}\left(  \underbrace{\left(  \sum\limits_{i\in
\mathbb{N}}\dfrac{1}{i!}a^{i}\right)  \otimes\left(  \sum\limits_{i\in
\mathbb{N}}\dfrac{1}{i!}a^{i}\right)  \otimes...\otimes\left(  \sum
\limits_{i\in\mathbb{N}}\dfrac{1}{i!}a^{i}\right)  }_{\ell\text{ times}%
}\right) \\
&  =\underbrace{\left(  \sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}a^{i}\right)
\otimes\left(  \sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}a^{i}\right)
\otimes...\otimes\left(  \sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}%
a^{i}\right)  }_{\ell\text{ times}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by the
definition of }\operatorname*{Prod}\right) \\
&  =\left(  \sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}a^{i}\right)
^{\otimes\ell},
\end{align*}
this rewrites as $\left(  \sum\limits_{i\in\mathbb{N}}\dfrac{1}{i!}%
a^{i}\right)  ^{\otimes\ell}=\sum\limits_{\left(  i_{1},i_{2},...,i_{\ell
}\right)  \in\mathbb{N}^{\ell}}\left(  \dfrac{1}{i_{1}!}a^{i_{1}}\right)
\otimes\left(  \dfrac{1}{i_{2}!}a^{i_{2}}\right)  \otimes...\otimes\left(
\dfrac{1}{i_{\ell}!}a^{i_{\ell}}\right)  $ in $\operatorname*{End}\left(
P^{\otimes\ell}\right)  $. This proves (\ref{pf.finitary.rhoRho.prodrule}).}.
Now,%
\begin{align}
\left(  \exp a\right)  ^{\otimes\ell}  &  =\left(  \sum\limits_{i\in
\mathbb{N}}\dfrac{1}{i!}a^{i}\right)  ^{\otimes\ell}=\sum\limits_{\left(
i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell}}\left(  \dfrac{1}%
{i_{1}!}a^{i_{1}}\right)  \otimes\left(  \dfrac{1}{i_{2}!}a^{i_{2}}\right)
\otimes...\otimes\left(  \dfrac{1}{i_{\ell}!}a^{i_{\ell}}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.finitary.rhoRho.prodrule}%
)}\right) \nonumber\\
&  =\sum\limits_{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell
}}\dfrac{1}{i_{1}!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes a^{i_{2}}\otimes
...\otimes a^{i_{\ell}}\nonumber\\
&  =\sum\limits_{m\in\mathbb{N}}\sum\limits_{\substack{\left(  i_{1}%
,i_{2},...,i_{\ell}\right)  \in\mathbb{N}^{\ell};\\i_{1}+i_{2}+...+i_{\ell}%
=m}}\dfrac{1}{i_{1}!i_{2}!...i_{\ell}!}a^{i_{1}}\otimes a^{i_{2}}%
\otimes...\otimes a^{i_{\ell}}\nonumber\\
&  =\sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\underbrace{\sum
\limits_{\substack{\left(  i_{1},i_{2},...,i_{\ell}\right)  \in\mathbb{N}%
^{\ell};\\i_{1}+i_{2}+...+i_{\ell}=m}}\dfrac{m!}{i_{1}!i_{2}!...i_{\ell}%
!}a^{i_{1}}\otimes a^{i_{2}}\otimes...\otimes a^{i_{\ell}}}%
_{\substack{=\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)
^{m}\\\text{(by (\ref{pf.finitary.rhoRho.mpower}))}}}=\sum\limits_{m\in
\mathbb{N}}\dfrac{1}{m!}\left(  \rho_{P,\ell}^{\prime}\left(  a\right)
\right)  ^{m}\nonumber\\
&  =\exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  .
\label{pf.finitary.rhoRho.expexp}%
\end{align}


Note that this shows that $\exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)
\right)  $ is well-defined. Thus, for every $w\in P^{\otimes\ell}$, the term
$\left(  \exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  \right)
w$ is well-defined. Hence, for every $x\in\wedge^{\ell}P$, the term $\left(
\exp\left(  \rho_{P,\ell}\left(  a\right)  \right)  \right)  x$ is
well-defined as well\footnote{\textit{Proof.} Let $x\in\wedge^{\ell}P$. Since
the map $\pi:P^{\otimes\ell}\rightarrow\wedge^{\ell}P$ is surjective, there
exists a $w\in P^{\otimes\ell}$ such that $x=\pi\left(  w\right)  $. Consider
this $w$. We know that $\left(  \exp\left(  \rho_{P,\ell}^{\prime}\left(
a\right)  \right)  \right)  w$ is well-defined. Since $\exp\left(
\rho_{P,\ell}^{\prime}\left(  a\right)  \right)  =\sum\limits_{m\in\mathbb{N}%
}\dfrac{1}{m!}\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{m}$,
we have $\left(  \exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)
\right)  w=\left(  \sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\left(
\rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{m}\right)  w=\sum
\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\left(  \rho_{P,\ell}^{\prime}\left(
a\right)  \right)  ^{m}\left(  w\right)  $. Thus,%
\begin{align*}
\pi\left(  \left(  \exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)
\right)  \right)  w\right)   &  =\pi\left(  \sum\limits_{m\in\mathbb{N}}%
\dfrac{1}{m!}\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)
^{m}\left(  w\right)  \right)  =\sum\limits_{m\in\mathbb{N}}\dfrac{1}%
{m!}\underbrace{\pi\left(  \left(  \rho_{P,\ell}^{\prime}\left(  a\right)
\right)  ^{m}\left(  w\right)  \right)  }_{=\left(  \pi\circ\left(
\rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{m}\right)  w}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\pi\text{ is linear}\right) \\
&  =\sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\underbrace{\left(  \pi
\circ\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{m}\right)
}_{\substack{=\left(  \rho_{P,\ell}\left(  a\right)  \right)  ^{m}\circ
\pi\\\text{(by (\ref{pf.finitary.rhoRho.projm}))}}}w=\sum\limits_{m\in
\mathbb{N}}\dfrac{1}{m!}\underbrace{\left(  \left(  \rho_{P,\ell}\left(
a\right)  \right)  ^{m}\circ\pi\right)  w}_{=\left(  \rho_{P,\ell}\left(
a\right)  \right)  ^{m}\left(  \pi\left(  w\right)  \right)  }\\
&  =\sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\left(  \rho_{P,\ell}\left(
a\right)  \right)  ^{m}\underbrace{\left(  \pi\left(  w\right)  \right)
}_{=x}=\underbrace{\sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\left(
\rho_{P,\ell}\left(  a\right)  \right)  ^{m}}_{=\exp\left(  \rho_{P,\ell
}\left(  a\right)  \right)  }\left(  x\right)  =\left(  \exp\left(
\rho_{P,\ell}\left(  a\right)  \right)  \right)  x.
\end{align*}
Thus, the term $\left(  \exp\left(  \rho_{P,\ell}\left(  a\right)  \right)
\right)  x$ is well-defined, qed.}. In other words, the endomorphism
$\exp\left(  \rho_{P,\ell}\left(  a\right)  \right)  $ of $\wedge^{\ell}P$ is
well-defined. Since $\exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)
\right)  =\sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\left(  \rho_{P,\ell
}^{\prime}\left(  a\right)  \right)  ^{m}$, we have%
\begin{align*}
\pi\circ\left(  \exp\left(  \rho_{P,\ell}^{\prime}\left(  a\right)  \right)
\right)   &  =\pi\circ\left(  \sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\left(
\rho_{P,\ell}^{\prime}\left(  a\right)  \right)  ^{m}\right)  =\sum
\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\underbrace{\pi\circ\left(  \rho_{P,\ell
}^{\prime}\left(  a\right)  \right)  ^{m}}_{\substack{=\left(  \rho_{P,\ell
}\left(  a\right)  \right)  ^{m}\circ\pi\\\text{(by
(\ref{pf.finitary.rhoRho.projm}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since composition of linear maps is
bilinear}\right) \\
&  =\sum\limits_{m\in\mathbb{N}}\dfrac{1}{m!}\left(  \rho_{P,\ell}\left(
a\right)  \right)  ^{m}\circ\pi=\underbrace{\left(  \sum\limits_{m\in
\mathbb{N}}\dfrac{1}{m!}\left(  \rho_{P,\ell}\left(  a\right)  \right)
^{m}\right)  }_{=\exp\left(  \rho_{P,\ell}\left(  a\right)  \right)  }\circ
\pi=\left(  \exp\left(  \rho_{P,\ell}\left(  a\right)  \right)  \right)
\circ\pi.
\end{align*}
Since $\left(  \exp a\right)  ^{\otimes\ell}=\exp\left(  \rho_{P,\ell}%
^{\prime}\left(  a\right)  \right)  $ (by (\ref{pf.finitary.rhoRho.expexp})),
this rewrites as $\pi\circ\left(  \exp a\right)  ^{\otimes\ell}=\left(
\exp\left(  \rho_{P,\ell}\left(  a\right)  \right)  \right)  \circ\pi$.
\end{verlong}

\begin{vershort}
On the other hand, since the projection $\pi:P^{\otimes\ell}\rightarrow
\wedge^{\ell}P$ is functorial in $P$, we have $\pi\circ\left(  \exp a\right)
^{\otimes\ell}=\left(  \wedge^{\ell}\left(  \exp a\right)  \right)  \circ\pi$.
Thus,
\[
\left(  \wedge^{\ell}\left(  \exp a\right)  \right)  \circ\pi=\pi\circ\left(
\exp a\right)  ^{\otimes\ell}=\left(  \exp\left(  \rho_{P,\ell}\left(
a\right)  \right)  \right)  \circ\pi.
\]
Since the morphism $\pi$ is right-cancellable (since it is surjective), this
yields $\wedge^{\ell}\left(  \exp a\right)  =\exp\left(  \rho_{P,\ell}\left(
a\right)  \right)  $. This proves Theorem \ref{thm.finitary.rhoRho}.
\end{vershort}

\begin{verlong}
On the other hand, since the projection $\pi:P^{\otimes\ell}\rightarrow
\wedge^{\ell}P$ is functorial in $P$, the diagram%
\[%
%TCIMACRO{\TeXButton{tensor vs wedge functoriality}{\xymatrixcolsep{4pc}
%\xymatrix{
%P^{\otimes\ell} \ar[r]^{\left(\exp a\right)^{\otimes\ell}} \ar@{->>}[d]_{\pi}
%& P^{\otimes\ell} \ar@{->>}[d]^{\pi} \\
%\wedge^{\ell} P \ar[r]^{\wedge^{\ell} \left(\exp a\right)} & \wedge^{\ell} P
%}}}%
%BeginExpansion
\xymatrixcolsep{4pc}
\xymatrix{
P^{\otimes\ell} \ar[r]^{\left(\exp a\right)^{\otimes\ell}} \ar@{->>}[d]_{\pi}
& P^{\otimes\ell} \ar@{->>}[d]^{\pi} \\
\wedge^{\ell} P \ar[r]^{\wedge^{\ell} \left(\exp a\right)} & \wedge^{\ell} P
}%
%EndExpansion
\]
commutes. In other words, $\pi\circ\left(  \exp a\right)  ^{\otimes\ell
}=\left(  \wedge^{\ell}\left(  \exp a\right)  \right)  \circ\pi$. Thus,
\[
\left(  \wedge^{\ell}\left(  \exp a\right)  \right)  \circ\pi=\pi\circ\left(
\exp a\right)  ^{\otimes\ell}=\left(  \exp\left(  \rho_{P,\ell}\left(
a\right)  \right)  \right)  \circ\pi.
\]
Since the morphism $\pi$ is right-cancellable (since it is surjective), this
yields $\wedge^{\ell}\left(  \exp a\right)  =\exp\left(  \rho_{P,\ell}\left(
a\right)  \right)  $. This proves Theorem \ref{thm.finitary.rhoRho}.
\end{verlong}

\begin{vershort}
\textit{Second proof of Theorem \ref{thm.finitary.rhoRho} (sketched).} Since
$a$ is nilpotent, it is known that the exponential $\exp a$ is a well-defined
unipotent element of $\operatorname*{GL}\left(  P\right)  $. But for every
$\ell\in\mathbb{N}$, the $\ell$-th exterior power of any unipotent element of
$\operatorname*{GL}\left(  P\right)  $ is a unipotent element of
$\operatorname*{GL}\left(  \wedge^{\ell}P\right)  $. Since $\exp a$ is a
unipotent element of $\operatorname*{GL}\left(  P\right)  $, this yields that
$\wedge^{\ell}\left(  \exp a\right)  $ is a unipotent element of
$\operatorname*{GL}\left(  \wedge^{\ell}P\right)  $ for every $\ell
\in\mathbb{N}$. Hence, the logarithm $\log\left(  \wedge^{\ell}\left(  \exp
a\right)  \right)  $ is well-defined for every $\ell\in\mathbb{N}$.
\end{vershort}

\begin{verlong}
\textit{Second proof of Theorem \ref{thm.finitary.rhoRho}.} Since $a$ is
nilpotent, it is known that the exponential $\exp a$ is a well-defined
unipotent element of $\operatorname*{GL}\left(  P\right)  $. But for every
$\ell\in\mathbb{N}$, the $\ell$-th exterior power of any unipotent element of
$\operatorname*{GL}\left(  P\right)  $ is a unipotent element of
$\operatorname*{GL}\left(  \wedge^{\ell}P\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $\ell\in\mathbb{N}$. Let $\alpha$ be a
unipotent element of $\operatorname*{GL}\left(  P\right)  $. We must prove
that $\wedge^{\ell}\alpha$ is a unipotent element of $\operatorname*{GL}%
\left(  \wedge^{\ell}P\right)  $.
\par
Since $\alpha$ is unipotent, $\alpha-1\in\operatorname*{End}P$ is nilpotent.
That is, there exists an $n\in\mathbb{N}$ such that $\left(  \alpha-1\right)
^{n}=0$. Consider this $n$.
\par
We will denote the identity map $\operatorname*{id}:P\rightarrow P$ by $1$
(since it is the unity of the algebra $\operatorname*{End}P$).
\par
For every $i\in\left\{  1,2,...,\ell\right\}  $, let $\alpha_{i}$ denote the
endomorphism $\underbrace{\alpha\otimes\alpha\otimes...\otimes\alpha
}_{i-1\text{ times }\alpha}\otimes\left(  \alpha-1\right)  \otimes
\underbrace{1\otimes1\otimes...\otimes1}_{\ell-i\text{ times }1}$ of
$P^{\otimes\ell}$. Then, the endomorphisms $\alpha_{1}$, $\alpha_{2}$, $...$,
$\alpha_{n}$ commute with each other (because $\alpha$, $\alpha-1$ and $1$
commute with each other). But for every $i\in\left\{  1,2,...,\ell\right\}  $,
we have%
\[
\alpha_{i}=\underbrace{\alpha\otimes\alpha\otimes...\otimes\alpha}_{i-1\text{
times }\alpha}\otimes\left(  \alpha-1\right)  \otimes\underbrace{1\otimes
1\otimes...\otimes1}_{\ell-i\text{ times }1},
\]
so that%
\begin{align*}
\alpha_{i}^{n}  &  =\left(  \underbrace{\alpha\otimes\alpha\otimes
...\otimes\alpha}_{i-1\text{ times }\alpha}\otimes\left(  \alpha-1\right)
\otimes\underbrace{1\otimes1\otimes...\otimes1}_{\ell-i\text{ times }1
}\right)  ^{n}\\
&  =\underbrace{\alpha^{n}\otimes\alpha^{n}\otimes...\otimes\alpha^{n}%
}_{i-1\text{ times }\alpha^{n}}\otimes\underbrace{\left(  \alpha-1\right)
^{n}}_{=0}\otimes\underbrace{1^{n}\otimes1^{n}\otimes...\otimes1^{n}}%
_{\ell-i\text{ times }1^{n}}=0,
\end{align*}
so that $\alpha_{i}$ is nilpotent.
\par
Meanwhile, it is known that if finitely many nilpotent elements of an algebra
commute with each other, then the sum of these elements must also be
nilpotent. Applying this result to the nilpotent elements $\alpha_{1}$,
$\alpha_{2}$, $...$, $\alpha_{\ell}$ of the algebra $\operatorname*{End}V$
(these elements commute with each other, as we know), we conclude that the sum
$\sum\limits_{i=1}^{\ell}\alpha_{i}$ is nilpotent. But every $i\in\left\{
1,2,...,\ell\right\}  $ satisfies%
\begin{align*}
\alpha_{i}  &  =\underbrace{\alpha\otimes\alpha\otimes...\otimes\alpha
}_{i-1\text{ times }\alpha}\otimes\left(  \alpha-1\right)  \otimes
\underbrace{1\otimes1\otimes...\otimes1}_{\ell-i\text{ times }1}\\
&  =\underbrace{\underbrace{\alpha\otimes\alpha\otimes...\otimes\alpha
}_{i-1\text{ times }\alpha}\otimes\alpha}_{=\underbrace{\alpha\otimes
\alpha\otimes...\otimes\alpha}_{i\text{ times }\alpha}}\otimes
\underbrace{1\otimes1\otimes...\otimes1}_{\ell-i\text{ times }1 }%
-\underbrace{\alpha\otimes\alpha\otimes...\otimes\alpha}_{i-1\text{ times
}\alpha}\otimes\underbrace{1\otimes\underbrace{1\otimes1\otimes...\otimes
1}_{\ell-i\text{ times }1}}_{=\underbrace{1\otimes1\otimes...\otimes1}%
_{\ell-i+1\text{ times }1}=\underbrace{1\otimes1\otimes...\otimes1}%
_{\ell-\left(  i-1\right)  \text{ times }1}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the multilinearity of the tensor
product}\right) \\
&  =\underbrace{\alpha\otimes\alpha\otimes...\otimes\alpha}_{i\text{ times
}\alpha}\otimes\underbrace{1\otimes1\otimes...\otimes1}_{\ell-i\text{ times
}1}-\underbrace{\alpha\otimes\alpha\otimes...\otimes\alpha}_{i-1\text{ times
}\alpha}\otimes\underbrace{1\otimes1\otimes...\otimes1}_{\ell-\left(
i-1\right)  \text{ times }1}.
\end{align*}
Hence,%
\begin{align*}
\sum\limits_{i=1}^{\ell}\alpha_{i}  &  =\sum\limits_{i=1}^{\ell}\left(
\underbrace{\alpha\otimes\alpha\otimes...\otimes\alpha}_{i\text{ times }%
\alpha}\otimes\underbrace{1\otimes1\otimes...\otimes1}_{\ell-i\text{ times }%
1}-\underbrace{\alpha\otimes\alpha\otimes...\otimes\alpha}_{i-1\text{ times
}\alpha}\otimes\underbrace{1\otimes1\otimes...\otimes1}_{\ell-\left(
i-1\right)  \text{ times }1}\right) \\
&  =\underbrace{\underbrace{\alpha\otimes\alpha\otimes...\otimes\alpha}%
_{\ell\text{ times }\alpha}}_{=\alpha^{\otimes\ell}}\otimes
\underbrace{\underbrace{1\otimes1\otimes...\otimes1}_{\ell-\ell\text{ times
}1}}_{=\left(  \text{empty tensor product}\right)  }%
-\underbrace{\underbrace{\alpha\otimes\alpha\otimes...\otimes\alpha}_{0\text{
times }\alpha}}_{=\left(  \text{empty tensor product}\right)  }\otimes
\underbrace{\underbrace{1\otimes1\otimes...\otimes1}_{\ell-0\text{ times }1}%
}_{=1^{\otimes\left(  \ell-0\right)  }=1^{\otimes\ell}=\operatorname*{id}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the telescope principle}\right) \\
&  =\alpha^{\otimes\ell}-\operatorname*{id}.
\end{align*}
Since we know that $\sum\limits_{i=1}^{\ell}\alpha_{i}$ is nilpotent, this
yields that $\alpha^{\otimes\ell}-\operatorname*{id}$ is nilpotent. In other
words, $\alpha^{\otimes\ell}$ is unipotent.
\par
But let $\pi$ be the canonical projection $P^{\otimes\ell}\rightarrow
\wedge^{\ell}P$. Then, we have a commutative diagram%
\[%
%TCIMACRO{\TeXButton{tensor vs wedge functoriality}{\xymatrixcolsep{4pc}
%\xymatrix{
%P^{\otimes\ell} \ar[r]^{\alpha^{\otimes\ell}} \ar@{->>}[d]_{\pi}
%& P^{\otimes\ell} \ar@{->>}[d]^{\pi} \\
%\wedge^{\ell} P \ar[r]^{\wedge^{\ell} \alpha} & \wedge^{\ell} P
%}}}%
%BeginExpansion
\xymatrixcolsep{4pc}
\xymatrix{
P^{\otimes\ell} \ar[r]^{\alpha^{\otimes\ell}} \ar@{->>}[d]_{\pi}
& P^{\otimes\ell} \ar@{->>}[d]^{\pi} \\
\wedge^{\ell} P \ar[r]^{\wedge^{\ell} \alpha} & \wedge^{\ell} P
}%
%EndExpansion
\]
(since the canonical projection $P^{\otimes\ell}\rightarrow\wedge^{\ell}P$ is
functorial). Thus, $\left(  \wedge^{\ell}\alpha\right)  \circ\pi=\pi
\circ\alpha^{\otimes\ell}$. Hence,
\[
\left(  \wedge^{\ell}\alpha-\operatorname*{id}\right)  \circ\pi
=\underbrace{\left(  \wedge^{\ell}\alpha\right)  \circ\pi}_{=\pi\circ
\alpha^{\otimes\ell}} - \underbrace{\operatorname*{id}\circ\pi}_{=\pi=\pi
\circ\operatorname*{id}} = \pi\circ\alpha^{\otimes\ell} - \pi\circ
\operatorname*{id} = \pi\circ\left(  \alpha^{\otimes\ell}-\operatorname*{id}%
\right)
\]
(since composition of linear maps is bilinear).
\par
Now, for every $m\in\mathbb{N}$, we have
\begin{equation}
\left(  \wedge^{\ell}\alpha-\operatorname*{id}\right)  ^{m}\circ\pi=\pi
\circ\left(  \alpha^{\otimes\ell}-\operatorname*{id}\right)  ^{m}.
\label{pf.finitary.rhoRho.m}%
\end{equation}
(\textit{Proof of (\ref{pf.finitary.rhoRho.m}):} We will prove
(\ref{pf.finitary.rhoRho.m}) by induction over $m$:
\par
\textit{Induction base:} Comparing $\underbrace{\left(  \wedge^{\ell}%
\alpha-\operatorname*{id}\right)  ^{0}}_{=\operatorname*{id}}\circ\pi=\pi$ and
$\pi\circ\underbrace{\left(  \alpha^{\otimes\ell}-\operatorname*{id}\right)
^{0}}_{=\operatorname*{id}}=\pi$, we obtain $\left(  \wedge^{\ell}%
\alpha-\operatorname*{id}\right)  ^{0}\circ\pi=\pi\circ\left(  \alpha
^{\otimes\ell}-\operatorname*{id}\right)  ^{0}$. Thus,
(\ref{pf.finitary.rhoRho.m}) holds for $m=0$. This completes the induction
base.
\par
\textit{Induction step:} Let $\mu\in\mathbb{N}$. Assume that
(\ref{pf.finitary.rhoRho.m}) holds for $m=\mu$. We must prove that
(\ref{pf.finitary.rhoRho.m}) holds for $m=\mu+1$ as well.
\par
Since (\ref{pf.finitary.rhoRho.m}) holds for $m=\mu$, we have $\left(
\wedge^{\ell}\alpha-\operatorname*{id}\right)  ^{\mu}\circ\pi=\pi\circ\left(
\alpha^{\otimes\ell}-\operatorname*{id}\right)  ^{\mu}$. Now,%
\begin{align*}
\underbrace{\left(  \wedge^{\ell}\alpha-\operatorname*{id}\right)  ^{\mu+1}%
}_{=\left(  \wedge^{\ell}\alpha-\operatorname*{id}\right)  ^{\mu}\circ\left(
\wedge^{\ell}\alpha-\operatorname*{id}\right)  }\circ\pi &  =\left(
\wedge^{\ell}\alpha-\operatorname*{id}\right)  ^{\mu}\circ\underbrace{\left(
\wedge^{\ell}\alpha-\operatorname*{id}\right)  \circ\pi}_{=\pi\circ\left(
\alpha^{\otimes\ell}-\operatorname*{id}\right)  }=\underbrace{\left(
\wedge^{\ell}\alpha-\operatorname*{id}\right)  ^{\mu}\circ\pi}_{=\pi
\circ\left(  \alpha^{\otimes\ell}-\operatorname*{id}\right)  ^{\mu}}%
\circ\left(  \alpha^{\otimes\ell}-\operatorname*{id}\right) \\
&  =\pi\circ\underbrace{\left(  \alpha^{\otimes\ell}-\operatorname*{id}%
\right)  ^{\mu}\circ\left(  \alpha^{\otimes\ell}-\operatorname*{id}\right)
}_{=\left(  \alpha^{\otimes\ell}-\operatorname*{id}\right)  ^{\mu+1}}=\pi
\circ\left(  \alpha^{\otimes\ell}-\operatorname*{id}\right)  ^{\mu+1}.
\end{align*}
Thus, (\ref{pf.finitary.rhoRho.m}) holds for $m=\mu+1$. This completes the
induction step. The induction proof of (\ref{pf.finitary.rhoRho.m}) is thus
complete.)
\par
But since $\alpha^{\otimes\ell}-\operatorname*{id}$ is nilpotent, there exists
some $m\in\mathbb{N}$ such that $\left(  \alpha^{\otimes\ell}%
-\operatorname*{id}\right)  ^{m}=0$. Consider this $m$. By
(\ref{pf.finitary.rhoRho.m}), we have $\left(  \wedge^{\ell}\alpha
-\operatorname*{id}\right)  ^{m}\circ\pi=\pi\circ\underbrace{\left(
\alpha^{\otimes\ell}-\operatorname*{id}\right)  ^{m}}_{=0}=\pi\circ0=0$. Since
$\pi$ is right-cancellable (because $\pi$ is a projection and thus
surjective), this yields $\left(  \wedge^{\ell}\alpha-\operatorname*{id}%
\right)  ^{m}=0$. Hence, $\wedge^{\ell}\alpha-\operatorname*{id}$ is
nilpotent, so that $\wedge^{\ell}\alpha$ is unipotent.
\par
We have thus shown that $\wedge^{\ell}\alpha$ is a unipotent element of
$\operatorname*{GL}\left(  \wedge^{\ell}P\right)  $ whenever $\alpha$ is a
unipotent element of $\operatorname*{GL}\left(  P\right)  $. In other words,
the $\ell$-th exterior power of any unipotent element of $\operatorname*{GL}%
\left(  P\right)  $ is a unipotent element of $\operatorname*{GL}\left(
\wedge^{\ell}P\right)  $, qed.}. Since $\exp a$ is a unipotent element of
$\operatorname*{GL}\left(  P\right)  $, this yields that $\wedge^{\ell}\left(
\exp a\right)  $ is a unipotent element of $\operatorname*{GL}\left(
\wedge^{\ell}P\right)  $ for every $\ell\in\mathbb{N}$. Hence, the logarithm
$\log\left(  \wedge^{\ell}\left(  \exp a\right)  \right)  $ is well-defined
for every $\ell\in\mathbb{N}$.
\end{verlong}

On the other hand, consider the map $\wedge\left(  \exp a\right)  :\wedge
P\rightarrow\wedge P$. This map is an algebra homomorphism (because generally,
if $Q$ and $R$ are two vector spaces, and $f:Q\rightarrow R$ is a linear map,
then $\wedge f:\wedge Q\rightarrow\wedge R$ is an algebra homomorphism) and
identical with the direct sum $\bigoplus\limits_{\ell\in\mathbb{N}}%
\wedge^{\ell}\left(  \exp a\right)  :\bigoplus\limits_{\ell\in\mathbb{N}%
}\wedge^{\ell}P\rightarrow\bigoplus\limits_{\ell\in\mathbb{N}}\wedge^{\ell}P$
of the linear maps $\wedge^{\ell}\left(  \exp a\right)  :\wedge^{\ell
}P\rightarrow\wedge^{\ell}P$.

Since $\wedge\left(  \exp a\right)  =\bigoplus\limits_{\ell\in\mathbb{N}%
}\wedge^{\ell}\left(  \exp a\right)  $, we have $\log\left(  \wedge\left(
\exp a\right)  \right)  =\log\left(  \bigoplus\limits_{\ell\in\mathbb{N}%
}\wedge^{\ell}\left(  \exp a\right)  \right)  =\bigoplus\limits_{\ell
\in\mathbb{N}}\log\left(  \wedge^{\ell}\left(  \exp a\right)  \right)  $
(because logarithms on direct sums are componentwise).\footnote{Note that the
map $\wedge\left(  \exp a\right)  $ needs not be unipotent, but the logarithm
$\log\left(  \wedge\left(  \exp a\right)  \right)  $ nevertheless makes sense
because the map $\wedge\left(  \exp a\right)  $ is a direct sum of unipotent
maps (and thus is locally unipotent).} As a consequence, every $\ell
\in\mathbb{N}$ and every $p_{1},p_{2},...,p_{\ell}\in P$ satisfy $p_{1}\wedge
p_{2}\wedge...\wedge p_{\ell}\in\wedge^{\ell}P$ and thus $\left(  \log\left(
\wedge\left(  \exp a\right)  \right)  \right)  \left(  p_{1}\wedge p_{2}%
\wedge...\wedge p_{\ell}\right)  =\left(  \log\left(  \wedge^{\ell}\left(
\exp a\right)  \right)  \right)  \left(  p_{1}\wedge p_{2}\wedge...\wedge
p_{\ell}\right)  $.

But it is well-known that if $A$ is an algebra and $f:A\rightarrow A$ is an
algebra endomorphism such that $\log f$ is well-defined, then $\log
f:A\rightarrow A$ is a derivation. Applied to $A=\wedge P$ and $f=\wedge
\left(  \exp a\right)  $, this yields that $\log\left(  \wedge\left(  \exp
a\right)  \right)  :\wedge P\rightarrow\wedge P$ is a derivation.

But every $p\in P$ satisfies
\begin{equation}
\left(  \log\left(  \wedge\left(  \exp a\right)  \right)  \right)  \left(
p\right)  =a\rightharpoonup p, \label{pf.finitary.rhoRho.deg1}%
\end{equation}
where $p$ is viewed as an element of $\wedge^{1}P\subseteq\wedge
P$.\ \ \ \ \footnote{\textit{Proof of (\ref{pf.finitary.rhoRho.deg1}):} Let
$p\in P$. Since $\log\left(  \wedge\left(  \exp a\right)  \right)
=\bigoplus\limits_{\ell\in\mathbb{N}}\log\left(  \wedge^{\ell}\left(  \exp
a\right)  \right)  $ and $p\in P=\wedge^{1}P$, we have
\[
\left(  \log\left(  \wedge\left(  \exp a\right)  \right)  \right)  \left(
p\right)  =\left(  \log\underbrace{\left(  \wedge^{1}\left(  \exp a\right)
\right)  }_{=\exp a}\right)  \left(  p\right)  =\underbrace{\left(
\log\left(  \exp a\right)  \right)  }_{=a}\left(  p\right)
=ap=a\rightharpoonup p.
\]
This proves (\ref{pf.finitary.rhoRho.deg1}).}

Now recall the Leibniz identity for derivations. In its general form, it says
that if $A$ is an algebra, $M$ is an $A$-bimodule, and $d:A\rightarrow M$ is a
derivation, then every $\ell\in\mathbb{N}$ and every $p_{1},p_{2},...,p_{\ell
}\in A$ satisfy
\[
d\left(  p_{1}p_{2}...p_{\ell}\right)  =\sum\limits_{k=1}^{\ell}p_{1}%
p_{2}...p_{k-1}d\left(  p_{k}\right)  p_{k+1}p_{k+2}...p_{\ell}.
\]
Applying this to $A=\wedge P$, $M=\wedge P$ and $d=\log\left(  \wedge\left(
\exp a\right)  \right)  $, we conclude that every $\ell\in\mathbb{N}$ and
every $p_{1},p_{2},...,p_{\ell}\in\wedge P$ satisfy%
\[
\left(  \log\left(  \wedge\left(  \exp a\right)  \right)  \right)  \left(
p_{1}p_{2}...p_{\ell}\right)  =\sum\limits_{k=1}^{\ell}p_{1}p_{2}%
...p_{k-1}\left(  \log\left(  \wedge\left(  \exp a\right)  \right)  \right)
\left(  p_{k}\right)  p_{k+1}p_{k+2}...p_{\ell}%
\]
(since $\log\left(  \wedge\left(  \exp a\right)  \right)  :\wedge
P\rightarrow\wedge P$ is a derivation). Thus, every $\ell\in\mathbb{N}$ and
every $p_{1},p_{2},...,p_{\ell}\in P$ satisfy
\begin{align}
\left(  \log\left(  \wedge\left(  \exp a\right)  \right)  \right)  \left(
p_{1}p_{2}...p_{\ell}\right)   &  =\sum\limits_{k=1}^{\ell}p_{1}%
p_{2}...p_{k-1}\underbrace{\left(  \log\left(  \wedge\left(  \exp a\right)
\right)  \right)  \left(  p_{k}\right)  }_{\substack{=a\rightharpoonup
p_{k}\\\text{(by (\ref{pf.finitary.rhoRho.deg1}), applied to }p=p_{k}\text{)}%
}}p_{k+1}p_{k+2}...p_{\ell}\nonumber\\
&  =\sum\limits_{k=1}^{\ell}\underbrace{p_{1}p_{2}...p_{k-1}\left(
a\rightharpoonup p_{k}\right)  p_{k+1}p_{k+2}...p_{\ell}}_{\substack{=p_{1}%
\wedge p_{2}\wedge...\wedge p_{k-1}\wedge\left(  a\rightharpoonup
p_{k}\right)  \wedge p_{k+1}\wedge p_{k+2}\wedge...\wedge p_{\ell
}\\\text{(since the multiplication in }\wedge P\text{ is given by the wedge
product)}}}\nonumber\\
&  =\sum\limits_{k=1}^{\ell}p_{1}\wedge p_{2}\wedge...\wedge p_{k-1}%
\wedge\left(  a\rightharpoonup p_{k}\right)  \wedge p_{k+1}\wedge
p_{k+2}\wedge...\wedge p_{\ell}\nonumber\\
&  =\left(  \rho_{P,\ell}\left(  a\right)  \right)  \left(  p_{1}\wedge
p_{2}\wedge...\wedge p_{\ell}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{def.finitary.rho})}\right)  . \label{pf.finitary.rhoRho.calc}%
\end{align}
On the other hand, every $\ell\in\mathbb{N}$ and every $p_{1},p_{2}%
,...,p_{\ell}\in P$ satisfy
\begin{align*}
&  \left(  \log\left(  \wedge\left(  \exp a\right)  \right)  \right)
\underbrace{\left(  p_{1}p_{2}...p_{\ell}\right)  }_{\substack{=p_{1}\wedge
p_{2}\wedge...\wedge p_{\ell}\\\text{(since the multiplication in }\wedge
P\text{ is given by the wedge product)}}}\\
&  =\left(  \log\left(  \wedge\left(  \exp a\right)  \right)  \right)  \left(
p_{1}\wedge p_{2}\wedge...\wedge p_{\ell}\right)  =\left(  \log\left(
\wedge^{\ell}\left(  \exp a\right)  \right)  \right)  \left(  p_{1}\wedge
p_{2}\wedge...\wedge p_{\ell}\right)  .
\end{align*}
Compared with (\ref{pf.finitary.rhoRho.calc}), this yields%
\[
\left(  \rho_{P,\ell}\left(  a\right)  \right)  \left(  p_{1}\wedge
p_{2}\wedge...\wedge p_{\ell}\right)  =\left(  \log\left(  \wedge^{\ell
}\left(  \exp a\right)  \right)  \right)  \left(  p_{1}\wedge p_{2}%
\wedge...\wedge p_{\ell}\right)
\]
for every $\ell\in\mathbb{N}$ and every $p_{1},p_{2},...,p_{\ell}\in P$.

Now fix $\ell\in\mathbb{N}$. We know that
\[
\left(  \rho_{P,\ell}\left(  a\right)  \right)  \left(  p_{1}\wedge
p_{2}\wedge...\wedge p_{\ell}\right)  =\left(  \log\left(  \wedge^{\ell
}\left(  \exp a\right)  \right)  \right)  \left(  p_{1}\wedge p_{2}%
\wedge...\wedge p_{\ell}\right)
\]
for every $p_{1},p_{2},...,p_{\ell}\in P$. Since the vector space
$\wedge^{\ell}P$ is spanned by elements of the form $p_{1}\wedge p_{2}%
\wedge...\wedge p_{\ell}$ with $p_{1},p_{2},...,p_{\ell}\in P$, this yields
that the two linear maps $\rho_{P,\ell}\left(  a\right)  $ and $\log\left(
\wedge^{\ell}\left(  \exp a\right)  \right)  $ are equal to each other on a
spanning set of the vector space $\wedge^{\ell}P$. Therefore, these two maps
must be identical (because if two linear maps are equal to each other on a
spanning set of their domain, then they must always be identical). In other
words, $\rho_{P,\ell}\left(  a\right)  =\log\left(  \wedge^{\ell}\left(  \exp
a\right)  \right)  $. Exponentiating this equality, we obtain $\exp\left(
\rho_{P,\ell}\left(  a\right)  \right)  =\wedge^{\ell}\left(  \exp a\right)
$. This proves Theorem \ref{thm.finitary.rhoRho}.

\subsubsection{\label{subsubsect.schur2.reduct}Reduction to fermions}

We are now going to reduce Theorem \ref{thm.schur} to a ``purely fermionic''
statement -- a statement (Theorem \ref{thm.schur.fermi}) not involving the
bosonic space $\mathcal{B}$ or the Boson-Fermion correspondence $\sigma$ in
any way. We will later (Subsection \ref{subsubsect.skewschur}) generalize this
statement, and yet later prove the generalization.

First, a definition:

\begin{definition}
Let $\mathbf{R}$ (not to be confused with the field $\mathbb{R}$) be a
commutative $\mathbb{Q}$-algebra. We denote by $\mathcal{A}_{\mathbf{R}}$ the
Heisenberg algebra defined over the ground ring $\mathbf{R}$ in lieu of
$\mathbb{C}$. We denote by $\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }$ the
$\mathcal{A}_{\mathbf{R}}$-module $\mathcal{B}^{\left(  0\right)  }$ defined
over the ground ring $\mathbf{R}$ in lieu of $\mathbb{C}$. We denote by
$\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }$ the $\mathcal{A}_{\mathbf{R}}%
$-module $\mathcal{F}^{\left(  0\right)  }$ defined over the ground ring
$\mathbf{R}$ in lieu of $\mathbb{C}$. We denote by $\sigma_{\mathbf{R}}$ the
map $\sigma$ defined over the ground ring $\mathbf{R}$ in lieu of $\mathbb{C}%
$. (This $\sigma_{\mathbf{R}}$ is thus a graded $\mathcal{A}_{\mathbf{R}}%
$-module homomorphism $\mathcal{B}_{\mathbf{R}}\rightarrow\mathcal{F}%
_{\mathbf{R}}$, where $\mathcal{B}_{\mathbf{R}}$ and $\mathcal{F}_{\mathbf{R}%
}$ are the $\mathcal{A}_{\mathbf{R}}$-modules $\mathcal{B}$ and $\mathcal{F}$
defined over the ground ring $\mathbf{R}$ in lieu of $\mathbb{C}$.)
\end{definition}

Next, some preparations:

\begin{proposition}
\label{prop.schur.fermi.welldef}Let $\mathbf{R}$ be a commutative $\mathbb{Q}%
$-algebra. Let $y_{1},y_{2},y_{3},...$ be some elements of $\mathbf{R}$.

\textbf{(a)} Let $M$ be a $\mathbb{Z}$-graded $\mathcal{A}_{\mathbf{R}}%
$-module concentrated in nonpositive degrees (i. e., satisfying $M\left[
n\right]  =0$ for all positive integers $n$). The map $\exp\left(  y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  :M\rightarrow M$ is well-defined, in
the following sense: For every $m\in M$, expanding the expression $\exp\left(
y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  m$ yields an infinite sum with
only finitely many nonzero addends.

\textbf{(b)} Let $M$ and $N$ be two $\mathbb{Z}$-graded $\mathcal{A}%
_{\mathbf{R}}$-modules concentrated in nonpositive degrees. Let $\eta
:M\rightarrow N$ be an $\mathcal{A}_{\mathbf{R}}$-module homomorphism. Then,%
\[
\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\circ\eta=\eta\circ\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  \right)
\]
as maps from $M$ to $N$.

\textbf{(c)} Consider the $\mathbb{Z}$-graded $\mathcal{A}_{\mathbf{R}}%
$-module $\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }$. This $\mathbb{Z}%
$-graded $\mathcal{A}_{\mathbf{R}}$-module $\mathcal{F}_{\mathbf{R}}^{\left(
0\right)  }$ is concentrated in nonpositive degrees. Hence, by Theorem
\ref{thm.schur.fermi}, the map $\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  :\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }%
\rightarrow\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }$ is well-defined.
Thus, $\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $ is well-defined
for every $0$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $.
\end{proposition}

\textit{Proof of Proposition \ref{prop.schur.fermi.welldef}.} \textbf{(a)} Let
$m\in M$. We will prove that expanding the expression $\exp\left(  y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  m$ yields an infinite sum with only
finitely many nonzero terms.

Since $M$ is $\mathbb{Z}$-graded, we can write $m$ in the form $m=\sum
\limits_{n\in\mathbb{Z}}m_{n}$ for a family $\left(  m_{n}\right)
_{n\in\mathbb{Z}}$ of elements of $M$ which satisfy $\left(  m_{n}\in M\left[
n\right]  \text{ for every }n\in\mathbb{Z}\right)  $ and $\left(
m_{n}=0\text{ for all but finitely many }n\in\mathbb{Z}\right)  $. Consider
this family $\left(  m_{n}\right)  _{n\in\mathbb{Z}}$. We know that $m_{n}=0$
for all but finitely many $n\in\mathbb{Z}$. In other words, there exists a
finite subset $I$ of $\mathbb{Z}$ such that every $n\in\mathbb{Z}\setminus I$
satisfies $m_{n}=0$. Consider this $I$. Let $s$ be an integer which is smaller
than every element of $I$. (Such an $s$ exists since $I$ is finite.) Then,%
\begin{equation}
fm=0\ \ \ \ \ \ \ \ \ \ \text{for every integer }q\geq-s\text{ and every }f\in
U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)  \left[  q\right]
\label{pf.schur.fermi.welldef.fm=0}%
\end{equation}
(where $U_{\mathbf{R}}$ means ``enveloping algebra over the ground ring
$\mathbf{R}$ '').\ \ \ \ \footnote{\textit{Proof of
(\ref{pf.schur.fermi.welldef.fm=0}):} Let $q\geq-s$ be an integer, and let
$f\in U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)  \left[  q\right]
$. Since $s$ is smaller than every element of $I$, we have $s<n$ for every
$n\in I$. Thus, $q\geq-\underbrace{s}_{<n}>-n$ for every $n\in I$, so that
$q+n>0$ for every $n\in I$ and thus $M\left[  q+n\right]  =0$ for every $n\in
I$ (since $M$ is concentrated in nonpositive degrees).
\par
Notice that $M$ is a graded $\mathcal{A}_{\mathbf{R}}$-module, thus a graded
$U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)  $-module. But%
\[
m=\sum\limits_{n\in\mathbb{Z}}m_{n}=\sum\limits_{n\in I}m_{n}+\sum
\limits_{n\in\mathbb{Z}\setminus I}\underbrace{m_{n}}%
_{\substack{=0\\\text{(since }n\in\mathbb{Z}\setminus I\text{)}}%
}=\sum\limits_{n\in I}m_{n}+\underbrace{\sum\limits_{n\in\mathbb{Z}\setminus
I}0}_{=0}=\sum\limits_{n\in I}m_{n},
\]
so that%
\[
fm=f\sum\limits_{n\in I}m_{n}=\sum\limits_{n\in I}\underbrace{fm_{n}%
}_{\substack{\in M\left[  q+n\right]  \\\text{(since }f\in U_{\mathbf{R}%
}\left(  \mathcal{A}_{\mathbf{R}}\right)  \left[  q\right]  \text{ and }%
m_{n}\in M\left[  n\right]  \text{,}\\\text{and since }M\text{ is a graded
}U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)  \text{-module)}}%
}\in\sum\limits_{n\in I}\underbrace{M\left[  q+n\right]  }%
_{\substack{=0\\\text{(since }n\in I\text{)}}}=\sum\limits_{n\in I}0=0,
\]
so that $fm=0$, qed.}

Expanding the expression $\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  m$, we obtain%
\begin{align*}
&  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  m\\
&  =\sum\limits_{i=0}^{\infty}\dfrac{1}{i!}\left(  \underbrace{y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...}_{=\sum\limits_{j\in\left\{
1,2,3,...\right\}  }y_{j}a_{j}}\right)  ^{i}m=\sum\limits_{i=0}^{\infty}%
\dfrac{1}{i!}\underbrace{\left(  \sum\limits_{j\in\left\{  1,2,3,...\right\}
}y_{j}a_{j}\right)  ^{i}}_{=\sum\limits_{\left(  j_{1},j_{2},...,j_{i}\right)
\in\left\{  1,2,3,...\right\}  ^{i}}y_{j_{1}}y_{j_{2}}...y_{j_{i}}a_{j_{1}%
}a_{j_{2}}...a_{j_{i}}}m\\
&  =\sum\limits_{i=0}^{\infty}\dfrac{1}{i!}\sum\limits_{\left(  j_{1}%
,j_{2},...,j_{i}\right)  \in\left\{  1,2,3,...\right\}  ^{i}}y_{j_{1}}%
y_{j_{2}}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}m\\
&  =\sum\limits_{\substack{i\in\mathbb{N};\\\left(  j_{1},j_{2},...,j_{i}%
\right)  \in\left\{  1,2,3,...\right\}  ^{i}}}\dfrac{1}{i!}y_{j_{1}}y_{j_{2}%
}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}m.
\end{align*}
But this infinite sum has only finitely many nonzero
addends\footnote{\textit{Proof.} Let $i\in\mathbb{N}$ and $\left(  j_{1}%
,j_{2},...,j_{i}\right)  \in\left\{  1,2,3,...\right\}  ^{i}$ be such that
$\dfrac{1}{i!}y_{j_{1}}y_{j_{2}}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}%
}m\neq0$. Since $a_{j_{k}}\in U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}%
}\right)  \left[  j_{k}\right]  $ for every $k\in\left\{  1,2,...,i\right\}
$, we have
\begin{align*}
a_{j_{1}}a_{j_{2}}...a_{j_{i}}  &  \in\left(  U_{\mathbf{R}}\left(
\mathcal{A}_{\mathbf{R}}\right)  \left[  j_{1}\right]  \right)  \left(
U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)  \left[  j_{2}\right]
\right)  ...\left(  U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)
\left[  j_{i}\right]  \right) \\
&  \subseteq U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)  \left[
j_{1}+j_{2}+...+j_{i}\right]  ,
\end{align*}
so that%
\[
\dfrac{1}{i!}y_{j_{1}}y_{j_{2}}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}%
\in\dfrac{1}{i!}y_{j_{1}}y_{j_{2}}...y_{j_{i}}U_{\mathbf{R}}\left(
\mathcal{A}_{\mathbf{R}}\right)  \left[  j_{1}+j_{2}+...+j_{i}\right]
\subseteq U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)  \left[
j_{1}+j_{2}+...+j_{i}\right]  .
\]
Hence, if $j_{1}+j_{2}+...+j_{i}\geq-s$, then $\dfrac{1}{i!}y_{j_{1}}y_{j_{2}%
}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}m=0$ (by
(\ref{pf.schur.fermi.welldef.fm=0}), applied to $f=\dfrac{1}{i!}y_{j_{1}%
}y_{j_{2}}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}$ and $q=j_{1}%
+j_{2}+...+j_{i}$), contradicting $\dfrac{1}{i!}y_{j_{1}}y_{j_{2}}...y_{j_{i}%
}a_{j_{1}}a_{j_{2}}...a_{j_{i}}m\neq0$. As a consequence, we cannot have
$j_{1}+j_{2}+...+j_{i}\geq-s$. We must thus have $j_{1}+j_{2}+...+j_{i}<-s$.
\par
Now forget that we fixed $i$ and $\left(  j_{1},j_{2},...,j_{i}\right)  $. We
thus have shown that every $i\in\mathbb{N}$ and $\left(  j_{1},j_{2}%
,...,j_{i}\right)  \in\left\{  1,2,3,...\right\}  ^{i}$ such that $\dfrac
{1}{i!}y_{j_{1}}y_{j_{2}}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}m\neq0$
must satisfy $j_{1}+j_{2}+...+j_{i}<-s$. Since there are only finitely many
pairs $\left(  i,\left(  j_{1},j_{2},...,j_{i}\right)  \right)  $ of
$i\in\mathbb{N}$ and $\left(  j_{1},j_{2},...,j_{i}\right)  \in\left\{
1,2,3,...\right\}  ^{i}$ satisfying $j_{1}+j_{2}+...+j_{i}<-s$, this yields
that there are only finitely many pairs $\left(  i,\left(  j_{1}%
,j_{2},...,j_{i}\right)  \right)  $ of $i\in\mathbb{N}$ and $\left(
j_{1},j_{2},...,j_{i}\right)  \in\left\{  1,2,3,...\right\}  ^{i}$ satisfying
$\dfrac{1}{i!}y_{j_{1}}y_{j_{2}}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}%
}m\neq0$. In other words, the infinite sum $\sum\limits_{\substack{i\in
\mathbb{N};\\\left(  j_{1},j_{2},...,j_{i}\right)  \in\left\{
1,2,3,...\right\}  ^{i}}}\dfrac{1}{i!}y_{j_{1}}y_{j_{2}}...y_{j_{i}}a_{j_{1}%
}a_{j_{2}}...a_{j_{i}}m$ has only finitely many nonzero addends, qed.}. Thus,
we have shown that for every $m\in M$, expanding the expression $\exp\left(
y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  m$ yields an infinite sum with
only finitely many nonzero addends. This proves Proposition
\ref{prop.schur.fermi.welldef} \textbf{(a)}.

\begin{verlong}
\textbf{(b)} Just as in the proof of Proposition
\ref{prop.schur.fermi.welldef} \textbf{(a)} above, we can show that%
\begin{equation}
\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  m=\sum
\limits_{\substack{i\in\mathbb{N};\\\left(  j_{1},j_{2},...,j_{i}\right)
\in\left\{  1,2,3,...\right\}  ^{i}}}\dfrac{1}{i!}y_{j_{1}}y_{j_{2}%
}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}m\ \ \ \ \ \ \ \ \ \ \text{for
every }m\in M, \label{pf.schur.fermi.welldef.b.1}%
\end{equation}
with the infinite sum $\sum\limits_{\substack{i\in\mathbb{N};\\\left(
j_{1},j_{2},...,j_{i}\right)  \in\left\{  1,2,3,...\right\}  ^{i}}}\dfrac
{1}{i!}y_{j_{1}}y_{j_{2}}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}m$ having
only finitely many nonzero addends (for every fixed $m$). Similarly,%
\begin{equation}
\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  n=\sum
\limits_{\substack{i\in\mathbb{N};\\\left(  j_{1},j_{2},...,j_{i}\right)
\in\left\{  1,2,3,...\right\}  ^{i}}}\dfrac{1}{i!}y_{j_{1}}y_{j_{2}%
}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}n\ \ \ \ \ \ \ \ \ \ \text{for
every }n\in N. \label{pf.schur.fermi.welldef.b.2}%
\end{equation}


Now, let $m\in M$. Since $\eta$ is an $\mathcal{A}_{\mathbf{R}}$-module
homomorphism, $\eta$ must also be an $U_{\mathbf{R}}\left(  \mathcal{A}%
_{\mathbf{R}}\right)  $-module homomorphism (since every $\mathcal{A}%
_{\mathbf{R}}$-module homomorphism is an $U_{\mathbf{R}}\left(  \mathcal{A}%
_{\mathbf{R}}\right)  $-module homomorphism). Thus,
\begin{equation}
\eta\left(  gm\right)  =g\cdot\eta\left(  m\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }g\in U_{\mathbf{R}}\left(  \mathcal{A}%
_{\mathbf{R}}\right)  . \label{pf.schur.fermi.welldef.b.3}%
\end{equation}
Since $\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  $ is not (in
general) an element of $U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)
$, we cannot directly apply this to $g=\exp\left(  y_{1}a_{1}+y_{2}a_{2}%
+y_{3}a_{3}+...\right)  $. However, we have%
\begin{align*}
&  \left(  \eta\circ\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  \right)  \right)  \left(  m\right) \\
&  =\eta\left(  \underbrace{\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  m}_{=\sum\limits_{\substack{i\in\mathbb{N};\\\left(
j_{1},j_{2},...,j_{i}\right)  \in\left\{  1,2,3,...\right\}  ^{i}}}\dfrac
{1}{i!}y_{j_{1}}y_{j_{2}}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}m}\right)
\\
&  =\eta\left(  \sum\limits_{\substack{i\in\mathbb{N};\\\left(  j_{1}%
,j_{2},...,j_{i}\right)  \in\left\{  1,2,3,...\right\}  ^{i}}}\dfrac{1}%
{i!}y_{j_{1}}y_{j_{2}}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}m\right) \\
&  =\sum\limits_{\substack{i\in\mathbb{N};\\\left(  j_{1},j_{2},...,j_{i}%
\right)  \in\left\{  1,2,3,...\right\}  ^{i}}}\dfrac{1}{i!}y_{j_{1}}y_{j_{2}%
}...y_{j_{i}}\underbrace{\eta\left(  a_{j_{1}}a_{j_{2}}...a_{j_{i}}m\right)
}_{\substack{=a_{j_{1}}a_{j_{2}}...a_{j_{i}}\eta\left(  m\right)  \\\text{(by
(\ref{pf.schur.fermi.welldef.b.3}), applied to }g=a_{j_{1}}a_{j_{2}%
}...a_{j_{i}}\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\eta\text{ is }\mathbf{R}\text{-linear, while the infinite sum
}\sum\limits_{\substack{i\in\mathbb{N};\\\left(  j_{1},j_{2},...,j_{i}\right)
\in\left\{  1,2,3,...\right\}  ^{i}}}\dfrac{1}{i!}y_{j_{1}}y_{j_{2}%
}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}m\\
\text{has only finitely many nonzero addends}%
\end{array}
\right) \\
&  =\sum\limits_{\substack{i\in\mathbb{N};\\\left(  j_{1},j_{2},...,j_{i}%
\right)  \in\left\{  1,2,3,...\right\}  ^{i}}}\dfrac{1}{i!}y_{j_{1}}y_{j_{2}%
}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}\eta\left(  m\right) \\
&  =\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \left(
\eta\left(  m\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since (\ref{pf.schur.fermi.welldef.b.2}) (applied to }n=\eta\left(
m\right)  \text{) yields}\\
\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \left(  \eta\left(
m\right)  \right)  =\sum\limits_{\substack{i\in\mathbb{N};\\\left(
j_{1},j_{2},...,j_{i}\right)  \in\left\{  1,2,3,...\right\}  ^{i}}}\dfrac
{1}{i!}y_{j_{1}}y_{j_{2}}...y_{j_{i}}a_{j_{1}}a_{j_{2}}...a_{j_{i}}\eta\left(
m\right)
\end{array}
\right) \\
&  =\left(  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
\right)  \circ\eta\right)  \left(  m\right)  .
\end{align*}


Now forget that we fixed $m$. We thus have proven that every $m\in M$
satisfies%
\[
\left(  \eta\circ\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  \right)  \right)  \left(  m\right)  =\left(  \left(
\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \circ
\eta\right)  \left(  m\right)  .
\]
In other words, $\eta\circ\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  \right)  =\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}%
+y_{3}a_{3}+...\right)  \right)  \circ\eta$. This proves Proposition
\ref{prop.schur.fermi.welldef} \textbf{(b)}.
\end{verlong}

\begin{vershort}
\textbf{(b)} In order to prove Proposition \ref{prop.schur.fermi.welldef}
\textbf{(b)}, we must clearly show that%
\begin{equation}
\eta\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  m\right)
=\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\eta\left(
m\right)  \label{pf.schur.fermi.welldef.b.short.1}%
\end{equation}
for every $m\in M$.

Fix $m\in M$. Since $\eta$ is an $\mathcal{A}_{\mathbf{R}}$-module
homomorphism, $\eta$ must also be an $U_{\mathbf{R}}\left(  \mathcal{A}%
_{\mathbf{R}}\right)  $-module homomorphism (since every $\mathcal{A}%
_{\mathbf{R}}$-module homomorphism is an $U_{\mathbf{R}}\left(  \mathcal{A}%
_{\mathbf{R}}\right)  $-module homomorphism). Thus,
\[
\eta\left(  gm\right)  =g\cdot\eta\left(  m\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }g\in U_{\mathbf{R}}\left(  \mathcal{A}%
_{\mathbf{R}}\right)  .
\]
If $\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  $ was an element
of $U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)  $, then we could
apply this to $g=\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  $
and conclude (\ref{pf.schur.fermi.welldef.b.short.1}) immediately.
Unfortunately, $\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  $ is
not an element of $U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)  $,
but this problem is easy to amend: By Proposition
\ref{prop.schur.fermi.welldef} \textbf{(a)}, we can find a finite partial sum
$g$ of the expanded power series $\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  $ satisfying%
\begin{align*}
\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  m  &
=gm\ \ \ \ \ \ \ \ \ \ \text{and}\\
\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\eta\left(
m\right)   &  =g\cdot\eta\left(  m\right)  .
\end{align*}
Consider such a $g$. Since $g$ is only a finite partial sum, we have $g\in
U_{\mathbf{R}}\left(  \mathcal{A}_{\mathbf{R}}\right)  $, and thus
$\eta\left(  gm\right)  =g\cdot\eta\left(  m\right)  $. Hence,%
\begin{align*}
\eta\left(  \underbrace{\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}%
+...\right)  m}_{=gm}\right)   &  =\eta\left(  gm\right)  =g\cdot\eta\left(
m\right) \\
&  =\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\eta\left(
m\right)  ,
\end{align*}
so that (\ref{pf.schur.fermi.welldef.b.short.1}) is proven. Thus, Proposition
\ref{prop.schur.fermi.welldef} \textbf{(b)} is proven.
\end{vershort}

\textbf{(c)} This is obvious.

Let us make a remark which we will only use in the ``finitary'' version of our
proof of Theorem \ref{thm.schur.fermi}. First, a definition:

\begin{definition}
For every commutative ring $\mathbf{R}$, let $\mathcal{A}_{+\mathbf{R}}$ be
the Lie algebra $\mathcal{A}_{+}$ defined for the ground ring $\mathbf{R}$
instead of $\mathbb{C}$.
\end{definition}

Now, it is easy to see that Proposition \ref{prop.schur.fermi.welldef} holds
with $\mathcal{A}_{\mathbf{R}}$ replaced by $\mathcal{A}_{+\mathbf{R}}$. We
will only use the analogues of parts \textbf{(a)} and \textbf{(b)}:

\begin{proposition}
\label{prop.schur.fermi.welldef.A+}Let $\mathbf{R}$ be a commutative
$\mathbb{Q}$-algebra. Let $y_{1},y_{2},y_{3},...$ be some elements of
$\mathbf{R}$.

\textbf{(a)} Let $M$ be a $\mathbb{Z}$-graded $\mathcal{A}_{+\mathbf{R}}%
$-module concentrated in nonpositive degrees (i. e., satisfying $M\left[
n\right]  =0$ for all positive integers $n$). The map $\exp\left(  y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  :M\rightarrow M$ is well-defined, in
the following sense: For every $m\in M$, expanding the expression $\exp\left(
y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  m$ yields an infinite sum with
only finitely many nonzero addends.

\textbf{(b)} Let $M$ and $N$ be two $\mathbb{Z}$-graded $\mathcal{A}%
_{+\mathbf{R}}$-modules concentrated in nonpositive degrees. Let
$\eta:M\rightarrow N$ be an $\mathcal{A}_{+\mathbf{R}}$-module homomorphism.
Then,%
\[
\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\circ\eta=\eta\circ\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  \right)
\]
as maps from $M$ to $N$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.schur.fermi.welldef.A+}.} In order to
obtain proofs of Proposition \ref{prop.schur.fermi.welldef.A+}, it is enough
to simply replace $\mathcal{A}_{\mathbf{R}}$ by $\mathcal{A}_{+\mathbf{R}}$
throughout the proof of parts \textbf{(a)} and \textbf{(b)} of Proposition
\ref{prop.schur.fermi.welldef}.

Now, let us state the ``fermionic'' version of Theorem \ref{thm.schur}:

\begin{theorem}
\label{thm.schur.fermi}Let $\mathbf{R}$ be a commutative $\mathbb{Q}$-algebra.
Let $y_{1},y_{2},y_{3},...$ be some elements of $\mathbf{R}$. Denote by $y$
the family $\left(  y_{1},y_{2},y_{3},...\right)  $. Let $\left(  i_{0}%
,i_{1},i_{2},...\right)  $ be a $0$-degression.

The $\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)  $-coordinate of
$\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $ (this is a
well-defined element of $\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }$ due to
Proposition \ref{prop.schur.fermi.welldef} \textbf{(c)}) with respect to the
basis\footnotemark\ $\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\right)  _{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression}}$ of $\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }$
equals $S_{\left(  i_{k}+k\right)  _{k\geq0}}\left(  y\right)  $. (Here, we
are using the fact that $\left(  i_{k}+k\right)  _{k\geq0}$ is a partition for
every $0$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $. This follows
from Proposition \ref{prop.glinf.wedge.grading}, applied to $m=0$.)
\end{theorem}

\footnotetext{Here, ``basis'' means ``$\mathbf{R}$-module basis'', not
``$\mathbb{C}$-vector space basis''.}Let us see how this yields Theorem
\ref{thm.schur}:

\textit{Proof of Theorem \ref{thm.schur} using Theorem \ref{thm.schur.fermi}.}
Fix a $0$-degression $\left(  i_{0},i_{1},i_{2},...\right)  $; then,
$i_{0}>i_{1}>i_{2}>...$ and $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\in\mathcal{F}^{\left(  0\right)  }$. Let $\lambda$ be the partition
$\left(  i_{0}+0,i_{1}+1,i_{2}+2,...\right)  $.

Denote the element $\sigma^{-1}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  \in\mathcal{B}^{\left(  0\right)  }$ by $P\left(
x\right)  $. We need to show that $P\left(  x\right)  =S_{\lambda}\left(
x\right)  $.

From now on, we let $y$ denote another countable family of indeterminates
$\left(  y_{1},y_{2},y_{3},...\right)  $ (rather than a finite family like the
$\left(  y_{1},y_{2},...,y_{N}\right)  $ of Definition \ref{def.schur.y}).
Thus, whenever $Q$ is a polynomial in countably many indeterminates, $Q\left(
y\right)  $ will mean $Q\left(  y_{1},y_{2},y_{3},...\right)  $.

Let $\mathbf{R}$ be the polynomial ring $\mathbb{C}\left[  y_{1},y_{2}%
,y_{3},...\right]  $. Then, $y$ is a family of elements of $\mathbf{R}$.

By the definition of $\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }$, we have
$\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }=\mathbf{R}\left[  x_{1}%
,x_{2},x_{3},...\right]  $ as a vector space, so that $\mathcal{B}%
_{\mathbf{R}}^{\left(  0\right)  }=\left(  \mathbb{C}\left[  y_{1},y_{2}%
,y_{3},...\right]  \right)  \left[  x_{1},x_{2},x_{3},...\right]  $ as a
vector space. Let us denote by $1\in\mathcal{B}^{\left(  0\right)  }$ the
unity of the algebra $\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $.
Clearly, $\mathcal{B}^{\left(  0\right)  }\subseteq\mathcal{B}_{\mathbf{R}%
}^{\left(  0\right)  }$, and thus $1\in\mathcal{B}^{\left(  0\right)
}\subseteq\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }$.

We still let $x$ denote the whole collection of variables $\left(  x_{1}%
,x_{2},x_{3},...\right)  $. Also, let $x+y$ denote the family $\left(
x_{1}+y_{1},x_{2}+y_{2},x_{3}+y_{3},...\right)  $ of elements of
$\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }$.

Recall the $\mathbb{C}$-bilinear form $\left(  \cdot,\cdot\right)  :F\times
F\rightarrow\mathbb{C}$ defined in Proposition \ref{prop.A.contravariantform}.
Since $F=\widetilde{F}=\mathcal{B}^{\left(  0\right)  }$ (as vector spaces),
this form $\left(  \cdot,\cdot\right)  $ is a $\mathbb{C}$-bilinear form
$\mathcal{B}^{\left(  0\right)  }\times\mathcal{B}^{\left(  0\right)
}\rightarrow\mathbb{C}$. Since the definition of the form did not depend of
the ground ring, we can analogously define an $\mathbf{R}$-bilinear form
$\left(  \cdot,\cdot\right)  :\mathcal{B}_{\mathbf{R}}^{\left(  0\right)
}\times\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }\rightarrow\mathbf{R}$.
The restriction of this latter $\mathbf{R}$-bilinear form $\left(  \cdot
,\cdot\right)  :\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }\times
\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }\rightarrow\mathbf{R}$ to
$\mathcal{B}^{\left(  0\right)  }\times\mathcal{B}^{\left(  0\right)  }$ is
clearly the former $\mathbb{C}$-bilinear form $\left(  \cdot,\cdot\right)
:\mathcal{B}^{\left(  0\right)  }\times\mathcal{B}^{\left(  0\right)
}\rightarrow\mathbb{C}$; therefore we will use the same notation for these two forms.

In the following, elements of $\mathcal{B}_{\mathbf{R}}^{\left(  0\right)
}=\mathbf{R}\left[  x_{1},x_{2},x_{3},...\right]  $ will be considered as
polynomials in the variables $x_{1},x_{2},x_{3},...$ over the ring
$\mathbf{R}$, and not as polynomials in the variables $x_{1},x_{2}%
,x_{3},...,y_{1},y_{2},y_{3},...$ over the field $\mathbb{C}$. Hence, for an
$R\in\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }$, the notation $R\left(
0,0,0,...\right)  $ will mean the result of substituting $0$ for the variables
$x_{1},x_{2},x_{3},...$ in $R$ (but the variables $y_{1},y_{2},y_{3},...$ will
stay unchanged!). We will abbreviate $R\left(  0,0,0,...\right)  $ by
$R\left(  0\right)  $.

Every polynomial $R\in\mathcal{B}^{\left(  0\right)  }$ satisfies:%
\begin{equation}
R\left(  0\right)  =\left(
\begin{array}
[c]{c}%
\text{the }\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)
\text{-coordinate of }\sigma\left(  R\right) \\
\text{with respect to the basis }\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge
v_{j_{2}}\wedge...\right)  _{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression}}\text{ of }\mathcal{F}^{\left(  0\right)  }%
\end{array}
\right)  \label{pf.schur.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.schur.1}).} Let $R\in\mathcal{B}^{\left(
0\right)  }$. Thus, $R\in\mathcal{B}^{\left(  0\right)  }=\widetilde{F}$.
\par
Let $p_{0,\mathcal{B}}$ be the canonical projection of the graded space
$\mathcal{B}^{\left(  0\right)  }$ onto its $0$-th homogeneous component
$\mathcal{B}^{\left(  0\right)  }\left[  0\right]  =\mathbb{C}\cdot1$, and let
$p_{0,\mathcal{F}}$ be the canonical projection of the graded space
$\mathcal{F}^{\left(  0\right)  }$ onto its $0$-th homogeneous component
$\mathcal{F}^{\left(  0\right)  }\left[  0\right]  =\mathbb{C}\psi_{0}$. Since
$\sigma_{0}:\mathcal{B}^{\left(  0\right)  }\rightarrow\mathcal{F}^{\left(
0\right)  }$ is a graded homomorphism, $\sigma_{0}$ commutes with the
projections on the $0$-th graded components; in other words, $\sigma_{0}\circ
p_{0,\mathcal{B}}=p_{0,\mathcal{F}}\circ\sigma_{0}$. Now, we know that
$p_{0,\mathcal{B}}\left(  R\right)  =R\left(  0\right)  \cdot1$ (since
$\mathcal{B}=\widetilde{F}=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $),
and thus $\left(  \sigma_{0}\circ p_{0,\mathcal{B}}\right)  \left(  R\right)
=\sigma_{0}\left(  \underbrace{p_{0,\mathcal{B}}\left(  R\right)  }_{=R\left(
1\right)  \cdot1}\right)  =\sigma_{0}\left(  R\left(  0\right)  \cdot1\right)
=R\left(  0\right)  \cdot\underbrace{\sigma_{0}\left(  1\right)  }_{=\psi_{0}%
}=R\left(  0\right)  \psi_{0}$.
\par
On the other hand, let $\kappa$ denote the $\left(  v_{0}\wedge v_{-1}\wedge
v_{-2}\wedge...\right)  $-coordinate of $\sigma\left(  R\right)  $ with
respect to the basis $\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\right)  _{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression}}$ of $\mathcal{F}^{\left(  0\right)  }$. Then, the
projection of $\sigma\left(  R\right)  $ onto the $0$-th graded component
$\mathcal{F}^{\left(  0\right)  }\left[  0\right]  $ of $\mathcal{F}^{\left(
0\right)  }$ is $\kappa\cdot v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...$
(because the basis $\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\right)  _{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression}}$ of $\mathcal{F}^{\left(  0\right)  }$ is a graded
basis, and the $0$-th graded component $\mathcal{F}^{\left(  0\right)
}\left[  0\right]  $ of $\mathcal{F}^{\left(  0\right)  }$ is spanned by
$\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)  $). In other words,
$p_{0,\mathcal{F}}\left(  \sigma\left(  R\right)  \right)  =\kappa
\cdot\underbrace{v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...}_{=\psi_{0}}%
=\kappa\psi_{0}$. Hence,%
\[
R\left(  0\right)  \psi_{0}=\underbrace{\left(  \sigma_{0}\circ
p_{0,\mathcal{B}}\right)  }_{=p_{0,\mathcal{F}}\circ\sigma_{0}}\left(
R\right)  =\left(  p_{0,\mathcal{F}}\circ\sigma_{0}\right)  \left(  R\right)
=p_{0,\mathcal{F}}\left(  \sigma\left(  R\right)  \right)  =\kappa\psi_{0}.
\]
Thus, $\left(  R\left(  0\right)  -\kappa\right)  \psi_{0}%
=\underbrace{R\left(  0\right)  \psi_{0}}_{=\kappa\psi_{0}}-\kappa\psi
_{0}=\kappa\psi_{0}-\kappa\psi_{0}=0$.
\par
But $\psi_{0}$ is an element of a basis of $\mathcal{F}^{\left(  0\right)  }$
(namely, of the basis $\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\right)  _{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression}}$). Thus, every scalar $\mu\in\mathbb{C}$ satisfying
$\mu\psi_{0}=0$ must satisfy $\mu=0$. Applying this to $\mu=R\left(  0\right)
-\kappa$, we obtain $R\left(  0\right)  -\kappa=0$ (since $\left(  R\left(
0\right)  -\kappa\right)  \psi_{0}=0$). Thus,%
\[
R\left(  0\right)  =\kappa=\left(
\begin{array}
[c]{c}%
\text{the }\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)
\text{-coordinate of }\sigma\left(  R\right) \\
\text{with respect to the basis }\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge
v_{j_{2}}\wedge...\right)  _{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression}}\text{ of }\mathcal{F}^{\left(  0\right)  }%
\end{array}
\right)  .
\]
This proves (\ref{pf.schur.1}).}. Since the proof of (\ref{pf.schur.1})
clearly does not depend on the ground ring, an analogous result holds over the
ring $\mathbf{R}$: Every polynomial $R\in\mathcal{B}_{\mathbf{R}}^{\left(
0\right)  }$ satisfies%
\begin{equation}
R\left(  0\right)  =\left(
\begin{array}
[c]{c}%
\text{the }\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)
\text{-coordinate of }\sigma_{\mathbf{R}}\left(  R\right) \\
\text{with respect to the basis }\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge
v_{j_{2}}\wedge...\right)  _{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression}}\text{ of }\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }%
\end{array}
\right)  \label{pf.schur.1R}%
\end{equation}
\footnote{Of course, ``basis'' means ``$\mathbf{R}$-module basis'' and no
longer ``$\mathbb{C}$-vector space basis'' in this statement.}.

On the other hand, for every polynomial $R\in\mathcal{B}^{\left(  0\right)  }%
$, we can view $R=R\left(  x\right)  $ as an element of $\mathcal{B}%
_{\mathbf{R}}^{\left(  0\right)  }$ (since $\mathcal{B}^{\left(  0\right)
}\subseteq\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }$), and this way we
obtain%
\begin{align}
&  \left(  1,\exp\left(  \underbrace{y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}%
+...}_{=\sum\limits_{s>0}y_{s}a_{s}}\right)  R\left(  x\right)  \right)
=\left(  1,\exp\left(  \sum\limits_{s>0}y_{s}a_{s}\right)  R\left(  x\right)
\right) \nonumber\\
&  =\left(  1,\exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial
x_{s}}\right)  R\left(  x\right)  \right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }a_{s}\text{ acts as }\dfrac{\partial}{\partial x_{s}}\text{ on
}\mathcal{B}^{\left(  0\right)  }\text{ for every }s\geq1\right) \nonumber\\
&  =\left(  1,R\left(  x+y\right)  \right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial
x_{s}}\right)  R\left(  x\right)  =R\left(  x+y\right) \\
\text{by Lemma \ref{lem.hirota.newton} (applied to }R\text{, }\left(
x_{1},x_{2},x_{3},...\right)  \text{ and }\mathbf{R}\\
\text{ instead of }P\text{, }\left(  z_{1},z_{2},z_{3},...\right)  \text{ and
}K\text{)}%
\end{array}
\right) \nonumber\\
&  =\left(  R\left(  x+y\right)  \right)  \left(  0\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because the analogue of Proposition \ref{prop.A.contravariantform}
\textbf{(b)} for}\\
\text{the ground ring }\mathbf{R}\text{ yields }\left(  1,Q\right)  =Q\left(
0\right)  \text{ for every }Q\in\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }%
\end{array}
\right) \nonumber\\
&  =R\left(  y\right)  \label{pf.schur.usingfermi.1}%
\end{align}
in $\mathbf{R}$.

Recall that the map $\sigma_{\mathbf{R}}$ is defined analogously to $\sigma$
but for the ground ring $\mathbf{R}$ instead of $\mathbb{C}$. Thus,
$\sigma_{\mathbf{R}}\left(  Q\right)  =\sigma\left(  Q\right)  $ for every
$Q\in\mathcal{B}^{\left(  0\right)  }$. Applied to $Q=P\left(  x\right)  $,
this yields $\sigma_{\mathbf{R}}\left(  P\left(  x\right)  \right)
=\sigma\left(  P\left(  x\right)  \right)  =v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...$ (since $P\left(  x\right)  =\sigma^{-1}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $).

On the other hand, since $\sigma_{\mathbf{R}}:\mathcal{B}_{\mathbf{R}%
}^{\left(  0\right)  }\rightarrow\mathcal{F}_{\mathbf{R}}^{\left(  0\right)
}$ is an $\mathcal{A}_{\mathbf{R}}$-module homomorphism, and since
$\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }$ and $\mathcal{F}_{\mathbf{R}%
}^{\left(  0\right)  }$ are two $\mathcal{A}_{\mathbf{R}}$-modules
concentrated in nonpositive degrees, we can apply Proposition
\ref{prop.schur.fermi.welldef} \textbf{(b)} to $\sigma_{\mathbf{R}}$,
$\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }$ and $\mathcal{F}_{\mathbf{R}%
}^{\left(  0\right)  }$ instead of $\eta$, $M$ and $N$. As a result, we obtain%
\[
\sigma_{\mathbf{R}}\circ\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  \right)  =\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}%
+y_{3}a_{3}+...\right)  \right)  \circ\sigma_{\mathbf{R}}%
\]
as maps from $\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }$ to $\mathcal{F}%
_{\mathbf{R}}^{\left(  0\right)  }$. This easily yields%
\begin{align}
\sigma_{\mathbf{R}}\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  P\left(  x\right)  \right)   &  =\underbrace{\left(
\sigma_{\mathbf{R}}\circ\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  \right)  \right)  }_{=\left(  \exp\left(  y_{1}a_{1}%
+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \circ\sigma_{\mathbf{R}}}\left(
P\left(  x\right)  \right) \nonumber\\
&  =\left(  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
\right)  \circ\sigma_{\mathbf{R}}\right)  \left(  P\left(  x\right)  \right)
\nonumber\\
&  =\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot
\underbrace{\sigma_{\mathbf{R}}\left(  P\left(  x\right)  \right)
}_{=v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...}\nonumber\\
&  =\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  .
\label{pf.schur.usingfermi.4}%
\end{align}


But (\ref{pf.schur.usingfermi.1}) (applied to $R=P$) yields%
\[
\left(  1,\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  P\left(
x\right)  \right)  =P\left(  y\right)  ,
\]
so that%
\begin{align*}
&  P\left(  y\right) \\
&  =\left(  1,\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
P\left(  x\right)  \right) \\
&  =\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  P\left(
x\right)  \right)  \left(  0\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because the analogue of Proposition \ref{prop.A.contravariantform}
\textbf{(b)} for}\\
\text{the ground ring }\mathbf{R}\text{ yields }\left(  1,Q\right)  =Q\left(
0\right)  \text{ for every }Q\in\mathcal{B}_{\mathbf{R}}^{\left(  0\right)  }%
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{c}%
\text{the }\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)
\text{-coordinate of }\sigma_{\mathbf{R}}\left(  \exp\left(  y_{1}a_{1}%
+y_{2}a_{2}+y_{3}a_{3}+...\right)  P\left(  x\right)  \right) \\
\text{with respect to the basis }\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge
v_{j_{2}}\wedge...\right)  _{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression}}\text{ of }\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.schur.1R}), applied to
}R=\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  P\left(  x\right)
\right) \\
&  =\left(
\begin{array}
[c]{c}%
\text{the }\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)
\text{-coordinate of}\\
\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
\text{with respect to the basis }\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge
v_{j_{2}}\wedge...\right)  _{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression}}\text{ of }\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.schur.usingfermi.4})}\right)
\\
&  =S_{\left(  i_{k}+k\right)  _{k\geq0}}\left(  y\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.schur.fermi}}\right) \\
&  =S_{\lambda}\left(  y\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\left(  i_{k}+k\right)  _{k\geq0}=\left(  i_{0}+0,i_{1}+1,i_{2}+2,...\right)
=\lambda\right)  .
\end{align*}
Substituting $x_{i}$ for $y_{i}$ in this equation, we obtain $P\left(
x\right)  =S_{\lambda}\left(  x\right)  $ (since both $P$ and $S_{\lambda}$
are polynomials in $\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $). Thus,%
\[
S_{\lambda}\left(  x\right)  =P\left(  x\right)  =\sigma^{-1}\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  .
\]
This proves Theorem \ref{thm.schur}.

\subsubsection{\label{subsubsect.skewschur}Skew Schur polynomials}

Rather than prove Theorem \ref{thm.schur.fermi} directly, let us formulate and
verify a stronger statement which will be in no way harder to prove. First, we
need a definition:

\begin{definition}
\label{def.skewschur}Let $\lambda$ and $\mu$ be two partitions.

\textbf{(a)} We write $\mu\subseteq\lambda$ if every $i\in\left\{
1,2,3,...\right\}  $ satisfies $\lambda_{i}\geq\mu_{i}$, where the partitions
$\lambda$ and $\mu$ have been written in the forms $\lambda=\left(
\lambda_{1},\lambda_{2},\lambda_{3},...\right)  $ and $\mu=\left(  \mu_{1}%
,\mu_{2},\mu_{3},...\right)  $.

\textbf{(b)} We define a polynomial $S_{\lambda\diagup\mu}\left(  x\right)
\in\mathbb{Q}\left[  x_{1},x_{2},x_{3},...\right]  $ as follows: Write
$\lambda$ and $\mu$ in the forms $\lambda=\left(  \lambda_{1},\lambda
_{2},...,\lambda_{m}\right)  $ and $\mu=\left(  \mu_{1},\mu_{2},...,\mu
_{m}\right)  $ for some $m\in\mathbb{N}$. Then, let $S_{\lambda\diagup\mu
}\left(  x\right)  $ be the polynomial%
\begin{align*}
&  \det\left(
\begin{array}
[c]{ccccc}%
S_{\lambda_{1}-\mu_{1}}\left(  x\right)  & S_{\lambda_{1}-\mu_{2}+1}\left(
x\right)  & S_{\lambda_{1}-\mu_{3}+2}\left(  x\right)  & ... & S_{\lambda
_{1}-\mu_{m}+m-1}\left(  x\right) \\
S_{\lambda_{2}-\mu_{1}-1}\left(  x\right)  & S_{\lambda_{2}-\mu_{2}}\left(
x\right)  & S_{\lambda_{2}-\mu_{3}+1}\left(  x\right)  & ... & S_{\lambda
_{2}-\mu_{m}+m-2}\left(  x\right) \\
S_{\lambda_{3}-\mu_{1}-2}\left(  x\right)  & S_{\lambda_{3}-\mu_{2}-1}\left(
x\right)  & S_{\lambda_{3}-\mu_{3}}\left(  x\right)  & ... & S_{\lambda
_{3}-\mu_{m}+m-3}\left(  x\right) \\
... & ... & ... & ... & ...\\
S_{\lambda_{m}-\mu_{1}-m+1}\left(  x\right)  & S_{\lambda_{m}-\mu_{2}%
-m+2}\left(  x\right)  & S_{\lambda_{m}-\mu_{3}-m+3}\left(  x\right)  & ... &
S_{\lambda_{m}-\mu_{m}}\left(  x\right)
\end{array}
\right) \\
&  =\det\left(  \left(  S_{\lambda_{i}-\mu_{j}+j-i}\left(  x\right)  \right)
_{1\leq i\leq m,\ 1\leq j\leq m}\right)  ,
\end{align*}
where $S_{j}$ denotes $0$ if $j<0$. (Note that this does not depend on the
choice of $m$ (that is, increasing $m$ at the cost of padding the partitions
$\lambda$ and $\mu$ with trailing zeroes does not change the value of
$\det\left(  \left(  S_{\lambda_{i}-\mu_{j}+j-i}\left(  x\right)  \right)
_{1\leq i\leq m,\ 1\leq j\leq m}\right)  $). This is because any nonnegative
integers $m$ and $\ell$, any $m\times m$-matrix $A$, any $m\times\ell$-matrix
$B$ and any upper unitriangular $\ell\times\ell$-matrix $C$ satisfy
$\det\left(
\begin{array}
[c]{cc}%
A & B\\
0 & C
\end{array}
\right)  =\det A$.)

We refer to $S_{\lambda\diagup\mu}\left(  x\right)  $ as the \textit{bosonic
Schur polynomial corresponding to the skew partition }$\lambda\diagup\mu$.
\end{definition}

Before we formulate the strengthening of Theorem \ref{thm.schur.fermi}, three remarks:

\begin{remark}
\label{rmk.skewschur.empty}Let $\varnothing$ denote the partition $\left(
0,0,0,...\right)  $. For every partition $\lambda$, we have $\varnothing
\subseteq\lambda$ and $S_{\lambda\diagup\varnothing}\left(  x\right)
=S_{\lambda}\left(  x\right)  $.
\end{remark}

\begin{remark}
\label{rmk.skewschur.0}Let $\lambda$ and $\mu$ be two partitions. Then,
$S_{\lambda\diagup\mu}\left(  x\right)  =0$ unless $\mu\subseteq\lambda$.
\end{remark}

\begin{remark}
\label{rmk.skewschur.infdet}Recall that in Definition \ref{def.infdet}
\textbf{(c)}, we defined the notion of an ``upper almost-unitriangular''
$\mathbb{N}\times\mathbb{N}$-matrix. In the same way, we can define the notion
of an ``upper almost-unitriangular'' $\left\{  1,2,3,...\right\}
\times\left\{  1,2,3,...\right\}  $-matrix.

In Definition \ref{def.infdet} \textbf{(e)}, we defined the determinant of an
upper almost-unitriangular $\mathbb{N}\times\mathbb{N}$-matrix. Analogously,
we can define the determinant of an upper almost-unitriangular $\left\{
1,2,3,...\right\}  \times\left\{  1,2,3,...\right\}  $-matrix.

Let $\lambda=\left(  \lambda_{1},\lambda_{2},\lambda_{3},...\right)  $ and
$\mu=\left(  \mu_{1},\mu_{2},\mu_{3},...\right)  $ be two partitions. Then,
the $\left\{  1,2,3,...\right\}  \times\left\{  1,2,3,...\right\}  $-matrix
$\left(  S_{\lambda_{i}-\mu_{j}+j-i}\left(  x\right)  \right)  _{\left(
i,j\right)  \in\left\{  1,2,3,...\right\}  ^{2}}$ is upper
almost-unitriangular, and we have%
\begin{align}
S_{\lambda\diagup\mu}\left(  x\right)   &  =\det\left(  \left(  S_{\lambda
_{i}-\mu_{j}+j-i}\left(  x\right)  \right)  _{\left(  i,j\right)  \in\left\{
1,2,3,...\right\}  ^{2}}\right) \label{rmk.skewschur.infdet.1}\\
&  =\det\left(
\begin{array}
[c]{cccc}%
S_{\lambda_{1}-\mu_{1}}\left(  x\right)  & S_{\lambda_{1}-\mu_{2}+1}\left(
x\right)  & S_{\lambda_{1}-\mu_{3}+2}\left(  x\right)  & ...\\
S_{\lambda_{2}-\mu_{1}-1}\left(  x\right)  & S_{\lambda_{2}-\mu_{2}}\left(
x\right)  & S_{\lambda_{2}-\mu_{3}+1}\left(  x\right)  & ...\\
S_{\lambda_{3}-\mu_{1}-2}\left(  x\right)  & S_{\lambda_{3}-\mu_{2}-1}\left(
x\right)  & S_{\lambda_{3}-\mu_{3}}\left(  x\right)  & ...\\
... & ... & ... & ...
\end{array}
\right)  .\nonumber
\end{align}

\end{remark}

All of the above three remarks follow easily from Definition
\ref{def.skewschur}.

Now, let us finally give the promised strengthening of Theorem
\ref{thm.schur.fermi}:

\begin{theorem}
\label{thm.schur.fermi.skew}Let $\mathbf{R}$ be a commutative $\mathbb{Q}%
$-algebra. Let $y_{1},y_{2},y_{3},...$ be some elements of $\mathbf{R}$.
Denote by $y$ the family $\left(  y_{1},y_{2},y_{3},...\right)  $. Let
$\left(  i_{0},i_{1},i_{2},...\right)  $ be a $0$-degression. Recall that
$\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $ is a well-defined
element of $\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }$ due to Proposition
\ref{prop.schur.fermi.welldef} \textbf{(c)}. Recall also that $\left(
j_{k}+k\right)  _{k\geq0}$ is a partition for every $0$-degression $\left(
j_{0},j_{1},j_{2},...\right)  $ (this follows from Proposition
\ref{prop.glinf.wedge.grading}, applied to $0$ and $\left(  j_{0},j_{1}%
,j_{2},...\right)  $ instead of $m$ and $\left(  i_{0},i_{1},i_{2},...\right)
$). In particular, $\left(  i_{k}+k\right)  _{k\geq0}$ is a partition.

We have%
\begin{align}
&  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge.... \label{thm.schur.fermi.skew.eq}%
\end{align}
(Note that the sum on the right hand side of (\ref{thm.schur.fermi.skew.eq})
is a finite sum, since only finitely many $0$-degressions $\left(  j_{0}%
,j_{1},j_{2},...\right)  $ satisfy $\left(  j_{k}+k\right)  _{k\geq0}%
\subseteq\left(  i_{k}+k\right)  _{k\geq0}$.)
\end{theorem}

Before we prove this, let us see how this yields Theorem \ref{thm.schur.fermi}:

\textit{Proof of Theorem \ref{thm.schur.fermi} using Theorem
\ref{thm.schur.fermi.skew}.} Remark \ref{rmk.skewschur.empty} (applied to
$\lambda=\left(  i_{k}+k\right)  _{k\geq0}$) yields $\varnothing
\subseteq\left(  i_{k}+k\right)  _{k\geq0}$ and $S_{\left(  i_{k}+k\right)
_{k\geq0}\diagup\varnothing}\left(  x\right)  =S_{\left(  i_{k}+k\right)
_{k\geq0}}\left(  x\right)  $. By substituting $y$ for $x$ in the equality
$S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\varnothing}\left(  x\right)
=S_{\left(  i_{k}+k\right)  _{k\geq0}}\left(  x\right)  $, we conclude
$S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\varnothing}\left(  y\right)
=S_{\left(  i_{k}+k\right)  _{k\geq0}}\left(  y\right)  $.

Theorem \ref{thm.schur.fermi.skew} yields that (\ref{thm.schur.fermi.skew.eq}) holds.

On the other hand, every $0$-degression $\left(  j_{0},j_{1},j_{2},...\right)
$ satisfying $\left(  j_{k}+k\right)  _{k\geq0}\not \subseteq \left(
i_{k}+k\right)  _{k\geq0}$ must satisfy%
\begin{equation}
S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq0}%
}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...=0
\label{pf.schur.fermi.notcontained}%
\end{equation}
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  j_{0},j_{1},j_{2},...\right)  $
be a $0$-degression satisfying $\left(  j_{k}+k\right)  _{k\geq0}%
\not \subseteq \left(  i_{k}+k\right)  _{k\geq0}$. We know that $\left(
i_{k}+k\right)  _{k\geq0}$ and $\left(  j_{k}+k\right)  _{k\geq0}$ are
partitions. Thus, Remark \ref{rmk.skewschur.0} (applied to $\lambda=\left(
i_{k}+k\right)  _{k\geq0}$ and $\mu=\left(  j_{k}+k\right)  _{k\geq0}$) yields
that $S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)
_{k\geq0}}\left(  x\right)  =0$ unless $\left(  j_{k}+k\right)  _{k\geq
0}\subseteq\left(  i_{k}+k\right)  _{k\geq0}$. Since we don't have $\left(
j_{k}+k\right)  _{k\geq0}\subseteq\left(  i_{k}+k\right)  _{k\geq0}$ (because
by assumption, we have $\left(  j_{k}+k\right)  _{k\geq0}\not \subseteq
\left(  i_{k}+k\right)  _{k\geq0}$), we thus know that $S_{\left(
i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq0}}\left(
x\right)  =0$. Substituting $y$ for $x$ in this equation, we obtain
$S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq0}%
}\left(  y\right)  =0$, so that $S_{\left(  i_{k}+k\right)  _{k\geq0}%
\diagup\left(  j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}%
}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...=0$, qed.}. Hence, each of the
addends of the infinite sum $\sum\limits_{\substack{\left(  j_{0},j_{1}%
,j_{2},...\right)  \text{ a }0\text{-degression;}\\\left(  j_{k}+k\right)
_{k\geq0}\not \subseteq \left(  i_{k}+k\right)  _{k\geq0}}}S_{\left(
i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq0}}\left(
y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...$ equals
$0$. Thus, the infinite sum $\sum\limits_{\substack{\left(  j_{0},j_{1}%
,j_{2},...\right)  \text{ a }0\text{-degression;}\\\left(  j_{k}+k\right)
_{k\geq0}\not \subseteq \left(  i_{k}+k\right)  _{k\geq0}}}S_{\left(
i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq0}}\left(
y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...$ is
well-defined and equals $0$. We thus have%
\begin{equation}
0=\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\not \subseteq \left(
i_{k}+k\right)  _{k\geq0}}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge.... \label{pf.schur.fermi.onlysubdiagrams}%
\end{equation}
Adding this equality to (\ref{thm.schur.fermi.skew.eq}), we obtain%
\begin{align*}
&  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{\left(  j_{0},j_{1}%
,j_{2},...\right)  \text{ a }0\text{-degression;}\\\left(  j_{k}+k\right)
_{k\geq0}\not \subseteq \left(  i_{k}+k\right)  _{k\geq0}}}S_{\left(
i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq0}}\left(
y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\\
&  =\sum\limits_{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a }%
0\text{-degression}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge....
\end{align*}
Hence, the $\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)
$-coordinate of $\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
\cdot\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  $ with
respect to the basis $\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\right)  _{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression}}$ of $\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }$
equals
\begin{align*}
S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  -k+k\right)  _{k\geq0}%
}\left(  y\right)   &  =S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup
\varnothing}\left(  y\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
-k+k\right)  _{k\geq0}=\left(  0\right)  _{k\geq0}=\left(  0,0,0,...\right)
=\varnothing\right) \\
&  =S_{\left(  i_{k}+k\right)  _{k\geq0}}\left(  y\right)  .
\end{align*}
This proves Theorem \ref{thm.schur.fermi} using Theorem
\ref{thm.schur.fermi.skew}.

\subsubsection{\label{subsubsect.schur2}Proof of Theorem
\ref{thm.schur.fermi.skew} using
\texorpdfstring{$\operatorname*{U}\left(  \infty\right)  $}{U-infinity}}

One final easy lemma:

\begin{lemma}
\label{lem.schur.fermi.skew.toeplitzmatrix}For every $n\in\mathbb{Z}$, let
$c_{n}$ be an element of $\mathbb{C}$. Assume that $c_{n}=0$ for every
negative $n\in\mathbb{Z}$. Consider the shift operator $T:V\rightarrow V$ of
Definition \ref{def.shiftoperator}. Then, $\sum\limits_{k\geq0}c_{k}%
T^{k}=\left(  c_{j-i}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$.
\end{lemma}

\begin{vershort}
The proof of this lemma is immediate from the definition of $T$.
\end{vershort}

\begin{verlong}
\textit{Proof of Lemma \ref{lem.schur.fermi.skew.toeplitzmatrix}.} By
Definition \ref{def.shiftoperator}, we have $Tv_{i+1}=v_{i}$ for every
$i\in\mathbb{Z}$. Substituting $i-1$ for $i$ in this equality, we obtain:
$Tv_{i}=v_{i-1}$ for every $i\in\mathbb{Z}$. Using this, we can readily find
that%
\begin{equation}
T^{k}v_{i}=v_{i-k}\ \ \ \ \ \ \ \ \ \text{for every }k\in\mathbb{N}\text{ and
}i\in\mathbb{Z}. \label{pf.schur.fermi.skew.toeplitzmatrix.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.schur.fermi.skew.toeplitzmatrix.1}):} We
will prove (\ref{pf.schur.fermi.skew.toeplitzmatrix.1}) by induction over $k$.
\par
\textit{Induction base:} For $k=0$, we have $\underbrace{T^{k}}_{=T^{0}%
=\operatorname*{id}}v_{i}=\operatorname*{id}v_{i}=v_{i}=v_{i-0}=v_{i-k}$
(since $0=k$) for every $i\in\mathbb{Z}$. In other words,
(\ref{pf.schur.fermi.skew.toeplitzmatrix.1}) is true for $k=0$. This completes
the induction base.
\par
\textit{Induction step:} Let $\ell\in\mathbb{N}$. Assume that
(\ref{pf.schur.fermi.skew.toeplitzmatrix.1}) holds for $k=\ell$. We must prove
that (\ref{pf.schur.fermi.skew.toeplitzmatrix.1}) also holds for $k=\ell+1$.
\par
Let $i\in\mathbb{Z}$. Since (\ref{pf.schur.fermi.skew.toeplitzmatrix.1}) holds
for $k=\ell$, we can apply (\ref{pf.schur.fermi.skew.toeplitzmatrix.1}) to
$\ell$ and $i-1$ instead of $k$ and $i$, and obtain $T^{\ell}v_{i-1}%
=v_{i-1-\ell}$. Now, $T^{\ell+1}=T^{\ell}T$, so that $T^{\ell+1}v_{i}=T^{\ell
}\underbrace{Tv_{i}}_{=v_{i-1}}=T^{\ell}v_{i-1}=v_{i-1-\ell}=v_{i-\left(
\ell+1\right)  }$. Now, forget that we fixed $i$. We thus have proven that
$T^{\ell+1}v_{i}=v_{i-\left(  \ell+1\right)  }$ for every $i\in\mathbb{Z}$.
Thus, (\ref{pf.schur.fermi.skew.toeplitzmatrix.1}) holds for $k=\ell+1$. This
completes the induction step. The induction proof of
(\ref{pf.schur.fermi.skew.toeplitzmatrix.1}) is thus finished.} Hence, every
$k\in\mathbb{N}$ and $i\in\mathbb{Z}$ satisfy%
\begin{align*}
&  \left(  \text{the }i\text{-th column of the matrix }\left(  \delta
_{v-u,k}\right)  _{\left(  u,v\right)  \in\mathbb{Z}^{2}}\right) \\
&  =\sum\limits_{u\in\mathbb{Z}}\delta_{i-u,k}v_{u}=\sum
\limits_{\substack{u\in\mathbb{Z};\\u\neq i-k}}\underbrace{\delta_{i-u,k}%
}_{\substack{=0\\\text{(since }i-u\neq k\\\text{(since }u\neq i-k\text{))}%
}}v_{u}+\underbrace{\delta_{i-\left(  i-k\right)  ,k}}%
_{\substack{=1\\\text{(since }i-\left(  i-k\right)  =k\text{)}}}v_{i-k}\\
&  =\underbrace{\sum\limits_{\substack{u\in\mathbb{Z};\\u\neq i-k}}0v_{u}%
}_{=0}+v_{i-k}=v_{i-k}=T^{k}v_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.schur.fermi.skew.toeplitzmatrix.1})}\right) \\
&  =\left(  \text{the }i\text{-th column of the matrix }T^{k}\right)  .
\end{align*}
Thus, every $k\in\mathbb{N}$ satisfies $\left(  \delta_{v-u,k}\right)
_{\left(  u,v\right)  \in\mathbb{Z}^{2}}=T^{k}$. Hence, $\sum\limits_{k\geq
0}c_{k}\underbrace{\left(  \delta_{v-u,k}\right)  _{\left(  u,v\right)
\in\mathbb{Z}^{2}}}_{=T^{k}}=\sum\limits_{k\geq0}c_{k}T^{k}$, so that%
\[
\sum\limits_{k\geq0}c_{k}T^{k}=\sum\limits_{k\geq0}c_{k}\left(  \delta
_{v-u,k}\right)  _{\left(  u,v\right)  \in\mathbb{Z}^{2}}=\left(
\sum\limits_{k\geq0}c_{k}\delta_{v-u,k}\right)  _{\left(  u,v\right)
\in\mathbb{Z}^{2}}.
\]
But recall that $c_{n}=0$ for every negative $n\in\mathbb{Z}$. In other words,
$c_{k}=0$ for every negative $k\in\mathbb{Z}$. Hence, $c_{k}\delta_{v-u,k}=0$
for every negative $k\in\mathbb{Z}$ and every $\left(  u,v\right)
\in\mathbb{Z}^{2}$. Thus, the sum $\sum\limits_{k<0}c_{k}\delta_{v-u,k}$ is
well-defined and equals $0$ for every $\left(  u,v\right)  \in\mathbb{Z}^{2}$.
Hence, in $\overline{\mathfrak{a}_{\infty}}$, we have $\left(  \sum
\limits_{k<0}c_{k}\delta_{v-u,k}\right)  _{\left(  u,v\right)  \in
\mathbb{Z}^{2}}=\left(  0\right)  _{\left(  u,v\right)  \in\mathbb{Z}^{2}}=0$.
Now,%
\begin{align*}
\sum\limits_{k\geq0}c_{k}T^{k}  &  =\underbrace{\sum\limits_{k\geq0}c_{k}%
T^{k}}_{=\left(  \sum\limits_{k\geq0}c_{k}\delta_{v-u,k}\right)  _{\left(
u,v\right)  \in\mathbb{Z}^{2}}}+\underbrace{0}_{=\left(  \sum\limits_{k<0}%
c_{k}\delta_{v-u,k}\right)  _{\left(  u,v\right)  \in\mathbb{Z}^{2}}}\\
&  =\left(  \sum\limits_{k\geq0}c_{k}\delta_{v-u,k}\right)  _{\left(
u,v\right)  \in\mathbb{Z}^{2}}+\left(  \sum\limits_{k<0}c_{k}\delta
_{v-u,k}\right)  _{\left(  u,v\right)  \in\mathbb{Z}^{2}}=\left(
\sum\limits_{k\geq0}c_{k}\delta_{v-u,k}+\sum\limits_{k<0}c_{k}\delta
_{v-u,k}\right)  _{\left(  u,v\right)  \in\mathbb{Z}^{2}}.
\end{align*}
But since every $\left(  u,v\right)  \in\mathbb{Z}^{2}$ satisfies%
\begin{align*}
\sum\limits_{k\geq0}c_{k}\delta_{v-u,k}+\sum\limits_{k<0}c_{k}\delta_{v-u,k}
&  =\sum\limits_{k\in\mathbb{Z}}c_{k}\delta_{v-u,k}=\sum
\limits_{\substack{k\in\mathbb{Z};\\k\neq v-u}}c_{k}\underbrace{\delta
_{v-u,k}}_{\substack{=0\\\text{(since }v-u\neq k\text{)}}}+c_{v-u}%
\underbrace{\delta_{v-u,v-u}}_{=1}\\
&  =\underbrace{\sum\limits_{\substack{k\in\mathbb{Z};\\k\neq v-u}}c_{k}%
0}_{=0}+c_{v-u}=c_{v-u},
\end{align*}
this rewrites as $\sum\limits_{k\geq0}c_{k}T^{k}=\left(  \underbrace{\sum
\limits_{k\geq0}c_{k}\delta_{v-u,k}+\sum\limits_{k<0}c_{k}\delta_{v-u,k}%
}_{=c_{v-u}}\right)  _{\left(  u,v\right)  \in\mathbb{Z}^{2}}=\left(
c_{v-u}\right)  _{\left(  u,v\right)  \in\mathbb{Z}^{2}}=\left(
c_{j-i}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$ (here, we renamed
$\left(  u,v\right)  $ as $\left(  i,j\right)  $). This proves Lemma
\ref{lem.schur.fermi.skew.toeplitzmatrix}.
\end{verlong}

We now give a proof of Theorem \ref{thm.schur.fermi.skew} using the actions
$\rho:\mathfrak{u}_{\infty}\rightarrow\operatorname*{End}\left(
\wedge^{\dfrac{\infty}{2},m}V\right)  $ and $\varrho:\operatorname*{U}\left(
\infty\right)  \rightarrow\operatorname*{GL}\left(  \wedge^{\dfrac{\infty}%
{2},m}V\right)  $ introduced in Subsection \ref{subsubsect.Uinf} and their properties.

\textit{First proof of Theorem \ref{thm.schur.fermi.skew}.} In order to
simplify notation, we assume that $\mathbf{R}=\mathbb{C}$. (All the arguments
that we will make in the following are independent of the ground ring, as long
as the ground ring is a commutative $\mathbb{Q}$-algebra. Therefore, we are
actually allowed to assume that $\mathbf{R}=\mathbb{C}$.) Since we assumed
that $\mathbf{R}=\mathbb{C}$, we have $\mathcal{A}_{\mathbf{R}}=\mathcal{A}$
and $\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }=\mathcal{F}^{\left(
0\right)  }$.

Now consider the shift operator $T:V\rightarrow V$ of Definition
\ref{def.shiftoperator}. As a matrix in $\overline{\mathfrak{a}_{\infty}}$,
this $T$ is the matrix which has $1$'s on the diagonal right above the main
one, and $0$'s everywhere else. The embedding $\mathcal{A}\rightarrow
\mathfrak{a}_{\infty}$ that we are using to define the action of $\mathcal{A}$
on $\mathcal{F}^{\left(  0\right)  }$ sends $a_{j}$ to $T^{j}$ for every
$j\in\mathbb{Z}$. Thus, every positive integer $j$ satisfies%
\begin{align*}
a_{j}\mid_{\mathcal{F}^{\left(  0\right)  }}  &  =T^{j}\mid_{\mathcal{F}%
^{\left(  0\right)  }}=\widehat{\rho}\left(  T^{j}\right)  =\rho\left(
T^{j}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Remark \ref{rmk.Uinf.rhorhohat},
applied to }m=0\text{ and }a=T^{j}\text{ (since }T^{j}\in\mathfrak{u}_{\infty
}\cap\overline{\mathfrak{a}_{\infty}}\text{)}\right)  .
\end{align*}
Since $y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...=\sum\limits_{j\geq1}y_{j}a_{j}$,
we have%
\begin{align*}
\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \mid_{\mathcal{F}%
^{\left(  0\right)  }}  &  =\left(  \sum\limits_{j\geq1}y_{j}a_{j}\right)
\mid_{\mathcal{F}^{\left(  0\right)  }}=\sum\limits_{j\geq1}y_{j}%
\underbrace{\left(  a_{j}\mid_{\mathcal{F}^{\left(  0\right)  }}\right)
}_{=\rho\left(  T^{j}\right)  }=\sum\limits_{j\geq1}y_{j}\rho\left(
T^{j}\right) \\
&  =\rho\left(  \sum\limits_{j\geq1}y_{j}T^{j}\right)  .
\end{align*}
Here, we have used the fact that $\sum\limits_{j\geq1}y_{j}T^{j}%
\in\mathfrak{u}_{\infty}$ (this ensures that $\rho\left(  \sum\limits_{j\geq
1}y_{j}T^{j}\right)  $ is well-defined).

On the other hand, substituting $y$ for $x$ in (\ref{def.schur.sk.genfun}), we
obtain%
\[
\sum\limits_{k\geq0}S_{k}\left(  y\right)  z^{k}=\exp\left(  \sum
\limits_{i\geq1}y_{i}z^{i}\right)  \ \ \ \ \ \ \ \ \ \ \text{in }%
\mathbb{C}\left[  \left[  z\right]  \right]  .
\]
Substituting $T$ for $z$ in this equality, we obtain $\sum\limits_{k\geq
0}S_{k}\left(  y\right)  T^{k}=\exp\left(  \sum\limits_{i\geq1}y_{i}%
T^{i}\right)  $. Thus,%
\begin{equation}
\exp\left(  \sum\limits_{j\geq1}y_{j}T^{j}\right)  =\exp\left(  \sum
\limits_{i\geq1}y_{i}T^{i}\right)  =\sum\limits_{k\geq0}S_{k}\left(  y\right)
T^{k}=\left(  S_{j-i}\left(  y\right)  \right)  _{\left(  i,j\right)
\in\mathbb{Z}^{2}} \label{pf.schur.fermi.skew.1}%
\end{equation}
(by Lemma \ref{lem.schur.fermi.skew.toeplitzmatrix}, applied to $c_{n}%
=S_{n}\left(  y\right)  $ (since $S_{n}\left(  y\right)  =0$ for every
negative $n\in\mathbb{Z}$)).

Now,%
\begin{align*}
&  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\mid_{\mathcal{F}^{\left(  0\right)  }}\\
&  =\exp\underbrace{\left(  \left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}%
+...\right)  \mid_{\mathcal{F}^{\left(  0\right)  }}\right)  }_{=\rho\left(
\sum\limits_{j\geq1}y_{j}T^{j}\right)  }\\
&  =\exp\left(  \rho\left(  \sum\limits_{j\geq1}y_{j}T^{j}\right)  \right)
=\varrho\left(  \exp\left(  \sum\limits_{j\geq1}y_{j}T^{j}\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since Theorem \ref{thm.Uinf.rhoRho} (applied to }a=\sum\limits_{j\geq
1}y_{j}T^{j}\text{) yields}\\
\varrho\left(  \exp\left(  \sum\limits_{j\geq1}y_{j}T^{j}\right)  \right)
=\exp\left(  \rho\left(  \sum\limits_{j\geq1}y_{j}T^{j}\right)  \right)
\end{array}
\right) \\
&  =\varrho\left(  \left(  S_{j-i}\left(  y\right)  \right)  _{\left(
i,j\right)  \in\mathbb{Z}^{2}}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.schur.fermi.skew.1})}\right)  .
\end{align*}
Denote the matrix $\left(  S_{j-i}\left(  y\right)  \right)  _{\left(
i,j\right)  \in\mathbb{Z}^{2}}\in\operatorname*{U}\left(  \infty\right)  $ by
$A$. Thus, we have%
\[
\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\mid_{\mathcal{F}^{\left(  0\right)  }}=\varrho\left(  \underbrace{\left(
S_{j-i}\left(  y\right)  \right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}%
}_{=A}\right)  =\varrho\left(  A\right)  .
\]
Hence,%
\begin{align}
&  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  =\left(  \varrho\left(  A\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  =\sum\limits_{\left(  j_{0}%
,j_{1},j_{2},...\right)  \text{ is a }0\text{-degression}}\det\left(  \left(
A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}\right)  ^{T}\right)
v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge
...\label{pf.schur.fermi.skew.3}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Remark \ref{rmk.Uinf.det}, applied to
}m=0\right)  .\nonumber
\end{align}


But a close look at the matrix $\left(  A_{j_{0},j_{1},j_{2},...}^{i_{0}%
,i_{1},i_{2},...}\right)  ^{T}$ proves that%
\begin{equation}
\det\left(  \left(  A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}\right)
^{T}\right)  =S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \ \ \ \ \ \ \ \ \ \ \text{for
every }0\text{-degression }\left(  j_{0},j_{1},j_{2},...\right)
\label{pf.schur.fermi.skew.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.schur.fermi.skew.2}):} Let $\left(
j_{0},j_{1},j_{2},...\right)  $ be a $0$-degression. Since $A=\left(
S_{j-i}\left(  y\right)  \right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$,
we have $A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}=\left(
S_{i_{v}-j_{u}}\left(  y\right)  \right)  _{\left(  u,v\right)  \in
\mathbb{N}^{2}}$, so that $\left(  A_{j_{0},j_{1},j_{2},...}^{i_{0}%
,i_{1},i_{2},...}\right)  ^{T}=\left(  S_{i_{v}-j_{u}}\left(  y\right)
\right)  _{\left(  v,u\right)  \in\mathbb{N}^{2}}$. But define two partitions
$\lambda$ and $\mu$ by $\lambda=\left(  i_{k}+k\right)  _{k\geq0}$ and
$\mu=\left(  j_{k}+k\right)  _{k\geq0}$. Write the partitions $\lambda$ and
$\mu$ in the forms $\lambda=\left(  \lambda_{1},\lambda_{2},\lambda
_{3},...\right)  $ and $\mu=\left(  \mu_{1},\mu_{2},\mu_{3},...\right)  $.
Then, $\lambda_{v}=i_{v-1}+\left(  v-1\right)  $ for every $v\in\left\{
1,2,3,...\right\}  $, and $\mu_{u}=j_{u-1}+\left(  u-1\right)  $ for every
$u\in\left\{  1,2,3,...\right\}  $. Thus, for every $\left(  u,v\right)
\in\left\{  1,2,3,...\right\}  ^{2}$, we have%
\begin{equation}
\underbrace{\lambda_{v}}_{=i_{v-1}+\left(  v-1\right)  }-\underbrace{\mu_{u}%
}_{=j_{u-1}+\left(  u-1\right)  }+u-v=\left(  i_{v-1}+\left(  v-1\right)
\right)  -\left(  j_{u-1}+\left(  u-1\right)  \right)  +u-v=i_{v-1}-j_{u-1}.
\label{pf.schur.fermi.skew.2.pf.1}%
\end{equation}
But (\ref{rmk.skewschur.infdet.1}) yields $S_{\lambda\diagup\mu}\left(
x\right)  =\det\left(  \left(  S_{\lambda_{i}-\mu_{j}+j-i}\left(  x\right)
\right)  _{\left(  i,j\right)  \in\left\{  1,2,3,...\right\}  ^{2}}\right)  $.
Substituting $y$ for $x$ in this equality, we obtain%
\begin{align*}
S_{\lambda\diagup\mu}\left(  y\right)   &  =\det\left(  \left(  S_{\lambda
_{i}-\mu_{j}+j-i}\left(  y\right)  \right)  _{\left(  i,j\right)  \in\left\{
1,2,3,...\right\}  ^{2}}\right)  =\det\left(  \left(  S_{\lambda_{v}-\mu
_{u}+u-v}\left(  y\right)  \right)  _{\left(  v,u\right)  \in\left\{
1,2,3,...\right\}  ^{2}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }\left(  v,u\right)
\text{ for }\left(  i,j\right)  \right) \\
&  =\det\left(  \left(  S_{i_{v-1}-j_{u-1}}\left(  y\right)  \right)
_{\left(  v,u\right)  \in\left\{  1,2,3,...\right\}  ^{2}}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.schur.fermi.skew.2.pf.1}%
)}\right) \\
&  =\det\left(  \underbrace{\left(  S_{i_{v}-j_{u}}\left(  y\right)  \right)
_{\left(  v,u\right)  \in\mathbb{N}^{2}}}_{=\left(  A_{j_{0},j_{1},j_{2}%
,...}^{i_{0},i_{1},i_{2},...}\right)  ^{T}}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }\left(  v,u\right)
\text{ for }\left(  v-1,u-1\right)  \right) \\
&  =\det\left(  \left(  A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2}%
,...}\right)  ^{T}\right)  .
\end{align*}
Since $\lambda=\left(  i_{k}+k\right)  _{k\geq0}$ and $\mu=\left(
j_{k}+k\right)  _{k\geq0}$, this rewrites as $S_{\left(  i_{k}+k\right)
_{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq0}}\left(  y\right)
=\det\left(  \left(  A_{j_{0},j_{1},j_{2},...}^{i_{0},i_{1},i_{2},...}\right)
^{T}\right)  $. This proves (\ref{pf.schur.fermi.skew.2}).}.

Now, (\ref{pf.schur.fermi.skew.3}) becomes%
\begin{align*}
&  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \cdot\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\sum\limits_{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression}}\underbrace{\det\left(  \left(  A_{j_{0},j_{1},j_{2}%
,...}^{i_{0},i_{1},i_{2},...}\right)  ^{T}\right)  }_{\substack{=S_{\left(
i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq0}}\left(
y\right)  \\\text{(by (\ref{pf.schur.fermi.skew.2}))}}}v_{j_{0}}\wedge
v_{j_{1}}\wedge v_{j_{2}}\wedge...\\
&  =\sum\limits_{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum\limits_{\substack{\left(  j_{0}%
,j_{1},j_{2},...\right)  \text{ a }0\text{-degression;}\\\left(
j_{k}+k\right)  _{k\geq0}\not \subseteq \left(  i_{k}+k\right)  _{k\geq0}%
}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq
0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge
...}_{\substack{=0\\\text{(by (\ref{pf.schur.fermi.onlysubdiagrams}))}}}\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge....
\end{align*}
This proves Theorem \ref{thm.schur.fermi.skew}.

We can now combine the above to obtain a proof of Theorem \ref{thm.schur}:

\textit{Second proof of Theorem \ref{thm.schur}.} We have proven Theorem
\ref{thm.schur.fermi} using Theorem \ref{thm.schur.fermi.skew}. Since we know
that Theorem \ref{thm.schur.fermi.skew} holds, this yields that Theorem
\ref{thm.schur.fermi} holds. This, in turn, entails that Theorem
\ref{thm.schur} holds (since we have proven Theorem \ref{thm.schur} using
Theorem \ref{thm.schur.fermi}).

\subsubsection{\label{subsubsect.schur2.finitary}``Finitary'' proof of Theorem
\ref{thm.schur.fermi.skew}}

The above second proof of Theorem \ref{thm.schur} had the drawback of
requiring a slew of new notions (those of $\mathfrak{u}_{\infty}$, of
$\operatorname*{U}\left(  \infty\right)  $, of the determinant of an almost
upper-triangular matrix etc.) and of their properties (Proposition
\ref{prop.uinf.Vhatwedge.welldef}, Remark \ref{rmk.Uinf.det}, Theorem
\ref{thm.Uinf.rhoRho} and others). We will now give a proof of Theorem
\ref{thm.schur} which is more or less equivalent to the second proof of
Theorem \ref{thm.schur} shown above, but avoiding these new notions. It will
eschew using infinite matrices other than those in $\overline{\mathfrak{a}%
_{\infty}}$, and instead work with finite objects most of the time.

Since we already know how to derive Theorem \ref{thm.schur} from Theorem
\ref{thm.schur.fermi.skew}, we only need to verify Theorem
\ref{thm.schur.fermi.skew}.

Let us first introduce some finite-dimensional subspaces of the vector space
$V$:

\begin{definition}
\label{def.finitary.Valphabeta}Let $\alpha$ and $\beta$ be integers such that
$\alpha-1\leq\beta$.

\textbf{(a)} Then, $V_{\left]  \alpha,\beta\right]  }$ will denote the vector
subspace of $V$ spanned by the vectors $v_{\alpha+1}$, $v_{\alpha+2}$, $...$,
$v_{\beta}$. It is clear that $\left(  v_{\alpha+1},v_{\alpha+2},...,v_{\beta
}\right)  $ is a basis of this vector space $V_{\left]  \alpha,\beta\right]
}$, so that $\dim\left(  V_{\left]  \alpha,\beta\right]  }\right)
=\beta-\alpha$.

\textbf{(b)} Let $T_{\left]  \alpha,\beta\right]  }$ be the endomorphism of
the vector space $V_{\left]  \alpha,\beta\right]  }$ defined by%
\[
\left(  T_{\left]  \alpha,\beta\right]  }\left(  v_{i}\right)  =\left\{
\begin{array}
[c]{l}%
v_{i-1},\ \ \ \ \ \ \ \ \ \ \text{if }i>\alpha+1;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i=\alpha+1
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{  \alpha+1,\alpha
+2,...,\beta\right\}  \right)  .
\]


\textbf{(c)} We let $\mathcal{A}_{+}$ be the Lie subalgebra $\left\langle
a_{1},a_{2},a_{3},...\right\rangle $ of $\mathcal{A}$. This Lie subalgebra
$\mathcal{A}_{+}$ is abelian. We define an $\mathcal{A}_{+}$-module structure
on the vector space $V_{\left]  \alpha,\beta\right]  }$ by letting $a_{i}$ act
as $T_{\left]  \alpha,\beta\right]  }^{i}$ for every positive integer $i$.
(This is well-defined, since the powers of $T_{\left]  \alpha,\beta\right]  }$
commute, just as the elements of $\mathcal{A}_{+}$.) Thus, for every $\ell
\in\mathbb{N}$, the $\ell$-th exterior power $\wedge^{\ell}\left(  V_{\left]
\alpha,\beta\right]  }\right)  $ is canonically equipped with an
$\mathcal{A}_{+}$-module structure.

\textbf{(d)} For every $\ell\in\mathbb{N}$, let $R_{\ell,\left]  \alpha
,\beta\right]  }:\wedge^{\ell}\left(  V_{\left]  \alpha,\beta\right]
}\right)  \rightarrow\wedge^{\dfrac{\infty}{2},\alpha+\ell}V$ be the linear
map defined by%
\[
\left(
\begin{array}
[c]{r}%
R_{\ell,\left]  \alpha,\beta\right]  }\left(  b_{1}\wedge b_{2}\wedge...\wedge
b_{\ell}\right)  =b_{1}\wedge b_{2}\wedge...\wedge b_{\ell}\wedge v_{\alpha
}\wedge v_{\alpha-1}\wedge v_{\alpha-2}\wedge...\\
\ \ \ \ \ \ \ \ \ \ \text{for any }b_{1},b_{2},...,b_{\ell}\in V_{\left]
\alpha,\beta\right]  }%
\end{array}
\right)  .
\]

\end{definition}

\begin{remark}
\label{rmk.finitary.Valphabeta}Let $\alpha$ and $\beta$ be integers such that
$\alpha-1\leq\beta$.

\textbf{(a)} The $\left(  \beta-\alpha\right)  $-tuple $\left(  v_{\beta
},v_{\beta-1},...,v_{\alpha+1}\right)  $ is a basis of this vector space
$V_{\left]  \alpha,\beta\right]  }$. With respect to this basis, the
endomorphism $T_{\left]  \alpha,\beta\right]  }$ of $V_{\left]  \alpha
,\beta\right]  }$ is represented by the $\left(  \beta-\alpha\right)
\times\left(  \beta-\alpha\right)  $ matrix $\left(
\begin{array}
[c]{cccccc}%
0 & 0 & 0 & ... & 0 & 0\\
1 & 0 & 0 & ... & 0 & 0\\
0 & 1 & 0 & ... & 0 & 0\\
0 & 0 & 1 & ... & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & 1 & 0
\end{array}
\right)  $.

\textbf{(b)} We have $T_{\left]  \alpha,\beta\right]  }^{\beta-\alpha}=0$.

\textbf{(c)} For every sequence $\left(  y_{1},y_{2},y_{3},...\right)  $ of
elements of $\mathbb{C}$, the endomorphism $\sum\limits_{i=1}^{\infty}%
y_{i}T_{\left]  \alpha,\beta\right]  }^{i}$ of $V_{\left]  \alpha
,\beta\right]  }$ is well-defined and nilpotent.

\textbf{(d)} For every sequence $\left(  y_{1},y_{2},y_{3},...\right)  $ of
elements of $\mathbb{C}$, the endomorphism $\exp\left(  \sum\limits_{i=1}%
^{\infty}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  $ of $V_{\left]
\alpha,\beta\right]  }$ is well-defined.

\textbf{(e)} For every sequence $\left(  y_{1},y_{2},y_{3},...\right)  $ of
elements of $\mathbb{C}$, the endomorphism $\exp\left(  y_{1}a_{1}+y_{2}%
a_{2}+y_{3}a_{3}+...\right)  $ of $V_{\left]  \alpha,\beta\right]  }$ is well-defined.

\textbf{(f)} Every $j\in\mathbb{N}$ satisfies%
\begin{equation}
T_{\left]  \alpha,\beta\right]  }^{j}v_{u}=\left\{
\begin{array}
[c]{l}%
v_{u-j},\ \ \ \ \ \ \ \ \ \ \text{if }u-j>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u-j\leq\alpha
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }u\in\left\{  \alpha
+1,\alpha+2,...,\beta\right\}  . \label{pf.finitary.Valphabeta.R.Tabjvu}%
\end{equation}


\textbf{(g)} For every $n\in\mathbb{Z}$, let $c_{n}$ be an element of
$\mathbb{C}$. Assume that $c_{n}=0$ for every negative $n\in\mathbb{Z}$. Then,
the sum $\sum\limits_{k\geq0}c_{k}T_{\left]  \alpha,\beta\right]  }^{k}$ is a
well-defined endomorphism of $V_{\left]  \alpha,\beta\right]  }$, and the
matrix representing this endomorphism with respect to the basis $\left(
v_{\beta},v_{\beta-1},...,v_{\alpha+1}\right)  $ of $V_{\left]  \alpha
,\beta\right]  }$ is $\left(  c_{i-j}\right)  _{\left(  i,j\right)
\in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}$.
\end{remark}

\begin{vershort}
\textit{Proof of Remark \ref{rmk.finitary.Valphabeta}.} Parts \textbf{(a)}
through \textbf{(f)} of Remark \ref{rmk.finitary.Valphabeta} are trivial, and
part \textbf{(g)} is just the finitary analogue of Lemma
\ref{lem.schur.fermi.skew.toeplitzmatrix} and proven in the same way.
\end{vershort}

\begin{verlong}
\textit{Proof of Remark \ref{rmk.finitary.Valphabeta}.} \textbf{(a)} We know
that $\left(  v_{\alpha+1},v_{\alpha+2},...,v_{\beta}\right)  $ is a basis of
this vector space $V_{\left]  \alpha,\beta\right]  }$. Thus, $\left(
v_{\beta},v_{\beta-1},...,v_{\alpha+1}\right)  $ also is a basis of this
vector space $V_{\left]  \alpha,\beta\right]  }$. With respect to this basis,
the endomorphism $T_{\left]  \alpha,\beta\right]  }$ of $V_{\left]
\alpha,\beta\right]  }$ is represented by the $\left(  \beta-\alpha\right)
\times\left(  \beta-\alpha\right)  $ matrix $\left(
\begin{array}
[c]{cccccc}%
0 & 0 & 0 & ... & 0 & 0\\
1 & 0 & 0 & ... & 0 & 0\\
0 & 1 & 0 & ... & 0 & 0\\
0 & 0 & 1 & ... & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & 1 & 0
\end{array}
\right)  $ (this follows readily from the definition of $T_{\left]
\alpha,\beta\right]  }$). This proves Remark \ref{rmk.finitary.Valphabeta}
\textbf{(a)}.

\textbf{(b)} We know (from Remark \ref{rmk.finitary.Valphabeta} \textbf{(a)})
that the endomorphism $T_{\left]  \alpha,\beta\right]  }$ of $V_{\left]
\alpha,\beta\right]  }$ is represented by the $\left(  \beta-\alpha\right)
\times\left(  \beta-\alpha\right)  $ matrix $\left(
\begin{array}
[c]{cccccc}%
0 & 0 & 0 & ... & 0 & 0\\
1 & 0 & 0 & ... & 0 & 0\\
0 & 1 & 0 & ... & 0 & 0\\
0 & 0 & 1 & ... & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & 1 & 0
\end{array}
\right)  $. This matrix is a strictly lower-triangular $\left(  \beta
-\alpha\right)  \times\left(  \beta-\alpha\right)  $ matrix, and therefore its
$\left(  \beta-\alpha\right)  $-th power is $0$\ \ \ \ \footnote{Here, we are
using the following simple fact from linear algebra: If $n$ is a nonnegative
integer, and $M$ is a strictly lower-triangular $n\times n$-matrix, then
$M^{n}=0$.}. That is, $T_{\left]  \alpha,\beta\right]  }^{\beta-\alpha}=0$.
This proves Remark \ref{rmk.finitary.Valphabeta} \textbf{(b)}.

\textbf{(c)} Let $\left(  y_{1},y_{2},y_{3},...\right)  $ be a sequence of
elements of $\mathbb{C}$. We have $T_{\left]  \alpha,\beta\right]  }%
^{\beta-\alpha}=0$. Thus, $T_{\left]  \alpha,\beta\right]  }$ is nilpotent, so
that the endomorphism $\sum\limits_{i=1}^{\infty}y_{i}T_{\left]  \alpha
,\beta\right]  }^{i}$ is well-defined.

We have $\sum\limits_{i=1}^{\infty}y_{i}\underbrace{T_{\left]  \alpha
,\beta\right]  }^{i}}_{=T_{\left]  \alpha,\beta\right]  }^{i-1}\circ
T_{\left]  \alpha,\beta\right]  }}=\left(  \sum\limits_{i=1}^{\infty}%
y_{i}T_{\left]  \alpha,\beta\right]  }^{i-1}\right)  \circ T_{\left]
\alpha,\beta\right]  }$ (here, the endomorphism $\sum\limits_{i=1}^{\infty
}y_{i}T_{\left]  \alpha,\beta\right]  }^{i-1}$ is well-defined, since
$T_{\left]  \alpha,\beta\right]  }$ is nilpotent), so that%
\begin{align*}
\left(  \sum\limits_{i=1}^{\infty}y_{i}T_{\left]  \alpha,\beta\right]  }%
^{i}\right)  ^{\beta-\alpha}  &  =\left(  \left(  \sum\limits_{i=1}^{\infty
}y_{i}T_{\left]  \alpha,\beta\right]  }^{i-1}\right)  \circ T_{\left]
\alpha,\beta\right]  }\right)  ^{\beta-\alpha}=\left(  \sum\limits_{i=1}%
^{\infty}y_{i}T_{\left]  \alpha,\beta\right]  }^{i-1}\right)  ^{\beta-\alpha
}\circ\underbrace{T_{\left]  \alpha,\beta\right]  }^{\beta-\alpha}}_{=0}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\sum\limits_{i=1}^{\infty}%
y_{i}T_{\left]  \alpha,\beta\right]  }^{i-1}\text{ and }T_{\left]
\alpha,\beta\right]  }\text{ commute}\right) \\
&  =0,
\end{align*}
so that the endomorphism $\sum\limits_{i=1}^{\infty}y_{i}T_{\left]
\alpha,\beta\right]  }^{i}$ is nilpotent. This proves Remark
\ref{rmk.finitary.Valphabeta} \textbf{(c)}.

\textbf{(d)} Let $\left(  y_{1},y_{2},y_{3},...\right)  $ be a sequence of
elements of $\mathbb{C}$. By Remark \ref{rmk.finitary.Valphabeta}
\textbf{(c)}, the endomorphism $\sum\limits_{i=1}^{\infty}y_{i}T_{\left]
\alpha,\beta\right]  }^{i}$ is nilpotent. Thus, the endomorphism $\exp\left(
\sum\limits_{i=1}^{\infty}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  $
of $V_{\left]  \alpha,\beta\right]  }$ is well-defined. This proves Remark
\ref{rmk.finitary.Valphabeta} \textbf{(d)}.

\textbf{(e)} Let $\left(  y_{1},y_{2},y_{3},...\right)  $ be a sequence of
elements of $\mathbb{C}$. We know that the endomorphism $\exp\left(
\sum\limits_{i=1}^{\infty}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  $
is well-defined. Since $T_{\left]  \alpha,\beta\right]  }^{i}$ is the action
of $a_{i}$ on $V_{\left]  \alpha,\beta\right]  }$ for every positive integer
$i$, this endomorphism rewrites as
\[
\exp\left(  \sum\limits_{i=1}^{\infty}y_{i}\underbrace{T_{\left]  \alpha
,\beta\right]  }^{i}}_{=a_{i}}\right)  =\exp\left(  \underbrace{\sum
\limits_{i=1}^{\infty}y_{i}a_{i}}_{=y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}%
+...}\right)  =\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  .
\]
Hence, the endomorphism $\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  $ of $V_{\left]  \alpha,\beta\right]  }$ is well-defined.
This proves Remark \ref{rmk.finitary.Valphabeta} \textbf{(e)}.

\textbf{(f)} We will prove (\ref{pf.finitary.Valphabeta.R.Tabjvu}) by
induction over $j$:

\textit{Induction base:} For every $u\in\left\{  \alpha+1,\alpha
+2,...,\beta\right\}  $, we have $\underbrace{T_{\left]  \alpha,\beta\right]
}^{0}}_{=\operatorname*{id}}v_{u}=\operatorname*{id}\left(  v_{u}\right)
=v_{u}$ and $\left\{
\begin{array}
[c]{l}%
v_{u-0},\ \ \ \ \ \ \ \ \ \ \text{if }u-0>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u-0\leq\alpha
\end{array}
\right.  =\left\{
\begin{array}
[c]{l}%
v_{u},\ \ \ \ \ \ \ \ \ \ \text{if }u>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u\leq\alpha
\end{array}
\right.  =v_{u}$ (since $u>\alpha$). Thus, for every $u\in\left\{
\alpha+1,\alpha+2,...,\beta\right\}  $, we have $T_{\left]  \alpha
,\beta\right]  }^{0}v_{u}=v_{u}=\left\{
\begin{array}
[c]{l}%
v_{u-0},\ \ \ \ \ \ \ \ \ \ \text{if }u-0>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u-0\leq\alpha
\end{array}
\right.  $. This proves (\ref{pf.finitary.Valphabeta.R.Tabjvu}) for $j=0$.
This completes the induction base.

\textit{Induction step:} Let $J\in\mathbb{N}$. Assume that
(\ref{pf.finitary.Valphabeta.R.Tabjvu}) holds for $j=J$. We now must prove
that (\ref{pf.finitary.Valphabeta.R.Tabjvu}) also holds for $j=J+1$.

Since (\ref{pf.finitary.Valphabeta.R.Tabjvu}) holds for $j=J$, we have%
\begin{equation}
T_{\left]  \alpha,\beta\right]  }^{J}v_{u}=\left\{
\begin{array}
[c]{l}%
v_{u-J},\ \ \ \ \ \ \ \ \ \ \text{if }u-J>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u-J\leq\alpha
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }u\in\left\{  \alpha
+1,\alpha+2,...,\beta\right\}  . \label{pf.finitary.Valphabeta.g.1}%
\end{equation}


Now let $u\in\left\{  \alpha+1,\alpha+2,...,\beta\right\}  $ be arbitrary. We
will prove that%
\begin{equation}
T_{\left]  \alpha,\beta\right]  }^{J+1}v_{u}=\left\{
\begin{array}
[c]{l}%
v_{u-\left(  J+1\right)  },\ \ \ \ \ \ \ \ \ \ \text{if }u-\left(  J+1\right)
>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u-\left(  J+1\right)  \leq\alpha
\end{array}
\right.  . \label{pf.finitary.Valphabeta.g.2}%
\end{equation}


In order to do so, we distinguish between two cases:

\textit{Case 1:} We have $u>\alpha+1$.

\textit{Case 2:} We have $u\leq\alpha+1$.

First, consider Case 1. In this case, $u>\alpha+1$. But the definition of
$T_{\left]  \alpha,\beta\right]  }$ yields $T_{\left]  \alpha,\beta\right]
}\left(  v_{u}\right)  =\left\{
\begin{array}
[c]{l}%
v_{u-1},\ \ \ \ \ \ \ \ \ \ \text{if }u>\alpha+1;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u=\alpha+1
\end{array}
\right.  =v_{u-1}$ (since $u>\alpha+1$). Since $T_{\left]  \alpha
,\beta\right]  }^{J+1}=T_{\left]  \alpha,\beta\right]  }^{J}\circ T_{\left]
\alpha,\beta\right]  }$, we have%
\begin{align*}
T_{\left]  \alpha,\beta\right]  }^{J+1}v_{u}  &  =\left(  T_{\left]
\alpha,\beta\right]  }^{J}\circ T_{\left]  \alpha,\beta\right]  }\right)
\left(  v_{u}\right)  =T_{\left]  \alpha,\beta\right]  }^{J}\left(
\underbrace{T_{\left]  \alpha,\beta\right]  }\left(  v_{u}\right)  }%
_{=v_{u-1}}\right)  =T_{\left]  \alpha,\beta\right]  }^{J}v_{u-1}\\
&  =\left\{
\begin{array}
[c]{l}%
v_{u-1-J},\ \ \ \ \ \ \ \ \ \ \text{if }u-1-J>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u-1-J\leq\alpha
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{according to
(\ref{pf.finitary.Valphabeta.g.1}), applied to }u-1\text{ instead of }u\right)
\\
&  =\left\{
\begin{array}
[c]{l}%
v_{u-\left(  J+1\right)  },\ \ \ \ \ \ \ \ \ \ \text{if }u-\left(  J+1\right)
>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u-\left(  J+1\right)  \leq\alpha
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }u-1-J=u-\left(  J+1\right)
\right)  .
\end{align*}
This proves (\ref{pf.finitary.Valphabeta.g.2}) in the Case 1.

Now, let us consider Case 2. In this case, $u\leq\alpha+1$. Thus, $u=\alpha+1$
(since $u\in\left\{  \alpha+1,\alpha+2,...,\beta\right\}  $). Hence,
$u-\left(  J+1\right)  =\left(  \alpha+1\right)  -\left(  J+1\right)
=\alpha-\underbrace{J}_{\geq0}\leq\alpha$, so that $\left\{
\begin{array}
[c]{l}%
v_{u-\left(  J+1\right)  },\ \ \ \ \ \ \ \ \ \ \text{if }u-\left(  J+1\right)
>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u-\left(  J+1\right)  \leq\alpha
\end{array}
\right.  =0$. But the definition of $T_{\left]  \alpha,\beta\right]  }$ yields
$T_{\left]  \alpha,\beta\right]  }\left(  v_{u}\right)  =\left\{
\begin{array}
[c]{l}%
v_{u-1},\ \ \ \ \ \ \ \ \ \ \text{if }u>\alpha+1;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u=\alpha+1
\end{array}
\right.  =0$ (since $u=\alpha+1$). Since $T_{\left]  \alpha,\beta\right]
}^{J+1}=T_{\left]  \alpha,\beta\right]  }^{J}\circ T_{\left]  \alpha
,\beta\right]  }$, we have%
\begin{align*}
T_{\left]  \alpha,\beta\right]  }^{J+1}v_{u}  &  =\left(  T_{\left]
\alpha,\beta\right]  }^{J}\circ T_{\left]  \alpha,\beta\right]  }\right)
\left(  v_{u}\right)  =T_{\left]  \alpha,\beta\right]  }^{J}\left(
\underbrace{T_{\left]  \alpha,\beta\right]  }\left(  v_{u}\right)  }%
_{=0}\right)  =T_{\left]  \alpha,\beta\right]  }^{J}\left(  0\right) \\
&  =0=\left\{
\begin{array}
[c]{l}%
v_{u-\left(  J+1\right)  },\ \ \ \ \ \ \ \ \ \ \text{if }u-\left(  J+1\right)
>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }u-\left(  J+1\right)  \leq\alpha
\end{array}
\right.  .
\end{align*}
This proves (\ref{pf.finitary.Valphabeta.g.2}) in the Case 2.

We have thus proven (\ref{pf.finitary.Valphabeta.g.2}) in each of the Cases 1
and 2. Thus, (\ref{pf.finitary.Valphabeta.g.2}) always holds.

We have thus proven (\ref{pf.finitary.Valphabeta.g.2}) for every $u\in\left\{
\alpha+1,\alpha+2,...,\beta\right\}  $. In other words,
(\ref{pf.finitary.Valphabeta.R.Tabjvu}) holds for $j=J+1$. This completes the
induction step. Thus, (\ref{pf.finitary.Valphabeta.R.Tabjvu}) is proven by
induction over $j$. In other words, Remark \ref{rmk.finitary.Valphabeta}
\textbf{(f)} is proven.

\textbf{(g)} Since $T_{\left]  \alpha,\beta\right]  }^{\beta-\alpha}=0$, the
endomorphism $T_{\left]  \alpha,\beta\right]  }$ is nilpotent. Thus, the
infinite sum $\sum\limits_{k\geq0}c_{k}T_{\left]  \alpha,\beta\right]  }^{k}$
converges (with respect to the discrete topology), and hence is a well-defined
endomorphism of $V_{\left]  \alpha,\beta\right]  }$.

Let us identify every endomorphism of the vector space $V_{\left]
\alpha,\beta\right]  }$ with the matrix representing this endomorphism with
respect to the basis $\left(  v_{\beta},v_{\beta-1},...,v_{\alpha+1}\right)  $
of $V_{\left]  \alpha,\beta\right]  }$. Then, every $k\in\mathbb{N}$ and
$i\in\left\{  \alpha+1,\alpha+2,...,\beta\right\}  $ satisfy%
\begin{align*}
&  \left(  \delta_{u-v,k}\right)  _{\left(  u,v\right)  \in\left\{
1,2,...,\beta-\alpha\right\}  ^{2}}\cdot v_{i}\\
&  =\left(  \text{the }\left(  \beta+1-i\right)  \text{-th column of the
matrix }\left(  \delta_{u-v,k}\right)  _{\left(  u,v\right)  \in\left\{
1,2,...,\beta-\alpha\right\}  ^{2}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }v_{i}\text{ is the }\left(
\beta+1-i\right)  \text{-th element of the basis }\left(  v_{\beta}%
,v_{\beta-1},...,v_{\alpha+1}\right)  \right) \\
&  =\sum\limits_{u\in\left\{  1,2,...,\beta-\alpha\right\}  }%
\underbrace{\delta_{u-\left(  \beta+1-i\right)  ,k}}_{\substack{=\delta
_{i-\left(  \beta+1-u\right)  ,k}\\\text{(since }u-\left(  \beta+1-i\right)
=i-\left(  \beta+1-u\right)  \text{)}}}v_{\beta+1-u}=\sum\limits_{u\in\left\{
1,2,...,\beta-\alpha\right\}  }\delta_{i-\left(  \beta+1-u\right)  ,k}%
v_{\beta+1-u}\\
&  =\sum\limits_{u\in\left\{  \alpha+1,\alpha+2,...,\beta\right\}  }%
\delta_{i-u,k}v_{u}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted
}u\text{ for }\beta+1-u\right) \\
&  =\sum\limits_{\substack{u\in\left\{  \alpha+1,\alpha+2,...,\beta\right\}
;\\u\neq i-k}}\underbrace{\delta_{i-u,k}}_{\substack{=0\\\text{(since }i-u\neq
k\\\text{(since }u\neq i-k\text{))}}}v_{u}+\sum\limits_{\substack{u\in\left\{
\alpha+1,\alpha+2,...,\beta\right\}  ;\\u=i-k}}\underbrace{\delta_{i-u,k}%
}_{\substack{=1\\\text{(since }i-u=k\\\text{(since }u=i-k\text{))}}}v_{u}\\
&  =\underbrace{\sum\limits_{\substack{u\in\left\{  \alpha+1,\alpha
+2,...,\beta\right\}  ;\\u\neq i-k}}0v_{u}}_{=0}+\sum\limits_{\substack{u\in
\left\{  \alpha+1,\alpha+2,...,\beta\right\}  ;\\u=i-k}}v_{u}=\sum
\limits_{\substack{u\in\left\{  \alpha+1,\alpha+2,...,\beta\right\}
;\\u=i-k}}v_{u}\\
&  =\left\{
\begin{array}
[c]{l}%
v_{i-k},\ \ \ \ \ \ \ \ \ \ \text{if }i-k\in\left\{  \alpha+1,\alpha
+2,...,\beta\right\}  ;\\
0,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right. \\
&  =\left\{
\begin{array}
[c]{l}%
v_{i-k},\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i-k>\alpha\text{ and }%
i-k\leq\beta\right)  ;\\
0,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }i-k\in\left\{  \alpha
+1,\alpha+2,...,\beta\right\}  \text{ is equivalent to }\left(  i-k>\alpha
\text{ and }i-k\leq\beta\right)  \right) \\
&  =\left\{
\begin{array}
[c]{l}%
v_{i-k},\ \ \ \ \ \ \ \ \ \ \text{if }i-k>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\left(  i-k>\alpha\text{ and }i-k\leq\beta\right)  \text{ is
equivalent to }i-k>\alpha\\
\text{(because }i-k\leq\beta\text{ is automatically satisfied (since }%
i\in\left\{  \alpha+1,\alpha+2,...,\beta\right\} \\
\text{yields }i\leq\beta\text{, while }k\in\mathbb{N}\text{ yields }%
k\geq0\text{, so that }\underbrace{i}_{\leq\beta}-\underbrace{k}_{\geq0}%
\leq\beta-0=\beta\text{))}%
\end{array}
\right) \\
&  =\left\{
\begin{array}
[c]{l}%
v_{i-k},\ \ \ \ \ \ \ \ \ \ \text{if }i-k>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i-k\leq\alpha
\end{array}
\right.  .
\end{align*}
and%
\[
T_{\left]  \alpha,\beta\right]  }^{k}v_{i}=\left\{
\begin{array}
[c]{l}%
v_{i-k},\ \ \ \ \ \ \ \ \ \ \text{if }i-k>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i-k\leq\alpha
\end{array}
\right.  .
\]
(by (\ref{pf.finitary.Valphabeta.R.Tabjvu}), applied to $k$ and $i$ instead of
$j$ and $u$). Hence, every $k\in\mathbb{N}$ and $i\in\left\{  \alpha
+1,\alpha+2,...,\beta\right\}  $ satisfy%
\[
\left(  \delta_{u-v,k}\right)  _{\left(  u,v\right)  \in\left\{
1,2,...,\beta-\alpha\right\}  ^{2}}\cdot v_{i}=\left\{
\begin{array}
[c]{l}%
v_{i-k},\ \ \ \ \ \ \ \ \ \ \text{if }i-k>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i-k\leq\alpha
\end{array}
\right.  =T_{\left]  \alpha,\beta\right]  }^{k}v_{i}.
\]
Thus, $\left(  \delta_{u-v,k}\right)  _{\left(  u,v\right)  \in\left\{
1,2,...,\beta-\alpha\right\}  ^{2}}=T_{\left]  \alpha,\beta\right]  }^{k}$ for
every $k\in\mathbb{N}$. Hence,%
\begin{equation}
\sum\limits_{k\geq0}c_{k}\underbrace{T_{\left]  \alpha,\beta\right]  }^{k}%
}_{=\left(  \delta_{u-v,k}\right)  _{\left(  u,v\right)  \in\left\{
1,2,...,\beta-\alpha\right\}  ^{2}}}=\sum\limits_{k\geq0}c_{k}\left(
\delta_{u-v,k}\right)  _{\left(  u,v\right)  \in\left\{  1,2,...,\beta
-\alpha\right\}  ^{2}}=\left(  \sum\limits_{k\geq0}c_{k}\delta_{u-v,k}\right)
_{\left(  u,v\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}.
\label{pf.finitary.Valphabeta.g.7}%
\end{equation}
On the other hand, recall that $c_{n}=0$ for every negative $n\in\mathbb{Z}$.
In other words, $c_{k}=0$ for every negative $k\in\mathbb{Z}$. Hence,
$c_{k}\delta_{u-v,k}=0$ for every negative $k\in\mathbb{Z}$ and every $\left(
u,v\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}$. Thus, the sum
$\sum\limits_{k<0}c_{k}\delta_{u-v,k}$ is well-defined and equals $0$ for
every $\left(  u,v\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}$.
Hence, in $\left(  \sum\limits_{k<0}c_{k}\delta_{u-v,k}\right)  _{\left(
u,v\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}=\left(  0\right)
_{\left(  u,v\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}=0$. In
other words, $0=\left(  \sum\limits_{k<0}c_{k}\delta_{u-v,k}\right)  _{\left(
u,v\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}$. Adding this
equality to (\ref{pf.finitary.Valphabeta.g.7}), we obtain%
\begin{align*}
\sum\limits_{k\geq0}c_{k}T_{\left]  \alpha,\beta\right]  }^{k}  &  =\left(
\sum\limits_{k\geq0}c_{k}\delta_{u-v,k}\right)  _{\left(  u,v\right)
\in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}+\left(  \sum\limits_{k<0}%
c_{k}\delta_{u-v,k}\right)  _{\left(  u,v\right)  \in\left\{  1,2,...,\beta
-\alpha\right\}  ^{2}}\\
&  =\left(  \sum\limits_{k<0}c_{k}\delta_{u-v,k}+\sum\limits_{k<0}c_{k}%
\delta_{u-v,k}\right)  _{\left(  u,v\right)  \in\left\{  1,2,...,\beta
-\alpha\right\}  ^{2}}.
\end{align*}
But since every $\left(  u,v\right)  \in\left\{  1,2,...,\beta-\alpha\right\}
^{2}$ satisfies%
\begin{align*}
\sum\limits_{k\geq0}c_{k}\delta_{u-v,k}+\sum\limits_{k<0}c_{k}\delta_{u-v,k}
&  =\sum\limits_{k\in\mathbb{Z}}c_{k}\delta_{u-v,k}=\sum
\limits_{\substack{k\in\mathbb{Z};\\k\neq u-v}}c_{k}\underbrace{\delta
_{u-v,k}}_{\substack{=0\\\text{(since }u-v\neq k\text{)}}}+c_{u-v}%
\underbrace{\delta_{u-v,u-v}}_{=1}\\
&  =\underbrace{\sum\limits_{\substack{k\in\mathbb{Z};\\k\neq u-v}}c_{k}%
0}_{=0}+c_{u-v}=c_{u-v},
\end{align*}
this rewrites as $\sum\limits_{k\geq0}c_{k}T_{\left]  \alpha,\beta\right]
}^{k}=\left(  \underbrace{\sum\limits_{k<0}c_{k}\delta_{u-v,k}+\sum
\limits_{k<0}c_{k}\delta_{u-v,k}}_{=c_{u-v}}\right)  _{\left(  u,v\right)
\in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}=\left(  c_{u-v}\right)
_{\left(  u,v\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}=\left(
c_{i-j}\right)  _{\left(  i,j\right)  \in\mathbb{Z}^{2}}$ (here, we renamed
$\left(  u,v\right)  $ as $\left(  i,j\right)  $). In other words, the matrix
representing the endomorphism $\sum\limits_{k\geq0}c_{k}T_{\left]
\alpha,\beta\right]  }^{k}$ of $V_{\left]  \alpha,\beta\right]  }$ with
respect to the basis $\left(  v_{\beta},v_{\beta-1},...,v_{\alpha+1}\right)  $
of $V_{\left]  \alpha,\beta\right]  }$ is $\left(  c_{i-j}\right)  _{\left(
i,j\right)  \in\mathbb{Z}^{2}}$. This proves Remark
\ref{rmk.finitary.Valphabeta} \textbf{(g)}.
\end{verlong}

This completes the proof of Remark \ref{rmk.finitary.Valphabeta}.

A less trivial observation is the following:

\begin{proposition}
\label{prop.finitary.Valphabeta.R}Let $\alpha$ and $\beta$ be integers such
that $\alpha-1\leq\beta$. Let $\ell\in\mathbb{N}$. Then, $R_{\ell,\left]
\alpha,\beta\right]  }:\wedge^{\ell}\left(  V_{\left]  \alpha,\beta\right]
}\right)  \rightarrow\mathcal{F}^{\left(  \alpha+\ell\right)  }$ is an
$\mathcal{A}_{+}$-module homomorphism (where the $\mathcal{A}_{+}$-module
structure on $\mathcal{F}^{\left(  \alpha+\ell\right)  }$ is obtained by
restricting the $\mathcal{A}$-module structure on $\mathcal{F}^{\left(
\alpha+\ell\right)  }$).
\end{proposition}

\textit{Proof of Proposition \ref{prop.finitary.Valphabeta.R}.} Let $T$ be the
shift operator defined in Definition \ref{def.shiftoperator}.

Let $j$ be a positive integer. Then, for every integer $i\leq0$, the $\left(
i,i\right)  $-th entry of the matrix $T^{j}$ is $0$ (in fact, the matrix
$T^{j}$ has only zeroes on its main diagonal). Moreover, from the definition
of $T$, it follows quickly that%
\begin{equation}
T^{j}v_{u}=v_{u-j}\ \ \ \ \ \ \ \ \ \ \text{for every }u\in\mathbb{Z}.
\label{pf.finitary.Valphabeta.R.Tjvu}%
\end{equation}


\begin{verlong}
(Actually, (\ref{pf.finitary.Valphabeta.R.Tjvu}) follows from
(\ref{pf.schur.fermi.skew.toeplitzmatrix.1}), applied to $j$ and $u$ instead
of $k$ and $i$.)
\end{verlong}

Let $\left(  i_{0},i_{1},...,i_{\ell-1}\right)  $ be an $\ell$-tuple of
elements of $\left\{  \alpha+1,\alpha+2,...,\beta\right\}  $ such that
$i_{0}>i_{1}>...>i_{\ell-1}$. We will prove that%
\begin{equation}
a_{j}\rightharpoonup\left(  R_{\ell,\left]  \alpha,\beta\right]  }\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  \right)
=R_{\ell,\left]  \alpha,\beta\right]  }\left(  a_{j}\rightharpoonup\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  \right)  .
\label{pf.finitary.Valphabeta.R.1}%
\end{equation}


Indeed, let us extend the $\ell$-tuple $\left(  i_{0},i_{1},...,i_{\ell
-1}\right)  $ to a sequence $\left(  i_{0},i_{1},i_{2},...\right)  $ of
integers by setting $\left(  i_{k}=\ell+\alpha-k\text{ for every }k\in\left\{
\ell,\ell+1,\ell+2,...\right\}  \right)  $. Then, $\left(  i_{0},i_{1}%
,i_{2},...\right)  =\left(  i_{0},i_{1},...,i_{\ell-1},\alpha,\alpha
-1,\alpha-2,...\right)  $. As a consequence, the sequence $\left(  i_{0}%
,i_{1},i_{2},...\right)  $ is strictly decreasing (since $i_{0}>i_{1}%
>...>i_{\ell-1}>\alpha>\alpha-1>\alpha-2>...$) and hence an $\left(
\alpha+\ell\right)  $-degression. Note that
\begin{equation}
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}-j}\wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...=0\ \ \ \ \ \ \ \ \ \ \text{for every
}k\in\mathbb{N}\text{ satisfying }k\geq\ell\label{pf.finitary.Valphabeta.R.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.finitary.Valphabeta.R.2}):} Let
$k\in\mathbb{N}$ satisfy $k\geq\ell$. Then, $k+j\geq\ell$ as well (since $j$
is positive), so that $i_{k+j}=\ell+\alpha-\left(  k+j\right)
=\underbrace{\left(  \ell+\alpha-k\right)  }_{\substack{=i_{k}\\\text{(since
}k\geq\ell\text{)}}}-j=i_{k}-j$. Thus, the sequence $\left(  i_{0}%
,i_{1},...,i_{k-1},i_{k}-j,i_{k+1},i_{k+2},...\right)  $ has two equal terms
(since $k+j\neq k$ (due to $j$ being positive)). Thus, $v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...=0$. This proves (\ref{pf.finitary.Valphabeta.R.2}).}.
Also,%
\begin{equation}
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}-j}\wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...=0\ \ \ \ \ \ \ \ \ \ \text{for every
}k\in\mathbb{N}\text{ satisfying }k<\ell\text{ and }i_{k}-j\leq\alpha.
\label{pf.finitary.Valphabeta.R.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.finitary.Valphabeta.R.3}):} Let
$k\in\mathbb{N}$ satisfy $k<\ell$ and $i_{k}-j\leq\alpha$. Every integer
$\leq\alpha$ is contained in the sequence $\left(  i_{0},i_{1},i_{2}%
,...\right)  $ (since $\left(  i_{0},i_{1},i_{2},...\right)  =\left(
i_{0},i_{1},...,i_{\ell-1},\alpha,\alpha-1,\alpha-2,...\right)  $). Since
$i_{k}-j\leq\alpha$, this yields that the integer $i_{k}-j$ is contained in
the sequence $\left(  i_{0},i_{1},i_{2},...\right)  $. Hence, there exists a
$p\in\mathbb{N}$ such that $i_{p}=i_{k}-j$. Consider this $p$.
\par
Since $k<\ell$, we have $i_{k}\in\left\{  \alpha+1,\alpha+2,...,\beta\right\}
$, so that $i_{k}>\alpha\geq i_{k}-j=i_{p}$, and hence $i_{k}\neq i_{p}$.
Thus, $k\neq p$. Since $i_{k}-j=i_{p}$ and $k\neq p$, the sequence $\left(
i_{0},i_{1},...,i_{k-1},i_{k}-j,i_{k+1},i_{k+2},...\right)  $ has two equal
terms. Thus, $v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge
v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...=0$, and this proves
(\ref{pf.finitary.Valphabeta.R.3}).}

\begin{vershort}
The definition of $R_{\ell,\left]  \alpha,\beta\right]  }$ yields%
\begin{align*}
&  R_{\ell,\left]  \alpha,\beta\right]  }\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{\ell-1}}\right) \\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\wedge v_{\alpha
}\wedge v_{\alpha-1}\wedge v_{\alpha-2}\wedge...\\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge
...\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{\ell
-1},\alpha,\alpha-1,\alpha-2,...\right)  =\left(  i_{0},i_{1},i_{2}%
,...\right)  \right)  ,
\end{align*}
so that%
\begin{align}
&  a_{j}\rightharpoonup\left(  R_{\ell,\left]  \alpha,\beta\right]  }\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  \right)
\nonumber\\
&  =a_{j}\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)  =\left(  \widehat{\rho}\left(  T^{j}\right)  \right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }a_{j}\mid_{\mathcal{F}^{\left(
\alpha+\ell\right)  }}=T^{j}\mid_{\mathcal{F}^{\left(  \alpha+\ell\right)  }%
}=\widehat{\rho}\left(  T^{j}\right)  \right) \nonumber\\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\underbrace{\left(  T^{j}\rightharpoonup v_{i_{k}}\right)
}_{\substack{=T^{j}v_{i_{k}}=v_{i_{k}-j}\\\text{(by
(\ref{pf.finitary.Valphabeta.R.Tjvu}), applied to }u=i_{k}\text{)}}}\wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Proposition \ref{prop.glinf.ainfact}, applied to }\left(  b_{0}%
,b_{1},b_{2},...\right)  =\left(  v_{i_{0}},v_{i_{1}},v_{i_{2}},...\right)
\text{ and }a=T^{j}\\
\text{(since for every integer }i\leq0\text{, the }\left(  i,i\right)
\text{-th entry of }T^{j}\text{ is }0\text{).}%
\end{array}
\right) \nonumber\\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge....\nonumber
\end{align}
The sum on the right hand side of this equation is infinite, but lots of its
terms vanish: Namely, all its terms with $k\geq\ell$ vanish (because of
(\ref{pf.finitary.Valphabeta.R.2})), and all its terms with $k<\ell$ and
$i_{k}-j\leq\alpha$ vanish (because of (\ref{pf.finitary.Valphabeta.R.3})). We
can thus replace the $\sum\limits_{k\geq0}$ sign by a $\sum
\limits_{\substack{k\geq0;\\k<\ell;\\i_{k}-j>\alpha}}$ sign, and obtain%
\begin{align}
&  a_{j}\rightharpoonup\left(  R_{\ell,\left]  \alpha,\beta\right]  }\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  \right)
\nonumber\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell;\\i_{k}-j>\alpha}}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge.... \label{pf.finitary.Valphabeta.R.oneside.short}%
\end{align}


On the other hand, by the definition of the $\mathcal{A}_{+}$-module
$\wedge^{\ell}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $, we have%
\begin{align*}
&  a_{j}\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{\ell-1}}\right) \\
&  =\sum\limits_{k=0}^{\ell-1}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\underbrace{\left(  a_{j}\rightharpoonup v_{i_{k}}\right)
}_{\substack{=T_{\left]  \alpha,\beta\right]  }^{j}v_{i_{k}}\\\text{(since
}a_{j}\text{ acts as }T_{\left]  \alpha,\beta\right]  }^{j}\\\text{on
}V_{\left]  \alpha,\beta\right]  }\text{)}}}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...\wedge v_{i_{\ell-1}}\\
&  =\sum\limits_{k=0}^{\ell-1}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  T_{\left]  \alpha,\beta\right]  }^{j}v_{i_{k}%
}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\wedge v_{i_{\ell-1}}.
\end{align*}
Applying the linear map $R_{\ell,\left]  \alpha,\beta\right]  }$ to this
equation, we obtain
\begin{align*}
&  R_{\ell,\left]  \alpha,\beta\right]  }\left(  a_{j}\rightharpoonup\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  \right) \\
&  =\sum\limits_{k=0}^{\ell-1}\underbrace{R_{\ell,\left]  \alpha,\beta\right]
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(
T_{\left]  \alpha,\beta\right]  }^{j}v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\wedge v_{i_{\ell-1}}\right)  }%
_{\substack{=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(
T_{\left]  \alpha,\beta\right]  }^{j}v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\wedge v_{i_{\ell-1}}\wedge v_{\alpha}\wedge
v_{\alpha-1}\wedge v_{\alpha-2}\wedge...\\\text{(by the definition of }%
R_{\ell,\left]  \alpha,\beta\right]  }\text{)}}}\\
&  =\underbrace{\sum\limits_{k=0}^{\ell-1}}_{=\sum\limits_{\substack{k\geq
0;\\k<\ell}}}\underbrace{v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  T_{\left]  \alpha,\beta\right]  }^{j}v_{i_{k}}\right)  \wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\wedge v_{i_{\ell-1}}\wedge v_{\alpha
}\wedge v_{\alpha-1}\wedge v_{\alpha-2}\wedge...}_{\substack{=v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(  T_{\left]  \alpha
,\beta\right]  }^{j}v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}%
}\wedge...\\\text{(since }\left(  i_{0},i_{1},...,i_{\ell-1},\alpha
,\alpha-1,\alpha-2,...\right)  =\left(  i_{0},i_{1},i_{2},...\right)
\text{)}}}\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell}}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\underbrace{\left(  T_{\left]  \alpha
,\beta\right]  }^{j}v_{i_{k}}\right)  }_{\substack{=\left\{
\begin{array}
[c]{l}%
v_{i_{k}-j},\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j\leq\alpha
\end{array}
\right.  \\\text{(by (\ref{pf.finitary.Valphabeta.R.Tabjvu}), applied to
}u=i_{k}\text{)}}}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell}}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left\{
\begin{array}
[c]{l}%
v_{i_{k}-j},\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j\leq\alpha
\end{array}
\right.  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell;\\i_{k}-j>\alpha}}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge....
\end{align*}
Compared with (\ref{pf.finitary.Valphabeta.R.oneside.short}), this yields%
\[
a_{j}\rightharpoonup\left(  R_{\ell,\left]  \alpha,\beta\right]  }\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  \right)
=R_{\ell,\left]  \alpha,\beta\right]  }\left(  a_{j}\rightharpoonup\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  \right)  .
\]
We have thus proven (\ref{pf.finitary.Valphabeta.R.1}).
\end{vershort}

\begin{verlong}
The definition of $R_{\ell,\left]  \alpha,\beta\right]  }$ yields%
\begin{align*}
&  R_{\ell,\left]  \alpha,\beta\right]  }\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{\ell-1}}\right) \\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\wedge v_{\alpha
}\wedge v_{\alpha-1}\wedge v_{\alpha-2}\wedge...\\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge
...\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{\ell
-1},\alpha,\alpha-1,\alpha-2,...\right)  =\left(  i_{0},i_{1},i_{2}%
,...\right)  \right)  ,
\end{align*}
so that%
\begin{align}
&  a_{j}\rightharpoonup\left(  R_{\ell,\left]  \alpha,\beta\right]  }\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  \right)
\nonumber\\
&  =a_{j}\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}%
}\wedge...\right)  =\left(  \widehat{\rho}\left(  T^{j}\right)  \right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }a_{j}\mid_{\mathcal{F}^{\left(
\alpha+\ell\right)  }}=T^{j}\mid_{\mathcal{F}^{\left(  \alpha+\ell\right)  }%
}=\widehat{\rho}\left(  T^{j}\right)  \right) \nonumber\\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\underbrace{\left(  T^{j}\rightharpoonup v_{i_{k}}\right)
}_{\substack{=T^{j}v_{i_{k}}=v_{i_{k}-j}\\\text{(by
(\ref{pf.finitary.Valphabeta.R.Tjvu}), applied to }u=i_{k}\text{)}}}\wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Proposition \ref{prop.glinf.ainfact}, applied to }\left(  b_{0}%
,b_{1},b_{2},...\right)  =\left(  v_{i_{0}},v_{i_{1}},v_{i_{2}},...\right)
\text{ and }a=T^{j}\\
\text{(since for every integer }i\leq0\text{, the }\left(  i,i\right)
\text{-th entry of }T^{j}\text{ is }0\text{).}%
\end{array}
\right) \nonumber\\
&  =\sum\limits_{k\geq0}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\nonumber\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell}}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{k\geq0;\\k\geq\ell
}}\underbrace{v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge
v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...}%
_{\substack{=0\\\text{(due to (\ref{pf.finitary.Valphabeta.R.2}))}%
}}\nonumber\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell}}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...\nonumber\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell;\\i_{k}-j>\alpha}}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{k\geq0;\\k<\ell;\\i_{k}%
-j\leq\alpha}}\underbrace{v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...}%
_{\substack{=0\\\text{(due to (\ref{pf.finitary.Valphabeta.R.3}))}%
}}\nonumber\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell;\\i_{k}-j>\alpha}}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge.... \label{pf.finitary.Valphabeta.R.oneside}%
\end{align}
On the other hand, by the definition of the $\mathcal{A}_{+}$-module
$\wedge^{\ell}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $, we have%
\begin{align*}
&  a_{j}\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{\ell-1}}\right) \\
&  =\sum\limits_{k=0}^{\ell-1}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\underbrace{\left(  a_{j}\rightharpoonup v_{i_{k}}\right)
}_{\substack{=T_{\left]  \alpha,\beta\right]  }^{j}v_{i_{k}}\\\text{(since
}a_{j}\text{ acts as }T_{\left]  \alpha,\beta\right]  }^{j}\\\text{on
}V_{\left]  \alpha,\beta\right]  }\text{)}}}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...\wedge v_{i_{\ell-1}}\\
&  =\sum\limits_{k=0}^{\ell-1}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge\left(  T_{\left]  \alpha,\beta\right]  }^{j}v_{i_{k}%
}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\wedge v_{i_{\ell-1}},
\end{align*}
so that%
\begin{align*}
&  R_{\ell,\left]  \alpha,\beta\right]  }\left(  a_{j}\rightharpoonup\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  \right) \\
&  =R_{\ell,\left]  \alpha,\beta\right]  }\left(  \sum\limits_{k=0}^{\ell
-1}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(
T_{\left]  \alpha,\beta\right]  }^{j}v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\wedge v_{i_{\ell-1}}\right) \\
&  =\sum\limits_{k=0}^{\ell-1}\underbrace{R_{\ell,\left]  \alpha,\beta\right]
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(
T_{\left]  \alpha,\beta\right]  }^{j}v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\wedge v_{i_{\ell-1}}\right)  }%
_{\substack{=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(
T_{\left]  \alpha,\beta\right]  }^{j}v_{i_{k}}\right)  \wedge v_{i_{k+1}%
}\wedge v_{i_{k+2}}\wedge...\wedge v_{i_{\ell-1}}\wedge v_{\alpha}\wedge
v_{\alpha-1}\wedge v_{\alpha-2}\wedge...\\\text{(by the definition of }%
R_{\ell,\left]  \alpha,\beta\right]  }\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }R_{\ell,\left]  \alpha
,\beta\right]  }\text{ is linear}\right) \\
&  =\underbrace{\sum\limits_{k=0}^{\ell-1}}_{=\sum\limits_{\substack{k\geq
0;\\k<\ell}}}\underbrace{v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}%
}\wedge\left(  T_{\left]  \alpha,\beta\right]  }^{j}v_{i_{k}}\right)  \wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\wedge v_{i_{\ell-1}}\wedge v_{\alpha
}\wedge v_{\alpha-1}\wedge v_{\alpha-2}\wedge...}_{\substack{=v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\left(  T_{\left]  \alpha
,\beta\right]  }^{j}v_{i_{k}}\right)  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}%
}\wedge...\\\text{(since }\left(  i_{0},i_{1},...,i_{\ell-1},\alpha
,\alpha-1,\alpha-2,...\right)  =\left(  i_{0},i_{1},i_{2},...\right)
\text{)}}}\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell}}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\underbrace{\left(  T_{\left]  \alpha
,\beta\right]  }^{j}v_{i_{k}}\right)  }_{\substack{=\left\{
\begin{array}
[c]{l}%
v_{i_{k}-j},\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j\leq\alpha
\end{array}
\right.  \\\text{(by (\ref{pf.finitary.Valphabeta.R.Tabjvu}), applied to
}u=i_{k}\text{)}}}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell}}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{k-1}}\wedge\left\{
\begin{array}
[c]{l}%
v_{i_{k}-j},\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j\leq\alpha
\end{array}
\right.  \wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell;\\i_{k}-j>\alpha}}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge\underbrace{\left\{
\begin{array}
[c]{l}%
v_{i_{k}-j},\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j\leq\alpha
\end{array}
\right.  }_{\substack{=v_{i_{k}-j}\\\text{(since }i_{k}-j>\alpha\text{)}%
}}\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{k\geq0;\\k<\ell;\\i_{k}%
-j\leq\alpha}}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{k-1}}%
\wedge\underbrace{\left\{
\begin{array}
[c]{l}%
v_{i_{k}-j},\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j>\alpha;\\
0,\ \ \ \ \ \ \ \ \ \ \text{if }i_{k}-j\leq\alpha
\end{array}
\right.  }_{\substack{=0\\\text{(since }i_{k}-j\leq\alpha\text{)}}}\wedge
v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell;\\i_{k}-j>\alpha}}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum\limits_{\substack{k\geq
0;\\k<\ell;\\i_{k}-j\leq\alpha}}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{k-1}}\wedge0\wedge v_{i_{k+1}}\wedge v_{i_{k+2}}\wedge...}_{=0}\\
&  =\sum\limits_{\substack{k\geq0;\\k<\ell;\\i_{k}-j>\alpha}}v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{k-1}}\wedge v_{i_{k}-j}\wedge v_{i_{k+1}}\wedge
v_{i_{k+2}}\wedge....
\end{align*}
Compared with (\ref{pf.finitary.Valphabeta.R.oneside}), this yields%
\[
a_{j}\rightharpoonup\left(  R_{\ell,\left]  \alpha,\beta\right]  }\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  \right)
=R_{\ell,\left]  \alpha,\beta\right]  }\left(  a_{j}\rightharpoonup\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  \right)  .
\]
We have thus proven (\ref{pf.finitary.Valphabeta.R.1}).
\end{verlong}

Now, forget that we fixed $j$ and $\left(  i_{0},i_{1},...,i_{\ell-1}\right)
$. We have thus proven the equality (\ref{pf.finitary.Valphabeta.R.1}) for
every positive integer $j$ and every $\ell$-tuple $\left(  i_{0}%
,i_{1},...,i_{\ell-1}\right)  $ of elements of $\left\{  \alpha+1,\alpha
+2,...,\beta\right\}  $ such that $i_{0}>i_{1}>...>i_{\ell-1}$.

\begin{vershort}
Since $\mathcal{A}_{+}=\left\langle a_{1},a_{2},a_{3},...\right\rangle $ and
since $\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)
_{\beta\geq i_{0}>i_{1}>...>i_{\ell-1}\geq\alpha+1}$ is a basis of the vector
space $\wedge^{\ell}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $, this
yields (by linearity) that $x\rightharpoonup\left(  R_{\ell,\left]
\alpha,\beta\right]  }\left(  w\right)  \right)  =R_{\ell,\left]  \alpha
,\beta\right]  }\left(  x\rightharpoonup w\right)  $ holds for every
$x\in\mathcal{A}_{+}$ and $w\in\wedge^{\ell}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  $. Thus, $R_{\ell,\left]  \alpha,\beta\right]  }$ is
an $\mathcal{A}_{+}$-module homomorphism. This proves Proposition
\ref{prop.finitary.Valphabeta.R}.
\end{vershort}

\begin{verlong}
From the above, we can easily obtain that%
\begin{equation}
x\rightharpoonup\left(  R_{\ell,\left]  \alpha,\beta\right]  }\left(
w\right)  \right)  =R_{\ell,\left]  \alpha,\beta\right]  }\left(
x\rightharpoonup w\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }%
x\in\mathcal{A}_{+}\text{ and }w\in\wedge^{\ell}\left(  V_{\left]
\alpha,\beta\right]  }\right)  . \label{pf.finitary.Valphabeta.R.7}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.finitary.Valphabeta.R.7}):} Let
$x\in\mathcal{A}_{+}$ and $w\in\wedge^{\ell}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  $. Since $x\in\mathcal{A}_{+}=\left\langle
a_{1},a_{2},a_{3},...\right\rangle $, there exists a sequence $\left(
\lambda_{1},\lambda_{2},\lambda_{3},...\right)  $ of elements of $\mathbb{C}$
such that $x=\sum\limits_{j=1}^{\infty}\lambda_{j}a_{j}$ and such that all but
finitely many positive integers $j$ satisfy $\lambda_{j}=0$. Consider this
sequence $\left(  \lambda_{1},\lambda_{2},\lambda_{3},...\right)  $.
\par
We know that $\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}%
}\right)  _{\beta\geq i_{0}>i_{1}>...>i_{\ell-1}\geq\alpha+1}$ is a basis of
the vector space $\wedge^{\ell}\left(  V_{\left]  \alpha,\beta\right]
}\right)  $ (because $\left(  v_{\beta},v_{\beta-1},...,v_{\alpha+1}\right)  $
is a basis of the vector space $V_{\left]  \alpha,\beta\right]  }$). Since
$w\in\wedge^{\ell}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $, there
must thus exist a family $\left(  \alpha_{\left(  i_{0},i_{1},...,i_{\ell
-1}\right)  }\right)  _{\beta\geq i_{0}>i_{1}>...>i_{\ell-1}\geq\alpha+1}$ of
elements of $\mathbb{C}$ such that $w=\sum\limits_{\beta\geq i_{0}%
>i_{1}>...>i_{\ell-1}\geq\alpha+1}\alpha_{\left(  i_{0},i_{1},...,i_{\ell
-1}\right)  }v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}$.
Consider this family $\left(  \alpha_{\left(  i_{0},i_{1},...,i_{\ell
-1}\right)  }\right)  _{\beta\geq i_{0}>i_{1}>...>i_{\ell-1}\geq\alpha+1}$.
\par
Since $x=\sum\limits_{j=1}^{\infty}\lambda_{j}a_{j}$ and $w=\sum
\limits_{\beta\geq i_{0}>i_{1}>...>i_{\ell-1}\geq\alpha+1}\alpha_{\left(
i_{0},i_{1},...,i_{\ell-1}\right)  }v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{\ell-1}}$, we have%
\begin{align*}
x\rightharpoonup\left(  R_{\ell,\left]  \alpha,\beta\right]  }\left(
w\right)  \right)   &  =\left(  \sum\limits_{j=1}^{\infty}\lambda_{j}%
a_{j}\right)  \rightharpoonup\left(  R_{\ell,\left]  \alpha,\beta\right]
}\left(  \sum\limits_{\beta\geq i_{0}>i_{1}>...>i_{\ell-1}\geq\alpha+1}%
\alpha_{\left(  i_{0},i_{1},...,i_{\ell-1}\right)  }v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{\ell-1}}\right)  \right) \\
&  =\sum\limits_{j=1}^{\infty}\lambda_{j}\sum\limits_{\beta\geq i_{0}%
>i_{1}>...>i_{\ell-1}\geq\alpha+1}\alpha_{\left(  i_{0},i_{1},...,i_{\ell
-1}\right)  }\underbrace{a_{j}\rightharpoonup\left(  R_{\ell,\left]
\alpha,\beta\right]  }\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{\ell-1}}\right)  \right)  }_{\substack{=R_{\ell,\left]  \alpha
,\beta\right]  }\left(  a_{j}\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{\ell-1}}\right)  \right)  \\\text{(by
(\ref{pf.finitary.Valphabeta.R.1}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the action of }\mathcal{A}%
_{+}\text{ is bilinear, and }R_{\ell,\left]  \alpha,\beta\right]  }\text{ is
linear}\right) \\
&  =\sum\limits_{j=1}^{\infty}\lambda_{j}\sum\limits_{\beta\geq i_{0}%
>i_{1}>...>i_{\ell-1}\geq\alpha+1}\alpha_{\left(  i_{0},i_{1},...,i_{\ell
-1}\right)  }R_{\ell,\left]  \alpha,\beta\right]  }\left(  a_{j}%
\rightharpoonup\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}%
}\right)  \right) \\
&  =R_{\ell,\left]  \alpha,\beta\right]  }\left(  \underbrace{\left(
\sum\limits_{j=1}^{\infty}\lambda_{j}a_{j}\right)  }_{=x}\rightharpoonup
\underbrace{\left(  \sum\limits_{\beta\geq i_{0}>i_{1}>...>i_{\ell-1}%
\geq\alpha+1}\alpha_{\left(  i_{0},i_{1},...,i_{\ell-1}\right)  }v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\ell-1}}\right)  }_{=w}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the action of }\mathcal{A}%
_{+}\text{ is bilinear, and }R_{\ell,\left]  \alpha,\beta\right]  }\text{ is
linear}\right) \\
&  =R_{\ell,\left]  \alpha,\beta\right]  }\left(  x\rightharpoonup w\right)  .
\end{align*}
This proves (\ref{pf.finitary.Valphabeta.R.7}).} As a consequence,
$R_{\ell,\left]  \alpha,\beta\right]  }:\wedge^{\ell}\left(  V_{\left]
\alpha,\beta\right]  }\right)  \rightarrow\mathcal{F}^{\left(  \alpha
+\ell\right)  }$ is an $\mathcal{A}_{+}$-module homomorphism. This proves
Proposition \ref{prop.finitary.Valphabeta.R}.
\end{verlong}

Now, we can turn to the promised proof:

\textit{Second proof of Theorem \ref{thm.schur.fermi.skew}.} In order to
simplify notation, we assume that $\mathbf{R}=\mathbb{C}$. (All the arguments
that we will make in the following are independent of the ground ring, as long
as the ground ring is a commutative $\mathbb{Q}$-algebra. Therefore, we are
actually allowed to assume that $\mathbf{R}=\mathbb{C}$.) Since we assumed
that $\mathbf{R}=\mathbb{C}$, we have $\mathcal{A}_{\mathbf{R}}=\mathcal{A}$
and $\mathcal{F}_{\mathbf{R}}^{\left(  0\right)  }=\mathcal{F}^{\left(
0\right)  }$.

Since $\left(  i_{0},i_{1},i_{2},...\right)  $ is a $0$-degression, every
sufficiently high $k\in\mathbb{N}$ satisfies $i_{k}+k=0$. In other words,
there exists some $K\in\mathbb{N}$ such that every $k\in\mathbb{N}$ satisfying
$k\geq K$ satisfies $i_{k}+k=0$. Consider this $K$. WLOG assume that $K>0$
(else, replace $K$ by $K+1$). Since every $k\in\mathbb{N}$ satisfying $k\geq
K$ satisfies $i_{k}+k=0$ and thus $i_{k}=-k$, we have $\left(  i_{0}%
,i_{1},i_{2},...\right)  =\left(  i_{0},i_{1},i_{2},...,i_{K-1},-K,-\left(
K+1\right)  ,-\left(  K+2\right)  ,...\right)  =\left(  i_{0},i_{1}%
,i_{2},...,i_{K-1},-K,-K-1,-K-2,...\right)  $. In particular, $i_{K}=-K$.

Let $\alpha=i_{K}$ and $\beta=i_{0}$. Since $\left(  i_{0},i_{1}%
,i_{2},...\right)  $ is a $0$-degression, we have $i_{0}>i_{1}>i_{2}>...$.
Thus, $i_{0}>i_{1}>i_{2}>...>i_{K-1}>i_{K}$. In other words, $i_{0}\geq
i_{0}>i_{1}>i_{2}>...>i_{K-1}>i_{K}$. Since $i_{0}=\beta$ and $i_{K}=\alpha$,
this rewrites as $\beta\geq i_{0}>i_{1}>i_{2}>...>i_{K-1}>\alpha$. Thus, the
integers $i_{0}$, $i_{1}$, $i_{2}$, $...$, $i_{K-1}$ lie in the set $\left\{
\alpha+1,\alpha+2,...,\beta\right\}  $. Hence, the vectors $v_{i_{0}}$,
$v_{i_{1}}$, $...$, $v_{i_{K-1}}$ lie in the vector space $V_{\left]
\alpha,\beta\right]  }$. Thus, the definition of the map $R_{K,\left]
\alpha,\beta\right]  }$ (defined according to Definition
\ref{def.finitary.Valphabeta} \textbf{(d)}) yields%
\begin{align}
R_{K,\left]  \alpha,\beta\right]  }\left(  v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{K-1}}\right)   &  =v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{K-1}}\wedge v_{\alpha}\wedge v_{\alpha-1}\wedge
v_{\alpha-2}\wedge...\nonumber\\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\wedge v_{-K}\wedge
v_{-K-1}\wedge v_{-K-2}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\alpha=i_{K}=-K\right) \nonumber\\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...
\label{pf.finitary.Valphabeta.R.8}%
\end{align}
(since $\left(  i_{0},i_{1},i_{2},...,i_{K-1},-K,-K-1,-K-2,...\right)
=\left(  i_{0},i_{1},i_{2},...\right)  $).

For every $p\in\left\{  1,2,...,K\right\}  $, define an integer $\widetilde{i}%
_{p}$ by $\widetilde{i}_{p}=\beta+1-i_{p-1}$.

Subtracting the chain of inequalities $\beta\geq i_{0}>i_{1}>i_{2}%
>...>i_{K-1}>\alpha$ from $\beta+1$, we obtain $\beta+1-\beta\leq\beta
+1-i_{0}<\beta+1-i_{1}<\beta+1-i_{2}<...<\beta+1-i_{K-1}<\beta+1-\alpha$.
Since $\beta+1-i_{p-1}=\widetilde{i}_{p}$ for every $p\in\left\{
1,2,...,K\right\}  $, this rewrites as $\beta+1-\beta\leq\widetilde{i}%
_{1}<\widetilde{i}_{2}<\widetilde{i}_{3}<...<\widetilde{i}_{K}<\beta+1-\alpha$.

This simplifies to $1\leq\widetilde{i}_{1}<\widetilde{i}_{2}<\widetilde{i}%
_{3}<...<\widetilde{i}_{K}<\beta+1-\alpha$. Since $\widetilde{i}_{K}$ and
$\beta+1-\alpha$ are integers, we obtain $\widetilde{i}_{K}\leq\beta-\alpha$
from $\widetilde{i}_{K}<\beta+1-\alpha$. Thus, $1\leq\widetilde{i}%
_{1}<\widetilde{i}_{2}<\widetilde{i}_{3}<...<\widetilde{i}_{K}\leq\beta
-\alpha$.

On the other hand, substituting $y$ for $x$ in (\ref{def.schur.sk.genfun}), we
obtain%
\[
\sum\limits_{k\geq0}S_{k}\left(  y\right)  z^{k}=\exp\left(  \sum
\limits_{i\geq1}y_{i}z^{i}\right)  \ \ \ \ \ \ \ \ \ \ \text{in }%
\mathbb{C}\left[  \left[  z\right]  \right]  .
\]
Substituting $T_{\left]  \alpha,\beta\right]  }$ for $z$ in this equality, we
obtain
\begin{equation}
\sum\limits_{k\geq0}S_{k}\left(  y\right)  T_{\left]  \alpha,\beta\right]
}^{k}=\exp\left(  \sum\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]
}^{i}\right)  . \label{pf.schur.fermi.skew.finitary.1}%
\end{equation}


From Remark \ref{rmk.finitary.Valphabeta}, we know that the endomorphisms
$\exp\left(  \sum\limits_{i=1}^{\infty}y_{i}T_{\left]  \alpha,\beta\right]
}^{i}\right)  $ and \newline$\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  $ of $V_{\left]  \alpha,\beta\right]  }$ are well-defined.
Denote the endomorphism \newline$\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  $ of $V_{\left]  \alpha,\beta\right]  }$ by $f$. Then,
\begin{align*}
f  &  =\exp\left(  \underbrace{y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...}%
_{=\sum\limits_{i=1}^{\infty}y_{i}a_{i}}\right)  =\exp\left(  \sum
\limits_{i=1}^{\infty}y_{i}\underbrace{a_{i}}_{\substack{=T_{\left]
\alpha,\beta\right]  }^{i}\\\text{(since the action of }a_{i}\text{ on
}V_{\left]  \alpha,\beta\right]  }\\\text{was defined to be }T_{\left]
\alpha,\beta\right]  }^{i}\text{)}}}\right)  =\exp\left(  \sum\limits_{i=1}%
^{\infty}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right) \\
&  =\exp\left(  \sum\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }%
^{i}\right)  =\sum\limits_{k\geq0}S_{k}\left(  y\right)  T_{\left]
\alpha,\beta\right]  }^{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.schur.fermi.skew.finitary.1})}\right)  .
\end{align*}
Note that $S_{n}\left(  y\right)  \in\mathbb{C}$ for every $n\in\mathbb{Z}$
(since we assumed that $\mathbf{R}=\mathbb{C}$). Also note that $S_{n}\left(
y\right)  =0$ for every negative $n\in\mathbb{Z}$ (since $S_{n}=0$ for every
negative $n$). Hence, according to Remark \ref{rmk.finitary.Valphabeta}
\textbf{(g)} (applied to $c_{n}=S_{n}\left(  y\right)  $), the sum
$\sum\limits_{k\geq0}S_{k}\left(  y\right)  T_{\left]  \alpha,\beta\right]
}^{k}$ is a well-defined endomorphism of $V_{\left]  \alpha,\beta\right]  }$,
and the matrix representing this endomorphism with respect to the basis
$\left(  v_{\beta},v_{\beta-1},...,v_{\alpha+1}\right)  $ of $V_{\left]
\alpha,\beta\right]  }$ is $\left(  S_{i-j}\left(  y\right)  \right)
_{\left(  i,j\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}$. Denote
this matrix $\left(  S_{i-j}\left(  y\right)  \right)  _{\left(  i,j\right)
\in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}$ by $A$. Let $n=\beta-\alpha$,
and denote the basis $\left(  v_{\beta},v_{\beta-1},...,v_{\alpha+1}\right)  $
of $V_{\left]  \alpha,\beta\right]  }$ by $\left(  e_{1},e_{2},...,e_{n}%
\right)  $. Then,%
\begin{equation}
e_{k}=v_{\beta+1-k}\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{
1,2,...,n\right\}  . \label{pf.schur.fermi.skew.finitary.2}%
\end{equation}
As a consequence,%
\begin{equation}
e_{\beta+1-k}=v_{k}\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{
\alpha+1,\alpha+2,...,\beta\right\}  \label{pf.schur.fermi.skew.finitary.2a}%
\end{equation}
(because for every $k\in\left\{  \alpha+1,\alpha+2,...,\beta\right\}  $, we
can apply (\ref{pf.schur.fermi.skew.finitary.2}) to $\beta+1-k$ instead of
$k$, and thus obtain $e_{\beta+1-k}=v_{\beta+1-\left(  \beta+1-k\right)
}=v_{k}$).

We have shown that the matrix representing the endomorphism $\sum
\limits_{k\geq0}S_{k}\left(  y\right)  T_{\left]  \alpha,\beta\right]  }^{k}$
of $V_{\left]  \alpha,\beta\right]  }$ with respect to the basis $\left(
v_{\beta},v_{\beta-1},...,v_{\alpha+1}\right)  $ of $V_{\left]  \alpha
,\beta\right]  }$ is $\left(  S_{i-j}\left(  y\right)  \right)  _{\left(
i,j\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}$. Since
$\sum\limits_{k\geq0}S_{k}\left(  y\right)  T_{\left]  \alpha,\beta\right]
}^{k}=f$, $\left(  v_{\beta},v_{\beta-1},...,v_{\alpha+1}\right)  =\left(
e_{1},e_{2},...,e_{n}\right)  $, and $\left(  S_{i-j}\left(  y\right)
\right)  _{\left(  i,j\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}%
}=A$, this rewrites as follows: The matrix representing the endomorphism $f$
of $V_{\left]  \alpha,\beta\right]  }$ with respect to the basis $\left(
e_{1},e_{2},...,e_{n}\right)  $ of $V_{\left]  \alpha,\beta\right]  }$ is $A$.
In other words, $A$ is the $n\times n$-matrix which represents the map $f$
with respect to the bases $\left(  e_{1},e_{2},...,e_{n}\right)  $ and
$\left(  e_{1},e_{2},...,e_{n}\right)  $ of $V_{\left]  \alpha,\beta\right]
}$ and $V_{\left]  \alpha,\beta\right]  }$. Therefore, we can apply
Proposition \ref{prop.GLinf.det.fin} to $n$, $V_{\left]  \alpha,\beta\right]
}$, $V_{\left]  \alpha,\beta\right]  }$, $\left(  e_{1},e_{2},...,e_{n}%
\right)  $, $\left(  e_{1},e_{2},...,e_{n}\right)  $, $K$, $f$, $A$ and
$\left(  \widetilde{i}_{1},\widetilde{i}_{2},\widetilde{i}_{3}%
,...,\widetilde{i}_{K}\right)  $ instead of $m$, $P$, $Q$, $\left(
e_{1},e_{2},...,e_{n}\right)  $, $\left(  f_{1},f_{2},...,f_{m}\right)  $,
$\ell$, $f$, $A$ and $\left(  i_{1},i_{2},...,i_{\ell}\right)  $. As a result,
we obtain%
\begin{equation}
\left(  \wedge^{K}\left(  f\right)  \right)  \left(  e_{\widetilde{i}_{1}%
}\wedge e_{\widetilde{i}_{2}}\wedge...\wedge e_{\widetilde{i}_{K}}\right)
=\sum\limits_{\substack{j_{1}\text{, }j_{2}\text{, }...\text{, }j_{K}\text{
are }K\text{ integers;}\\1\leq j_{1}<j_{2}<...<j_{K}\leq\beta-\alpha}%
}\det\left(  A_{j_{1},j_{2},...,j_{K}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  e_{j_{1}}\wedge e_{j_{2}}\wedge...\wedge
e_{j_{K}}. \label{pf.schur.fermi.skew.finitary.3}%
\end{equation}


But every $p\in\left\{  1,2,...,K\right\}  $ satisfies $e_{\widetilde{i}_{p}%
}=v_{i_{p-1}}$\ \ \ \ \footnote{This is because $\widetilde{i}_{p}%
=\beta+1-i_{p-1}$, so that $\beta+1-\widetilde{i}_{p}=i_{p-1}$ and now%
\begin{align*}
e_{\widetilde{i}_{p}}  &  =v_{\beta+1-\widetilde{i}_{p}}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.schur.fermi.skew.finitary.2}),
applied to }\widetilde{i}_{p}\text{ instead of }k\right) \\
&  =v_{i_{p-1}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\beta+1-\widetilde{i}%
_{p}=i_{p-1}\right)  .
\end{align*}
}. Hence, $\left(  e_{\widetilde{i}_{1}},e_{\widetilde{i}_{2}}%
,...,e_{\widetilde{i}_{K}}\right)  =\left(  v_{i_{0}},v_{i_{1}},...,v_{i_{K-1}%
}\right)  $. Consequently, $e_{\widetilde{i}_{1}}\wedge e_{\widetilde{i}_{2}%
}\wedge...\wedge e_{\widetilde{i}_{K}}=v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{K-1}}$. Thus, (\ref{pf.schur.fermi.skew.finitary.3})
rewrites as%
\begin{equation}
\left(  \wedge^{K}\left(  f\right)  \right)  \left(  v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{K-1}}\right)  =\sum\limits_{\substack{j_{1}\text{,
}j_{2}\text{, }...\text{, }j_{K}\text{ are }K\text{ integers;}\\1\leq
j_{1}<j_{2}<...<j_{K}\leq\beta-\alpha}}\det\left(  A_{j_{1},j_{2},...,j_{K}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
e_{j_{1}}\wedge e_{j_{2}}\wedge...\wedge e_{j_{K}}.
\label{pf.schur.fermi.skew.finitary.4}%
\end{equation}


But $\sum\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}%
\in\mathfrak{gl}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $ is a
nilpotent linear map (by Remark \ref{rmk.finitary.Valphabeta} \textbf{(c)}).
Hence, Theorem \ref{thm.finitary.rhoRho} (applied to $P=V_{\left]
\alpha,\beta\right]  }$, $a=\sum\limits_{i\geq1}y_{i}T_{\left]  \alpha
,\beta\right]  }^{i}$ and $\ell=K$) yields that the exponential $\exp\left(
\sum\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  $ is a
well-defined element of $\operatorname*{U}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  $ and satisfies $\wedge^{K}\left(  \exp\left(
\sum\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)
\right)  =\exp\left(  \rho_{V_{\left]  \alpha,\beta\right]  },K}\left(
\sum\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)
\right)  $. Since $\exp\left(  \sum\limits_{i\geq1}y_{i}T_{\left]
\alpha,\beta\right]  }^{i}\right)  =f$, this rewrites as%
\begin{equation}
\wedge^{K}\left(  f\right)  =\exp\left(  \rho_{V_{\left]  \alpha,\beta\right]
},K}\left(  \sum\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }%
^{i}\right)  \right)  . \label{pf.schur.fermi.skew.finitary.11}%
\end{equation}


\begin{vershort}
But it is easy to see that%
\begin{equation}
\rho_{V_{\left]  \alpha,\beta\right]  },K}\left(  \sum\limits_{i\geq1}%
y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  =y_{1}a_{1}+y_{2}%
a_{2}+y_{3}a_{3}+... \label{pf.schur.fermi.skew.finitary.12.short}%
\end{equation}
as endomorphisms of $\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]
}\right)  $\ \ \ \ \footnote{\textit{Proof of
(\ref{pf.schur.fermi.skew.finitary.12.short}).} By the definition of
$\rho_{V_{\left]  \alpha,\beta\right]  },K}$, we know that $\rho_{V_{\left]
\alpha,\beta\right]  },K}:\mathfrak{gl}\left(  V_{\left]  \alpha,\beta\right]
}\right)  \rightarrow\operatorname*{End}\left(  \wedge^{K}\left(  V_{\left]
\alpha,\beta\right]  }\right)  \right)  $ denotes the representation of the
Lie algebra $\mathfrak{gl}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $
on the $K$-th exterior power of the defining representation $V_{\left]
\alpha,\beta\right]  }$ of $\mathfrak{gl}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  $. Hence,
\[
\rho_{V_{\left]  \alpha,\beta\right]  },K}\left(  \sum\limits_{i\geq1}%
y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  =\left(  \sum
\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)
\mid_{\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]  }\right)  }.
\]
Hence, every $\xi_{1},\xi_{2},...,\xi_{K}\in V_{\left]  \alpha,\beta\right]
}$ satisfy%
\begin{align*}
&  \left(  \rho_{V_{\left]  \alpha,\beta\right]  },K}\left(  \sum
\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  \right)
\left(  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi_{K}\right) \\
&  =\left(  \sum\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }%
^{i}\right)  \rightharpoonup\left(  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi
_{K}\right) \\
&  =\sum\limits_{i\geq1}y_{i}\underbrace{T_{\left]  \alpha,\beta\right]  }%
^{i}\rightharpoonup\left(  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi_{K}\right)
}_{\substack{=\sum\limits_{k=1}^{K}\xi_{1}\wedge\xi_{2}\wedge...\wedge
\xi_{k-1}\wedge\left(  T_{\left]  \alpha,\beta\right]  }^{i}\rightharpoonup
\xi_{k}\right)  \wedge\xi_{k+1}\wedge\xi_{k+2}\wedge...\wedge\xi
_{K}\\\text{(by the definition of the }\mathfrak{gl}\left(  V_{\left]
\alpha,\beta\right]  }\right)  \text{-module }\wedge^{K}\left(  V_{\left]
\alpha,\beta\right]  }\right)  \text{)}}}\\
&  =\sum\limits_{i\geq1}y_{i}\sum\limits_{k=1}^{K}\xi_{1}\wedge\xi_{2}%
\wedge...\wedge\xi_{k-1}\wedge\underbrace{\left(  T_{\left]  \alpha
,\beta\right]  }^{i}\rightharpoonup\xi_{k}\right)  }_{=T_{\left]  \alpha
,\beta\right]  }^{i}\xi_{k}}\wedge\xi_{k+1}\wedge\xi_{k+2}\wedge...\wedge
\xi_{K}\\
&  =\sum\limits_{i\geq1}y_{i}\sum\limits_{k=1}^{K}\xi_{1}\wedge\xi_{2}%
\wedge...\wedge\xi_{k-1}\wedge T_{\left]  \alpha,\beta\right]  }^{i}\xi
_{k}\wedge\xi_{k+1}\wedge\xi_{k+2}\wedge...\wedge\xi_{K}.
\end{align*}
On the other hand, every $\xi_{1},\xi_{2},...,\xi_{K}\in V_{\left]
\alpha,\beta\right]  }$ satisfy%
\begin{align*}
&  \underbrace{\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  }%
_{=\sum\limits_{i\geq1}y_{i}a_{i}}\left(  \xi_{1}\wedge\xi_{2}\wedge
...\wedge\xi_{K}\right) \\
&  =\sum\limits_{i\geq1}y_{i}\underbrace{a_{i}\left(  \xi_{1}\wedge\xi
_{2}\wedge...\wedge\xi_{K}\right)  }_{\substack{=\sum\limits_{k=1}^{K}\xi
_{1}\wedge\xi_{2}\wedge...\wedge\xi_{k-1}\wedge a_{i}\xi_{k}\wedge\xi
_{k+1}\wedge\xi_{k+2}\wedge...\wedge\xi_{K}\\\text{(by the definition of the
}\mathcal{A}_{+}\text{-module }\wedge^{K}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  \text{)}}}\\
&  =\sum\limits_{i\geq1}y_{i}\sum\limits_{k=1}^{K}\xi_{1}\wedge\xi_{2}%
\wedge...\wedge\xi_{k-1}\wedge\underbrace{a_{i}\xi_{k}}_{\substack{=T_{\left]
\alpha,\beta\right]  }^{i}\xi_{k}\\\text{(since the element }a_{i}\text{ of
}\mathcal{A}_{+}\text{ acts}\\\text{on }V_{\left]  \alpha,\beta\right]
}\text{ by }T_{\left]  \alpha,\beta\right]  }^{i}\text{)}}}\wedge\xi
_{k+1}\wedge\xi_{k+2}\wedge...\wedge\xi_{K}\\
&  =\sum\limits_{i\geq1}y_{i}\sum\limits_{k=1}^{K}\xi_{1}\wedge\xi_{2}%
\wedge...\wedge\xi_{k-1}\wedge T_{\left]  \alpha,\beta\right]  }^{i}\xi
_{k}\wedge\xi_{k+1}\wedge\xi_{k+2}\wedge...\wedge\xi_{K}\\
&  =\left(  \rho_{V_{\left]  \alpha,\beta\right]  },K}\left(  \sum
\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  \right)
\left(  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi_{K}\right)  .
\end{align*}
In other words, the two endomorphisms $y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...$
and $\rho_{V_{\left]  \alpha,\beta\right]  },K}\left(  \sum\limits_{i\geq
1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  $ of $\wedge^{K}\left(
V_{\left]  \alpha,\beta\right]  }\right)  $ are equal to each other on the set
$\left\{  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi_{K}\ \mid\ \xi_{1},\xi
_{2},...,\xi_{K}\in V_{\left]  \alpha,\beta\right]  }\right\}  $. Since the
set $\left\{  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi_{K}\ \mid\ \xi_{1}%
,\xi_{2},...,\xi_{K}\in V_{\left]  \alpha,\beta\right]  }\right\}  $ is a
spanning set of the vector space $\wedge^{K}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  $, this entails that the two endomorphisms
$y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...$ and $\rho_{V_{\left]  \alpha
,\beta\right]  },K}\left(  \sum\limits_{i\geq1}y_{i}T_{\left]  \alpha
,\beta\right]  }^{i}\right)  $ of $\wedge^{K}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  $ are identical. This proves
(\ref{pf.schur.fermi.skew.finitary.12.short}).}. Hence,
(\ref{pf.schur.fermi.skew.finitary.11}) rewrites as%
\[
\wedge^{K}\left(  f\right)  =\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}%
a_{3}+...\right)  .
\]
Hence,%
\begin{align}
&  \underbrace{\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}%
+...\right)  \right)  }_{=\wedge^{K}\left(  f\right)  }\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \nonumber\\
&  =\left(  \wedge^{K}\left(  f\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \nonumber\\
&  =\sum\limits_{\substack{j_{1}\text{, }j_{2}\text{, }...\text{, }j_{K}\text{
are }K\text{ integers;}\\1\leq j_{1}<j_{2}<...<j_{K}\leq\beta-\alpha}%
}\det\left(  A_{j_{1},j_{2},...,j_{K}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  e_{j_{1}}\wedge e_{j_{2}}\wedge...\wedge
e_{j_{K}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.schur.fermi.skew.finitary.4})}\right) \nonumber\\
&  =\sum\limits_{\substack{j_{0}\text{, }j_{1}\text{, }...\text{, }%
j_{K-1}\text{ are }K\text{ integers;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}%
<...<\beta+1-j_{K-1}\leq\beta-\alpha}}\det\left(  A_{\beta+1-j_{0}%
,\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \underbrace{e_{\beta+1-j_{0}}\wedge
e_{\beta+1-j_{1}}\wedge...\wedge e_{\beta+1-j_{K-1}}}_{\substack{=v_{j_{0}%
}\wedge v_{j_{1}}\wedge...\wedge v_{j_{K-1}}\\\text{(due to
(\ref{pf.schur.fermi.skew.finitary.2a}))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }\left(
\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}\right)  \text{ for }\left(
j_{1},j_{2},...,j_{K}\right)  \right) \nonumber\\
&  =\sum\limits_{\substack{j_{0}\text{, }j_{1}\text{, }...\text{, }%
j_{K-1}\text{ are }K\text{ integers;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}%
<...<\beta+1-j_{K-1}\leq\beta-\alpha}}\det\left(  A_{\beta+1-j_{0}%
,\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{K-1}}. \label{pf.schur.fermi.skew.finitary.22.short}%
\end{align}
But every $K$-tuple $\left(  j_{0},j_{1},...,j_{K-1}\right)  $ of integers
such that $1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha$ satisfies%
\[
\sum\limits_{\substack{j_{K}\text{, }j_{K+1}\text{, }j_{K+2}\text{, }...\text{
are integers;}\\j_{k}=-k\text{ for every }k\geq K}}\det\left(  A_{\beta
+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  =\det\left(  A_{\beta+1-j_{0}%
,\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)
\]
(since the sum $\sum\limits_{\substack{j_{K}\text{, }j_{K+1}\text{, }%
j_{K+2}\text{, }...\text{ are integers;}\\j_{k}=-k\text{ for every }k\geq
K}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)  $ has
only one addend). Thus, (\ref{pf.schur.fermi.skew.finitary.22.short}) becomes%
\begin{align*}
&  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \\
&  =\sum\limits_{\substack{j_{0}\text{, }j_{1}\text{, }...\text{, }%
j_{K-1}\text{ are }K\text{ integers;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}%
<...<\beta+1-j_{K-1}\leq\beta-\alpha}}\underbrace{\det\left(  A_{\beta
+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  }_{=\sum\limits_{\substack{j_{K}\text{,
}j_{K+1}\text{, }j_{K+2}\text{, }...\text{ are integers;}\\j_{k}=-k\text{ for
every }k\geq K}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta
+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}%
}\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ v_{j_{0}}\wedge v_{j_{1}}%
\wedge...\wedge v_{j_{K-1}}\\
&  =\underbrace{\sum\limits_{\substack{j_{0}\text{, }j_{1}\text{, }...\text{,
}j_{K-1}\text{ are }K\text{ integers;}\\1\leq\beta+1-j_{0}<\beta
+1-j_{1}<...<\beta+1-j_{K-1}\leq\beta-\alpha}}\sum\limits_{\substack{j_{K}%
\text{, }j_{K+1}\text{, }j_{K+2}\text{, }...\text{ are integers;}%
\\j_{k}=-k\text{ for every }k\geq K}}}_{=\sum\limits_{\substack{\left(
j_{0},j_{1},j_{2},...\right)  \in\mathbb{Z}^{\mathbb{N}}\text{;}%
\\j_{k}=-k\text{ for every }k\geq K\text{;}\\1\leq\beta+1-j_{0}<\beta
+1-j_{1}<...<\beta+1-j_{K-1}\leq\beta-\alpha}}}\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \det\left(  A_{\beta+1-j_{0}%
,\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{K-1}}\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \in
\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{K-1}}.
\end{align*}
Applying the linear map $R_{K,\left]  \alpha,\beta\right]  }$ to this
equality, we obtain%
\begin{align}
&  R_{K,\left]  \alpha,\beta\right]  }\left(  \left(  \exp\left(  y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right)  \right) \nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \in
\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \underbrace{R_{K,\left]
\alpha,\beta\right]  }\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{K-1}}\right)  }_{\substack{=v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{K-1}}\wedge v_{\alpha}\wedge v_{\alpha-1}\wedge v_{\alpha-2}%
\wedge...\\\text{(by the definition of }R_{K,\left]  \alpha,\beta\right]
}\text{)}}}\nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \in
\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ v_{j_{0}}\wedge v_{j_{1}}%
\wedge...\wedge v_{j_{K-1}}\wedge v_{\alpha}\wedge v_{\alpha-1}\wedge
v_{\alpha-2}\wedge...\nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \in
\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \underbrace{v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{K-1}}\wedge v_{-K}\wedge v_{-K-1}\wedge
v_{-K-2}\wedge...}_{\substack{=v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\\\text{(since every }k\geq K\text{ satisfies }-k=j_{k}\text{, and
therefore}\\\left(  j_{0},j_{1},...,j_{K-1},-K,-K-1,-K-2,...\right)  =\left(
j_{0},j_{1},...,j_{K-1},j_{K},j_{K+1},j_{K+2},...\right)  =\left(  j_{0}%
,j_{1},j_{2},...\right)  \text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\alpha=i_{K}=-K\right) \nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \in
\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge....
\label{pf.schur.fermi.skew.finitary.32.short}%
\end{align}
Let us now notice that when $\left(  j_{0},j_{1},j_{2},...\right)
\in\mathbb{Z}^{\mathbb{N}}$ is a sequence of integers satisfying $\left(
j_{k}=-k\text{ for every }k\geq K\right)  $, then a very straightforward
argument shows that $1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}%
\leq\beta-\alpha$ holds if and only if $\left(  j_{0},j_{1},j_{2},...\right)
$ is a $0$-degression satisfying $j_{0}\leq\beta$. Hence, we can replace the
sum sign $\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)
\in\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}$ by $\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2}%
,...\right)  \text{ is a }0\text{-degression;}\\j_{k}=-k\text{ for every
}k\geq K\text{;}\\j_{0}\leq\beta}}$ in
(\ref{pf.schur.fermi.skew.finitary.32.short}). Hence,
(\ref{pf.schur.fermi.skew.finitary.32.short}) becomes%
\begin{align}
&  R_{K,\left]  \alpha,\beta\right]  }\left(  \left(  \exp\left(  y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right)  \right) \nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\j_{k}=-k\text{ for every }k\geq K\text{;}\\j_{0}%
\leq\beta}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge....
\label{pf.schur.fermi.skew.finitary.35.short}%
\end{align}
But it is easily revealed that%
\begin{equation}
\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}%
_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)  =S_{\left(
i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq0}}\left(
y\right)  \label{pf.schur.fermi.skew.finitary.36.short}%
\end{equation}
for any $0$-degression $\left(  j_{0},j_{1},j_{2},...\right)  $ satisfying
$\left(  j_{k}=-k\text{ for every }k\geq K\right)  $ and $j_{0}\leq\beta
$\ \ \ \ \footnote{The proof of (\ref{pf.schur.fermi.skew.finitary.36.short})
is completely straightforward and left to the reader. The ingredients of the
proof are the equality $A=\left(  S_{i-j}\left(  y\right)  \right)  _{\left(
i,j\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}}$ (which we used to
define $A$), the definition of $\widetilde{i}_{v}$ (namely, $\widetilde{i}%
_{v}=\beta+1-i_{v-1}$ for every $v\in\left\{  1,2,...,K\right\}  $), and the
definition of the skew Schur function $S_{\lambda\diagup\mu}\left(  x\right)
$ as a determinant of a (finite!) matrix.}. Therefore,
(\ref{pf.schur.fermi.skew.finitary.35.short}) simplifies to%
\begin{align}
&  R_{K,\left]  \alpha,\beta\right]  }\left(  \left(  \exp\left(  y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right)  \right) \nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\j_{k}=-k\text{ for every }k\geq K\text{;}\\j_{0}%
\leq\beta}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)
_{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}%
}\wedge.... \label{pf.schur.fermi.skew.finitary.42.short}%
\end{align}

\end{vershort}

\begin{verlong}
But it is easy to see that%
\begin{equation}
\rho_{V_{\left]  \alpha,\beta\right]  },K}\left(  \sum\limits_{i\geq1}%
y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  =y_{1}a_{1}+y_{2}%
a_{2}+y_{3}a_{3}+... \label{pf.schur.fermi.skew.finitary.12}%
\end{equation}
as endomorphisms of $\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]
}\right)  $\ \ \ \ \footnote{\textit{Proof of
(\ref{pf.schur.fermi.skew.finitary.12}).} By the definition of $\rho
_{V_{\left]  \alpha,\beta\right]  },K}$, we know that $\rho_{V_{\left]
\alpha,\beta\right]  },K}:\mathfrak{gl}\left(  V_{\left]  \alpha,\beta\right]
}\right)  \rightarrow\operatorname*{End}\left(  \wedge^{K}\left(  V_{\left]
\alpha,\beta\right]  }\right)  \right)  $ denotes the representation of the
Lie algebra $\mathfrak{gl}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $
on the $K$-th exterior power of the defining representation $V_{\left]
\alpha,\beta\right]  }$ of $\mathfrak{gl}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  $. Hence,
\[
\rho_{V_{\left]  \alpha,\beta\right]  },K}\left(  \sum\limits_{i\geq1}%
y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  =\left(  \sum
\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)
\mid_{\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]  }\right)  }.
\]
Hence, every $\xi_{1},\xi_{2},...,\xi_{K}\in V_{\left]  \alpha,\beta\right]
}$ satisfy%
\begin{align*}
&  \left(  \rho_{V_{\left]  \alpha,\beta\right]  },K}\left(  \sum
\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  \right)
\left(  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi_{K}\right) \\
&  =\left(  \left(  \sum\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]
}^{i}\right)  \mid_{\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]
}\right)  }\right)  \left(  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi_{K}\right)
\\
&  =\left(  \sum\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }%
^{i}\right)  \rightharpoonup\left(  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi
_{K}\right) \\
&  =\sum\limits_{i\geq1}y_{i}\underbrace{T_{\left]  \alpha,\beta\right]  }%
^{i}\rightharpoonup\left(  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi_{K}\right)
}_{\substack{=\sum\limits_{k=1}^{K}\xi_{1}\wedge\xi_{2}\wedge...\wedge
\xi_{k-1}\wedge\left(  T_{\left]  \alpha,\beta\right]  }^{i}\rightharpoonup
\xi_{k}\right)  \wedge\xi_{k+1}\wedge\xi_{k+2}\wedge...\wedge\xi
_{K}\\\text{(by the definition of the }\mathfrak{gl}\left(  V_{\left]
\alpha,\beta\right]  }\right)  \text{-module }\wedge^{K}\left(  V_{\left]
\alpha,\beta\right]  }\right)  \text{)}}}\\
&  =\sum\limits_{i\geq1}y_{i}\sum\limits_{k=1}^{K}\xi_{1}\wedge\xi_{2}%
\wedge...\wedge\xi_{k-1}\wedge\underbrace{\left(  T_{\left]  \alpha
,\beta\right]  }^{i}\rightharpoonup\xi_{k}\right)  }_{=T_{\left]  \alpha
,\beta\right]  }^{i}\xi_{k}}\wedge\xi_{k+1}\wedge\xi_{k+2}\wedge...\wedge
\xi_{K}\\
&  =\sum\limits_{i\geq1}y_{i}\sum\limits_{k=1}^{K}\xi_{1}\wedge\xi_{2}%
\wedge...\wedge\xi_{k-1}\wedge T_{\left]  \alpha,\beta\right]  }^{i}\xi
_{k}\wedge\xi_{k+1}\wedge\xi_{k+2}\wedge...\wedge\xi_{K}.
\end{align*}
On the other hand, every $\xi_{1},\xi_{2},...,\xi_{K}\in V_{\left]
\alpha,\beta\right]  }$ satisfy%
\begin{align*}
&  \underbrace{\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  }%
_{=\sum\limits_{i\geq1}y_{i}a_{i}}\left(  \xi_{1}\wedge\xi_{2}\wedge
...\wedge\xi_{K}\right) \\
&  =\sum\limits_{i\geq1}y_{i}\underbrace{a_{i}\left(  \xi_{1}\wedge\xi
_{2}\wedge...\wedge\xi_{K}\right)  }_{\substack{=\sum\limits_{k=1}^{K}\xi
_{1}\wedge\xi_{2}\wedge...\wedge\xi_{k-1}\wedge a_{i}\xi_{k}\wedge\xi
_{k+1}\wedge\xi_{k+2}\wedge...\wedge\xi_{K}\\\text{(by the definition of the
}\mathcal{A}_{+}\text{-module }\wedge^{K}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  \text{)}}}\\
&  =\sum\limits_{i\geq1}y_{i}\sum\limits_{k=1}^{K}\xi_{1}\wedge\xi_{2}%
\wedge...\wedge\xi_{k-1}\wedge\underbrace{a_{i}\xi_{k}}_{\substack{=T_{\left]
\alpha,\beta\right]  }^{i}\xi_{k}\\\text{(since the element }a_{i}\text{ of
}\mathcal{A}_{+}\text{ acts}\\\text{on }V_{\left]  \alpha,\beta\right]
}\text{ by }T_{\left]  \alpha,\beta\right]  }^{i}\text{)}}}\wedge\xi
_{k+1}\wedge\xi_{k+2}\wedge...\wedge\xi_{K}\\
&  =\sum\limits_{i\geq1}y_{i}\sum\limits_{k=1}^{K}\xi_{1}\wedge\xi_{2}%
\wedge...\wedge\xi_{k-1}\wedge T_{\left]  \alpha,\beta\right]  }^{i}\xi
_{k}\wedge\xi_{k+1}\wedge\xi_{k+2}\wedge...\wedge\xi_{K}\\
&  =\left(  \rho_{V_{\left]  \alpha,\beta\right]  },K}\left(  \sum
\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  \right)
\left(  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi_{K}\right)  .
\end{align*}
In other words, the two endomorphisms $y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...$
and $\rho_{V_{\left]  \alpha,\beta\right]  },K}\left(  \sum\limits_{i\geq
1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  $ of $\wedge^{K}\left(
V_{\left]  \alpha,\beta\right]  }\right)  $ are equal to each other on the set
$\left\{  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi_{K}\ \mid\ \xi_{1},\xi
_{2},...,\xi_{K}\in V_{\left]  \alpha,\beta\right]  }\right\}  $. Since the
set $\left\{  \xi_{1}\wedge\xi_{2}\wedge...\wedge\xi_{K}\ \mid\ \xi_{1}%
,\xi_{2},...,\xi_{K}\in V_{\left]  \alpha,\beta\right]  }\right\}  $ is a
spanning set of the vector space $\wedge^{K}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  $, this yields the following: The two endomorphisms
$y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...$ and $\rho_{V_{\left]  \alpha
,\beta\right]  },K}\left(  \sum\limits_{i\geq1}y_{i}T_{\left]  \alpha
,\beta\right]  }^{i}\right)  $ of $\wedge^{K}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  $ are equal to each other on a spanning set of the
vector space $\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $.
But when two linear maps from the same domain are equal to each other on a
spanning set of their domain, then these two maps must be identical. In
particular, if two endomorphisms of a vector space are equal to each other on
a spanning set of this vector space, then these two endomorphisms must be
identical. Applying this to the two endomorphisms $y_{1}a_{1}+y_{2}a_{2}%
+y_{3}a_{3}+...$ and $\rho_{V_{\left]  \alpha,\beta\right]  },K}\left(
\sum\limits_{i\geq1}y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  $ of
$\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $, we conclude
that the two endomorphisms $y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...$ and
$\rho_{V_{\left]  \alpha,\beta\right]  },K}\left(  \sum\limits_{i\geq1}%
y_{i}T_{\left]  \alpha,\beta\right]  }^{i}\right)  $ of $\wedge^{K}\left(
V_{\left]  \alpha,\beta\right]  }\right)  $ are identical. This proves
(\ref{pf.schur.fermi.skew.finitary.12}).}. Hence,
(\ref{pf.schur.fermi.skew.finitary.11}) rewrites as%
\[
\wedge^{K}\left(  f\right)  =\exp\left(  \underbrace{\rho_{V_{\left]
\alpha,\beta\right]  },K}\left(  \sum\limits_{i\geq1}y_{i}T_{\left]
\alpha,\beta\right]  }^{i}\right)  }_{\substack{=y_{1}a_{1}+y_{2}a_{2}%
+y_{3}a_{3}+...\\\text{(by (\ref{pf.schur.fermi.skew.finitary.12}))}}}\right)
=\exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  .
\]
Hence,%
\begin{align}
&  \underbrace{\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}%
+...\right)  \right)  }_{=\wedge^{K}\left(  f\right)  }\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \nonumber\\
&  =\left(  \wedge^{K}\left(  f\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \nonumber\\
&  =\sum\limits_{\substack{j_{1}\text{, }j_{2}\text{, }...\text{, }j_{K}\text{
are }K\text{ integers;}\\1\leq j_{1}<j_{2}<...<j_{K}\leq\beta-\alpha}%
}\det\left(  A_{j_{1},j_{2},...,j_{K}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  e_{j_{1}}\wedge e_{j_{2}}\wedge...\wedge
e_{j_{K}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.schur.fermi.skew.finitary.4})}\right) \nonumber\\
&  =\sum\limits_{\substack{j_{1}\text{, }j_{2}\text{, }...\text{, }j_{K}\text{
are }K\text{ integers;}\\1\leq\beta+1-j_{1}<\beta+1-j_{2}<...<\beta
+1-j_{K}\leq\beta-\alpha}}\det\left(  A_{\beta+1-j_{1},\beta+1-j_{2}%
,...,\beta+1-j_{K}}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}%
_{K}}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \underbrace{e_{\beta+1-j_{1}}\wedge
e_{\beta+1-j_{2}}\wedge...\wedge e_{\beta+1-j_{K}}}_{\substack{=v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\wedge v_{j_{K}}\\\text{(since every }p\in\left\{
1,2,...,K\right\}  \text{ satisfies }e_{\beta+1-j_{p}}=v_{j_{p}}\text{ (by
(\ref{pf.schur.fermi.skew.finitary.2a}), applied to }k=j_{p}\text{),}%
\\\text{so that }\left(  e_{\beta+1-j_{1}},e_{\beta+1-j_{2}},...,e_{\beta
+1-j_{K}}\right)  =\left(  v_{j_{1}},v_{j_{2}},...,v_{j_{K}}\right)  \text{
and thus}\\e_{\beta+1-j_{1}}\wedge e_{\beta+1-j_{2}}\wedge...\wedge
e_{\beta+1-j_{K}}=v_{j_{1}}\wedge v_{j_{2}}\wedge...\wedge v_{j_{K}}\text{)}%
}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }\left(
\beta+1-j_{1},\beta+1-j_{2},...,\beta+1-j_{K}\right)  \text{ for }\left(
j_{1},j_{2},...,j_{K}\right)  \right) \nonumber\\
&  =\sum\limits_{\substack{j_{1}\text{, }j_{2}\text{, }...\text{, }j_{K}\text{
are }K\text{ integers;}\\1\leq\beta+1-j_{1}<\beta+1-j_{2}<...<\beta
+1-j_{K}\leq\beta-\alpha}}\det\left(  A_{\beta+1-j_{1},\beta+1-j_{2}%
,...,\beta+1-j_{K}}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}%
_{K}}\right)  v_{j_{1}}\wedge v_{j_{2}}\wedge...\wedge v_{j_{K}}\nonumber\\
&  =\sum\limits_{\substack{j_{0}\text{, }j_{1}\text{, }...\text{, }%
j_{K-1}\text{ are }K\text{ integers;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}%
<...<\beta+1-j_{K-1}\leq\beta-\alpha}}\det\left(  A_{\beta+1-j_{0}%
,\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{K-1}}\label{pf.schur.fermi.skew.finitary.22}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed }\left(  j_{1}%
,j_{2},...,j_{K}\right)  \text{ as }\left(  j_{0},j_{1},...,j_{K-1}\right)
\right)  .\nonumber
\end{align}
But every $K$-tuple $\left(  j_{0},j_{1},...,j_{K-1}\right)  $ of integers
such that $1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha$ satisfies%
\begin{align}
&  \sum\limits_{\substack{j_{K}\text{, }j_{K+1}\text{, }j_{K+2}\text{,
}...\text{ are integers;}\\j_{k}=-k\text{ for every }k\geq K}}\det\left(
A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}%
_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right) \nonumber\\
&  =\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
\label{pf.schur.fermi.skew.finitary.23}%
\end{align}
(since the sum $\sum\limits_{\substack{j_{K}\text{, }j_{K+1}\text{, }%
j_{K+2}\text{, }...\text{ are integers;}\\j_{k}=-k\text{ for every }k\geq
K}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)  $ has
only one addend). Thus, (\ref{pf.schur.fermi.skew.finitary.22}) becomes%
\begin{align*}
&  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \\
&  =\sum\limits_{\substack{j_{0}\text{, }j_{1}\text{, }...\text{, }%
j_{K-1}\text{ are }K\text{ integers;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}%
<...<\beta+1-j_{K-1}\leq\beta-\alpha}}\underbrace{\det\left(  A_{\beta
+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  }_{=\sum\limits_{\substack{j_{K}\text{,
}j_{K+1}\text{, }j_{K+2}\text{, }...\text{ are integers;}\\j_{k}=-k\text{ for
every }k\geq K}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta
+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}%
}\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ v_{j_{0}}\wedge v_{j_{1}}%
\wedge...\wedge v_{j_{K-1}}\\
&  =\underbrace{\sum\limits_{\substack{j_{0}\text{, }j_{1}\text{, }...\text{,
}j_{K-1}\text{ are }K\text{ integers;}\\1\leq\beta+1-j_{0}<\beta
+1-j_{1}<...<\beta+1-j_{K-1}\leq\beta-\alpha}}\sum\limits_{\substack{j_{K}%
\text{, }j_{K+1}\text{, }j_{K+2}\text{, }...\text{ are integers;}%
\\j_{k}=-k\text{ for every }k\geq K}}}_{=\sum\limits_{\substack{j_{0}\text{,
}j_{1}\text{, }j_{2}\text{, }...\text{ are integers;}\\j_{k}=-k\text{ for
every }k\geq K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta
+1-j_{K-1}\leq\beta-\alpha}}=\sum\limits_{\substack{\left(  j_{0},j_{1}%
,j_{2},...\right)  \in\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for
every }k\geq K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta
+1-j_{K-1}\leq\beta-\alpha}}}\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \det\left(  A_{\beta+1-j_{0}%
,\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{K-1}}\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \in
\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{K-1}},
\end{align*}
so that
\begin{align}
&  R_{K,\left]  \alpha,\beta\right]  }\left(  \left(  \exp\left(  y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right)  \right) \nonumber\\
&  =R_{K,\left]  \alpha,\beta\right]  }\left(  \sum\limits_{\substack{\left(
j_{0},j_{1},j_{2},...\right)  \in\mathbb{Z}^{\mathbb{N}}\text{;}%
\\j_{k}=-k\text{ for every }k\geq K\text{;}\\1\leq\beta+1-j_{0}<\beta
+1-j_{1}<...<\beta+1-j_{K-1}\leq\beta-\alpha}}\det\left(  A_{\beta
+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{K-1}}\right) \nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \in
\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \underbrace{R_{K,\left]
\alpha,\beta\right]  }\left(  v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{K-1}}\right)  }_{\substack{=v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge
v_{j_{K-1}}\wedge v_{\alpha}\wedge v_{\alpha-1}\wedge v_{\alpha-2}%
\wedge...\\\text{(by the definition of }R_{K,\left]  \alpha,\beta\right]
}\text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }R_{K,\left]  \alpha,\beta\right]
}\text{ is linear}\right) \nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \in
\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ v_{j_{0}}\wedge v_{j_{1}}%
\wedge...\wedge v_{j_{K-1}}\wedge v_{\alpha}\wedge v_{\alpha-1}\wedge
v_{\alpha-2}\wedge...\nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \in
\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \underbrace{v_{j_{0}}\wedge
v_{j_{1}}\wedge...\wedge v_{j_{K-1}}\wedge v_{-K}\wedge v_{-K-1}\wedge
v_{-K-2}\wedge...}_{\substack{=v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\\\text{(since every }k\geq K\text{ satisfies }-k=j_{k}\text{,
hence}\\\left(  -K,-K-1,-K-2,...\right)  =\left(  j_{K},j_{K+1},j_{K+2}%
,...\right)  \text{ and thus}\\\left(  j_{0},j_{1},...,j_{K-1}%
,-K,-K-1,-K-2,...\right)  =\left(  j_{0},j_{1},...,j_{K-1},j_{K}%
,j_{K+1},j_{K+2},...\right)  =\left(  j_{0},j_{1},j_{2},...\right)
\\\text{and thus }v_{j_{0}}\wedge v_{j_{1}}\wedge...\wedge v_{j_{K-1}}\wedge
v_{-K}\wedge v_{-K-1}\wedge v_{-K-2}\wedge...=v_{j_{0}}\wedge v_{j_{1}}\wedge
v_{j_{2}}\wedge...\text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\alpha=i_{K}=-K\right) \nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \in
\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge....
\label{pf.schur.fermi.skew.finitary.32}%
\end{align}
Let us now notice that when $\left(  j_{0},j_{1},j_{2},...\right)
\in\mathbb{Z}^{\mathbb{N}}$ is a sequence of integers satisfying $\left(
j_{k}=-k\text{ for every }k\geq K\right)  $, then $1\leq\beta+1-j_{0}%
<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq\beta-\alpha$ holds if and only if
$\left(  j_{0},j_{1},j_{2},...\right)  $ is a $0$-degression satisfying
$j_{0}\leq\beta$\ \ \ \ \footnote{\textit{Proof.} Let $\left(  j_{0}%
,j_{1},j_{2},...\right)  \in\mathbb{Z}^{\mathbb{N}}$ be a sequence of integers
satisfying $\left(  j_{k}=-k\text{ for every }k\geq K\right)  $. We need to
prove the following two assertions:
\par
\textit{Assertion 1:} If $1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta
+1-j_{K-1}\leq\beta-\alpha$, then $\left(  j_{0},j_{1},j_{2},...\right)  $ is
a $0$-degression satisfying $j_{0}\leq\beta$.
\par
\textit{Assertion 2:} If $\left(  j_{0},j_{1},j_{2},...\right)  $ is a
$0$-degression satisfying $j_{0}\leq\beta$, then $1\leq\beta+1-j_{0}%
<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq\beta-\alpha$.
\par
\textit{Proof of Assertion 1:} Assume that $1\leq\beta+1-j_{0}<\beta
+1-j_{1}<...<\beta+1-j_{K-1}\leq\beta-\alpha$. Subtracting this chain of
inequalities from $\beta+1$, we obtain%
\[
\beta+1-1\geq\beta+1-\left(  \beta+1-j_{0}\right)  >\beta+1-\left(
\beta+1-j_{1}\right)  >...>\beta+1-\left(  \beta+1-j_{K-1}\right)  \geq
\beta+1-\left(  \beta+\alpha\right)  .
\]
This simplifies to $\beta\geq j_{0}>j_{1}>...>j_{K-1}\geq\alpha+1$ (since
$\beta+1-1=\beta$, since $\beta+1-\left(  \beta+1-j_{p}\right)  =j_{p}$ for
every $p\in\left\{  0,1,...,K-1\right\}  $, and since $\beta+1-\left(
\beta+\alpha\right)  =\alpha+1$). Thus, $\beta\geq j_{0}$, so that $j_{0}%
\leq\beta$. Also $-K>-\left(  K+1\right)  >-\left(  K+2\right)  >...$. Since
$j_{k}=-k$ for every $k\geq K$, this rewrites as $j_{K}>j_{K+1}>j_{K+2}>...$.
Also, since $j_{k}=-k$ for every $k\geq K$, we have $j_{K}=-K$. Compared with
$i_{K}=-K$, this yields $j_{K}=i_{K}=\alpha$. Thus, $j_{K-1}\geq
\alpha+1>\alpha=j_{K}$. Combined with $j_{0}>j_{1}>...>j_{K-1}$, this yields
$j_{0}>j_{1}>...>j_{K}$. Combined with $j_{K}>j_{K+1}>j_{K+2}>...$, this
yields $j_{0}>j_{1}>j_{2}>...$. Thus, $\left(  j_{0},j_{1},j_{2},...\right)  $
is a strictly decreasing sequence of integers. Since we know that $\left(
j_{k}=-k\text{ for every }k\geq K\right)  $, this yields that $\left(
j_{0},j_{1},j_{2},...\right)  $ is a $0$-degression. Recall also that
$j_{0}\leq\beta$. This proves Assertion 1.
\par
\textit{Proof of Assertion 2:} Assume that $\left(  j_{0},j_{1},j_{2}%
,...\right)  $ is a $0$-degression satisfying $j_{0}\leq\beta$. Since $\left(
j_{0},j_{1},j_{2},...\right)  $ is a $0$-degression, we have $j_{0}%
>j_{1}>...>j_{K-1}>j_{K}$.
\par
Recall that $\left(  j_{k}=-k\text{ for every }k\geq K\right)  $. Applied to
$k=K$, this yields $j_{K}=-K$. Compared with $i_{K}=-K$, this yields
$j_{K}=i_{K}=\alpha$. Thus, $j_{K-1}>j_{K}=\alpha$. Since $j_{K-1}$ and
$\alpha$ are integers, this yields $j_{K-1}\geq\alpha+1$. Combined with
$j_{0}>j_{1}>...>j_{K-1}$, this becomes $j_{0}>j_{1}>...>j_{K-1}\geq\alpha+1$.
Combined with $\beta\geq j_{0}$ (since $j_{0}\leq\beta$), this becomes
$\beta\geq j_{0}>j_{1}>...>j_{K-1}\geq\alpha+1$. Subtracting this chain of
inequalities from $\beta+1$, we obtain $\beta+1-1\leq\beta+1-j_{0}%
<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq\beta+1-\left(  \alpha+1\right)  $.
Since $\beta+1-1=\beta$ and $\beta+1-\left(  \alpha+1\right)  =\beta-\alpha$,
this simplifies to $1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}%
\leq\beta-\alpha$. This proves Assertion 2.
\par
Now, both Assertions 1 and 2 are proven. Combining these two assertions, we
conclude that $1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha$ holds if and only if $\left(  j_{0},j_{1},j_{2},...\right)  $ is
a $0$-degression satisfying $j_{0}\leq\beta$, qed.}. Hence, we can replace the
sum sign $\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)
\in\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}$ by $\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2}%
,...\right)  \in\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every
}k\geq K\text{;}\\\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression such that }j_{0}\leq\beta}}$ in
(\ref{pf.schur.fermi.skew.finitary.32}). Hence,
(\ref{pf.schur.fermi.skew.finitary.32}) becomes%
\begin{align}
&  R_{K,\left]  \alpha,\beta\right]  }\left(  \left(  \exp\left(  y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right)  \right) \nonumber\\
&  =\underbrace{\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)
\in\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\1\leq\beta+1-j_{0}<\beta+1-j_{1}<...<\beta+1-j_{K-1}\leq
\beta-\alpha}}}_{=\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)
\in\mathbb{Z}^{\mathbb{N}}\text{;}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a }%
0\text{-degression such that }j_{0}\leq\beta}}=\sum\limits_{\substack{\left(
j_{0},j_{1},j_{2},...\right)  \text{ is a }0\text{-degression;}\\j_{k}%
=-k\text{ for every }k\geq K\text{;}\\j_{0}\leq\beta}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \det\left(  A_{\beta+1-j_{0}%
,\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}%
_{2},...,\widetilde{i}_{K}}\right)  v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}%
}\wedge...\nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\j_{k}=-k\text{ for every }k\geq K\text{;}\\j_{0}%
\leq\beta}}\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge....
\label{pf.schur.fermi.skew.finitary.35}%
\end{align}
But it is easily revealed that%
\begin{equation}
\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}%
_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)  =S_{\left(
i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq0}}\left(
y\right)  \label{pf.schur.fermi.skew.finitary.36}%
\end{equation}
for any $0$-degression $\left(  j_{0},j_{1},j_{2},...\right)  $ satisfying
$\left(  j_{k}=-k\text{ for every }k\geq K\right)  $ and $j_{0}\leq\beta
$\ \ \ \ \footnote{\textit{Proof of (\ref{pf.schur.fermi.skew.finitary.36}):}
Let $\left(  j_{0},j_{1},j_{2},...\right)  $ be a $0$-degression satisfying
$\left(  j_{k}=-k\text{ for every }k\geq K\right)  $ and $j_{0}\leq\beta$. By
the definition of the matrix $A$, we have $A=\left(  S_{i-j}\left(  y\right)
\right)  _{\left(  i,j\right)  \in\left\{  1,2,...,\beta-\alpha\right\}  ^{2}%
}$. Hence,%
\begin{align*}
&  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}%
_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\\
&  =\left(  S_{\left(  \beta+1-j_{u-1}\right)  -\widetilde{i}_{v}}\left(
y\right)  \right)  _{\left(  u,v\right)  \in\left\{  1,2,...,K\right\}  ^{2}%
}=\left(  S_{\left(  \beta+1-j_{u-1}\right)  -\left(  \beta+1-i_{v-1}\right)
}\left(  y\right)  \right)  _{\left(  u,v\right)  \in\left\{
1,2,...,K\right\}  ^{2}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\widetilde{i}_{v}=\beta
+1-i_{v-1}\text{ (by the definition of }\widetilde{i}_{v}\text{) for every
}v\in\left\{  1,2,...,K\right\}  \right) \\
&  =\left(  S_{i_{v-1}-j_{u-1}}\left(  y\right)  \right)  _{\left(
u,v\right)  \in\left\{  1,2,...,K\right\}  ^{2}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  \beta+1-j_{u-1}\right)
-\left(  \beta+1-i_{v-1}\right)  =i_{v-1}-j_{u-1}\text{ for every }\left(
u,v\right)  \in\left\{  1,2,...,K\right\}  ^{2}\right)  .
\end{align*}
\par
Now, define two partitions $\lambda$ and $\mu$ by $\lambda=\left(
i_{k}+k\right)  _{k\geq0}$ and $\mu=\left(  j_{k}+k\right)  _{k\geq0}$. Write
the partitions $\lambda$ and $\mu$ in the forms $\lambda=\left(  \lambda
_{1},\lambda_{2},\lambda_{3},...\right)  $ and $\mu=\left(  \mu_{1},\mu
_{2},\mu_{3},...\right)  $. Then, $\lambda_{v}=i_{v-1}+\left(  v-1\right)  $
for every $v\in\left\{  1,2,3,...\right\}  $, and $\mu_{u}=j_{u-1}+\left(
u-1\right)  $ for every $u\in\left\{  1,2,3,...\right\}  $. Thus, for every
$\left(  u,v\right)  \in\left\{  1,2,...,K\right\}  ^{2}$, we have%
\begin{equation}
\underbrace{\lambda_{v}}_{=i_{v-1}+\left(  v-1\right)  }-\underbrace{\mu_{u}%
}_{=j_{u-1}+\left(  u-1\right)  }+u-v=\left(  i_{v-1}+\left(  v-1\right)
\right)  -\left(  j_{u-1}+\left(  u-1\right)  \right)  +u-v=i_{v-1}-j_{u-1}.
\label{pf.schur.fermi.skew.finitary.37}%
\end{equation}
\par
But every integer $v\geq K+1$ satisfies $\lambda_{v}=i_{v-1}+\left(
v-1\right)  =0$ (because every integer $v\geq K+1$ satisfies $v-1\geq K$, and
therefore $i_{v-1}+\left(  v-1\right)  =0$ (due to the fact that $\left(
i_{k}+k=0\text{ for every }k\geq K\right)  $, applied to $k=v-1$). Hence, the
partition $\left(  \lambda_{1},\lambda_{2},\lambda_{3},...\right)  $ can be
written in the form $\left(  \lambda_{1},\lambda_{2},...,\lambda_{K}\right)
$.
\par
Also, every integer $u\geq K+1$ satisfies $\mu_{u}=j_{u-1}+\left(  u-1\right)
=0$ (because every integer $u\geq K+1$ satisfies $u-1\geq K$, and therefore
$j_{u-1}+\left(  u-1\right)  =0$ (due to the fact that $\left(  j_{k}%
+k=0\text{ for every }k\geq K\right)  $, applied to $k=u-1$). Hence, the
partition $\left(  \mu_{1},\mu_{2},\mu_{3},...\right)  $ can be written in the
form $\left(  \mu_{1},\mu_{2},...,\mu_{K}\right)  $.
\par
Since $\lambda=\left(  \lambda_{1},\lambda_{2},\lambda_{3},...\right)
=\left(  \lambda_{1},\lambda_{2},...,\lambda_{K}\right)  $ and $\mu=\left(
\mu_{1},\mu_{2},\mu_{3},...\right)  =\left(  \mu_{1},\mu_{2},...,\mu
_{K}\right)  $, the definition of $S_{\lambda\diagup\mu}\left(  x\right)  $
yields $S_{\lambda\diagup\mu}\left(  x\right)  =\det\left(
\underbrace{\left(  S_{\lambda_{i}-\mu_{j}+j-i}\left(  x\right)  \right)
_{1\leq i\leq K,\ 1\leq j\leq K}}_{=\left(  S_{\lambda_{i}-\mu_{j}+j-i}\left(
x\right)  \right)  _{\left(  i,j\right)  \in\left\{  1,2,...,K\right\}  ^{2}}%
}\right)  =\det\left(  \left(  S_{\lambda_{i}-\mu_{j}+j-i}\left(  x\right)
\right)  _{\left(  i,j\right)  \in\left\{  1,2,...,K\right\}  ^{2}}\right)
=\left(  \left(  S_{\lambda_{v}-\mu_{u}+u-v}\left(  y\right)  \right)
_{\left(  v,u\right)  \in\left\{  1,2,...,K\right\}  ^{2}}\right)  $ (here, we
substituted $\left(  v,u\right)  $ for $\left(  i,j\right)  $). Substituting
$y$ for $x$ in this equality, we obtain%
\begin{align*}
S_{\lambda\diagup\mu}\left(  y\right)   &  =\det\left(  \left(  S_{\lambda
_{v}-\mu_{u}+u-v}\left(  y\right)  \right)  _{\left(  v,u\right)  \in\left\{
1,2,...,K\right\}  ^{2}}\right) \\
&  =\det\left(  \underbrace{\left(  S_{i_{v-1}-j_{u-1}}\left(  y\right)
\right)  _{\left(  v,u\right)  \in\left\{  1,2,...,K\right\}  ^{2}}%
}_{=A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}}^{\widetilde{i}%
_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.schur.fermi.skew.finitary.37}%
)}\right) \\
&  =\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)  .
\end{align*}
Thus, $\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1},...,\beta+1-j_{K-1}%
}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}_{K}}\right)
=S_{\lambda\diagup\mu}\left(  y\right)  =S_{\left(  i_{k}+k\right)  _{k\geq
0}\diagup\left(  j_{k}+k\right)  _{k\geq0}}\left(  y\right)  $ (since
$\lambda=\left(  i_{k}+k\right)  _{k\geq0}$ and $\mu=\left(  j_{k}+k\right)
_{k\geq0}$). This proves (\ref{pf.schur.fermi.skew.finitary.36}).}. Therefore,
(\ref{pf.schur.fermi.skew.finitary.35}) becomes%
\begin{align}
&  R_{K,\left]  \alpha,\beta\right]  }\left(  \left(  \exp\left(  y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right)  \right) \nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\j_{k}=-k\text{ for every }k\geq K\text{;}\\j_{0}%
\leq\beta}}\underbrace{\det\left(  A_{\beta+1-j_{0},\beta+1-j_{1}%
,...,\beta+1-j_{K-1}}^{\widetilde{i}_{1},\widetilde{i}_{2},...,\widetilde{i}%
_{K}}\right)  }_{\substack{=S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \\\text{(by
(\ref{pf.schur.fermi.skew.finitary.36}))}}}v_{j_{0}}\wedge v_{j_{1}}\wedge
v_{j_{2}}\wedge...\nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\j_{k}=-k\text{ for every }k\geq K\text{;}\\j_{0}%
\leq\beta}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)
_{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}%
}\wedge.... \label{pf.schur.fermi.skew.finitary.42}%
\end{align}

\end{verlong}

But Proposition \ref{prop.finitary.Valphabeta.R} (applied to $K$ instead of
$\ell$) yields that $R_{K,\left]  \alpha,\beta\right]  }:\wedge^{K}\left(
V_{\left]  \alpha,\beta\right]  }\right)  \rightarrow\mathcal{F}^{\left(
\alpha+K\right)  }$ is an $\mathcal{A}_{+}$-module homomorphism. Since
$\underbrace{\alpha}_{=i_{K}=-K}+K=-K+K=0$, this rewrites as follows:
$R_{K,\left]  \alpha,\beta\right]  }:\wedge^{K}\left(  V_{\left]  \alpha
,\beta\right]  }\right)  \rightarrow\mathcal{F}^{\left(  0\right)  }$ is an
$\mathcal{A}_{+}$-module homomorphism.

\begin{vershort}
Now, $\mathcal{A}_{+}$ is a graded Lie subalgebra of $\mathcal{A}$, and it is
easy to define a grading on the $\mathcal{A}_{+}$-module $\wedge^{K}\left(
V_{\left]  \alpha,\beta\right]  }\right)  $ such that $\wedge^{K}\left(
V_{\left]  \alpha,\beta\right]  }\right)  $ is concentrated in nonpositive
degrees.\footnote{Indeed, let us define a grading on the vector space
$V_{\left]  \alpha,\beta\right]  }$ by setting the degree of $v_{i}$ to be
$\alpha+1-i$ for every $i\in\left\{  \alpha+1,\alpha+2,...,\beta\right\}  $.
Then, the vector space $V_{\left]  \alpha,\beta\right]  }$ is concentrated in
nonpositive degrees, so that its $K$-th exterior power $\wedge^{K}\left(
V_{\left]  \alpha,\beta\right]  }\right)  $ is also concentrated in
nonpositive degrees. On the other hand, $V_{\left]  \alpha,\beta\right]  }$ is
a graded $\mathcal{A}_{+}$-module (this is very easy to check), so that its
$K$-th exterior power $\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]
}\right)  $ is also a graded $\mathcal{A}_{+}$-module.} Thus, applying
Proposition \ref{prop.schur.fermi.welldef.A+} \textbf{(b)} to $\mathbb{C}$,
$\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $, $\mathcal{F}%
^{\left(  0\right)  }$ and $R_{K,\left]  \alpha,\beta\right]  }$ instead of
$\mathbf{R}$, $M$, $N$ and $\eta$, we obtain%
\[
\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\circ R_{K,\left]  \alpha,\beta\right]  }=R_{K,\left]  \alpha,\beta\right]
}\circ\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
\right)
\]
as maps from $\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $ to
$\mathcal{F}^{\left(  0\right)  }$. Hence,%
\begin{align*}
&  \left(  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
\right)  \circ R_{K,\left]  \alpha,\beta\right]  }\right)  \left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \\
&  =\left(  R_{K,\left]  \alpha,\beta\right]  }\circ\left(  \exp\left(
y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \\
&  =R_{K,\left]  \alpha,\beta\right]  }\left(  \left(  \exp\left(  y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right)  \right) \\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\j_{k}=-k\text{ for every }k\geq K\text{;}\\j_{0}%
\leq\beta}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)
_{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}%
}\wedge...\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.schur.fermi.skew.finitary.42.short})}\right)  .
\end{align*}
Compared with%
\begin{align*}
&  \left(  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
\right)  \circ R_{K,\left]  \alpha,\beta\right]  }\right)  \left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \\
&  =\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\underbrace{\left(  R_{K,\left]  \alpha,\beta\right]  }\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right)  \right)  }_{\substack{=v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\\\text{(by
(\ref{pf.finitary.Valphabeta.R.8}))}}}\\
&  =\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  ,
\end{align*}
this becomes%
\begin{align}
&  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\j_{k}=-k\text{ for every }k\geq K\text{;}\\j_{0}%
\leq\beta}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)
_{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}%
}\wedge...\nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}\\j_{k}=-k\text{ for every }k\geq K\text{;}%
\\j_{0}\leq\beta}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge... \label{pf.schur.fermi.skew.finitary.48.short}%
\end{align}
(here, we deprived the sum of all addends for which $\left(  j_{k}+k\right)
_{k\geq0}\not \subseteq \left(  i_{k}+k\right)  _{k\geq0}$, because
(\ref{pf.schur.fermi.notcontained}) shows that all such addends are $0$).
\end{vershort}

\begin{verlong}
Let us define a grading on the vector space $V_{\left]  \alpha,\beta\right]
}$ by setting the degree of $v_{i}$ to be $\alpha+1-i$ for every $i\in\left\{
\alpha+1,\alpha+2,...,\beta\right\}  $. Then, the vector space $V_{\left]
\alpha,\beta\right]  }$ is concentrated in nonpositive degrees, so that its
$K$-th exterior power $\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]
}\right)  $ is also concentrated in nonpositive degrees. On the other hand,
$\mathcal{A}_{+}$ is a graded Lie subalgebra of $\mathcal{A}$, and $V_{\left]
\alpha,\beta\right]  }$ is a graded $\mathcal{A}_{+}$-module (this is very
easy to check), so that its $K$-th exterior power $\wedge^{K}\left(
V_{\left]  \alpha,\beta\right]  }\right)  $ is also a graded $\mathcal{A}_{+}$-module.

Now, applying Proposition \ref{prop.schur.fermi.welldef.A+} \textbf{(b)} to
$\mathbb{C}$, $\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $,
$\mathcal{F}^{\left(  0\right)  }$ and $R_{K,\left]  \alpha,\beta\right]  }$
instead of $\mathbf{R}$, $M$, $N$ and $\eta$, we obtain%
\[
\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\circ R_{K,\left]  \alpha,\beta\right]  }=R_{K,\left]  \alpha,\beta\right]
}\circ\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
\right)
\]
as maps from $\wedge^{K}\left(  V_{\left]  \alpha,\beta\right]  }\right)  $ to
$\mathcal{F}^{\left(  0\right)  }$. Hence,%
\begin{align*}
&  \left(  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
\right)  \circ R_{K,\left]  \alpha,\beta\right]  }\right)  \left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \\
&  =\left(  R_{K,\left]  \alpha,\beta\right]  }\circ\left(  \exp\left(
y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \right)  \left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \\
&  =R_{K,\left]  \alpha,\beta\right]  }\left(  \left(  \exp\left(  y_{1}%
a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)  \left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right)  \right) \\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\j_{k}=-k\text{ for every }k\geq K\text{;}\\j_{0}%
\leq\beta}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)
_{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}%
}\wedge...\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.schur.fermi.skew.finitary.42})}\right)  .
\end{align*}
Compared with%
\begin{align*}
&  \left(  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)
\right)  \circ R_{K,\left]  \alpha,\beta\right]  }\right)  \left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right) \\
&  =\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\underbrace{\left(  R_{K,\left]  \alpha,\beta\right]  }\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{K-1}}\right)  \right)  }_{\substack{=v_{i_{0}%
}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\\\text{(by
(\ref{pf.finitary.Valphabeta.R.8}))}}}\\
&  =\left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  ,
\end{align*}
this becomes%
\begin{align}
&  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\j_{k}=-k\text{ for every }k\geq K\text{;}\\j_{0}%
\leq\beta}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)
_{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}%
}\wedge...\nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}\\j_{k}=-k\text{ for every }k\geq K\text{;}%
\\j_{0}\leq\beta}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum\limits_{\substack{\left(  j_{0},j_{1}%
,j_{2},...\right)  \text{ is a }0\text{-degression;}\\\left(  j_{k}+k\right)
_{k\geq0}\not \subseteq \left(  i_{k}+k\right)  _{k\geq0}\\j_{k}=-k\text{ for
every }k\geq K\text{;}\\j_{0}\leq\beta}}\underbrace{S_{\left(  i_{k}+k\right)
_{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot
v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...}_{\substack{=0\\\text{(by
(\ref{pf.schur.fermi.notcontained}), since }\left(  j_{0},j_{1},j_{2}%
,...\right)  \text{ is a }0\text{-degression}\\\text{satisfying }\left(
j_{k}+k\right)  _{k\geq0}\not \subseteq \left(  i_{k}+k\right)  _{k\geq
0}\text{)}}}\nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}\\j_{k}=-k\text{ for every }k\geq K\text{;}%
\\j_{0}\leq\beta}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...+\underbrace{\sum\limits_{\substack{\left(
j_{0},j_{1},j_{2},...\right)  \text{ is a }0\text{-degression;}\\\left(
j_{k}+k\right)  _{k\geq0}\not \subseteq \left(  i_{k}+k\right)  _{k\geq
0}\\j_{k}=-k\text{ for every }k\geq K\text{;}\\j_{0}\leq\beta}}0}%
_{=0}\nonumber\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}\\j_{k}=-k\text{ for every }k\geq K\text{;}%
\\j_{0}\leq\beta}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge.... \label{pf.schur.fermi.skew.finitary.48}%
\end{align}

\end{verlong}

\begin{vershort}
But for any $0$-degression $\left(  j_{0},j_{1},j_{2},...\right)  $ satisfying
$\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(  i_{k}+k\right)  _{k\geq0}$,
we automatically have $\left(  j_{k}=-k\text{ for every }k\geq K\right)  $ and
$j_{0}\leq\beta$ (this is very easy to see). Hence, we can replace the
summation sign $\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)
\text{ is a }0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}%
\subseteq\left(  i_{k}+k\right)  _{k\geq0}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\j_{0}\leq\beta}}$ on the right hand side of
(\ref{pf.schur.fermi.skew.finitary.48.short}) by a $\sum
\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}}}$ sign. Thus,
(\ref{pf.schur.fermi.skew.finitary.48.short}) simplifies to%
\begin{align*}
&  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge....
\end{align*}
This proves Theorem \ref{thm.schur.fermi.skew}.
\end{vershort}

\begin{verlong}
But for any $0$-degression $\left(  j_{0},j_{1},j_{2},...\right)  $ satisfying
$\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(  i_{k}+k\right)  _{k\geq0}$,
we automatically have $\left(  j_{k}=-k\text{ for every }k\geq K\right)  $ and
$j_{0}\leq\beta$\ \ \ \ \footnote{\textit{Proof.} Let $\left(  j_{0}%
,j_{1},j_{2},...\right)  $ be a $0$-degression satisfying $\left(
j_{k}+k\right)  _{k\geq0}\subseteq\left(  i_{k}+k\right)  _{k\geq0}$. Since
$\left(  j_{k}+k\right)  _{k\geq0}$ is a partition, every $k\geq0$ satisfies
$j_{k}+k\geq0$.
\par
Now recall that $\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}$. In other words, $j_{k}+k\leq i_{k}+k$ for every
$k\geq0$. In other words, $j_{k}\leq i_{k}$ for every $k\geq0$. Applied to
$k=0$, this yields $j_{0}\leq i_{0}=\beta$.
\par
Now, let $k\in\mathbb{N}$ satisfy $k\geq K$. Then, $j_{k}\leq i_{k}$ (since
$k\geq0$) and $i_{k}=-k$ (since $k\geq K$). Hence, $j_{k}\leq i_{k}=-k$.
Combined with $j_{k}\geq-k$ (since $j_{k}+k=0$), this yields $j_{k}=-k$. Now
forget that we fixed $k$. We thus have proven that $\left(  j_{k}=-k\text{ for
every }k\geq K\right)  $.
\par
Altogether, we now know that $\left(  j_{k}=-k\text{ for every }k\geq
K\right)  $ and $j_{0}\leq\beta$, qed.}. Hence, we can replace the summation
sign $\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}\\j_{k}=-k\text{ for every }k\geq K\text{;}%
\\j_{0}\leq\beta}}$ on the right hand side of
(\ref{pf.schur.fermi.skew.finitary.48}) by a $\sum\limits_{\substack{\left(
j_{0},j_{1},j_{2},...\right)  \text{ is a }0\text{-degression;}\\\left(
j_{k}+k\right)  _{k\geq0}\subseteq\left(  i_{k}+k\right)  _{k\geq0}}}$ sign.
Thus, (\ref{pf.schur.fermi.skew.finitary.48}) becomes%
\begin{align*}
&  \left(  \exp\left(  y_{1}a_{1}+y_{2}a_{2}+y_{3}a_{3}+...\right)  \right)
\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right) \\
&  =\underbrace{\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)
\text{ is a }0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}%
\subseteq\left(  i_{k}+k\right)  _{k\geq0}\\j_{k}=-k\text{ for every }k\geq
K\text{;}\\j_{0}\leq\beta}}}_{=\sum\limits_{\substack{\left(  j_{0}%
,j_{1},j_{2},...\right)  \text{ is a }0\text{-degression;}\\\left(
j_{k}+k\right)  _{k\geq0}\subseteq\left(  i_{k}+k\right)  _{k\geq0}}%
}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(  j_{k}+k\right)  _{k\geq
0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\\
&  =\sum\limits_{\substack{\left(  j_{0},j_{1},j_{2},...\right)  \text{ is a
}0\text{-degression;}\\\left(  j_{k}+k\right)  _{k\geq0}\subseteq\left(
i_{k}+k\right)  _{k\geq0}}}S_{\left(  i_{k}+k\right)  _{k\geq0}\diagup\left(
j_{k}+k\right)  _{k\geq0}}\left(  y\right)  \cdot v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge....
\end{align*}
This proves Theorem \ref{thm.schur.fermi.skew}.
\end{verlong}

\subsection{Applications to integrable systems}

Let us show how these things can be applied to partial differential equations.

\begin{Convention}
If $v$ is a function in several variables $x_{1}$, $x_{2}$, $...$, $x_{k}$,
then, for every $i\in\left\{  1,2,...,k\right\}  $, the derivative of $v$ by
the variable $x_{i}$ will be denoted by $\partial_{x_{i}}v$ and by $v_{x_{i}}%
$. In other words, $\partial_{x_{i}}v=v_{x_{i}}=\dfrac{\partial}{\partial
x_{i}}v$. (For example, if $v$ is a function in two variables $x$ and $t$,
then $v_{t}$ will mean the derivative of $v$ by $t$.)
\end{Convention}

The PDE (partial differential equation) we will be concerned with is the
\textbf{Korteweg-de Vries equation} (abbreviated as \textbf{KdV equation}%
)\textbf{:} This is the equation $u_{t}=\dfrac{3}{2}uu_{x}+\dfrac{1}{4}%
u_{xxx}$ for a function $u\left(  t,x\right)  $.\ \ \ \ \footnote{There seems
to be no consistent definition of the KdV equation across literature. We
defined the KdV equation as $u_{t}=\dfrac{3}{2}uu_{x}+\dfrac{1}{4}u_{xxx}$
because this is the form most suited to our approach. Some other authors,
instead, define the KdV equation as $v_{t}=v_{xxx}+6vv_{x}$ for a function
$v\left(  t,x\right)  $. Others define it as $w_{t}+ww_{x}+w_{xxx}=0$ for a
function $w\left(  t,x\right)  $. Yet others define it as $q_{t}%
+q_{xxx}+6qq_{x}=0$ for a function $q\left(  t,x\right)  $. These equations
are not literally equivalent, but can be transformed into each other by very
simple substitutions. In fact, for a function $u\left(  t,x\right)  $, we have
the following equivalence of assertions:%
\begin{align*}
&  \ \left(  \text{the function }u\left(  t,x\right)  \text{ satisfies the
equation }u_{t}=\dfrac{3}{2}uu_{x}+\dfrac{1}{4}u_{xxx}\right) \\
&  \Longleftrightarrow\ \left(  \text{the function }v\left(  t,x\right)
:=u\left(  4t,x\right)  \text{ satisfies the equation }v_{t}=v_{xxx}%
+6vv_{x}\right) \\
&  \Longleftrightarrow\ \left(  \text{the function }w\left(  t,x\right)
:=6u\left(  -4t,x\right)  \text{ satisfies the equation }w_{t}+ww_{x}%
+w_{xxx}=0\right) \\
&  \Longleftrightarrow\ \left(  \text{the function }q\left(  t,x\right)
:=u\left(  -4t,x\right)  \text{ satisfies the equation }q_{t}+q_{xxx}%
+6qq_{x}=0\right)  .
\end{align*}
}

We will discuss several interesting solutions of this equation. Here is the
most basic family of solutions:%
\[
u\left(  t\right)  =\dfrac{2a^{2}}{\cosh^{2}\left(  a\left(  x+a^{2}t\right)
\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{for }a\text{ being arbitrary but
fixed}\right)  .
\]
These are so-called ``traveling wave solutions''. It is a peculiar kind of
wave: it has only one bump; it is therefore called a \textit{soliton} (or
\textit{solitary wave}). Such waves never occur in linear systems. Note that
when we speak of ``wave'', we are imagining a time-dependent 2-dimensional
graph with the x-axis showing $t$, the y-axis showing $u\left(  t\right)  $,
and the time parameter being $x$. So when we speak of ``traveling wave'', we
mean that it is a wave for any fixed time $x$ and ``travels'' when $x$ moves.

The first to study this kind of waves was J. S. Russell in 1834, describing
the motion of water in a shallow canal (tsunami waves are similar). The first
models for these waves were found by Korteweg-de Vries in 1895.

The term $\dfrac{1}{4}u_{xxx}$ in the Korteweg-de Vries equation $u_{t}%
=\dfrac{3}{2}uu_{x}+\dfrac{1}{4}u_{xxx}$ is called the \textit{dispersion
term}.

\textbf{Exercise:} Solve the equation $u_{t}=\dfrac{3}{2}uu_{x}$. (Note that
the waves solving this equation develop shocks, in contrast to those solving
the Korteweg-de Vries equation.)

The Korteweg-de Vries equation is famous for having lots of explicit solutions
(unexpectedly for a nonlinear partial differential equation). We will
construct some of them using infinite-dimensional Lie algebras. (There are
many other ways to construct solutions. In some sense, every field of
mathematics is related to some of its solutions.)

We will also study the \textbf{Kadomtsev-Petviashvili equation} (abbreviated
as \textbf{KP equation})
\[
u_{yy}=\left(  u_{t}-\dfrac{3}{2}uu_{x}-\dfrac{1}{4}u_{xxx}\right)  _{x}%
\]
(or, after some rescaling, $\dfrac{3}{4}\partial_{y}^{2}u=\partial_{x}\left(
\partial_{t}u-\dfrac{3}{2}u\partial_{x}u-\dfrac{1}{4}\partial_{x}^{3}u\right)
$) on a function $u\left(  t,x,y\right)  $. We will obtain functions which
solve this equation (among others).

We are going to use the \textit{infinite Grassmannian} for this. First, recall
what the \textit{finite Grassmannian} is:

\subsubsection{The finite Grassmannian}

\begin{definition}
Let $k$ and $n$ be integers satisfying $0\leq k\leq n$. Let $V$ be the
$\mathbb{C}$-vector space $\mathbb{C}^{n}$. Let $\left(  v_{1},v_{2}%
,...,v_{n}\right)  $ be the standard basis of $\mathbb{C}^{n}$. Recall that
$\wedge^{k}V$ is a representation of $\operatorname*{GL}\left(  V\right)  $
with a highest-weight vector $v_{1}\wedge v_{2}\wedge...\wedge v_{k}$. Denote
by $\Omega$ the orbit of $v_{1}\wedge v_{2}\wedge...\wedge v_{k}$ under
$\operatorname*{GL}\left(  V\right)  $.
\end{definition}

\begin{proposition}
Let $k$ and $n$ be integers satisfying $0\leq k\leq n$. We have $\Omega
=\left\{  x\in\wedge^{k}V\text{ nonzero}\ \mid\ x=x_{1}\wedge x_{2}%
\wedge...\wedge x_{k}\text{ for some }x_{i}\in V\right\}  $. Also,
$x_{1}\wedge x_{2}\wedge...\wedge x_{k}\neq0$ if and only if $x_{1}$, $x_{2}$,
$...$, $x_{k}$ are linearly independent.
\end{proposition}

\textit{Proof.} Very easy.

\begin{definition}
Let $V$ be a $\mathbb{C}$-vector space. Let $k$ be a nonnegative integer. The
$k$\textit{-Grassmannian of }$V$ is defined to be the set of all
$k$-dimensional vector subspaces of $V$. This set is denoted by
$\operatorname*{Gr}\left(  k,V\right)  $.
\end{definition}

When $V$ is a finite-dimensional $\mathbb{C}$-vector space, there is a way to
define the structure of a projective variety on the Grassmannian
$\operatorname*{Gr}\left(  k,V\right)  $. While we won't ever need the
existence of this structure, we will need the so-called Pl\"{u}cker embedding
which is the main ingredient in defining this structure:\footnote{In the
following definition (and further below), we use the notation $\mathbb{P}%
\left(  W\right)  $ for the projective space of a $\mathbb{C}$-vector space
$W$. This projective space is defined to be the quotient set $\left(  W
\setminus\left\{  0\right\}  \right)  / \sim$, where $\sim$ is the
proportionality relation (i.e., two vectors $w_{1}$ and $w_{2}$ in $W
\setminus\left\{  0\right\}  $ satisfy $w_{1} \sim w_{2}$ if and only if they
are linearly dependent).}

\begin{definition}
Let $k$ and $n$ be integers satisfying $0\leq k\leq n$. Let $V$ be the
$\mathbb{C}$-vector space $\mathbb{C}^{n}$. The \textit{Pl\"{u}cker embedding}
(corresponding to $n$ and $k$) is defined as the map%
\begin{align*}
\operatorname*{Pl}:\operatorname*{Gr}\left(  k,V\right)   &  \rightarrow
\mathbb{P}\left(  \wedge^{k}V\right)  ,\\
\left(
\begin{array}
[c]{c}%
k\text{-dimensional subspace of }V\\
\text{with basis }x_{1},x_{2},...,x_{k}%
\end{array}
\right)   &  \mapsto\left(
\begin{array}
[c]{c}%
\text{projection of}\\
x_{1}\wedge x_{2}\wedge...\wedge x_{k}\in\wedge^{k}V\diagdown\left\{
0\right\} \\
\text{on }\mathbb{P}\left(  \wedge^{k}V\right)
\end{array}
\right)  .
\end{align*}
It is easy to see that this is well-defined (i. e., that the projection of
$x_{1}\wedge x_{2}\wedge...\wedge x_{k}\in\wedge^{k}V\diagdown\left\{
0\right\}  $ on $\mathbb{P}\left(  \wedge^{k}V\right)  $ does not depend on
the choice of basis $x_{1},x_{2},...,x_{k}$). The image of this map is
$\operatorname*{Im}\operatorname*{Pl}=\Omega\diagup\left(  \text{scalars}%
\right)  $.
\end{definition}

\begin{proposition}
\label{prop.plu.injective}This map $\operatorname*{Pl}$ is injective.
\end{proposition}

\textit{Proof of Proposition \ref{prop.plu.injective}.} Proving Proposition
\ref{prop.plu.injective} boils down to showing that if $\lambda$ is a complex
number and $v_{1}$, $v_{2}$, $...$, $v_{k}$, $w_{1}$, $w_{2}$, $...$, $w_{k}$
are any vectors in a vector space $U$ satisfying $v_{1}\wedge v_{2}%
\wedge...\wedge v_{k}=\lambda\cdot w_{1}\wedge w_{2}\wedge...\wedge w_{k}%
\neq0$, then the vector subspace of $U$ spanned by the vectors $v_{1}$,
$v_{2}$, $...$, $v_{k}$ is identical with the vector subspace of $U$ spanned
by the vectors $w_{1}$, $w_{2}$, $...$, $w_{k}$. This is a well-known fact.
The details are left to the reader.

Thus, $\operatorname*{Gr}\left(  k,V\right)  \cong\Omega\diagup\left(
\text{scalars}\right)  $. (For algebraic geometers: $\Omega$ is the total
space of the determinant bundle on $\operatorname*{Gr}\left(  k,V\right)  $
(but only the nonzero elements).)

We are now going to describe the image $\operatorname*{Im}\operatorname*{Pl}$
by algebraic equations. These equations go under the name \textit{Pl\"{u}cker
relations}.

First, we define (in analogy to Definition \ref{def.createdestroy})
``wedging'' and ``contraction'' operators on the exterior algebra of $V$:

\begin{definition}
\label{def.createdestroy.fin}Let $n\in\mathbb{N}$. Let $k\in\mathbb{Z}$. Let
$V$ be the vector space $\mathbb{C}^{n}$. Let $\left(  v_{1},v_{2}%
,...,v_{n}\right)  $ be the standard basis of $V$. Let $i\in\left\{
1,2,...,n\right\}  $.

\textbf{(a)} We define the so-called $i$\textit{-th wedging operator}
$\widehat{v_{i}}:\wedge^{k}V\rightarrow\wedge^{k+1}V$ by%
\[
\widehat{v_{i}}\cdot\psi=v_{i}\wedge\psi\ \ \ \ \ \ \ \ \ \ \text{for all
}\psi\in\wedge^{k}V.
\]


\textbf{(b)} We define the so-called $i$\textit{-th contraction operator}
$\overset{\vee}{v_{i}}:\wedge^{k}V\rightarrow\wedge^{k-1}V$ as follows:

For every $k$-tuple $\left(  i_{1},i_{2},...,i_{k}\right)  $ of integers
satisfying $1\leq i_{1}<i_{2}<...<i_{k}\leq n$, we let $\overset{\vee}{v_{i}%
}\left(  v_{i_{1}}\wedge v_{i_{2}}\wedge...\wedge v_{i_{k}}\right)  $ be%
\[
\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  i_{1},i_{2},...,i_{k}\right\}
;\\
\left(  -1\right)  ^{j-1}v_{i_{1}}\wedge v_{i_{2}}\wedge...\wedge v_{i_{j-1}%
}\wedge v_{i_{j+1}}\wedge v_{i_{j+2}}\wedge...\wedge v_{i_{k}}%
,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  i_{1},i_{2},...,i_{k}\right\}
\end{array}
\right.  ,
\]
where, in the case $i\in\left\{  i_{1},i_{2},...,i_{k}\right\}  $, we denote
by $j$ the integer $\ell$ satisfying $i_{\ell}=i$. Thus, the map
$\overset{\vee}{v_{i}}$ is defined on a basis of the vector space $\wedge
^{k}V$; we extend this to a map $\wedge^{k}V\rightarrow\wedge^{k-1}V$ by linearity.

Note that, for every negative $\ell\in\mathbb{Z}$, we understand $\wedge
^{\ell}V$ to mean the zero space.
\end{definition}

Now we can formulate the Pl\"{u}cker relations as follows:

\begin{theorem}
\label{thm.plu}Let $n\in\mathbb{N}$. Let $k\in\mathbb{Z}$. We consider the
vector space $V=\mathbb{C}^{n}$ with its standard basis $\left(  v_{1}%
,v_{2},...,v_{n}\right)  $. Let $S=\sum\limits_{i=1}^{n}\widehat{v_{i}}%
\otimes\overset{\vee}{v_{i}}:\wedge^{k}V\otimes\wedge^{k}V\rightarrow
\wedge^{k+1}V\otimes\wedge^{k-1}V$.

\textbf{(a)} This map $S$ does not depend on the choice of the basis and is
$\operatorname*{GL}\left(  V\right)  $-invariant\footnotemark. In other words,
for \textbf{any} basis $\left(  w_{1},w_{2},...,w_{n}\right)  $ of $V$, we
have $S=\sum\limits_{i=1}^{n}\widehat{w_{i}}\otimes\overset{\vee}{w_{i}}$
(where the maps $\widehat{w_{i}}$ and $\overset{\vee}{w_{i}}$ are defined just
as $\widehat{v_{i}}$ and $\overset{\vee}{v_{i}}$, but with respect to the
basis $\left(  w_{1},w_{2},...,w_{n}\right)  $).

\textbf{(b)} Let $k\in\left\{  1,2,...,n\right\}  $. A nonzero element
$\tau\in\wedge^{k}V$ belongs to $\Omega$ if and only if $S\left(  \tau
\otimes\tau\right)  =0$.

\textbf{(c)} The map $S$ is $\operatorname*{M}\left(  V\right)  $-invariant.
(Here, $\operatorname*{M}\left(  V\right)  $ denotes the multiplicative monoid
of all endomorphisms of $V$.)
\end{theorem}

\footnotetext{The word ``$\operatorname*{GL}\left(  V\right)  $-invariant''
here means ``invariant under the action of $\operatorname*{GL}\left(
V\right)  $ on the space of all linear operators $\wedge^{k}V\otimes\wedge
^{k}V\rightarrow\wedge^{k+1}V\otimes\wedge^{k-1}V$''. So, for an operator from
$\wedge^{k}V\otimes\wedge^{k}V$ to $\wedge^{k+1}V\otimes\wedge^{k-1}V$ to be
$\operatorname*{GL}\left(  V\right)  $-invariant means the same as for it to
be $\operatorname*{GL}\left(  V\right)  $-equivariant.}Part \textbf{(b)} of
this theorem is what is actually called the Pl\"{u}cker relations, although it
is not how these relations are usually formulated in literature. For a more
classical formulation, see Theorem \ref{thm.plu.coo}. Of course, Theorem
\ref{thm.plu} \textbf{(b)} not only shows when an element of $\wedge^{k}V$
belongs to $\Omega$, but also shows when an element of $\mathbb{P}\left(
\wedge^{k}V\right)  $ lies in $\operatorname*{Im}\operatorname*{Pl}$ (because
an element of $\mathbb{P}\left(  \wedge^{k}V\right)  $ is an equivalence class
of elements of $\wedge^{k}V\diagdown\left\{  0\right\}  $, and lies in
$\operatorname*{Im}\operatorname*{Pl}$ if and only if its representatives lie
in $\Omega$).

\textit{Proof of Theorem \ref{thm.plu}.} Before we start proving the theorem,
let us introduce some notations.

First of all, for every basis $\left(  e_{1},e_{2},...,e_{n}\right)  $ of $V$,
let $\left(  e_{1}^{\ast},e_{2}^{\ast},...,e_{n}^{\ast}\right)  $ denote its
dual basis (this is a basis of $V^{\ast}$).

Next, for any element $v\in V$ we define the so called $v$\textit{-wedging
operator} $\widehat{v}:\wedge^{k}V\rightarrow\wedge^{k+1}V$ by%
\[
\widehat{v}\cdot\psi=v\wedge\psi\ \ \ \ \ \ \ \ \ \ \text{for all }\psi
\in\wedge^{k}V.
\]
Of course, this definition does not conflict with Definition
\ref{def.createdestroy.fin} \textbf{(a)}. (In fact, for every $i\in\left\{
1,2,...,n\right\}  $, the $v_{i}$-wedging operator that we just defined is
exactly identical with the $i$-th wedging operator defined in Definition
\ref{def.createdestroy.fin} \textbf{(a)}, and hence there is no harm from
denoting both of them by $\widehat{v_{i}}$.)

Further, for any $f\in V^{\ast}$, we define the so called $f$%
\textit{-contraction operator} $\overset{\vee}{f}:\wedge^{k}V\rightarrow
\wedge^{k-1}V$ by%
\begin{align*}
\overset{\vee}{f}\cdot\left(  u_{1}\wedge u_{2}\wedge...\wedge u_{k}\right)
&  =\sum\limits_{i=1}^{k}\left(  -1\right)  ^{i-1}f\left(  u_{i}\right)  \cdot
u_{1}\wedge u_{2}\wedge...\wedge u_{i-1}\wedge u_{i+1}\wedge u_{i+2}%
\wedge...\wedge u_{k}\\
&  \ \ \ \ \ \ \ \ \ \ \text{for all }u_{1},u_{2},...,u_{k}\in V.
\end{align*}
\footnote{In order to prove that this is well-defined, we need to check that
the term $\sum\limits_{i=1}^{k}\left(  -1\right)  ^{i-1}f\left(  u_{i}\right)
\cdot u_{1}\wedge u_{2}\wedge...\wedge u_{i-1}\wedge u_{i+1}\wedge
u_{i+2}\wedge...\wedge u_{k}$ depends multilinearly and antisymmetrically on
$u_{1},u_{2},...,u_{k}$. This is easy and left to the reader.} These
contraction operators are connected to the contraction operators defined in
Definition \ref{def.createdestroy.fin} \textbf{(b)}: Namely, $\overset{\vee
}{v_{i}}=\overset{\vee}{v_{i}^{\ast}}$ for every $i\in\left\{
1,2,...,n\right\}  $. More generally, $\overset{\vee}{e_{i}}=\overset{\vee
}{e_{i}^{\ast}}$ for every basis $\left(  e_{1},e_{2},...,e_{n}\right)  $ of
$V$ (where the maps $\widehat{e_{i}}$ and $\overset{\vee}{e_{i}}$ are defined
just as $\widehat{v_{i}}$ and $\overset{\vee}{v_{i}}$, but with respect to the
basis $\left(  e_{1},e_{2},...,e_{n}\right)  $).

The $f$-contraction operators, however, have a major advantage against the
contraction operators defined in Definition \ref{def.createdestroy.fin}
\textbf{(b)}: In fact, the former are canonical (i. e., they can be defined in
the same way for every vector space instead of $V$, and then they are
canonical maps that don't depend on any choice of basis), while the latter
have the basis $\left(  v_{1},v_{2},...,v_{n}\right)  $ ``hard-coded'' into them.

Note that many sources denote the $f$-contraction operator by $i_{f}$ and call
it the \textit{interior product operator} with $f$.

It is easy to see that%
\begin{equation}
\overset{\vee}{f}\widehat{v}+\widehat{v}\overset{\vee}{f}=f\left(  v\right)
\cdot\operatorname*{id}\ \ \ \ \ \ \ \ \ \ \text{for all }f\in V^{\ast}\text{
and }v\in V \label{pf.plu.fv+vf}%
\end{equation}
(where, in the case $k=0$, we interpret $\widehat{v}\overset{\vee}{f}$ as $0$).

\textbf{(a)} We will give a basis-free definition of $S$. This will prove the
basis independence.

There is a unique vector space isomorphism $\Phi:V^{\ast}\otimes
V\rightarrow\operatorname*{End}V$ which satisfies%
\[
\Phi\left(  f\otimes v\right)  =\left(  \text{the map }V\rightarrow V\text{
sending each }w\text{ to }f\left(  w\right)  v\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }f\in V^{\ast}\text{ and }v\in V.
\]
This $\Phi$ and its inverse isomorphism $\Phi^{-1}$ are actually basis-independent.

Now, define a map
\[
T:V^{\ast}\otimes V\otimes\wedge^{k}V\otimes\wedge^{k}V\rightarrow\wedge
^{k+1}V\otimes\wedge^{k-1}V
\]
by%
\[
T\left(  f\otimes v\otimes\psi\otimes\phi\right)  =\left(  \widehat{v}%
\cdot\psi\right)  \otimes\left(  \overset{\vee}{f}\cdot\phi\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }f\in V^{\ast}\text{, }v\in V\text{, }%
\psi\in\wedge^{k}V\text{ and }\phi\in\wedge^{k}V.
\]
This map $T$ is clearly well-defined (because $\widehat{v}\cdot\psi$ depends
bilinearly on $v$ and $\psi$, and because $\overset{\vee}{f}\cdot\phi$ depends
bilinearly on $f$ and $\phi$).

It is now easy to show that $S$ is the map $\wedge^{k}V\otimes\wedge
^{k}V\rightarrow\wedge^{k+1}V\otimes\wedge^{k-1}V$ which sends $\psi
\otimes\phi$ to $T\left(  \Phi^{-1}\left(  \operatorname*{id}\nolimits_{V}%
\right)  \otimes\psi\otimes\phi\right)  $ for all $\psi\in\wedge^{k}V$ and
$\phi\in\wedge^{k}V$.\ \ \ \ \footnote{\textit{Proof.} Consider the map
$\wedge^{k}V\otimes\wedge^{k}V\rightarrow\wedge^{k+1}V\otimes\wedge^{k-1}V$
which sends $\psi\otimes\phi$ to $T\left(  \Phi^{-1}\left(  \operatorname*{id}%
\nolimits_{V}\right)  \otimes\psi\otimes\phi\right)  $ for all $\psi\in
\wedge^{k}V$ and $\phi\in\wedge^{k}V$. This map is clearly well-defined. Now,
since $\Phi^{-1}\left(  \operatorname*{id}\nolimits_{V}\right)  =\sum
\limits_{i=1}^{n}v_{i}^{\ast}\otimes v_{i}$ (because every $w\in V$ satisfies
\begin{align*}
\left(  \Phi\left(  \sum\limits_{i=1}^{n}v_{i}^{\ast}\otimes v_{i}\right)
\right)  \left(  w\right)   &  =\sum\limits_{i=1}^{n}\underbrace{\left(
\Phi\left(  v_{i}^{\ast}\otimes v_{i}\right)  \right)  \left(  w\right)
}_{\substack{=v_{i}^{\ast}\left(  w\right)  v_{i}\\\text{(by the definition of
}\Phi\text{)}}}=\sum\limits_{i=1}^{n}v_{i}^{\ast}\left(  w\right)  v_{i}=w\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  v_{1}^{\ast},v_{2}^{\ast
},...,v_{n}^{\ast}\right)  \text{ is the dual basis of }\left(  v_{1}%
,v_{2},...,v_{n}\right)  \right) \\
&  =\operatorname*{id}\nolimits_{V}\left(  w\right)  ,
\end{align*}
so that $\Phi\left(  \sum\limits_{i=1}^{n}v_{i}^{\ast}\otimes v_{i}\right)
=\operatorname*{id}\nolimits_{V}$), this map sends $\psi\otimes\phi$ to%
\begin{align*}
T\left(  \underbrace{\Phi^{-1}\left(  \operatorname*{id}\nolimits_{V}\right)
}_{=\sum\limits_{i=1}^{n}v_{i}^{\ast}\otimes v_{i}}\otimes\psi\otimes
\phi\right)   &  =T\left(  \sum\limits_{i=1}^{n}v_{i}^{\ast}\otimes
v_{i}\otimes\psi\otimes\phi\right)  =\sum\limits_{i=1}^{n}\underbrace{T\left(
v_{i}^{\ast}\otimes v_{i}\otimes\psi\otimes\phi\right)  }_{\substack{=\left(
\widehat{v_{i}}\cdot\psi\right)  \otimes\left(  \overset{\vee}{v_{i}^{\ast}%
}\cdot\phi\right)  \\\text{(by the definition of }T\text{)}}}\\
&  =\sum\limits_{i=1}^{n}\left(  \widehat{v_{i}}\cdot\psi\right)
\otimes\left(  \underbrace{\overset{\vee}{v_{i}^{\ast}}}_{=\overset{\vee
}{v_{i}}}\cdot\phi\right)  =\sum\limits_{i=1}^{n}\left(  \widehat{v_{i}}%
\cdot\psi\right)  \otimes\left(  \overset{\vee}{v_{i}}\cdot\phi\right)
\end{align*}
for all $\psi\in\wedge^{k}V$ and $\phi\in\wedge^{k}V$. In other words, this
map is the map $\sum\limits_{i=1}^{n}\widehat{v_{i}}\otimes\overset{\vee
}{v_{i}}=S$. So we have shown that $S$ is the map $\wedge^{k}V\otimes
\wedge^{k}V\rightarrow\wedge^{k+1}V\otimes\wedge^{k-1}V$ which sends
$\psi\otimes\phi$ to $T\left(  \Phi^{-1}\left(  \operatorname*{id}%
\nolimits_{V}\right)  \otimes\psi\otimes\phi\right)  $ for all $\psi\in
\wedge^{k}V$ and $\phi\in\wedge^{k}V$, qed.} This shows immediately that $S$
is basis-independent (since $T$ and $\Phi^{-1}$ are basis-independent).

Since $S$ is basis-independent, it is clear that $S$ is $\operatorname*{GL}%
\left(  V\right)  $-invariant (because the action of $\operatorname*{GL}%
\left(  V\right)  $ transforms $S$ into the same operator $S$ but constructed
for a different basis; but since $S$ is basis-independent, this other $S$ must
be the $S$ that we started with). This proves Theorem \ref{thm.plu}
\textbf{(a)}.

\textbf{(b)} Let $\tau\in\Omega$ be nonzero.

\textbf{1)} First let us show that if $\tau\in\Omega$, then $S\left(
\tau\otimes\tau\right)  =0$.

In order to show this, it is enough to prove that $S\left(  \tau\otimes
\tau\right)  =0$ holds in the case $\tau=v_{1}\wedge v_{2}\wedge...\wedge
v_{k}$ (since $S$ is $\operatorname*{GL}\left(  V\right)  $-invariant, and
$\Omega$ is the $\operatorname*{GL}\left(  V\right)  $-orbit of $v_{1}\wedge
v_{2}\wedge...\wedge v_{k}$).

But this is obvious, because for every $i\in\left\{  1,2,...,n\right\}  $,
either $\widehat{v_{i}}$ or $\overset{\vee}{v_{i}}$ annihilates $v_{1}\wedge
v_{2}\wedge...\wedge v_{k}$.

\textbf{2)} Let us now (conversely) prove that if $S\left(  \tau\otimes
\tau\right)  =0$, then $\tau\in\Omega$.

(There is a combinatorial proof of this in the infinite setting in the
Kac-Raina book, but we will make a different proof here.)

Define $E\subseteq V$ to be the set $\left\{  v\in V\ \mid\ \widehat{v}%
\tau=0\right\}  $. Define $E^{\prime}\subseteq V^{\ast}$ to be the set
$\left\{  f\in V^{\ast}\ \mid\ \overset{\vee}{f}\tau=0\right\}  $. Clearly,
$E$ is a subspace of $V$, and $E^{\prime}$ is a subspace of $V^{\ast}$.

We know that all $v\in E$ and $f\in E^{\prime}$ satisfy $\left(
\overset{\vee}{f}\widehat{v}+\widehat{v}\overset{\vee}{f}\right)  \tau=0$
(since the definition of $E$ yields $\widehat{v}\tau=0$, and the definition of
$E^{\prime}$ yields $\overset{\vee}{f}\tau=0$). But $\underbrace{\left(
\overset{\vee}{f}\widehat{v}+\widehat{v}\overset{\vee}{f}\right)
}_{\substack{=f\left(  v\right)  \operatorname*{id}\\\text{(by
(\ref{pf.plu.fv+vf}))}}}\tau=f\left(  v\right)  \tau$, so this yields
$f\left(  v\right)  \tau=0$, and thus $f\left(  v\right)  =0$ (since $\tau
\neq0$). Thus, $E\subseteq E^{\prime\perp}$.

Let $m=\dim E$ and $r=\dim\left(  E^{\prime\perp}\right)  $. Pick a basis
$\left(  e_{1},e_{2},...,e_{n}\right)  $ of $V$ such that $\left(  e_{1}%
,e_{2},...,e_{m}\right)  $ is a basis of $E$ and such that $\left(
e_{1},e_{2},...,e_{r}\right)  $ is a basis of $E^{\prime\perp}$. (Such a basis
clearly exists.)

Clearly, for every $i\in\left\{  1,2,...,m\right\}  $, we have $e_{i}\in E$
and thus $\widehat{e_{i}}\tau=0$ (by the definition of $E$).

Also, for every $i\in\left\{  r+1,r+2,...,n\right\}  $, we have $\overset{\vee
}{e_{i}^{\ast}}\tau=0$ (because $i>r$, so that $e_{i}^{\ast}\left(
e_{j}\right)  =0$ for all $j\in\left\{  1,2,...,r\right\}  $, so that
$e_{i}^{\ast}\left(  E^{\prime}\right)  =0$ (since $\left(  e_{1}%
,e_{2},...,e_{r}\right)  $ is a basis of $E^{\prime\perp}$), so that
$e_{i}^{\ast}\in\left(  E^{\prime\perp}\right)  ^{\perp}=E^{\prime}$).

The vectors $\widehat{e_{i}}\tau$ for $i\in\left\{  m+1,m+2,...,n\right\}  $
are linearly independent (because if some linear combination of them was zero,
then some linear combination of the $e_{i}$ with $i\in\left\{
m+1,m+2,...,n\right\}  $ would lie in $\left\{  v\in V\ \mid\ \widehat{v}%
\tau=0\right\}  =E$, but this contradicts the fact that $\left(  e_{1}%
,e_{2},...,e_{m}\right)  $ is a basis of $E$). Hence, the vectors
$\widehat{e_{i}}\tau$ for $i\in\left\{  m+1,m+2,...,r\right\}  $ are linearly independent.

We defined $S$ using the basis $\left(  v_{1},v_{2},...,v_{n}\right)  $ of $V$
by the formula $S=\sum\limits_{i=1}^{n}\widehat{v_{i}}\otimes\overset{\vee
}{v_{i}}$. Since $S$ did not depend on the basis, we get the same $S$ if we
define it using the basis $\left(  e_{1},e_{2},...,e_{n}\right)  $. Thus, we
have $S=\sum\limits_{i=1}^{n}\widehat{e_{i}}\otimes\overset{\vee}{e_{i}}$.
Hence,%
\begin{align*}
S\left(  \tau\otimes\tau\right)   &  =\sum\limits_{i=1}^{m}%
\underbrace{\widehat{e_{i}}\tau}_{\substack{=0\\\text{(since }i\in\left\{
1,2,...,m\right\}  \text{)}}}\otimes\overset{\vee}{e_{i}^{\ast}}\tau
+\sum\limits_{i=m+1}^{r}\widehat{e_{i}}\tau\otimes\overset{\vee}{e_{i}^{\ast}%
}\tau+\sum\limits_{i=r+1}^{n}\widehat{e_{i}}\tau\otimes
\underbrace{\overset{\vee}{e_{i}^{\ast}}\tau}_{\substack{=0\\\text{(since
}i\in\left\{  r+1,r+2,...,n\right\}  \text{)}}}\\
&  =\sum\limits_{i=m+1}^{r}\widehat{e_{i}}\tau\otimes\overset{\vee
}{e_{i}^{\ast}}\tau.
\end{align*}
Thus, $S\left(  \tau\otimes\tau\right)  =0$ rewrites as $\sum\limits_{i=m+1}%
^{r}\widehat{e_{i}}\tau\otimes\overset{\vee}{e_{i}^{\ast}}\tau=0$. But since
the vectors $\widehat{e_{i}}\tau$ for $i\in\left\{  m+1,m+2,...,r\right\}  $
are linearly independent, this yields that $\overset{\vee}{e_{i}^{\ast}}%
\tau=0$ for any $i\in\left\{  m+1,m+2,...,r\right\}  $. Thus, for every
$i\in\left\{  m+1,m+2,...,r\right\}  $, we have $e_{i}^{\ast}\in\left\{  f\in
V^{\ast}\ \mid\ \overset{\vee}{f}\tau=0\right\}  =E^{\prime}$, so that
$e_{i}^{\ast}\left(  E^{\prime\perp}\right)  =0$. But on the other hand, for
every $i\in\left\{  m+1,m+2,...,r\right\}  $, we have $e_{i}\in E^{\prime
\perp}$ (since $\left(  e_{1},e_{2},...,e_{r}\right)  $ is a basis of
$E^{\prime\perp}$, and since $i\leq r$). Thus, for every $i\in\left\{
m+1,m+2,...,r\right\}  $, we have $1=e_{i}^{\ast}\left(  \underbrace{e_{i}%
}_{\in E^{\prime\perp}}\right)  \in e_{i}^{\ast}\left(  E^{\prime\perp
}\right)  =0$. This is a contradiction unless there are no $i\in\left\{
m+1,m+2,...,r\right\}  $ at all.

So we conclude that there are no $i\in\left\{  m+1,m+2,...,r\right\}  $ at
all. In other words, $m=r$. Thus, $\dim E=m=r=\dim\left(  E^{\prime\perp
}\right)  $. Combined with $E\subseteq E^{\prime\perp}$, this yields
$E=E^{\prime\perp}$.

Now, recall that $\left(  e_{i_{1}}\wedge e_{i_{2}}\wedge...\wedge e_{i_{k}%
}\right)  _{1\leq i_{1}<i_{2}<...<i_{k}\leq n}$ is a basis of $\wedge^{k}V$.
Hence, we can write $\tau$ in the form $\tau=\sum\limits_{1\leq i_{1}%
<i_{2}<...<i_{k}\leq n}\lambda_{i_{1},i_{2},...,i_{k}}e_{i_{1}}\wedge
e_{i_{2}}\wedge...\wedge e_{i_{k}}$ for some scalars $\lambda_{i_{1}%
,i_{2},...,i_{k}}\in\mathbb{C}$.

Now, we will prove:

\textit{Observation 1:} For every $k$-tuple $\left(  j_{1},j_{2}%
,...,j_{k}\right)  $ of integers satisfying $1\leq j_{1}<j_{2}<...<j_{k}\leq
n$ and $\left\{  1,2,...,m\right\}  \not \subseteq \left\{  j_{1}%
,j_{2},...,j_{k}\right\}  $, we have $\lambda_{j_{1},j_{2},...,j_{k}}=0$.

\textit{Proof of Observation 1:} Let $\left(  j_{1},j_{2},...,j_{k}\right)  $
be a $k$-tuple of integers satisfying $1\leq j_{1}<j_{2}<...<j_{k}\leq n$ and
$\left\{  1,2,...,m\right\}  \not \subseteq \left\{  j_{1},j_{2}%
,...,j_{k}\right\}  $. Then, there exists an $i\in\left\{  1,2,...,m\right\}
$ such that $i\notin\left\{  j_{1},j_{2},...,j_{k}\right\}  $. Consider this
$i$. As we saw above, this yields $\widehat{e_{i}}\tau=0$. Thus,%
\begin{align*}
0  &  =\widehat{e_{i}}\tau=e_{i}\wedge\tau=\sum\limits_{1\leq i_{1}%
<i_{2}<...<i_{k}\leq n}\lambda_{i_{1},i_{2},...,i_{k}}e_{i}\wedge e_{i_{1}%
}\wedge e_{i_{2}}\wedge...\wedge e_{i_{k}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\tau=\sum\limits_{1\leq
i_{1}<i_{2}<...<i_{k}\leq n}\lambda_{i_{1},i_{2},...,i_{k}}e_{i_{1}}\wedge
e_{i_{2}}\wedge...\wedge e_{i_{k}}\right) \\
&  =\sum\limits_{\substack{1\leq i_{1}<i_{2}<...<i_{k}\leq n;\\i\notin\left\{
i_{1},i_{2},...,i_{k}\right\}  }}\lambda_{i_{1},i_{2},...,i_{k}}e_{i}\wedge
e_{i_{1}}\wedge e_{i_{2}}\wedge...\wedge e_{i_{k}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since all terms of the sum with }%
i\in\left\{  i_{1},i_{2},...,i_{k}\right\}  \text{ are }0\right)  .
\end{align*}
Thus, for every $k$-tuple $\left(  i_{1},i_{2},...,i_{k}\right)  $ of integers
satisfying $1\leq i_{1}<i_{2}<...<i_{k}\leq n$ and $i\notin\left\{
i_{1},i_{2},...,i_{k}\right\}  $, we must have $\lambda_{i_{1},i_{2}%
,...,i_{k}}=0$ (because the wedge products $e_{i}\wedge e_{i_{1}}\wedge
e_{i_{2}}\wedge...\wedge e_{i_{k}}$ for all such $k$-tuples are linearly
independent elements of $\wedge^{k+1}V$). Applied to $\left(  i_{1}%
,i_{2},...,i_{k}\right)  =\left(  j_{1},j_{2},...,j_{k}\right)  $, this yields
that $\lambda_{j_{1},j_{2},...,j_{k}}=0$. Observation 1 is proven.

\textit{Observation 2:} For every $k$-tuple $\left(  j_{1},j_{2}%
,...,j_{k}\right)  $ of integers satisfying $1\leq j_{1}<j_{2}<...<j_{k}\leq
n$ and $\left\{  j_{1},j_{2},...,j_{k}\right\}  \not \subseteq \left\{
1,2,...,m\right\}  $, we have $\lambda_{j_{1},j_{2},...,j_{k}}=0$.

\textit{Proof of Observation 2:} Let $\left(  j_{1},j_{2},...,j_{k}\right)  $
be a $k$-tuple of integers satisfying $1\leq j_{1}<j_{2}<...<j_{k}\leq n$ and
$\left\{  j_{1},j_{2},...,j_{k}\right\}  \not \subseteq \left\{
1,2,...,m\right\}  $. Then, there exists an $i\in\left\{  j_{1},j_{2}%
,...,j_{k}\right\}  $ such that $i\notin\left\{  1,2,...,m\right\}  $.
Consider this $i$. Then, $i\notin\left\{  1,2,...,m\right\}  $, so that
$i>m=r$, so that $i\in\left\{  r+1,r+2,...,n\right\}  $. As we saw above, this
yields $\overset{\vee}{e_{i}^{\ast}}\tau=0$. Thus,
\begin{align*}
0  &  =\underbrace{\overset{\vee}{e_{i}^{\ast}}}_{=\overset{\vee}{e_{i}}}%
\tau=\overset{\vee}{e_{i}}\tau=\sum\limits_{1\leq i_{1}<i_{2}<...<i_{k}\leq
n}\lambda_{i_{1},i_{2},...,i_{k}}\overset{\vee}{e_{i}}\cdot\left(  e_{i_{1}%
}\wedge e_{i_{2}}\wedge...\wedge e_{i_{k}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\tau=\sum\limits_{1\leq
i_{1}<i_{2}<...<i_{k}\leq n}\lambda_{i_{1},i_{2},...,i_{k}}e_{i_{1}}\wedge
e_{i_{2}}\wedge...\wedge e_{i_{k}}\right) \\
&  =\sum\limits_{\substack{1\leq i_{1}<i_{2}<...<i_{k}\leq n;\\i\in\left\{
i_{1},i_{2},...,i_{k}\right\}  }}\lambda_{i_{1},i_{2},...,i_{k}}%
\overset{\vee}{e_{i}}\cdot\left(  e_{i_{1}}\wedge e_{i_{2}}\wedge...\wedge
e_{i_{k}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since all terms of the sum with }%
i\notin\left\{  i_{1},i_{2},...,i_{k}\right\}  \text{ are }0\right)  .
\end{align*}
Thus, for every $k$-tuple $\left(  i_{1},i_{2},...,i_{k}\right)  $ of integers
satisfying $1\leq i_{1}<i_{2}<...<i_{k}\leq n$ and $i\in\left\{  i_{1}%
,i_{2},...,i_{k}\right\}  $, we must have $\lambda_{i_{1},i_{2},...,i_{k}}=0$
(because the wedge products $\overset{\vee}{e_{i}}\cdot\left(  e_{i_{1}}\wedge
e_{i_{2}}\wedge...\wedge e_{i_{k}}\right)  $ for all such $k$-tuples are
linearly independent elements of $\wedge^{k-1}V$\ \ \ \ \footnote{To check
this, it is enough to recall how $\overset{\vee}{e_{i}}\cdot\left(  e_{i_{1}%
}\wedge e_{i_{2}}\wedge...\wedge e_{i_{k}}\right)  $ was defined: It was
defined to be $\left(  -1\right)  ^{j-1}e_{i_{1}}\wedge e_{i_{2}}%
\wedge...\wedge e_{i_{j-1}}\wedge e_{i_{j+1}}\wedge e_{i_{j+2}}\wedge...\wedge
e_{i_{k}}$, where $j$ is the integer $\ell$ satisfying $i_{\ell}=i$.}).
Applied to $\left(  i_{1},i_{2},...,i_{k}\right)  =\left(  j_{1}%
,j_{2},...,j_{k}\right)  $, this yields that $\lambda_{j_{1},j_{2},...,j_{k}%
}=0$. Observation 2 is proven.

Now, every $k$-tuple $\left(  j_{1},j_{2},...,j_{k}\right)  $ of integers
satisfying $1\leq j_{1}<j_{2}<...<j_{k}\leq n$ must satisfy either $\left\{
1,2,...,m\right\}  \not \subseteq \left\{  j_{1},j_{2},...,j_{k}\right\}  $,
or $\left\{  j_{1},j_{2},...,j_{k}\right\}  \not \subseteq \left\{
1,2,...,m\right\}  $, or $\left(  1,2,...,m\right)  =\left(  j_{1}%
,j_{2},...,j_{k}\right)  $. In the first of these three cases, we have
$\lambda_{j_{1},j_{2},...,j_{k}}=0$ by Observation 1; in the second case, we
have $\lambda_{j_{1},j_{2},...,j_{k}}=0$ by Observation 2. Hence, the only
case where $\lambda_{j_{1},j_{2},...,j_{k}}$ can be nonzero is the third case,
i. e., the case when $\left(  1,2,...,m\right)  =\left(  j_{1},j_{2}%
,...,j_{k}\right)  $. Hence, the only nonzero addend that the sum
$\sum\limits_{1\leq i_{1}<i_{2}<...<i_{k}\leq n}\lambda_{i_{1},i_{2}%
,...,i_{k}}e_{i_{1}}\wedge e_{i_{2}}\wedge...\wedge e_{i_{k}}$ can have is the
addend for $\left(  i_{1},i_{2},...,i_{k}\right)  =\left(  1,2,...,m\right)
$. Thus, all other addends of this sum can be removed, and therefore
$\tau=\sum\limits_{1\leq i_{1}<i_{2}<...<i_{k}\leq n}\lambda_{i_{1}%
,i_{2},...,i_{k}}e_{i_{1}}\wedge e_{i_{2}}\wedge...\wedge e_{i_{k}}$ rewrites
as $\tau=\lambda_{1,2,...,m}e_{1}\wedge e_{2}\wedge...\wedge e_{m}$. Since
$\tau\neq0$, we thus have $\lambda_{1,2,...,m}\neq0$. Hence, $m=k$ (because
$\lambda_{1,2,...,m}e_{1}\wedge e_{2}\wedge...\wedge e_{m}=\tau\in\wedge^{k}%
V$). Hence,%
\[
\tau=\lambda_{1,2,...,m}e_{1}\wedge e_{2}\wedge...\wedge e_{m}=\lambda
_{1,2,...,m}e_{1}\wedge e_{2}\wedge...\wedge e_{k}=\left(  \lambda
_{1,2,...,m}e_{1}\right)  \wedge e_{2}\wedge e_{3}\wedge...\wedge e_{k}.
\]


Now, since $\lambda_{1,2,...,m}\neq0$, the $n$-tuple $\left(  \lambda
_{1,2,...,m}e_{1},e_{2},e_{3},...,e_{n}\right)  $ is a basis of $V$. Thus,
there exists an element of $\operatorname*{GL}\left(  V\right)  $ which sends
$\left(  v_{1},v_{2},...,v_{n}\right)  $ to $\left(  \lambda_{1,2,...,m}%
e_{1},e_{2},e_{3},...,e_{n}\right)  $. This element therefore sends
$v_{1}\wedge v_{2}\wedge...\wedge v_{k}$ to $\left(  \lambda_{1,2,...,m}%
e_{1}\right)  \wedge e_{2}\wedge e_{3}\wedge...\wedge e_{k}=\tau$. Hence,
$\tau$ lies in the $\operatorname*{GL}\left(  V\right)  $-orbit of
$v_{1}\wedge v_{2}\wedge...\wedge v_{k}$. Since this orbit was called $\Omega
$, this becomes $\tau\in\Omega$.

We thus have shown that if $S\left(  \tau\otimes\tau\right)  =0$, then
$\tau\in\Omega$. This completes the proof of Theorem \ref{thm.plu}
\textbf{(b)}.

\textbf{(c)} We know from Theorem \ref{thm.plu} \textbf{(a)} that $S$ is
$\operatorname*{GL}\left(  V\right)  $-invariant. Since $\operatorname*{GL}%
\left(  V\right)  $ is Zariski-dense in $\operatorname*{M}\left(  V\right)  $,
this yields that $S$ is $\operatorname*{M}\left(  V\right)  $-invariant
(because the $\operatorname*{M}\left(  V\right)  $-invariance of $S$ can be
written as a collection of polynomial identities). This proves Theorem
\ref{thm.plu} \textbf{(c)}.

We can rewrite Theorem \ref{thm.plu} \textbf{(b)} in coordinates:

\begin{theorem}
\label{thm.plu.coo}Let $n\in\mathbb{N}$. Let $k\in\left\{  1,2,...,n\right\}
$. We consider the vector space $V=\mathbb{C}^{n}$ with its standard basis
$\left(  v_{1},v_{2},...,v_{n}\right)  $.

Let $\tau\in\wedge^{k}V$ be nonzero.

For every subset $K$ of $\left\{  1,2,...,n\right\}  $, let $v_{K}$ denote the
element of $\wedge^{\left\vert K\right\vert }V$ defined by $v_{K}=v_{k_{1}%
}\wedge v_{k_{2}}\wedge...\wedge v_{k_{\ell}}$ where $k_{1}$, $k_{2}$, $...$,
$k_{\ell}$ are the elements of $K$ in increasing order. We know that $\left(
v_{K}\right)  _{K\subseteq\left\{  1,2,...,n\right\}  ,\ \left\vert
K\right\vert =k}$ is a basis of the vector space $\wedge^{k}V$. For every
subset $K$ of $\left\{  1,2,...,n\right\}  $ satisfying $\left\vert
K\right\vert =k$, let $P_{K}$ be the $K$-coordinate of $\tau$ with respect to
this basis.

Then, $\tau\in\Omega$ if and only if
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{for all }I\subseteq\left\{  1,2,...,n\right\}  \text{ with }\left\vert
I\right\vert =k-1\text{ and all }J\subseteq\left\{  1,2,...,n\right\} \\
\text{with }\left\vert J\right\vert =k+1\text{, we have }\sum\limits_{j\in
J;\ j\notin I}\left(  -1\right)  ^{\mu\left(  j\right)  }\left(  -1\right)
^{\nu\left(  j\right)  -1}P_{I\cup\left\{  j\right\}  }P_{J\diagdown\left\{
j\right\}  }=0
\end{array}
\right)  , \label{thm.plu.coo.plu}%
\end{equation}
where $\nu\left(  j\right)  $ is the integer $\ell$ for which $j$ is the
$\ell$-th smallest element of the set $J$, and where $\mu\left(  j\right)  $
is the number of elements of the set $I$ which are smaller than $j$.
\end{theorem}

\textit{Proof of Theorem \ref{thm.plu.coo} (sketched).} We know that $\left(
v_{K}\right)  _{K\subseteq\left\{  1,2,...,n\right\}  ,\ \left\vert
K\right\vert =k+1}$ is a basis of $\wedge^{k+1}V$, and $\left(  v_{K}\right)
_{K\subseteq\left\{  1,2,...,n\right\}  ,\ \left\vert K\right\vert =k-1}$ is a
basis of $\wedge^{k-1}V$. Hence, $\left(  v_{K}\otimes v_{L}\right)
_{\substack{K\subseteq\left\{  1,2,...,n\right\}  ,\ \left\vert K\right\vert
=k+1,\\L\subseteq\left\{  1,2,...,n\right\}  ,\ \left\vert L\right\vert
=k-1}}$ is a basis of $\wedge^{k+1}V\otimes\wedge^{k-1}V$. It is not hard to
check that the $v_{J}\otimes v_{I}$-coordinate (with respect to this basis) of
$S\left(  \tau\otimes\tau\right)  $ is precisely $\sum\limits_{j\in
J;\ j\notin I}\left(  -1\right)  ^{\mu\left(  j\right)  }\left(  -1\right)
^{\nu\left(  j\right)  -1}P_{I\cup\left\{  j\right\}  }P_{J\diagdown\left\{
j\right\}  }$ for all $I\subseteq\left\{  1,2,...,n\right\}  $ with
$\left\vert I\right\vert =k-1$ and all $J\subseteq\left\{  1,2,...,n\right\}
$ with $\left\vert J\right\vert =k+1$. Hence, (\ref{thm.plu.coo.plu}) holds if
and only if every coordinate of $S\left(  \tau\otimes\tau\right)  $ is zero,
i. e., if $S\left(  \tau\otimes\tau\right)  =0$, but the latter condition is
equivalent to $\tau\in\Omega$ (because of Theorem \ref{thm.plu} \textbf{(b)}).
This proves Theorem \ref{thm.plu.coo}.

Note that the $\Longrightarrow$ direction of Theorem \ref{thm.plu.coo} can be
formulated as a determinantal identity:

\begin{corollary}
\label{cor.plu.matrix}Let $n\in\mathbb{N}$. Let $k\in\left\{
1,2,...,n\right\}  $. Let $\left(
\begin{array}
[c]{cccc}%
x_{11} & x_{12} & ... & x_{1k}\\
x_{21} & x_{22} & ... & x_{2k}\\
\vdots & \vdots & \ddots & \vdots\\
x_{n1} & x_{n2} & ... & x_{nk}%
\end{array}
\right)  $ be any matrix with $n$ rows and $k$ columns.

For every $I\subseteq\left\{  1,2,...,n\right\}  $ with $\left\vert
I\right\vert =k$, let $P_{I}$ be the minor of this matrix obtained by only
keeping the rows whose indices lie in $I$ (and throwing all other rows away).

Then, for all $I\subseteq\left\{  1,2,...,n\right\}  $ with $\left\vert
I\right\vert =k-1$ and all $J\subseteq\left\{  1,2,...,n\right\}  $ with
$\left\vert J\right\vert =k+1$, we have $\sum\limits_{j\in J;\ j\notin
I}\left(  -1\right)  ^{\mu\left(  j\right)  }\left(  -1\right)  ^{\nu\left(
j\right)  -1}P_{I\cup\left\{  j\right\}  }P_{J\diagdown\left\{  j\right\}
}=0$ (where $\mu\left(  j\right)  $ and $\nu\left(  j\right)  $ are defined as
in Theorem \ref{thm.plu.coo}).
\end{corollary}

\textit{Example:} If $n=4$ and $k=2$, then the claim of Corollary
\ref{cor.plu.matrix} is easily simplified to the single equation $P_{12}%
P_{34}+P_{14}P_{23}-P_{13}P_{24}=0$ (where we abbreviate two-element sets
$\left\{  i,j\right\}  $ by $ij$).

\textit{Proof of Corollary \ref{cor.plu.matrix} (sketched).} WLOG assume
$k\leq n$ (else, everything is vacuously true).

For every $i\in\left\{  1,2,...,k\right\}  $, let $x_{i}\in V$ be the vector
$\left(
\begin{array}
[c]{c}%
x_{1i}\\
x_{2i}\\
\vdots\\
x_{ni}%
\end{array}
\right)  $, where $V$ is as in Theorem \ref{thm.plu.coo}. Since Corollary
\ref{cor.plu.matrix} is a collection of polynomial identities, we can WLOG
assume that the vectors $x_{1}$, $x_{2}$, $...$, $x_{k}$ are linearly
independent (since the set of linearly independent $k$-tuples $\left(
x_{1},x_{2},...,x_{k}\right)  $ of vectors in $V$ is Zariski-dense in $V^{k}%
$). Then, there exists an element of $\operatorname*{GL}\left(  V\right)  $
which maps $v_{1}$, $v_{2}$, $...$, $v_{k}$ to $x_{1}$, $x_{2}$, $...$,
$x_{k}$. Thus, $x_{1}\wedge x_{2}\wedge...\wedge x_{k}\in\Omega$ (since
$\Omega$ is the orbit of $v_{1}\wedge v_{2}\wedge...\wedge v_{k}$ under
$\operatorname*{GL}\left(  V\right)  $). Now, apply Theorem \ref{thm.plu.coo}
to $\tau=x_{1}\wedge x_{2}\wedge...\wedge x_{k}$, and Corollary
\ref{cor.plu.matrix} follows.

Of course, this was not the easiest way to prove Corollary
\ref{cor.plu.matrix}. We could just as well have derived Corollary
\ref{cor.plu.matrix} from the Cauchy-Binet identity, and thus given a new
proof for the $\Longrightarrow$ direction of Theorem \ref{thm.plu.coo}; but
the $\Longleftarrow$ direction is not that easy.

\subsubsection{\label{subsubsect.infgrass}The semiinfinite Grassmannian:
preliminary work}

Now we prepare for the semiinfinite Grassmannian:

Let $\psi_{0}$ denote the elementary semiinfinite wedge $v_{0}\wedge
v_{-1}\wedge v_{-2}\wedge...\in\mathcal{F}^{\left(  0\right)  }$. We recall
the action $\varrho:\operatorname*{M}\left(  \infty\right)  \rightarrow
\operatorname*{End}\left(  \mathcal{F}^{\left(  m\right)  }\right)  $ of the
monoid $\operatorname*{M}\left(  \infty\right)  $ on $\mathcal{F}^{\left(
m\right)  }$ for every $m\in\mathbb{Z}$. This action was defined in Definition
\ref{def.GLinf.act}.

\begin{definition}
From now on, $\Omega$ denotes the subset $\operatorname*{GL}\left(
\infty\right)  \cdot\psi_{0}$ of $\mathcal{F}^{\left(  0\right)  }$. (Here and
in the following, we abbreviate $\left(  \varrho\left(  A\right)  \right)  v$
by $Av$ for every $A\in\operatorname*{M}\left(  \infty\right)  $ and
$v\in\mathcal{F}^{\left(  m\right)  }$ and every $m\in\mathbb{Z}$. In
particular, $\operatorname*{GL}\left(  \infty\right)  \psi_{0}$ means $\left(
\varrho\left(  \operatorname*{GL}\left(  \infty\right)  \right)  \right)
\psi_{0}$.)
\end{definition}

\begin{proposition}
\label{prop.plu.inf.pure}For all $0$-degressions $\left(  i_{0},i_{1}%
,i_{2},...\right)  $, we have $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\in\Omega$.
\end{proposition}

\textit{Proof of Proposition \ref{prop.plu.inf.pure}.} Let $\left(
i_{0},i_{1},i_{2},...\right)  $ be a $0$-degression. Then, there exists a
permutation $\sigma:\mathbb{Z}\rightarrow\mathbb{Z}$ which fixes all but
finitely many integers (i. e., is a finitary permutation of $\mathbb{Z}$), and
satisfies $i_{k}=\sigma\left(  -k\right)  $ for every $k\in\mathbb{N}$. Since
$\sigma$ fixes all but finitely many integers, we can represent $\sigma$ by a
matrix in $\operatorname*{GL}\left(  \infty\right)  $. Let us (by abuse of
notation) denote this matrix by $\sigma$ again. Then, every $k\in\mathbb{N}$
satisfies $v_{i_{k}}=v_{\sigma\left(  -k\right)  }=\sigma v_{-k}$. Thus,
\[
v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...=\sigma v_{0}\wedge\sigma
v_{-1}\wedge\sigma v_{-2}\wedge...=\sigma\underbrace{\left(  v_{0}\wedge
v_{-1}\wedge v_{-2}\wedge...\right)  }_{=\psi_{0}}=\sigma\psi_{0}%
\in\operatorname*{GL}\left(  \infty\right)  \psi_{0}=\Omega.
\]
This proves Proposition \ref{prop.plu.inf.pure}.

Next, an ``infinite'' analogue of Theorem \ref{thm.plu}:

\begin{theorem}
\label{thm.plu.inf}For every $m\in\mathbb{Z}$, define a map $S:\mathcal{F}%
^{\left(  m\right)  }\otimes\mathcal{F}^{\left(  m\right)  }\rightarrow
\mathcal{F}^{\left(  m+1\right)  }\otimes\mathcal{F}^{\left(  m-1\right)  }$
by $S=\sum\limits_{i\in\mathbb{Z}}\widehat{v_{i}}\otimes\overset{\vee}{v_{i}}%
$. (Note that the map $S$ is well-defined because, for every $T\in
\mathcal{F}^{\left(  m\right)  }\otimes\mathcal{F}^{\left(  m\right)  }$, only
finitely many terms of the infinite sum $\sum\limits_{i\in\mathbb{Z}}\left(
\widehat{v_{i}}\otimes\overset{\vee}{v_{i}}\right)  \left(  T\right)  $ are nonzero.)

\textbf{(a)} For every $m\in\mathbb{Z}$, this map $S$ is $\operatorname*{GL}%
\left(  \infty\right)  $-invariant.

\textbf{(b)} Let $\tau\in\mathcal{F}^{\left(  0\right)  }$ be nonzero. Then,
$\tau\in\Omega$ if and only if $S\left(  \tau\otimes\tau\right)  =0$.

\textbf{(c)} For every $m\in\mathbb{Z}$, the map $S$ is $\operatorname*{M}%
\left(  \infty\right)  $-invariant.
\end{theorem}

We are going to prove this theorem by reducing it to its ``finite-dimensional
version'' (i. e., Theorem \ref{thm.plu}). This reduction requires us to link
the set $\Omega$ with its finite-dimensional analoga. To do this, we set up
some definitions:

\subsubsection{Proof of Theorem \ref{thm.plu.inf}}

While the following definitions and results are, superficially seen, auxiliary
to the proof of Theorem \ref{thm.plu.inf}, their use is not confined to this
proof. They can be used to derive various results about semiinfinite wedges
(elements of $\mathcal{F}^{\left(  m\right)  }$ for integer $m$) from similar
statements about finite wedges (elements of $\wedge^{k}W$ for integer $k$ and
finite-dimensional $W$). Our proof of Theorem \ref{thm.plu.inf} below will be
just one example of such a derivation.

Note that most of the proofs in this subsection are straightforward and boring
and are easier to do by the reader than to understand from these notes.

\begin{definition}
\label{def.plu.inf.VN}Let $V$ be the vector space $\mathbb{C}^{\left(
\mathbb{Z}\right)  }=\left\{  \left(  x_{i}\right)  _{i\in\mathbb{Z}}%
\text{\ }\mid\ x_{i}\in\mathbb{C}\text{; only finitely many }x_{i}\text{ are
nonzero}\right\}  $ as defined in Definition \ref{def.glinf.V}. Let $\left(
v_{j}\right)  _{j\in\mathbb{Z}}$ be the basis of $V$ introduced in Definition
\ref{def.glinf.V}.

For every $N\in\mathbb{N}$, let $V_{N}$ denote the $\left(  2N+1\right)
$-dimensional vector subspace $\left\langle v_{-N},v_{-N+1},...,v_{N}%
\right\rangle $ of $V$. It is clear that $V_{0}\subseteq V_{1}\subseteq
V_{2}\subseteq...$ and $V=\bigcup\limits_{N\in\mathbb{N}}V_{N}$.
\end{definition}

It should be noticed that this vector subspace $V_{N}$ is what has been called
$V_{\left]  -N-1,N\right]  }$ in Definition \ref{def.finitary.Valphabeta}.

\begin{definition}
\label{def.plu.inf.iN}Let $N\in\mathbb{N}$. Let $\operatorname*{M}\left(
V_{N}\right)  $ denote the set of all $\left(  2N+1\right)  \times\left(
2N+1\right)  $-matrices over $\mathbb{C}$ whose rows are indexed by elements
of $\left\{  -N,-N+1,...,N\right\}  $ and whose columns are also indexed by
elements of $\left\{  -N,-N+1,...,N\right\}  $. Define a map $i_{N}%
:\operatorname*{M}\left(  V_{N}\right)  \rightarrow\operatorname*{M}\left(
\infty\right)  $ as follows: For every matrix $A\in\operatorname*{M}\left(
V_{N}\right)  $, let $i_{N}\left(  A\right)  $ be the infinite matrix (with
rows and columns indexed by integers) such that%
\[
\left(
\begin{array}
[c]{l}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }i_{N}\left(
A\right)  \right) \\
=\left\{
\begin{array}
[c]{l}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)  \in\left\{
-N,-N+1,...,N\right\}  ^{2};\\
\delta_{i,j}\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)
\in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}  ^{2}%
\end{array}
\right. \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{for
every }\left(  i,j\right)  \in\mathbb{Z}^{2}%
\end{array}
\right)  .
\]
It is easy to see that this map $i_{N}$ is well-defined (i. e., for every
$A\in\operatorname*{M}\left(  V_{N}\right)  $, the matrix $i_{N}\left(
A\right)  $ that we just defined really lies in $\operatorname*{M}\left(
\infty\right)  $), injective and a monoid homomorphism.

The vector space $V_{N}$ has a basis $\left(  v_{-N},v_{-N+1},...,v_{N}%
\right)  $ which is indexed by the set $\left\{  -N,-N+1,...,N\right\}  $.
Thus, we can identify matrices in $\operatorname*{M}\left(  V_{N}\right)  $
with endomorphisms of the vector space $V_{N}$ in the obvious way. Hence, the
invertible elements of $\operatorname*{M}\left(  V_{N}\right)  $ are
identified with the invertible endomorphisms of the vector space $V_{N}$, i.
e., with the elements of $\operatorname*{GL}\left(  V_{N}\right)  $. The
injective map $i_{N}:\operatorname*{M}\left(  V_{N}\right)  \rightarrow
\operatorname*{M}\left(  \infty\right)  $ restricts to an injective map
$i_{N}\mid_{\operatorname*{GL}\left(  V_{N}\right)  }:\operatorname*{GL}%
\left(  V_{N}\right)  \rightarrow\operatorname*{GL}\left(  \infty\right)  $.
\end{definition}

\begin{remark}
Here is a more lucid way to describe the map $i_{N}$ we just defined:

Let $I_{-\infty}$ be the infinite identity matrix whose rows are indexed by
all negative integers, and whose columns are indexed by all negative integers.

Let $I_{\infty}$ be the infinite identity matrix whose rows are indexed by all
positive integers, and whose columns are indexed by all positive integers.

For any matrix $A\in\operatorname*{M}\left(  V_{N}\right)  $, we define
$i_{N}\left(  A\right)  $ to be the block-diagonal matrix $\left(
\begin{array}
[c]{ccc}%
I_{-\infty} & 0 & 0\\
0 & A & 0\\
0 & 0 & I_{\infty}%
\end{array}
\right)  $ whose diagonal blocks are $I_{-\infty}$, $A$ and $I_{\infty}$,
where the first block covers the rows with indices smaller than $-N$ (and
therefore also the columns with indices smaller than $-N$), the second block
covers the rows with indices in $\left\{  -N,-N+1,...,N\right\}  $ (and
therefore also the columns with indices in $\left\{  -N,-N+1,...,N\right\}
$), and the third block covers the rows with indices larger than $N$ (and
therefore also the columns with indices larger than $N$). From this
definition, it becomes clear why $i_{N}$ is a monoid homomorphism. (In fact,
it is clear that the block-diagonal matrix $\left(
\begin{array}
[c]{ccc}%
I_{-\infty} & 0 & 0\\
0 & I_{2N+1} & 0\\
0 & 0 & I_{\infty}%
\end{array}
\right)  $ is the identity matrix, and using the rules for computing with
block matrices it is also easy to see that $\left(
\begin{array}
[c]{ccc}%
I_{-\infty} & 0 & 0\\
0 & A & 0\\
0 & 0 & I_{\infty}%
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}%
I_{-\infty} & 0 & 0\\
0 & B & 0\\
0 & 0 & I_{\infty}%
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
I_{-\infty} & 0 & 0\\
0 & AB & 0\\
0 & 0 & I_{\infty}%
\end{array}
\right)  $ for all $A\in\operatorname*{M}\left(  V_{N}\right)  $ and
$B\in\operatorname*{M}\left(  V_{N}\right)  $.)
\end{remark}

\begin{remark}
\label{rmk.plu.inf.iN}\textbf{(a)} Every $N\in\mathbb{N}$ satisfies%
\[
i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)  \right)  =\left\{
A\in\operatorname*{M}\left(  \infty\right)  \ \mid\ \left(
\begin{array}
[c]{c}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
=\delta_{i,j}\text{ for every}\\
\left(  i,j\right)  \in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}
^{2}%
\end{array}
\right)  \right\}  .
\]


\textbf{(b)} We have $i_{0}\left(  \operatorname*{M}\left(  V_{0}\right)
\right)  \subseteq i_{1}\left(  \operatorname*{M}\left(  V_{1}\right)
\right)  \subseteq i_{2}\left(  \operatorname*{M}\left(  V_{2}\right)
\right)  \subseteq...$.

\textbf{(c)} We have $\operatorname*{M}\left(  \infty\right)  =\bigcup
\limits_{N\in\mathbb{N}}i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)
\right)  $.
\end{remark}

\textit{Proof of Remark \ref{rmk.plu.inf.iN}.} \textbf{(a)} Let $N\in
\mathbb{N}$. Then,%
\[
\left\{  A\in\operatorname*{M}\left(  \infty\right)  \ \mid\ \left(
\begin{array}
[c]{c}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
=\delta_{i,j}\text{ for every}\\
\left(  i,j\right)  \in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}
^{2}%
\end{array}
\right)  \right\}  \subseteq i_{N}\left(  \operatorname*{M}\left(
V_{N}\right)  \right)
\]
\footnote{\textit{Proof.} To prove this, it is clearly enough to show that
every matrix $A\in\operatorname*{M}\left(  \infty\right)  $ which satisfies
\begin{equation}
\left(  \left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
=\delta_{i,j}\text{ for every }\left(  i,j\right)  \in\mathbb{Z}^{2}%
\diagdown\left\{  -N,-N+1,...,N\right\}  ^{2}\right)  \label{pf.plu.inf.iN.1}%
\end{equation}
lies in $i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)  \right)  $. So
let $A\in\operatorname*{M}\left(  \infty\right)  $ be a matrix which satisfies
(\ref{pf.plu.inf.iN.1}). We must prove that $A\in i_{N}\left(
\operatorname*{M}\left(  V_{N}\right)  \right)  $.
\par
Indeed, let $B\in\operatorname*{M}\left(  V_{N}\right)  $ be the matrix
defined by%
\begin{equation}
\left(  \left(  \text{the }\left(  i,j\right)  \text{-th entry of }B\right)
=\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)  \text{
for every }\left(  i,j\right)  \in\left\{  -N,-N+1,...,N\right\}  ^{2}\right)
. \label{pf.plu.inf.iN.2}%
\end{equation}
Then, $i_{N}\left(  B\right)  =A$ (because for every $\left(  i,j\right)
\in\mathbb{Z}^{2}$, we have%
\begin{align*}
&  \left(  \text{the }\left(  i,j\right)  \text{-th entry of }i_{N}\left(
B\right)  \right) \\
&  =\left\{
\begin{array}
[c]{l}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }B\right)
\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)  \in\left\{
-N,-N+1,...,N\right\}  ^{2};\\
\delta_{i,j}\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)
\in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}  ^{2}%
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }i_{N}\left(
B\right)  \right) \\
&  =\left\{
\begin{array}
[c]{l}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)  \in\left\{
-N,-N+1,...,N\right\}  ^{2};\\
\delta_{i,j}\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)
\in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}  ^{2}%
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.plu.inf.iN.2})}\right) \\
&  =\left\{
\begin{array}
[c]{l}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)  \in\left\{
-N,-N+1,...,N\right\}  ^{2};\\
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
\text{,}\ \ \ \ \ \ \ \ \ \ \text{if }\left(  i,j\right)  \in\mathbb{Z}%
^{2}\diagdown\left\{  -N,-N+1,...,N\right\}  ^{2}%
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\delta_{i,j}=\left(  \text{the
}\left(  i,j\right)  \text{-th entry of }A\right)  \text{ for every }\left(
i,j\right)  \in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}
^{2}\text{ (by (\ref{pf.plu.inf.iN.1}))}\right) \\
&  =\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
\end{align*}
). Thus, $A=i_{N}\left(  B\right)  \in i_{N}\left(  \operatorname*{M}\left(
V_{N}\right)  \right)  $ (since $B\in\operatorname*{M}\left(  V_{N}\right)
$), qed.} and%
\[
i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)  \right)  \subseteq\left\{
A\in\operatorname*{M}\left(  \infty\right)  \ \mid\ \left(
\begin{array}
[c]{c}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
=\delta_{i,j}\text{ for every}\\
\left(  i,j\right)  \in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}
^{2}%
\end{array}
\right)  \right\}
\]
(by the definition of $i_{N}$). Combining these two relations, we obtain
\[
i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)  \right)  =\left\{
A\in\operatorname*{M}\left(  \infty\right)  \ \mid\ \left(
\begin{array}
[c]{c}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
=\delta_{i,j}\text{ for every}\\
\left(  i,j\right)  \in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}
^{2}%
\end{array}
\right)  \right\}  .
\]
This proves Remark \ref{rmk.plu.inf.iN} \textbf{(a)}.

\textbf{(b)} By Remark \ref{rmk.plu.inf.iN} \textbf{(a)}, for any
$N\in\mathbb{N}$, the set $i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)
\right)  $ is the set of all matrices $A\in\operatorname*{M}\left(
\infty\right)  $ satisfying the condition%
\[
\left(  \left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
=\delta_{i,j}\text{ for every }\left(  i,j\right)  \in\mathbb{Z}^{2}%
\diagdown\left\{  -N,-N+1,...,N\right\}  ^{2}\right)  .
\]
If this condition is satisfied for some $N$, then it is (all the more)
satisfied for $N+1$ instead of $N$. Hence, $i_{N}\left(  \operatorname*{M}%
\left(  V_{N}\right)  \right)  \subseteq i_{N+1}\left(  \operatorname*{M}%
\left(  V_{N+1}\right)  \right)  $ for any $N\in\mathbb{N}$. Thus,
$i_{0}\left(  \operatorname*{M}\left(  V_{0}\right)  \right)  \subseteq
i_{1}\left(  \operatorname*{M}\left(  V_{1}\right)  \right)  \subseteq
i_{2}\left(  \operatorname*{M}\left(  V_{2}\right)  \right)  \subseteq...$.
This proves Remark \ref{rmk.plu.inf.iN} \textbf{(b)}.

\textbf{(c)} Let $B\in\operatorname*{M}\left(  \infty\right)  $ be arbitrary.
We will now construct an $N\in\mathbb{N}$ such that $B\in i_{N}\left(
\operatorname*{M}\left(  V_{N}\right)  \right)  $.

Since $B\in\operatorname*{M}\left(  \infty\right)  =\operatorname*{id}%
+\mathfrak{gl}_{\infty}$, there exists a $b\in\mathfrak{gl}_{\infty}$ such
that $B=\operatorname*{id}+b$. Consider this $b$.

For any $\left(  i,j\right)  \in\mathbb{Z}^{2}$, let $b_{i,j}$ denote the
$\left(  i,j\right)  $-th entry of the matrix $b$.

Since $b\in\mathfrak{gl}_{\infty}$, only finitely many entries of the matrix
$b$ are nonzero. In other words, only finitely many $\left(  u,v\right)
\in\mathbb{Z}^{2}$ satisfy $\left(  \left(  u,v\right)  \text{-th entry of
}b\right)  \neq0$. In other words, only finitely many $\left(  u,v\right)
\in\mathbb{Z}^{2}$ satisfy $b_{u,v}\neq0$ (since $\left(  \left(  u,v\right)
\text{-th entry of }b\right)  =b_{u,v}$). In other words, the set $\left\{
\max\left\{  \left\vert u\right\vert ,\left\vert v\right\vert \right\}
\ \mid\ \left(  u,v\right)  \in\mathbb{Z}^{2};\ b_{u,v}\neq0\right\}  $ is finite.

Let
\[
N=\max\left\{  \max\left\{  \left\vert u\right\vert ,\left\vert v\right\vert
\right\}  \ \mid\ \left(  u,v\right)  \in\mathbb{Z}^{2};\ b_{u,v}%
\neq0\right\}  .
\]
\footnote{Here, we set $\max\left\{  \max\left\{  \left\vert u\right\vert
,\left\vert v\right\vert \right\}  \ \mid\ \left(  u,v\right)  \in
\mathbb{Z}^{2};\ b_{u,v}\neq0\right\}  $ to be $0$ if the set $\left\{
\max\left\{  \left\vert u\right\vert ,\left\vert v\right\vert \right\}
\ \mid\ \left(  u,v\right)  \in\mathbb{Z}^{2};\ b_{u,v}\neq0\right\}  $ is
empty.} This $N$ is a well-defined nonnegative integer (since the set
$\left\{  \max\left\{  \left\vert u\right\vert ,\left\vert v\right\vert
\right\}  \ \mid\ \left(  u,v\right)  \in\mathbb{Z}^{2};\ b_{u,v}%
\neq0\right\}  $ is finite).

Let $\left(  i,j\right)  \in\mathbb{Z}^{2}\diagdown\left\{
-N,-N+1,...,N\right\}  ^{2}$. Then, $\left(  i,j\right)  \notin\left\{
-N,-N+1,...,N\right\}  ^{2}$. We are now going to show that $b_{i,j}=0$.

In fact, assume (for the sake of contradiction) that $b_{i,j}\neq0$. Thus,
$\left(  i,j\right)  \in\left\{  \left(  u,v\right)  \in\mathbb{Z}^{2}%
\ \mid\ b_{u,v}\neq0\right\}  $. Hence,%
\[
\max\left\{  \left\vert i\right\vert ,\left\vert j\right\vert \right\}
\in\left\{  \max\left\{  \left\vert u\right\vert ,\left\vert v\right\vert
\right\}  \ \mid\ \left(  u,v\right)  \in\mathbb{Z}^{2};\ b_{u,v}%
\neq0\right\}  .
\]
Since any element of a finite set is less or equal to the maximum of the set,
this yields%
\[
\max\left\{  \left\vert i\right\vert ,\left\vert j\right\vert \right\}
\leq\max\left\{  \max\left\{  \left\vert u\right\vert ,\left\vert v\right\vert
\right\}  \ \mid\ \left(  u,v\right)  \in\mathbb{Z}^{2};\ b_{u,v}%
\neq0\right\}  =N.
\]
Thus, $\left\vert i\right\vert \leq\max\left\{  \left\vert i\right\vert
,\left\vert j\right\vert \right\}  \leq N$, so that $i\in\left\{
-N,-N+1,...,N\right\}  $ and similarly $j\in\left\{  -N,-N+1,...,N\right\}  $.
Hence, $\left(  i,j\right)  \in\left\{  -N,-N+1,...,N\right\}  ^{2}$ (because
$i\in\left\{  -N,-N+1,...,N\right\}  $ and $j\in\left\{
-N,-N+1,...,N\right\}  $), which contradicts $\left(  i,j\right)
\notin\left\{  -N,-N+1,...,N\right\}  ^{2}$. This contradiction shows that our
assumption (that $b_{i,j}\neq0$) was wrong. We thus have $b_{i,j}=0$.

Since $B=\operatorname*{id}+b$, we have:%
\[
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }B\right)
=\underbrace{\left(  \text{the }\left(  i,j\right)  \text{-th entry of
}\operatorname*{id}\right)  }_{=\delta_{i,j}}+\underbrace{\left(  \text{the
}\left(  i,j\right)  \text{-th entry of }b\right)  }_{=b_{i,j}=0}=\delta
_{i,j}.
\]


Now, forget that we fixed $\left(  i,j\right)  $. We thus have shown that
$\left(  \text{the }\left(  i,j\right)  \text{-th entry of }B\right)
=\delta_{i,j}$ for every $\left(  i,j\right)  \in\mathbb{Z}^{2}\diagdown
\left\{  -N,-N+1,...,N\right\}  ^{2}$. In other words,%
\begin{align*}
B  &  \in\left\{  A\in\operatorname*{M}\left(  \infty\right)  \ \mid\ \left(
\begin{array}
[c]{c}%
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }A\right)
=\delta_{i,j}\text{ for every}\\
\left(  i,j\right)  \in\mathbb{Z}^{2}\diagdown\left\{  -N,-N+1,...,N\right\}
^{2}%
\end{array}
\right)  \right\}  =i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Remark \ref{rmk.plu.inf.iN}
\textbf{(a)}}\right) \\
&  \subseteq\bigcup\limits_{P\in\mathbb{N}}i_{P}\left(  \operatorname*{M}%
\left(  V_{P}\right)  \right)  .
\end{align*}
Now forget that we fixed $B$. We thus have proven that every $B\in
\operatorname*{M}\left(  \infty\right)  $ satisfies $B\in\bigcup
\limits_{P\in\mathbb{N}}i_{P}\left(  \operatorname*{M}\left(  V_{P}\right)
\right)  $. In other words, $\operatorname*{M}\left(  \infty\right)
\subseteq\bigcup\limits_{P\in\mathbb{N}}i_{P}\left(  \operatorname*{M}\left(
V_{P}\right)  \right)  =\bigcup\limits_{N\in\mathbb{N}}i_{N}\left(
\operatorname*{M}\left(  V_{N}\right)  \right)  $ (here, we renamed the index
$P$ as $N$). Combined with the obvious inclusion $\bigcup\limits_{N\in
\mathbb{N}}i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)  \right)
\subseteq\operatorname*{M}\left(  \infty\right)  $, this yields
$\operatorname*{M}\left(  \infty\right)  =\bigcup\limits_{N\in\mathbb{N}}%
i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)  \right)  $. Remark
\ref{rmk.plu.inf.iN} \textbf{(c)} is therefore proven.

\begin{definition}
\label{def.plu.inf.jmN}Let $N\in\mathbb{N}$ and $m\in\mathbb{Z}$. We define a
linear map $j_{N}^{\left(  m\right)  }:\wedge^{N+m+1}\left(  V_{N}\right)
\rightarrow\mathcal{F}^{\left(  m\right)  }$ by setting%
\[
\left(
\begin{array}
[c]{r}%
j_{N}^{\left(  m\right)  }\left(  b_{0}\wedge b_{1}\wedge...\wedge
b_{N+m}\right)  =b_{0}\wedge b_{1}\wedge...\wedge b_{N+m}\wedge v_{-N-1}\wedge
v_{-N-2}\wedge v_{-N-3}\wedge...\\
\text{for any }b_{0},b_{1},...,b_{N+m}\in V_{N}%
\end{array}
\right)  .
\]
This map $j_{N}^{\left(  m\right)  }$ is well-defined (because $b_{0}\wedge
b_{1}\wedge...\wedge b_{N+m}\wedge v_{-N-1}\wedge v_{-N-2}\wedge
v_{-N-3}\wedge...$ is easily seen to lie in $\mathcal{F}^{\left(  m\right)  }$
and depend multilinearly and antisymmetrically on $b_{0},b_{1},...,b_{N+m}$)
and injective (because the elements of the basis $\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)  _{N\geq i_{0}>i_{1}%
>...>i_{N+m}\geq-N}$ of $\wedge^{N+m+1}\left(  V_{N}\right)  $ are sent by
$j_{N}^{\left(  m\right)  }$ to pairwise distinct elements of the basis
$\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)  _{\left(
i_{0},i_{1},i_{2},...\right)  \text{ is an }m\text{-degression}}$ of
$\mathcal{F}^{\left(  m\right)  }$).
\end{definition}

In the terminology of Definition \ref{def.finitary.Valphabeta}, the map
$j_{N}^{\left(  m\right)  }$ that we have just defined is the map
$R_{N+m+1,\left]  -N-1,N\right]  }$.

Our definitions of $j_{N}^{\left(  m\right)  }$ and of $i_{N}$ satisfy
reasonable compatibilities:

\begin{proposition}
\label{prop.plu.inf.iNjmN}Let $N\in\mathbb{N}$ and $m\in\mathbb{Z}$. For any
$u\in\wedge^{N+m+1}\left(  V_{N}\right)  $ and $A\in\operatorname*{M}\left(
V_{N}\right)  $, we have%
\[
i_{N}\left(  A\right)  \cdot j_{N}^{\left(  m\right)  }\left(  u\right)
=j_{N}^{\left(  m\right)  }\left(  Au\right)  .
\]
(Here, of course, $i_{N}\left(  A\right)  \cdot j_{N}^{\left(  m\right)
}\left(  u\right)  $ stands for $\left(  \varrho\left(  i_{N}\left(  A\right)
\right)  \right)  \left(  j_{N}^{\left(  m\right)  }\left(  u\right)  \right)
$.)
\end{proposition}

\textit{Proof of Proposition \ref{prop.plu.inf.iNjmN}.} Let $A\in
\operatorname*{M}\left(  V_{N}\right)  $ and $u\in\wedge^{N+m+1}\left(
V_{N}\right)  $. We must prove the equality $i_{N}\left(  A\right)  \cdot
j_{N}^{\left(  m\right)  }\left(  u\right)  =j_{N}^{\left(  m\right)  }\left(
Au\right)  $. Since this equality is linear in $u$, we can WLOG assume that
$u$ is an element of the basis $\left(  v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{N+m}}\right)  _{N\geq i_{0}>i_{1}>...>i_{N+m}\geq-N}$ of
$\wedge^{N+m+1}\left(  V_{N}\right)  $. Assume this. Then, there exists an
$N+m+1$-tuple $\left(  i_{0},i_{1},...,i_{N+m}\right)  $ of integers such that
$N\geq i_{0}>i_{1}>...>i_{N+m}\geq-N$ and $u=v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{N+m}}$. Consider this $N+m+1$-tuple.

By the definition of $i_{N}\left(  A\right)  $, we have
\begin{equation}
\left(  i_{N}\left(  A\right)  \cdot v_{k}=Av_{k}\ \ \ \ \ \ \ \ \ \ \text{for
every }k\in\left\{  -N,-N+1,...,N\right\}  \right)
\label{pf.plu.inf.iNjmN.in}%
\end{equation}
and
\begin{equation}
\left(  i_{N}\left(  A\right)  \cdot v_{k}=v_{k}\ \ \ \ \ \ \ \ \ \ \text{for
every }k\in\mathbb{Z}\diagdown\left\{  -N,-N+1,...,N\right\}  \right)  .
\label{pf.plu.inf.iNjmN.out}%
\end{equation}


Note that every $\ell\in\left\{  0,1,...,N+m\right\}  $ satisfies $i_{\ell}%
\in\left\{  -N,-N+1,...,N\right\}  $ (since $N\geq i_{0}>i_{1}>...>i_{N+m}%
\geq-N$ and thus $N\geq i_{\ell}\geq-N$) and thus
\begin{equation}
i_{N}\left(  A\right)  \cdot v_{i_{\ell}}=Av_{i_{\ell}}
\label{pf.plu.inf.iNjmN.in2}%
\end{equation}
(by (\ref{pf.plu.inf.iNjmN.in}), applied to $k=i_{\ell}$). Also, every
positive integer $r$ satisfies $-N-r\in\mathbb{Z}\diagdown\left\{
-N,-N+1,...,N\right\}  $ and thus%
\begin{equation}
i_{N}\left(  A\right)  \cdot v_{-N-r}=v_{-N-r} \label{pf.plu.inf.iNjmN.out2}%
\end{equation}
(by (\ref{pf.plu.inf.iNjmN.out}), applied to $k=-N-r$).

Now, since $u=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}$, we have%
\begin{align*}
j_{N}^{\left(  m\right)  }\left(  u\right)   &  =j_{N}^{\left(  m\right)
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right) \\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge
v_{-N-2}\wedge v_{-N-3}\wedge...
\end{align*}
(by the definition of $j_{N}^{\left(  m\right)  }$), so that%
\begin{align}
&  i_{N}\left(  A\right)  \cdot j_{N}^{\left(  m\right)  }\left(  u\right)
\nonumber\\
&  =i_{N}\left(  A\right)  \cdot\left(  v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}%
\wedge...\right) \nonumber\\
&  =\underbrace{i_{N}\left(  A\right)  \cdot v_{i_{0}}\wedge i_{N}\left(
A\right)  \cdot v_{i_{1}}\wedge...\wedge i_{N}\left(  A\right)  \cdot
v_{i_{N+m}}}_{\substack{=Av_{i_{0}}\wedge Av_{i_{1}}\wedge...\wedge
Av_{i_{N+m}}\\\text{(because every }\ell\in\left\{  0,1,...,N+m\right\}
\text{ satisfies }i_{N}\left(  A\right)  \cdot v_{i_{\ell}}=Av_{i_{\ell}%
}\text{ (by (\ref{pf.plu.inf.iNjmN.in2})))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \wedge\underbrace{i_{N}\left(  A\right)  \cdot
v_{-N-1}\wedge i_{N}\left(  A\right)  \cdot v_{-N-2}\wedge i_{N}\left(
A\right)  \cdot v_{-N-3}\wedge...}_{\substack{=v_{-N-1}\wedge v_{-N-2}\wedge
v_{-N-3}\wedge...\\\text{(because every positive integer }r\text{ satisfies
}i_{N}\left(  A\right)  \cdot v_{-N-r}=v_{-N-r}\text{ (by
(\ref{pf.plu.inf.iNjmN.out2})))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of the action }%
\varrho:\operatorname*{M}\left(  \infty\right)  \rightarrow\operatorname*{End}%
\left(  \mathcal{F}^{\left(  m\right)  }\right)  \right) \nonumber\\
&  =Av_{i_{0}}\wedge Av_{i_{1}}\wedge...\wedge Av_{i_{N+m}}\wedge
v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}\wedge.... \label{pf.plu.inf.iNjmN.left}%
\end{align}
On the other hand, since $u=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{N+m}}$, we have $Au=Av_{i_{0}}\wedge Av_{i_{1}}\wedge...\wedge
Av_{i_{N+m}}$, so that%
\begin{align*}
j_{N}^{\left(  m\right)  }\left(  Au\right)   &  =j_{N}^{\left(  m\right)
}\left(  Av_{i_{0}}\wedge Av_{i_{1}}\wedge...\wedge Av_{i_{N+m}}\right) \\
&  =Av_{i_{0}}\wedge Av_{i_{1}}\wedge...\wedge Av_{i_{N+m}}\wedge
v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}\wedge...
\end{align*}
(by the definition of $j_{N}^{\left(  m\right)  }$). Compared with
(\ref{pf.plu.inf.iNjmN.left}), this yields $i_{N}\left(  A\right)  \cdot
j_{N}^{\left(  m\right)  }\left(  u\right)  =j_{N}^{\left(  m\right)  }\left(
Au\right)  $. This proves Proposition \ref{prop.plu.inf.iNjmN}.

An important property of the maps $j_{N}^{\left(  m\right)  }$ is that their
images (for fixed $m$ and varying $N$) cover (not just span, but actually
cover) all of $\mathcal{F}^{\left(  m\right)  }$:

\begin{proposition}
\label{prop.plu.inf.cover}Let $m\in\mathbb{Z}$.

\textbf{(a)} We have%
\[
j_{0}^{\left(  m\right)  }\left(  \wedge^{0+m+1}\left(  V_{0}\right)  \right)
\subseteq j_{1}^{\left(  m\right)  }\left(  \wedge^{1+m+1}\left(
V_{1}\right)  \right)  \subseteq j_{2}^{\left(  m\right)  }\left(
\wedge^{2+m+1}\left(  V_{2}\right)  \right)  \subseteq....
\]


\textbf{(b)} For every $Q\in\mathbb{N}$, we have $\mathcal{F}^{\left(
m\right)  }=\bigcup\limits_{\substack{N\in\mathbb{N};\\N\geq Q}}j_{N}^{\left(
m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)  \right)  $.
\end{proposition}

Actually, the ``$N\geq Q$'' in Proposition \ref{prop.plu.inf.cover}
\textbf{(b)} doesn't have much effect since Proposition
\ref{prop.plu.inf.cover} \textbf{(a)} yields $\bigcup\limits_{\substack{N\in
\mathbb{N};\\N\geq Q}}j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(
V_{N}\right)  \right)  =\bigcup\limits_{N\in\mathbb{N}}j_{N}^{\left(
m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)  \right)  $; but we
prefer to put it in because it is needed in our application.

\textit{Proof of Proposition \ref{prop.plu.inf.cover}.} \textbf{(a)} Let
$N\in\mathbb{N}$. From the definitions of $j_{N}$ and $j_{N+1}$, it is easy to
see that%
\[
j_{N}^{\left(  m\right)  }\left(  b_{0}\wedge b_{1}\wedge...\wedge
b_{N+m}\right)  =j_{N+1}^{\left(  m\right)  }\left(  b_{0}\wedge b_{1}%
\wedge...\wedge b_{N+m}\wedge v_{-N-1}\right)
\]
for any $b_{0},b_{1},...,b_{N+m}\in V_{N}$. Due to linearity, this yields that
$j_{N}^{\left(  m\right)  }\left(  a\right)  =j_{N+1}^{\left(  m\right)
}\left(  a\wedge v_{-N-1}\right)  $ for any $a\in\wedge^{N+m+1}\left(
V_{N}\right)  $. Hence, $j_{N}^{\left(  m\right)  }\left(  a\right)
=j_{N+1}^{\left(  m\right)  }\left(  a\wedge v_{-N-1}\right)  \in
j_{N+1}^{\left(  m\right)  }\left(  \wedge^{\left(  N+1\right)  +m+1}\left(
V_{N+1}\right)  \right)  $ for any $a\in\wedge^{N+m+1}\left(  V_{N}\right)  $.
In other words, $j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(
V_{N}\right)  \right)  \subseteq j_{N+1}^{\left(  m\right)  }\left(
\wedge^{\left(  N+1\right)  +m+1}\left(  V_{N+1}\right)  \right)  $.

We thus have proven that every $N\in\mathbb{N}$ satisfies%
\[
j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)  \right)
\subseteq j_{N+1}^{\left(  m\right)  }\left(  \wedge^{\left(  N+1\right)
+m+1}\left(  V_{N+1}\right)  \right)  .
\]
In other words,%
\[
j_{0}^{\left(  m\right)  }\left(  \wedge^{0+m+1}\left(  V_{0}\right)  \right)
\subseteq j_{1}^{\left(  m\right)  }\left(  \wedge^{1+m+1}\left(
V_{1}\right)  \right)  \subseteq j_{2}^{\left(  m\right)  }\left(
\wedge^{2+m+1}\left(  V_{2}\right)  \right)  \subseteq....
\]
Proposition \ref{prop.plu.inf.cover} \textbf{(a)} is proven.

\textbf{(b)} We need three notations:

\begin{itemize}
\item For any $m$-degression $\mathbf{i}$, define a nonnegative integer
$\operatorname*{exting}\left(  \mathbf{i}\right)  $ as the largest
$k\in\mathbb{N}$ satisfying $i_{k}+k\neq m$\ \ \ \ \footnote{If no such $k$
exists, then we set $\operatorname*{exting}\left(  \mathbf{i}\right)  $ to be
$0$.}, where $\mathbf{i}$ is written in the form $\left(  i_{0},i_{1}%
,i_{2},...\right)  $. (Such a largest $k$ indeed exists, because (by the
definition of an $m$-degression) every sufficiently high $k\in\mathbb{N}$
satisfies $i_{k}+k=m$.)

\item For any $m$-degression $\mathbf{i}$, define an integer
$\operatorname*{head}\left(  \mathbf{i}\right)  $ by $\operatorname*{head}%
\left(  \mathbf{i}\right)  =i_{0}$, where $\mathbf{i}$ is written in the form
$\left(  i_{0},i_{1},i_{2},...\right)  $.

\item For any $m$-degression $\mathbf{i}$, define an element $v_{\mathbf{i}}$
of $\mathcal{F}^{\left(  m\right)  }$ by $v_{\mathbf{i}}=v_{i_{0}}\wedge
v_{i_{1}}\wedge v_{i_{2}}\wedge...$, where $\mathbf{i}$ is written in the form
$\left(  i_{0},i_{1},i_{2},...\right)  $.
\end{itemize}

Thus, $\left(  v_{\mathbf{i}}\right)  _{\mathbf{i}\text{ is an }%
m\text{-degression}}=\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...\right)  _{\left(  i_{0},i_{1},i_{2},...\right)  \text{ is an
}m\text{-degression}}$. Since \newline$\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...\right)  _{\left(  i_{0},i_{1},i_{2},...\right)  \text{ is
an }m\text{-degression}}$ is a basis of the vector space $\mathcal{F}^{\left(
m\right)  }$, we thus conclude that $\left(  v_{\mathbf{i}}\right)
_{\mathbf{i}\text{ is an }m\text{-degression}}$ is a basis of the vector space
$\mathcal{F}^{\left(  m\right)  }$.

Now we prove a simple fact:%
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{If }\mathbf{i}\text{ is an }m\text{-degression, and }P\text{ is an
integer such that }\\
P\geq\max\left\{  0,\operatorname*{exting}\left(  \mathbf{i}\right)
-m,\operatorname*{head}\left(  \mathbf{i}\right)  \right\}  \text{, then
}v_{\mathbf{i}}\in j_{P}^{\left(  m\right)  }\left(  \wedge^{P+m+1}\left(
V_{P}\right)  \right)
\end{array}
\right)  . \label{pf.plu.inf.cover.1}%
\end{equation}


\textit{Proof of (\ref{pf.plu.inf.cover.1}):} Let $\mathbf{i}$ be an
$m$-degression, and $P$ be an integer such that $P\geq\max\left\{
0,\operatorname*{exting}\left(  \mathbf{i}\right)  -m,\operatorname*{head}%
\left(  \mathbf{i}\right)  \right\}  $. Write $\mathbf{i}$ in the form
$\left(  i_{0},i_{1},i_{2},...\right)  $. Then, $\operatorname*{exting}\left(
\mathbf{i}\right)  $ is the largest $k\in\mathbb{N}$ satisfying $i_{k}+k\neq
m$ (by the definition of $\operatorname*{exting}\left(  \mathbf{i}\right)  $).
Hence,%
\begin{equation}
\text{every }k\in\mathbb{N}\text{ such that }k>\operatorname*{exting}\left(
\mathbf{i}\right)  \text{ satisfies }i_{k}+k=m. \label{pf.plu.inf.cover.2}%
\end{equation}


Since $P\geq\max\left\{  0,\operatorname*{exting}\left(  \mathbf{i}\right)
-m,\operatorname*{head}\left(  \mathbf{i}\right)  \right\}  \geq0$, the map
$j_{P}^{\left(  m\right)  }$ and the space $V_{P}$ are well-defined.

Since $P\geq\max\left\{  0,\operatorname*{exting}\left(  \mathbf{i}\right)
-m,\operatorname*{head}\left(  \mathbf{i}\right)  \right\}  \geq
\operatorname*{exting}\left(  \mathbf{i}\right)  -m$, we have $P+m\geq
\operatorname*{exting}\left(  \mathbf{i}\right)  \geq0$. Now,%
\begin{equation}
\text{every positive integer }\ell\text{ satisfies }i_{P+m+\ell}%
=-P-\ell\label{pf.plu.inf.cover.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.plu.inf.cover.3}):} Let $\ell\in
\mathbb{N}$ be a positive integer. Then, $P+m+\underbrace{\ell}_{>0}%
>P+m\geq\operatorname*{exting}\left(  \mathbf{i}\right)  $. Hence,
(\ref{pf.plu.inf.cover.2}) (applied to $k=P+m+\ell$) yields $i_{P+m+\ell
}+P+m+\ell=m$. In other words, $i_{P+m+\ell}=-P-\ell$. This proves
(\ref{pf.plu.inf.cover.3}).}. Applied to $\ell=1$, this yields $i_{P+m+1}%
=-P-1$.

Notice also that $P\geq\max\left\{  0,\operatorname*{exting}\left(
\mathbf{i}\right)  -m,\operatorname*{head}\left(  \mathbf{i}\right)  \right\}
\geq\operatorname*{head}\left(  \mathbf{i}\right)  =i_{0}$ (by the definition
of $\operatorname*{head}\left(  \mathbf{i}\right)  $). Now it is easy to see
that%
\begin{equation}
\text{every }k\in\mathbb{N}\text{ such that }k\leq P+m\text{ satisfies
}v_{i_{k}}\in V_{P}. \label{pf.plu.inf.cover.4}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.plu.inf.cover.4}):} Let $k\in\mathbb{N}$
be such that $k\leq P+m$. Thus, $k<P+m+1$.
\par
Since $\left(  i_{0},i_{1},i_{2},...\right)  =\mathbf{i}$ is an $m$%
-degression, the sequence $\left(  i_{0},i_{1},i_{2},...\right)  $ is strictly
decreasing, i. e., we have $i_{0}>i_{1}>i_{2}>...$. As a consequence,
$i_{0}\geq i_{k}$ (since $0\leq k$) and $i_{k}>i_{P+m+1}$ (since $k<P+m+1$).
Since $i_{k}>i_{P+m+1}=-P-1$, we have $i_{k}\geq-P$ (since both $i_{k}$ and
$-P$ are integers). Combining $P\geq i_{0}\geq i_{k}$ with $i_{k}\geq-P$, we
obtain $P\geq i_{k}\geq-P$. Hence, $v_{i_{k}}\in\left\langle v_{-P}%
,v_{-P+1},...,v_{P}\right\rangle =V_{P}$ (because $V_{P}$ is defined as
$\left\langle v_{-P},v_{-P+1},...,v_{P}\right\rangle $). This proves
(\ref{pf.plu.inf.cover.4}).} Hence, $v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{P+m}}\in\wedge^{P+m+1}\left(  V_{P}\right)  $. Now, by the definition of
$j_{P}^{\left(  m\right)  }$, we have%
\begin{align*}
&  j_{P}^{\left(  m\right)  }\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{P+m}}\right) \\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{P+m}}\wedge
\underbrace{v_{-P-1}\wedge v_{-P-2}\wedge v_{-P-3}\wedge...}%
_{\substack{=v_{i_{P+m+1}}\wedge v_{i_{P+m+2}}\wedge v_{i_{P+m+3}}%
\wedge...\\\text{(because every positive integer }\ell\\\text{satisfies
}-P-\ell=i_{P+m+\ell}\text{ (by (\ref{pf.plu.inf.cover.3})))}}}\\
&  =v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{P+m}}\wedge v_{i_{P+m+1}%
}\wedge v_{i_{P+m+2}}\wedge v_{i_{P+m+3}}\wedge...=v_{i_{0}}\wedge v_{i_{1}%
}\wedge v_{i_{2}}\wedge...=v_{\mathbf{i}}%
\end{align*}
(since $v_{\mathbf{i}}$ was defined as $v_{i_{0}}\wedge v_{i_{1}}\wedge
v_{i_{2}}\wedge...$). Thus, $v_{\mathbf{i}}=j_{P}^{\left(  m\right)  }\left(
\underbrace{v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{P+m}}}_{\in
\wedge^{P+m+1}\left(  V_{P}\right)  }\right)  \in j_{P}^{\left(  m\right)
}\left(  \wedge^{P+m+1}\left(  V_{P}\right)  \right)  $. This proves
(\ref{pf.plu.inf.cover.1}).

Now, fix an arbitrary $Q\in\mathbb{N}$.

Let $w$ be any element of $\mathcal{F}^{\left(  m\right)  }$. Since $\left(
v_{\mathbf{i}}\right)  _{\mathbf{i}\text{ is an }m\text{-degression}}$ is a
basis of $\mathcal{F}^{\left(  m\right)  }$, we can write $w$ as a linear
combination of elements of the family $\left(  v_{\mathbf{i}}\right)
_{\mathbf{i}\text{ is an }m\text{-degression}}$. Since every linear
combination contains only finitely many vectors, this yields that we can write
$w$ as a linear combination of \textbf{finitely many} elements of the family
$\left(  v_{\mathbf{i}}\right)  _{\mathbf{i}\text{ is an }m\text{-degression}%
}$. In other words, there exists a finite set $S$ of $m$-degressions such that
$w$ is a linear combination of the family $\left(  v_{\mathbf{i}}\right)
_{\mathbf{i}\in S}$. Consider this $S$. Since $w$ is a linear combination of
the family $\left(  v_{\mathbf{i}}\right)  _{\mathbf{i}\in S}$, we can find a
scalar $\lambda_{\mathbf{i}}\in\mathbb{C}$ for every $\mathbf{i}\in S$ such
that $w=\sum\limits_{\mathbf{i}\in S}\lambda_{\mathbf{i}}v_{\mathbf{i}}$.
Consider these scalars $\lambda_{\mathbf{i}}$. Let
\[
P=\max\left\{  Q,\max\left\{  \max\left\{  0,\operatorname*{exting}\left(
\mathbf{j}\right)  -m,\operatorname*{head}\left(  \mathbf{j}\right)  \right\}
\ \mid\ \mathbf{j}\in S\right\}  \right\}
\]
(where the maximum of the empty set is to be understood as $0$). Then, first
of all, $P\geq Q$. Second, every $\mathbf{i}\in S$ satisfies%
\begin{align*}
P  &  =\max\left\{  Q,\max\left\{  \max\left\{  0,\operatorname*{exting}%
\left(  \mathbf{j}\right)  -m,\operatorname*{head}\left(  \mathbf{j}\right)
\right\}  \ \mid\ \mathbf{j}\in S\right\}  \right\} \\
&  \geq\max\left\{  \max\left\{  0,\operatorname*{exting}\left(
\mathbf{j}\right)  -m,\operatorname*{head}\left(  \mathbf{j}\right)  \right\}
\ \mid\ \mathbf{j}\in S\right\} \\
&  \geq\max\left\{  0,\operatorname*{exting}\left(  \mathbf{i}\right)
-m,\operatorname*{head}\left(  \mathbf{i}\right)  \right\} \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\max\left\{  0,\operatorname*{exting}\left(  \mathbf{i}\right)
-m,\operatorname*{head}\left(  \mathbf{i}\right)  \right\}  \text{ is an
element of the set}\\
\left\{  \max\left\{  0,\operatorname*{exting}\left(  \mathbf{j}\right)
-m,\operatorname*{head}\left(  \mathbf{j}\right)  \right\}  \ \mid
\ \mathbf{j}\in S\right\}  \text{ (because }\mathbf{i}\in S\text{),}\\
\text{and the maximum of a set is }\geq\text{ to any element of this set}%
\end{array}
\right)
\end{align*}
and thus $v_{\mathbf{i}}\in j_{P}^{\left(  m\right)  }\left(  \wedge
^{P+m+1}\left(  V_{P}\right)  \right)  $ (by (\ref{pf.plu.inf.cover.1})).
Hence,%
\begin{align*}
w  &  =\sum\limits_{\mathbf{i}\in S}\lambda_{\mathbf{i}}%
\underbrace{v_{\mathbf{i}}}_{\in j_{P}^{\left(  m\right)  }\left(
\wedge^{P+m+1}\left(  V_{P}\right)  \right)  }\in\sum\limits_{\mathbf{i}\in
S}\lambda_{\mathbf{i}}j_{P}^{\left(  m\right)  }\left(  \wedge^{P+m+1}\left(
V_{P}\right)  \right)  \subseteq j_{P}^{\left(  m\right)  }\left(
\wedge^{P+m+1}\left(  V_{P}\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }j_{P}^{\left(  m\right)  }\left(
\wedge^{P+m+1}\left(  V_{P}\right)  \right)  \text{ is a vector space}\right)
\\
&  \subseteq\bigcup\limits_{\substack{N\in\mathbb{N};\\N\geq Q}}j_{N}^{\left(
m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)  \right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }P\geq Q\right)  .
\end{align*}


Now, forget that we fixed $w$. We thus have proven that every $w\in
\mathcal{F}^{\left(  m\right)  }$ satisfies $w\in\bigcup
\limits_{\substack{N\in\mathbb{N};\\N\geq Q}}j_{N}^{\left(  m\right)  }\left(
\wedge^{N+m+1}\left(  V_{N}\right)  \right)  $. Thus, $\mathcal{F}^{\left(
m\right)  }\subseteq\bigcup\limits_{\substack{N\in\mathbb{N};\\N\geq Q}%
}j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)
\right)  $. Combined with the obvious inclusion $\bigcup
\limits_{\substack{N\in\mathbb{N};\\N\geq Q}}j_{N}^{\left(  m\right)  }\left(
\wedge^{N+m+1}\left(  V_{N}\right)  \right)  \subseteq\mathcal{F}^{\left(
m\right)  }$, this yields $\mathcal{F}^{\left(  m\right)  }=\bigcup
\limits_{\substack{N\in\mathbb{N};\\N\geq Q}}j_{N}^{\left(  m\right)  }\left(
\wedge^{N+m+1}\left(  V_{N}\right)  \right)  $. Proposition
\ref{prop.plu.inf.cover} \textbf{(b)} is thus proven.

\begin{verlong}
And a corollary of Proposition \ref{prop.plu.inf.cover} (that we won't need):

\begin{corollary}
\label{cor.plu.inf.cover.tensor}Let $m\in\mathbb{Z}$. We have $\mathcal{F}%
^{\left(  m\right)  }\otimes\mathcal{F}^{\left(  m\right)  }=\bigcup
\limits_{N\in\mathbb{N}}\left(  j_{N}^{\left(  m\right)  }\left(
\wedge^{N+m+1}\left(  V_{N}\right)  \right)  \otimes j_{N}^{\left(  m\right)
}\left(  \wedge^{N+m+1}\left(  V_{N}\right)  \right)  \right)  $.
\end{corollary}

To prove this, we need the following lemma:

\begin{lemma}
\label{lem.plu.inf.cover.tensor}Let $W$ be a vector space, and $\left(
W_{n}\right)  _{n\in\mathbb{N}}$ a family of vector subspaces of $W$ such that
$W_{0}\subseteq W_{1}\subseteq W_{2}\subseteq...$ and $W=\bigcup
\limits_{n\in\mathbb{N}}W_{n}$. Let $U$ be a vector space, and $\left(
U_{n}\right)  _{n\in\mathbb{N}}$ a family of vector subspaces of $U$ such that
$U_{0}\subseteq U_{1}\subseteq U_{2}\subseteq...$ and $U=\bigcup
\limits_{n\in\mathbb{N}}U_{n}$. Then, $U\otimes W=\bigcup\limits_{n\in
\mathbb{N}}\left(  U_{n}\otimes W_{n}\right)  $.
\end{lemma}

\textit{Proof of Lemma \ref{lem.plu.inf.cover.tensor}.} Let $t\in U\otimes W$
be arbitrary. Since $t$ is a tensor, we can write $t$ in the form
$t=\sum\limits_{i=1}^{m}u_{i}\otimes w_{i}$ for some $m\in\mathbb{N}$, some
elements $u_{1}$, $u_{2}$, $...$, $u_{m}$ of $U$, and some elements $w_{1}$,
$w_{2}$, $...$, $w_{m}$ of $W$. Consider this $u$, these $u_{1}$, $u_{2}$,
$...$, $u_{m}$ and these $w_{1}$, $w_{2}$, $...$, $w_{m}$.

For every $i\in\left\{  1,2,...,m\right\}  $, there exists some $\alpha_{i}%
\in\mathbb{N}$ such that $u_{i}\in U_{\alpha_{i}}$ (since $u_{i}\in
U=\bigcup\limits_{n\in\mathbb{N}}U_{n}$). Consider this $\alpha_{i}$.

For every $i\in\left\{  1,2,...,m\right\}  $, there exists some $\beta_{i}%
\in\mathbb{N}$ such that $w_{i}\in W_{\beta_{i}}$ (since $w_{i}\in
W=\bigcup\limits_{n\in\mathbb{N}}W_{n}$). Consider this $\beta_{i}$.

Let $N=\max\left(  \left\{  \alpha_{1},\alpha_{2},...,\alpha_{m}\right\}
\cup\left\{  \beta_{1},\beta_{2},...,\beta_{m}\right\}  \right)  $. Then,
every $i\in\left\{  1,2,...,m\right\}  $ satisfies $\alpha_{i}\in\left\{
\alpha_{1},\alpha_{2},...,\alpha_{m}\right\}  \subseteq\left\{  \alpha
_{1},\alpha_{2},...,\alpha_{m}\right\}  \cup\left\{  \beta_{1},\beta
_{2},...,\beta_{m}\right\}  $, so that%
\begin{align*}
\alpha_{i}  &  \leq\max\left(  \left\{  \alpha_{1},\alpha_{2},...,\alpha
_{m}\right\}  \cup\left\{  \beta_{1},\beta_{2},...,\beta_{m}\right\}  \right)
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since any element of a finite set is
}\leq\text{ to the maximum of this set}\right) \\
&  =N
\end{align*}
and thus $U_{\alpha_{i}}\subseteq U_{N}$ (since $U_{0}\subseteq U_{1}\subseteq
U_{2}\subseteq...$), so that $u_{i}\in U_{\alpha_{i}}\subseteq U_{N}$.
Similarly, every $i\in\left\{  1,2,...,m\right\}  $ satisfies $w_{i}\in W_{N}%
$. Thus,%
\begin{align*}
t  &  =\sum\limits_{i=1}^{m}\underbrace{u_{i}}_{\in U_{N}}\otimes
\underbrace{w_{i}}_{\in W_{N}}\in\sum\limits_{i=1}^{m}U_{N}\otimes
W_{N}\subseteq U_{N}\otimes W_{N}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }U_{N}\otimes W_{N}\text{ is a
}k\text{-vector space}\right) \\
&  \subseteq\bigcup\limits_{n\in\mathbb{N}}\left(  U_{n}\otimes W_{n}\right)
.
\end{align*}


Now, forget that we fixed $t$. We thus have proven that every $t\in U\otimes
W$ satisfies $t\in\bigcup\limits_{n\in\mathbb{N}}\left(  U_{n}\otimes
W_{n}\right)  $. In other words, $U\otimes W\subseteq\bigcup\limits_{n\in
\mathbb{N}}\left(  U_{n}\otimes W_{n}\right)  $. Combined with the obvious
inclusion $\bigcup\limits_{n\in\mathbb{N}}\left(  U_{n}\otimes W_{n}\right)
\subseteq U\otimes W$, this yields $U\otimes W=\bigcup\limits_{n\in\mathbb{N}%
}\left(  U_{n}\otimes W_{n}\right)  $, so that Lemma
\ref{lem.plu.inf.cover.tensor} is proven.

\textit{Proof of Corollary \ref{cor.plu.inf.cover.tensor}.} Proposition
\ref{prop.plu.inf.cover} \textbf{(b)} (applied to $Q=0$) yields
\[
\mathcal{F}^{\left(  m\right)  }=\bigcup\limits_{\substack{N\in\mathbb{N}%
;\\N\geq0}}j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(
V_{N}\right)  \right)  =\bigcup\limits_{N\in\mathbb{N}}j_{N}^{\left(
m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)  \right)  .
\]
Proposition \ref{prop.plu.inf.cover} \textbf{(a)} yields $j_{0}^{\left(
m\right)  }\left(  \wedge^{0+m+1}\left(  V_{0}\right)  \right)  \subseteq
j_{1}^{\left(  m\right)  }\left(  \wedge^{1+m+1}\left(  V_{1}\right)  \right)
\subseteq j_{2}^{\left(  m\right)  }\left(  \wedge^{2+m+1}\left(
V_{2}\right)  \right)  \subseteq...$. Thus, Lemma
\ref{lem.plu.inf.cover.tensor} (applied to $W=\mathcal{F}^{\left(  m\right)
}$, $W_{i}=j_{i}^{\left(  m\right)  }\left(  \wedge^{i+m+1}\left(
V_{1}\right)  \right)  $, $U=\mathcal{F}^{\left(  m\right)  }$ and
$U_{i}=j_{i}^{\left(  m\right)  }\left(  \wedge^{i+m+1}\left(  V_{1}\right)
\right)  $) yields $\mathcal{F}^{\left(  m\right)  }\otimes\mathcal{F}%
^{\left(  m\right)  }=\bigcup\limits_{N\in\mathbb{N}}\left(  j_{N}^{\left(
m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)  \right)  \otimes
j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(  V_{N}\right)  \right)
\right)  $. This proves Corollary \ref{cor.plu.inf.cover.tensor}.
\end{verlong}

What comes next is almost a carbon copy of Definition
\ref{def.createdestroy.fin}:

\begin{definition}
\label{def.plu.inf.createdestroy}Let $N\in\mathbb{N}$. Let $k\in\mathbb{Z}$.
Let $i\in\left\{  -N,-N+1,...,N\right\}  $.

\textbf{(a)} We define the so-called $i$\textit{-th wedging operator}
$\widehat{v_{i}^{\left(  N\right)  }}:\wedge^{k}\left(  V_{N}\right)
\rightarrow\wedge^{k+1}\left(  V_{N}\right)  $ by%
\[
\widehat{v_{i}^{\left(  N\right)  }}\cdot\psi=v_{i}\wedge\psi
\ \ \ \ \ \ \ \ \ \ \text{for all }\psi\in\wedge^{k}\left(  V_{N}\right)  .
\]


\textbf{(b)} We define the so-called $i$\textit{-th contraction operator}
$\overset{\vee}{v_{i}^{\left(  N\right)  }}:\wedge^{k}\left(  V_{N}\right)
\rightarrow\wedge^{k-1}\left(  V_{N}\right)  $ as follows:

For every $k$-tuple $\left(  i_{1},i_{2},...,i_{k}\right)  $ of integers
satisfying $N\geq i_{1}>i_{2}>...>i_{k}\geq-N$, we let $\overset{\vee
}{v_{i}^{\left(  N\right)  }}\left(  v_{i_{1}}\wedge v_{i_{2}}\wedge...\wedge
v_{i_{k}}\right)  $ be%
\[
\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  i_{1},i_{2},...,i_{k}\right\}
;\\
\left(  -1\right)  ^{j-1}v_{i_{1}}\wedge v_{i_{2}}\wedge...\wedge v_{i_{j-1}%
}\wedge v_{i_{j+1}}\wedge v_{i_{j+2}}\wedge...\wedge v_{i_{k}}%
,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  i_{1},i_{2},...,i_{k}\right\}
\end{array}
\right.  ,
\]
where, in the case $i\in\left\{  i_{1},i_{2},...,i_{k}\right\}  $, we denote
by $j$ the integer $\ell$ satisfying $i_{\ell}=i$. Thus, the map
$\overset{\vee}{v_{i}^{\left(  N\right)  }}$ is defined on a basis of the
vector space $\wedge^{k}\left(  V_{N}\right)  $; we extend this to a map
$\wedge^{k}\left(  V_{N}\right)  \rightarrow\wedge^{k-1}\left(  V_{N}\right)
$ by linearity.

Note that, for every negative $\ell\in\mathbb{Z}$, we understand $\wedge
^{\ell}\left(  V_{N}\right)  $ to mean the zero space.
\end{definition}

Also:

\begin{definition}
For every $N\in\mathbb{N}$ and $k\in\left\{  1,2,...,2N+1\right\}  $, let
$\Omega_{N}^{\left(  k\right)  }$ denote the orbit of $v_{N}\wedge
v_{N-1}\wedge...\wedge v_{N-k+1}$ under the action of $\operatorname*{GL}%
\left(  V_{N}\right)  $.
\end{definition}

The following lemma, then, is an easy corollary of Theorem \ref{thm.plu}:

\begin{lemma}
\label{lem.plu.inf.plu}Let $N\in\mathbb{N}$ and $k\in\mathbb{Z}$. Let
$S_{N}^{\left(  k\right)  }=\sum\limits_{i=-N}^{N}\widehat{v_{i}^{\left(
N\right)  }}\otimes\overset{\vee}{v_{i}^{\left(  N\right)  }}:\wedge
^{k}\left(  V_{N}\right)  \otimes\wedge^{k}\left(  V_{N}\right)
\rightarrow\wedge^{k+1}\left(  V_{N}\right)  \otimes\wedge^{k-1}\left(
V_{N}\right)  $.

\textbf{(a)} This map $S_{N}^{\left(  k\right)  }$ does not depend on the
choice of the basis of $V_{N}$, and is $\operatorname*{GL}\left(
V_{N}\right)  $-invariant. In other words, for \textbf{any} basis $\left(
w_{N},w_{N-1},...,w_{-N}\right)  $ of $V_{N}$, we have $S_{N}^{\left(
k\right)  }=\sum\limits_{i=-N}^{N}\widehat{w_{i}^{\left(  N\right)  }}%
\otimes\overset{\vee}{w_{i}^{\left(  N\right)  }}$ (where the maps
$\widehat{w_{i}^{\left(  N\right)  }}$ and $\overset{\vee}{w_{i}^{\left(
N\right)  }}$ are defined just as $\widehat{v_{i}^{\left(  N\right)  }}$ and
$\overset{\vee}{v_{i}^{\left(  N\right)  }}$, but with respect to the basis
$\left(  w_{N},w_{N-1},...,w_{-N}\right)  $).

\textbf{(b)} Let $k\in\left\{  1,2,...,2N+1\right\}  $. A nonzero element
$\tau\in\wedge^{k}\left(  V_{N}\right)  $ belongs to $\Omega_{N}^{\left(
k\right)  }$ if and only if $S_{N}^{\left(  k\right)  }\left(  \tau\otimes
\tau\right)  =0$.

\textbf{(c)} The map $S_{N}^{\left(  k\right)  }$ is $\operatorname*{M}\left(
V_{N}\right)  $-invariant.
\end{lemma}

\textit{Proof of Lemma \ref{lem.plu.inf.plu}.} If we set $n=2N+1$ in Theorem
\ref{thm.plu}, and do the following renaming operations:

\begin{itemize}
\item rename the standard basis $\left(  v_{1},v_{2},...,v_{n}\right)  $ as
$\left(  v_{N},v_{N-1},...,v_{-N}\right)  $;

\item rename the vector space $V$ as $V_{N}$;

\item rename the map $S$ as $S_{N}^{\left(  k\right)  }$;

\item rename the basis $\left(  w_{1},w_{2},...,w_{n}\right)  $ as $\left(
w_{N},w_{N-1},...,w_{-N}\right)  $;

\item rename the maps $\widehat{v_{i}}$ as $\widehat{v_{i}^{\left(  N\right)
}}$;

\item rename the maps $\overset{\vee}{v_{i}}$ as $\overset{\vee}{v_{i}%
^{\left(  N\right)  }}$;

\item rename the maps $\widehat{w_{i}}$ as $\widehat{w_{i}^{\left(  N\right)
}}$;

\item rename the maps $\overset{\vee}{w_{i}}$ as $\overset{\vee}{w_{i}%
^{\left(  N\right)  }}$;

\item rename the set $\Omega$ as $\Omega_{N}^{\left(  k\right)  }$;
\end{itemize}

then what we obtain is exactly the statement of Lemma \ref{lem.plu.inf.plu}.
Thus, Lemma \ref{lem.plu.inf.plu} is proven.

The maps $S_{N}^{\left(  k\right)  }$ have their own compatibility relation
with the $j_{N}^{\left(  m\right)  }$:

\begin{lemma}
\label{lem.plu.inf.S.comp}Let $N\in\mathbb{N}$ and $m\in\mathbb{Z}$. Define
the notation $S_{N}^{\left(  N+m+1\right)  }$ as in Lemma
\ref{lem.plu.inf.plu}. Then,%
\[
\left(  j_{N}^{\left(  m+1\right)  }\otimes j_{N}^{\left(  m-1\right)
}\right)  \circ S_{N}^{\left(  N+m+1\right)  }=S\circ\left(  j_{N}^{\left(
m\right)  }\otimes j_{N}^{\left(  m\right)  }\right)  .
\]

\end{lemma}

\textit{Proof of Lemma \ref{lem.plu.inf.S.comp}.} Define the maps
$\widehat{v_{i}^{\left(  N\right)  }}$ and $\overset{\vee}{v_{i}^{\left(
N\right)  }}$ (for all $i\in\left\{  -N,-N+1,...,N\right\}  $) as in
Definition \ref{def.plu.inf.createdestroy}. Define the maps $\widehat{v_{i}}$
and $\overset{\vee}{v_{i}}$ (for all $i\in\mathbb{Z}$) as in Definition
\ref{def.createdestroy}.

\textbf{a)} Let us first show that%
\begin{equation}
j_{N}^{\left(  m+1\right)  }\circ\widehat{v_{i}^{\left(  N\right)  }%
}=\widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\ \ \ \ \ \ \ \ \ \ \text{for
every }i\in\left\{  N,N-1,...,-N\right\}  . \label{pf.plu.inf.S.comp.a}%
\end{equation}


\textit{Proof of (\ref{pf.plu.inf.S.comp.a}):} Let $i\in\left\{
N,N-1,...,-N\right\}  $. In order to prove (\ref{pf.plu.inf.S.comp.a}), it is
clearly enough to show that $\left(  j_{N}^{\left(  m+1\right)  }%
\circ\widehat{v_{i}^{\left(  N\right)  }}\right)  \left(  u\right)  =\left(
\widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(  u\right)  $
for every $u\in\wedge^{N+m+1}\left(  V_{N}\right)  $.

So let $u$ be any element of $\wedge^{N+m+1}\left(  V_{N}\right)  $. We must
prove the equality $\left(  j_{N}^{\left(  m+1\right)  }\circ\widehat{v_{i}%
^{\left(  N\right)  }}\right)  \left(  u\right)  =\left(  \widehat{v_{i}}\circ
j_{N}^{\left(  m\right)  }\right)  \left(  u\right)  $. Since this equality is
linear in $u$, we can WLOG assume that $u$ is an element of the basis $\left(
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)  _{N\geq
i_{0}>i_{1}>...>i_{N+m}\geq-N}$ of $\wedge^{N+m+1}\left(  V_{N}\right)  $.
Assume this. Then, there exists an $N+m+1$-tuple $\left(  i_{0},i_{1}%
,...,i_{N+m}\right)  $ of integers such that $N\geq i_{0}>i_{1}>...>i_{N+m}%
\geq-N$ and $u=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}$. Consider
this $N+m+1$-tuple.

Comparing%
\begin{align*}
\left(  j_{N}^{\left(  m+1\right)  }\circ\widehat{v_{i}^{\left(  N\right)  }%
}\right)  \left(  u\right)   &  =j_{N}^{\left(  m+1\right)  }%
\underbrace{\left(  \widehat{v_{i}^{\left(  N\right)  }}\left(  u\right)
\right)  }_{\substack{=v_{i}\wedge u\\\text{(by the definition of
}\widehat{v_{i}^{\left(  N\right)  }}\text{)}}}=j_{N}^{\left(  m+1\right)
}\left(  v_{i}\wedge\underbrace{u}_{=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{N+m}}}\right) \\
&  =j_{N}^{\left(  m+1\right)  }\left(  v_{i}\wedge v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{N+m}}\right) \\
&  =v_{i}\wedge v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge
v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }j_{N}^{\left(
m+1\right)  }\right)
\end{align*}
with%
\begin{align*}
\left(  \widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)   &  =\widehat{v_{i}}\left(  j_{N}^{\left(  m\right)  }\left(
u\right)  \right)  =v_{i}\wedge j_{N}^{\left(  m\right)  }\left(
\underbrace{u}_{=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\widehat{v_{i}}\right)
\\
&  =v_{i}\wedge\underbrace{j_{N}^{\left(  m\right)  }\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)  }_{\substack{=v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge v_{-N-2}\wedge
v_{-N-3}\wedge...\\\text{(by the definition of }j_{N}^{\left(  m\right)
}\text{)}}}\\
&  =v_{i}\wedge v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge
v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}\wedge...,
\end{align*}
we obtain $\left(  j_{N}^{\left(  m+1\right)  }\circ\widehat{v_{i}^{\left(
N\right)  }}\right)  \left(  u\right)  =\left(  \widehat{v_{i}}\circ
j_{N}^{\left(  m\right)  }\right)  \left(  u\right)  $. This is exactly what
we needed to prove in order to complete the proof of
(\ref{pf.plu.inf.S.comp.a}). The proof of (\ref{pf.plu.inf.S.comp.a}) is thus finished.

\textbf{b)} Let us next show that%
\begin{equation}
j_{N}^{\left(  m+1\right)  }\circ\overset{\vee}{v_{i}^{\left(  N\right)  }%
}=\overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }%
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  N,N-1,...,-N\right\}  .
\label{pf.plu.inf.S.comp.b}%
\end{equation}


\textit{Proof of (\ref{pf.plu.inf.S.comp.b}):} Let $i\in\left\{
N,N-1,...,-N\right\}  $. In order to prove (\ref{pf.plu.inf.S.comp.b}), it is
clearly enough to show that $\left(  j_{N}^{\left(  m+1\right)  }%
\circ\overset{\vee}{v_{i}^{\left(  N\right)  }}\right)  \left(  u\right)
=\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)  $ for every $u\in\wedge^{N+m+1}\left(  V_{N}\right)  $.

So let $u$ be any element of $\wedge^{N+m+1}\left(  V_{N}\right)  $. We must
prove the equality $\left(  j_{N}^{\left(  m+1\right)  }\circ\overset{\vee
}{v_{i}^{\left(  N\right)  }}\right)  \left(  u\right)  =\left(
\overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)  $. Since this equality is linear in $u$, we can WLOG assume that $u$
is an element of the basis $\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{N+m}}\right)  _{N\geq i_{0}>i_{1}>...>i_{N+m}\geq-N}$ of $\wedge
^{N+m+1}\left(  V_{N}\right)  $. Assume this. Then, there exists an
$N+m+1$-tuple $\left(  i_{0},i_{1},...,i_{N+m}\right)  $ of integers such that
$N\geq i_{0}>i_{1}>...>i_{N+m}\geq-N$ and $u=v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{N+m}}$. Consider this $N+m+1$-tuple.

Let $\left(  j_{0},j_{1},j_{2},...\right)  $ be the sequence $\left(
i_{0},i_{1},...,i_{N+m},-N-1,-N-2,-N-3,...\right)  $. From $u=v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}$, we obtain%
\begin{align}
j_{N}^{\left(  m\right)  }\left(  u\right)   &  =j_{N}^{\left(  m\right)
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)
=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge
v_{-N-2}\wedge v_{-N-3}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }j_{N}^{\left(
m\right)  }\right) \nonumber\\
&  =v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{N+m}%
,-N-1,-N-2,-N-3,...\right)  =\left(  j_{0},j_{1},j_{2},...\right)  \right)  .
\label{pf.plu.inf.S.comp.b.0}%
\end{align}


We distinguish between two cases:

\textit{Case 1:} We have $i\notin\left\{  i_{0},i_{1},...,i_{N+m}\right\}  $.

\textit{Case 2:} We have $i\in\left\{  i_{0},i_{1},...,i_{N+m}\right\}  $.

Let us first consider Case 1. In this case, from $u=v_{i_{0}}\wedge v_{i_{1}%
}\wedge...\wedge v_{i_{N+m}}$, we obtain%
\begin{align*}
&  \overset{\vee}{v_{i}^{\left(  N\right)  }}\left(  u\right) \\
&  =\overset{\vee}{v_{i}^{\left(  N\right)  }}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  i_{0},i_{1},...,i_{N+m}%
\right\}  ;\\
\left(  -1\right)  ^{j-1}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{\left(  j-1\right)  -1}}\wedge v_{i_{\left(  j-1\right)  +1}}\wedge
v_{i_{\left(  j-1\right)  +2}}\wedge...\wedge v_{i_{N+m}}%
,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  i_{0},i_{1},...,i_{N+m}\right\}
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\overset{\vee
}{v_{i}^{\left(  N\right)  }}\right)  ,
\end{align*}
where, in the case $i\in\left\{  i_{0},i_{1},...,i_{N+m}\right\}  $, we denote
by $j$ the integer $\ell$ satisfying $i_{\ell-1}=i$.\ \ \ \ \footnote{If you
are wondering where the $-1$ (for example, in $i_{\ell-1}$ and in $i_{\left(
j-1\right)  -1}$) comes from: It comes from the fact that the indexing of our
$N+m+1$-tuple $\left(  v_{i_{0}},v_{i_{1}},...,v_{i_{N+m}}\right)  $ begins
with $0$, and not with $1$ as in Definition \ref{def.createdestroy.fin}.}
Since $i\notin\left\{  i_{0},i_{1},...,i_{N+m}\right\}  $ (because we are in
Case 1), this simplifies to%
\[
\overset{\vee}{v_{i}^{\left(  N\right)  }}\left(  u\right)  =0.
\]


On the other hand, combining $i\notin\left\{  -N-1,-N-2,-N-3,...\right\}  $
(which is because $i\in\left\{  N,N-1,...,-N\right\}  $) with $i\notin\left\{
i_{0},i_{1},...,i_{N+m}\right\}  $ (which is because we are in Case 1), we
obtain%
\begin{align*}
i  &  \notin\left\{  i_{0},i_{1},...,i_{N+m}\right\}  \cup\left\{
-N-1,-N-2,-N-3,...\right\} \\
&  =\left\{  i_{0},i_{1},...,i_{N+m},-N-1,-N-2,-N-3,...\right\}  =\left\{
j_{0},j_{1},j_{2},...\right\} \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{N+m}%
,-N-1,-N-2,-N-3,...\right)  =\left(  j_{0},j_{1},j_{2},...\right)  \right)  .
\end{align*}
Now,%
\begin{align*}
\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)   &  =\overset{\vee}{v_{i}}\left(  j_{N}^{\left(  m\right)  }\left(
u\right)  \right)  =\overset{\vee}{v_{i}}\left(  v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }j_{N}^{\left(  m\right)  }\left(
u\right)  =v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\text{ by
(\ref{pf.plu.inf.S.comp.b.0})}\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  j_{0},j_{1},j_{2},...\right\}
;\\
\left(  -1\right)  ^{j}v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\wedge v_{j_{j-1}}\wedge v_{j_{j+1}}\wedge v_{j_{j+2}}\wedge
...,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  j_{0},j_{1},j_{2},...\right\}
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\overset{\vee
}{v_{i}}\right)  ,
\end{align*}
where, in the case $i\in\left\{  j_{0},j_{1},j_{2},...\right\}  $, we denote
by $j$ the integer $k$ satisfying $j_{k}=i$. Since $i\notin\left\{
j_{0},j_{1},j_{2},...\right\}  $, this simplifies to%
\[
\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)  =0.
\]
Compared with%
\[
\left(  j_{N}^{\left(  m+1\right)  }\circ\overset{\vee}{v_{i}^{\left(
N\right)  }}\right)  \left(  u\right)  =j_{N}^{\left(  m+1\right)
}\underbrace{\left(  \overset{\vee}{v_{i}^{\left(  N\right)  }}\left(
u\right)  \right)  }_{=0}=0,
\]
this yields $\left(  j_{N}^{\left(  m+1\right)  }\circ\overset{\vee
}{v_{i}^{\left(  N\right)  }}\right)  \left(  u\right)  =\left(
\overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)  $. We have thus proven $\left(  j_{N}^{\left(  m+1\right)  }%
\circ\overset{\vee}{v_{i}^{\left(  N\right)  }}\right)  \left(  u\right)
=\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)  $ in Case 1.

Next, let us consider Case 2. In this case, $i\in\left\{  i_{0},i_{1}%
,...,i_{N+m}\right\}  $, so there exists an $\ell\in\left\{
0,1,...,N+m\right\}  $ such that $i_{\ell}=i$. Denote this $\ell$ by $\kappa$.
Then, $i_{\kappa}=i$. Clearly,
\begin{align}
&  \left(  i_{0},i_{1},...,i_{\kappa-1},i_{\kappa+1},i_{\kappa+2}%
,...,i_{N+m},-N-1,-N-2,-N-3,...\right) \nonumber\\
&  =\left(  \text{result of removing the }\kappa+1\text{-th term from the
sequence} \phantom{\dfrac{\dfrac{I}{I}}{I}} \right. \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left.  \underbrace{\left(
i_{0},i_{1},...,i_{N+m},-N-1,-N-2,-N-3,...\right)  }_{=\left(  j_{0}%
,j_{1},j_{2},...\right)  }\right) \nonumber\\
&  =\left(  \text{result of removing the }\kappa+1\text{-th term from the
sequence }\left(  j_{0},j_{1},j_{2},...\right)  \right) \nonumber\\
&  =\left(  j_{0},j_{1},...,j_{\kappa-1},j_{\kappa+1},j_{\kappa+2},...\right)
. \label{pf.plu.inf.S.comp.b.2.triv}%
\end{align}


From $u=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}$, we obtain%
\begin{align*}
&  \overset{\vee}{v_{i}^{\left(  N\right)  }}\left(  u\right) \\
&  =\overset{\vee}{v_{i}^{\left(  N\right)  }}\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  i_{0},i_{1},...,i_{N+m}%
\right\}  ;\\
\left(  -1\right)  ^{j-1}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{\left(  j-1\right)  -1}}\wedge v_{i_{\left(  j-1\right)  +1}}\wedge
v_{i_{\left(  j-1\right)  +2}}\wedge...\wedge v_{i_{N+m}}%
,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  i_{0},i_{1},...,i_{N+m}\right\}
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\overset{\vee
}{v_{i}^{\left(  N\right)  }}\right)  ,
\end{align*}
where, in the case $i\in\left\{  i_{0},i_{1},...,i_{N+m}\right\}  $, we denote
by $j$ the integer $\ell$ satisfying $i_{\ell-1}=i$.\ \ \ \ \footnote{If you
are wondering where the $-1$ (for example, in $i_{\ell-1}$ and in $i_{\left(
j-1\right)  -1}$) comes from: It comes from the fact that the indexing of our
$N+m+1$-tuple $\left(  v_{i_{0}},v_{i_{1}},...,v_{i_{N+m}}\right)  $ begins
with $0$, and not with $1$ as in Definition \ref{def.createdestroy.fin}.}
Since $i\in\left\{  i_{0},i_{1},...,i_{N+m}\right\}  $, this simplifies to%
\[
\overset{\vee}{v_{i}^{\left(  N\right)  }}\left(  u\right)  =\left(
-1\right)  ^{j-1}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\left(
j-1\right)  -1}}\wedge v_{i_{\left(  j-1\right)  +1}}\wedge v_{i_{\left(
j-1\right)  +2}}\wedge...\wedge v_{i_{N+m}},
\]
where we denote by $j$ the integer $\ell$ satisfying $i_{\ell-1}=i$. Since the
integer $\ell$ satisfying $i_{\ell-1}=i$ is $\kappa+1$ (because $i_{\left(
\kappa+1\right)  -1}=i_{\kappa}=i$), this rewrites as%
\begin{align*}
\overset{\vee}{v_{i}^{\left(  N\right)  }}\left(  u\right)   &  =\left(
-1\right)  ^{\left(  \kappa+1\right)  -1}v_{i_{0}}\wedge v_{i_{1}}%
\wedge...\wedge v_{i_{\left(  \left(  \kappa+1\right)  -1\right)  -1}}\wedge
v_{i_{\left(  \left(  \kappa+1\right)  -1\right)  +1}}\wedge v_{i_{\left(
\left(  \kappa+1\right)  -1\right)  +2}}\wedge...\wedge v_{i_{N+m}}\\
&  =\left(  -1\right)  ^{\kappa}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{\kappa-1}}\wedge v_{i_{\kappa+1}}\wedge v_{i_{\kappa+2}}\wedge...\wedge
v_{i_{N+m}}%
\end{align*}
(since $\left(  \kappa+1\right)  -1=\kappa$). Thus,%
\begin{align}
&  \left(  j_{N}^{\left(  m+1\right)  }\circ\overset{\vee}{v_{i}^{\left(
N\right)  }}\right)  \left(  u\right) \nonumber\\
&  =j_{N}^{\left(  m+1\right)  }\left(  \underbrace{\overset{\vee
}{v_{i}^{\left(  N\right)  }}\left(  u\right)  }_{=\left(  -1\right)
^{\kappa}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\kappa-1}}\wedge
v_{i_{\kappa+1}}\wedge v_{i_{\kappa+2}}\wedge...\wedge v_{i_{N+m}}}\right)
\nonumber\\
&  =j_{N}^{\left(  m+1\right)  }\left(  \left(  -1\right)  ^{\kappa}v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{\kappa-1}}\wedge v_{i_{\kappa+1}}\wedge
v_{i_{\kappa+2}}\wedge...\wedge v_{i_{N+m}}\right) \nonumber\\
&  =\left(  -1\right)  ^{\kappa}v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge
v_{i_{\kappa-1}}\wedge v_{i_{\kappa+1}}\wedge v_{i_{\kappa+2}}\wedge...\wedge
v_{i_{N+m}}\wedge v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }j_{N}^{\left(
m+1\right)  }\right) \nonumber\\
&  =\left(  -1\right)  ^{\kappa}v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}%
}\wedge...\wedge v_{j_{\kappa-1}}\wedge v_{j_{\kappa+1}}\wedge v_{j_{\kappa
+2}}\wedge...\label{pf.plu.inf.S.comp.b.2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since (\ref{pf.plu.inf.S.comp.b.2.triv}) yields}\\
\left(  i_{0},i_{1},...,i_{\kappa-1},i_{\kappa+1},i_{\kappa+2},...,i_{N+m}%
,-N-1,-N-2,-N-3,...\right) \\
=\left(  j_{0},j_{1},...,j_{\kappa-1},j_{\kappa+1},j_{\kappa+2},...\right)
\end{array}
\right)  .\nonumber
\end{align}


On the other hand,
\begin{align*}
i  &  \in\left\{  i_{0},i_{1},...,i_{N+m}\right\}  \subseteq\left\{
i_{0},i_{1},...,i_{N+m},-N-1,-N-2,-N-3,...\right\}  =\left\{  j_{0}%
,j_{1},j_{2},...\right\} \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{N+m}%
,-N-1,-N-2,-N-3,...\right)  =\left(  j_{0},j_{1},j_{2},...\right)  \right)  .
\end{align*}
Moreover, the integer $k$ satisfying $j_{k}=i$ is $\kappa$%
\ \ \ \ \footnote{because%
\begin{align*}
j_{\kappa}  &  =i_{\kappa}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
i_{0},i_{1},...,i_{N+m},-N-1,-N-2,-N-3,...\right)  =\left(  j_{0},j_{1}%
,j_{2},...\right)  \text{ and }\kappa\in\left\{  0,1,...,N+m\right\}  \right)
\\
&  =i
\end{align*}
}. Now,
\begin{align*}
\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)   &  =\overset{\vee}{v_{i}}\left(  j_{N}^{\left(  m\right)  }\left(
u\right)  \right)  =\overset{\vee}{v_{i}}\left(  v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }j_{N}^{\left(  m\right)  }\left(
u\right)  =v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\text{ by
(\ref{pf.plu.inf.S.comp.b.0})}\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  j_{0},j_{1},j_{2},...\right\}
;\\
\left(  -1\right)  ^{j}v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\wedge v_{j_{j-1}}\wedge v_{j_{j+1}}\wedge v_{j_{j+2}}\wedge
...,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  j_{0},j_{1},j_{2},...\right\}
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\overset{\vee
}{v_{i}}\right)  ,
\end{align*}
where, in the case $i\in\left\{  j_{0},j_{1},j_{2},...\right\}  $, we denote
by $j$ the integer $k$ satisfying $j_{k}=i$. Since $i\in\left\{  j_{0}%
,j_{1},j_{2},...\right\}  $, this simplifies to%
\[
\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)  =\left(  -1\right)  ^{j}v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}%
}\wedge...\wedge v_{j_{j-1}}\wedge v_{j_{j+1}}\wedge v_{j_{j+2}}\wedge...,
\]
where we denote by $j$ the integer $k$ satisfying $j_{k}=i$. Since the integer
$k$ satisfying $j_{k}=i$ is $\kappa$, this rewrites as%
\[
\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)  =\left(  -1\right)  ^{\kappa}v_{j_{0}}\wedge v_{j_{1}}\wedge
v_{j_{2}}\wedge...\wedge v_{j_{\kappa-1}}\wedge v_{j_{\kappa+1}}\wedge
v_{j_{\kappa+2}}\wedge....
\]
Compared with (\ref{pf.plu.inf.S.comp.b.2}), this yields $\left(
j_{N}^{\left(  m+1\right)  }\circ\widehat{v_{i}^{\left(  N\right)  }}\right)
\left(  u\right)  =\left(  \widehat{v_{i}}\circ j_{N}^{\left(  m\right)
}\right)  \left(  u\right)  $. This is exactly what we needed to prove in
order to complete the proof of (\ref{pf.plu.inf.S.comp.b}). The proof of
(\ref{pf.plu.inf.S.comp.b}) is thus finished.

\textbf{c)} Let us next show that%
\begin{equation}
\widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }=0\ \ \ \ \ \ \ \ \ \ \text{for
every }i\in\left\{  -N-1,-N-2,-N-3,...\right\}  . \label{pf.plu.inf.S.comp.c}%
\end{equation}


\textit{Proof of (\ref{pf.plu.inf.S.comp.c}):} Let $i\in\left\{
-N-1,-N-2,-N-3,...\right\}  $. In order to prove (\ref{pf.plu.inf.S.comp.c}),
it is clearly enough to show that $\left(  \widehat{v_{i}}\circ j_{N}^{\left(
m\right)  }\right)  \left(  u\right)  =0$ for every $u\in\wedge^{N+m+1}\left(
V_{N}\right)  $.

So let $u$ be any element of $\wedge^{N+m+1}\left(  V_{N}\right)  $. We must
prove the equality $\left(  \widehat{v_{i}}\circ j_{N}^{\left(  m\right)
}\right)  \left(  u\right)  =0$. Since this equality is linear in $u$, we can
WLOG assume that $u$ is an element of the basis $\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)  _{N\geq i_{0}>i_{1}%
>...>i_{N+m}\geq-N}$ of $\wedge^{N+m+1}\left(  V_{N}\right)  $. Assume this.
Then, there exists an $N+m+1$-tuple $\left(  i_{0},i_{1},...,i_{N+m}\right)  $
of integers such that $N\geq i_{0}>i_{1}>...>i_{N+m}\geq-N$ and $u=v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}$. Consider this $N+m+1$-tuple.

The vector $v_{i}$ occurs twice in the semiinfinite wedge $v_{i}\wedge
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge
v_{-N-2}\wedge v_{-N-3}\wedge...$ (namely, it occurs once in the very
beginning of this wedge, and then it occurs again in the $v_{-N-1}\wedge
v_{-N-2}\wedge v_{-N-3}\wedge...$ part (because $i\in\left\{
-N-1,-N-2,-N-3,...\right\}  $)). Hence, the semiinfinite wedge $v_{i}\wedge
v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge
v_{-N-2}\wedge v_{-N-3}\wedge...$ equals $0$ (since a semiinfinite wedge in
which a vector occurs more than once must always be equal to $0$).

Now,%
\begin{align*}
\left(  \widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)   &  =\widehat{v_{i}}\left(  j_{N}^{\left(  m\right)  }\left(
u\right)  \right)  =v_{i}\wedge j_{N}^{\left(  m\right)  }\left(
\underbrace{u}_{=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\widehat{v_{i}}\right)
\\
&  =v_{i}\wedge\underbrace{j_{N}^{\left(  m\right)  }\left(  v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)  }_{\substack{=v_{i_{0}}\wedge
v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge v_{-N-2}\wedge
v_{-N-3}\wedge...\\\text{(by the definition of }j_{N}^{\left(  m\right)
}\text{)}}}\\
&  =v_{i}\wedge v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge
v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}\wedge...\\
&  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{as we proved above}\right)  .
\end{align*}
This is exactly what we needed to prove in order to complete the proof of
(\ref{pf.plu.inf.S.comp.c}). The proof of (\ref{pf.plu.inf.S.comp.c}) is thus finished.

\textbf{d)} Let us now show that
\begin{equation}
\overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }%
=0\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  N+1,N+2,N+3,...\right\}  .
\label{pf.plu.inf.S.comp.d}%
\end{equation}


\textit{Proof of (\ref{pf.plu.inf.S.comp.d}):} Let $i\in\left\{
N+1,N+2,N+3,...\right\}  $. In order to prove (\ref{pf.plu.inf.S.comp.b}), it
is clearly enough to show that $\left(  \overset{\vee}{v_{i}}\circ
j_{N}^{\left(  m\right)  }\right)  \left(  u\right)  =0$ for every $u\in
\wedge^{N+m+1}\left(  V_{N}\right)  $.

So let $u$ be any element of $\wedge^{N+m+1}\left(  V_{N}\right)  $. We must
prove the equality $\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(
m\right)  }\right)  \left(  u\right)  =0$. Since this equality is linear in
$u$, we can WLOG assume that $u$ is an element of the basis $\left(  v_{i_{0}%
}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)  _{N\geq i_{0}%
>i_{1}>...>i_{N+m}\geq-N}$ of $\wedge^{N+m+1}\left(  V_{N}\right)  $. Assume
this. Then, there exists an $N+m+1$-tuple $\left(  i_{0},i_{1},...,i_{N+m}%
\right)  $ of integers such that $N\geq i_{0}>i_{1}>...>i_{N+m}\geq-N$ and
$u=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}$. Consider this
$N+m+1$-tuple.

Notice that $i\in\left\{  N+1,N+2,N+3,...\right\}  $, so that $i\notin\left\{
N,N-1,...,-N\right\}  $ and $i\notin\left\{  N,N-1,N-2,...\right\}  $.

Since $N\geq i_{0}>i_{1}>...>i_{N+m}\geq-N$, we have $\left\{  i_{0}%
,i_{1},...,i_{N+m}\right\}  \subseteq\left\{  N,N-1,...,-N\right\}  $ and thus
$i\notin\left\{  i_{0},i_{1},...,i_{N+m}\right\}  $ (because $i\notin\left\{
N,N-1,...,-N\right\}  $).

Let $\left(  j_{0},j_{1},j_{2},...\right)  $ be the sequence $\left(
i_{0},i_{1},...,i_{N+m},-N-1,-N-2,-N-3,...\right)  $. Then,%
\begin{align*}
\left\{  j_{0},j_{1},j_{2},...\right\}   &  =\left\{  i_{0},i_{1}%
,...,i_{N+m},-N-1,-N-2,-N-3,...\right\} \\
&  =\underbrace{\left\{  i_{0},i_{1},...,i_{N+m}\right\}  }_{\subseteq\left\{
N,N-1,...,-N\right\}  }\cup\left\{  -N-1,-N-2,-N-3,...\right\} \\
&  \subseteq\left\{  N,N-1,...,-N\right\}  \cup\left\{
-N-1,-N-2,-N-3,...\right\}  =\left\{  N,N-1,N-2,...\right\}  .
\end{align*}
Thus, $i\notin\left\{  j_{0},j_{1},j_{2},...\right\}  $ (since $i\notin%
\left\{  N,N-1,N-2,...\right\}  $).

From $u=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}$, we obtain%
\begin{align}
j_{N}^{\left(  m\right)  }\left(  u\right)   &  =j_{N}^{\left(  m\right)
}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\right)
=v_{i_{0}}\wedge v_{i_{1}}\wedge...\wedge v_{i_{N+m}}\wedge v_{-N-1}\wedge
v_{-N-2}\wedge v_{-N-3}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }j_{N}^{\left(
m\right)  }\right) \nonumber\\
&  =v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i_{0},i_{1},...,i_{N+m}%
,-N-1,-N-2,-N-3,...\right)  =\left(  j_{0},j_{1},j_{2},...\right)  \right)  ,
\label{pf.plu.inf.S.comp.d.0}%
\end{align}
so that%
\begin{align*}
\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(
u\right)   &  =\overset{\vee}{v_{i}}\left(  j_{N}^{\left(  m\right)  }\left(
u\right)  \right)  =\overset{\vee}{v_{i}}\left(  v_{j_{0}}\wedge v_{j_{1}%
}\wedge v_{j_{2}}\wedge...\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }j_{N}^{\left(  m\right)  }\left(
u\right)  =v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}\wedge...\text{ by
(\ref{pf.plu.inf.S.comp.d.0})}\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  j_{0},j_{1},j_{2},...\right\}
;\\
\left(  -1\right)  ^{j}v_{j_{0}}\wedge v_{j_{1}}\wedge v_{j_{2}}%
\wedge...\wedge v_{j_{j-1}}\wedge v_{j_{j+1}}\wedge v_{j_{j+2}}\wedge
...,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{  j_{0},j_{1},j_{2},...\right\}
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\overset{\vee
}{v_{i}}\right)  ,
\end{align*}
where, in the case $i\in\left\{  j_{0},j_{1},j_{2},...\right\}  $, we denote
by $j$ the integer $k$ satisfying $j_{k}=i$. Since $i\notin\left\{
j_{0},j_{1},j_{2},...\right\}  $, this simplifies to $\left(  \overset{\vee
}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \left(  u\right)  =0$.

This is exactly what we needed to prove in order to complete the proof of
(\ref{pf.plu.inf.S.comp.d}). The proof of (\ref{pf.plu.inf.S.comp.d}) is thus finished.

\textbf{e)} Now it is the time to draw conclusions.

We have $S=\sum\limits_{i\in\mathbb{Z}}\widehat{v_{i}}\otimes\overset{\vee
}{v_{i}}$ (by the definition of $S$). Thus,%
\begin{align*}
&  S\circ\left(  j_{N}^{\left(  m\right)  }\otimes j_{N}^{\left(  m\right)
}\right)  =\left(  \sum\limits_{i\in\mathbb{Z}}\widehat{v_{i}}\otimes
\overset{\vee}{v_{i}}\right)  \circ\left(  j_{N}^{\left(  m\right)  }\otimes
j_{N}^{\left(  m\right)  }\right)  =\sum\limits_{i\in\mathbb{Z}}%
\underbrace{\left(  \widehat{v_{i}}\otimes\overset{\vee}{v_{i}}\right)
\circ\left(  j_{N}^{\left(  m\right)  }\otimes j_{N}^{\left(  m\right)
}\right)  }_{=\left(  \widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)
\otimes\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)
}\\
&  =\sum\limits_{i\in\mathbb{Z}}\left(  \widehat{v_{i}}\circ j_{N}^{\left(
m\right)  }\right)  \otimes\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(
m\right)  }\right) \\
&  =\sum\limits_{i=-\infty}^{-N-1}\underbrace{\left(  \widehat{v_{i}}\circ
j_{N}^{\left(  m\right)  }\right)  }_{\substack{=0\\\text{(by
(\ref{pf.plu.inf.S.comp.c}))}}}\otimes\left(  \overset{\vee}{v_{i}}\circ
j_{N}^{\left(  m\right)  }\right)  +\sum\limits_{i=-N}^{N}\underbrace{\left(
\widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  }_{\substack{=j_{N}%
^{\left(  m+1\right)  }\circ\widehat{v_{i}^{\left(  N\right)  }}\\\text{(by
(\ref{pf.plu.inf.S.comp.a}))}}}\otimes\underbrace{\left(  \overset{\vee
}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  }_{\substack{=j_{N}^{\left(
m+1\right)  }\circ\overset{\vee}{v_{i}^{\left(  N\right)  }}\\\text{(by
(\ref{pf.plu.inf.S.comp.b}))}}}+\sum\limits_{i=N+1}^{\infty}\left(
\widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  \otimes
\underbrace{\left(  \overset{\vee}{v_{i}}\circ j_{N}^{\left(  m\right)
}\right)  }_{\substack{=0\\\text{(by (\ref{pf.plu.inf.S.comp.d}))}}}\\
&  =\underbrace{\sum\limits_{i=-\infty}^{-N-1}0\otimes\left(  \overset{\vee
}{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)  }_{=0}+\sum\limits_{i=-N}%
^{N}\underbrace{\left(  j_{N}^{\left(  m+1\right)  }\circ\widehat{v_{i}%
^{\left(  N\right)  }}\right)  \otimes\left(  j_{N}^{\left(  m+1\right)
}\circ\overset{\vee}{v_{i}^{\left(  N\right)  }}\right)  }_{=\left(
j_{N}^{\left(  m+1\right)  }\otimes j_{N}^{\left(  m-1\right)  }\right)
\circ\left(  \widehat{v_{i}^{\left(  N\right)  }}\otimes\overset{\vee
}{v_{i}^{\left(  N\right)  }}\right)  }+\underbrace{\sum\limits_{i=N+1}%
^{\infty}\left(  \widehat{v_{i}}\circ j_{N}^{\left(  m\right)  }\right)
\otimes0}_{=0}\\
&  =\sum\limits_{i=-N}^{N}\left(  j_{N}^{\left(  m+1\right)  }\otimes
j_{N}^{\left(  m-1\right)  }\right)  \circ\left(  \widehat{v_{i}^{\left(
N\right)  }}\otimes\overset{\vee}{v_{i}^{\left(  N\right)  }}\right)  =\left(
j_{N}^{\left(  m+1\right)  }\otimes j_{N}^{\left(  m-1\right)  }\right)
\circ\left(  \sum\limits_{i=-N}^{N}\widehat{v_{i}^{\left(  N\right)  }}%
\otimes\overset{\vee}{v_{i}^{\left(  N\right)  }}\right)  .
\end{align*}
But since $S_{N}^{\left(  N+m+1\right)  }=\sum\limits_{i=-N}^{N}%
\widehat{v_{i}^{\left(  N\right)  }}\otimes\overset{\vee}{v_{i}^{\left(
N\right)  }}$ (by the definition of $S_{N}^{\left(  N+m+1\right)  }$), this
rewrites as%
\[
S\circ\left(  j_{N}^{\left(  m\right)  }\otimes j_{N}^{\left(  m\right)
}\right)  =\left(  j_{N}^{\left(  m+1\right)  }\otimes j_{N}^{\left(
m-1\right)  }\right)  \circ\underbrace{\left(  \sum\limits_{i=-N}%
^{N}\widehat{v_{i}^{\left(  N\right)  }}\otimes\overset{\vee}{v_{i}^{\left(
N\right)  }}\right)  }_{=S_{N}^{\left(  N+m+1\right)  }}=\left(
j_{N}^{\left(  m+1\right)  }\otimes j_{N}^{\left(  m-1\right)  }\right)  \circ
S_{N}^{\left(  N+m+1\right)  }.
\]
This proves Lemma \ref{lem.plu.inf.S.comp}.

Now we can finally come to proving Theorem \ref{thm.plu.inf}:

\textit{Proof of Theorem \ref{thm.plu.inf}.} Let $\varrho^{\prime
}:\operatorname*{M}\left(  \infty\right)  \rightarrow\operatorname*{End}%
\left(  \mathcal{F}\otimes\mathcal{F}\right)  $ be the action of the monoid
$\operatorname*{M}\left(  \infty\right)  $ on the tensor product of the
$\operatorname*{M}\left(  \infty\right)  $-module $\mathcal{F}$ with itself.
Clearly,%
\[
\varrho^{\prime}\left(  M\right)  =\varrho\left(  M\right)  \otimes
\varrho\left(  M\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }M\in
\operatorname*{M}\left(  \infty\right)
\]
(because this is how one defines the tensor product of two modules over a monoid).

\textbf{(c)} Let $m\in\mathbb{Z}$. Let $M\in\operatorname*{M}\left(
\infty\right)  $. Let $v\in\mathcal{F}^{\left(  m\right)  }$ and
$w\in\mathcal{F}^{\left(  m\right)  }$. We are going to prove that $\left(
S\circ\varrho^{\prime}\left(  M\right)  \right)  \left(  v\otimes w\right)
=\left(  \varrho^{\prime}\left(  M\right)  \circ S\right)  \left(  v\otimes
w\right)  $.

Since $M\in\operatorname*{M}\left(  \infty\right)  =\bigcup\limits_{N\in
\mathbb{N}}i_{N}\left(  \operatorname*{M}\left(  V_{N}\right)  \right)  $ (by
Remark \ref{rmk.plu.inf.iN} \textbf{(c)}), there exists an $R\in\mathbb{N}$
such that $M\in i_{R}\left(  \operatorname*{M}\left(  V_{R}\right)  \right)
$. Consider this $R$.

Since $v\in\mathcal{F}^{\left(  m\right)  }=\bigcup\limits_{\substack{N\in
\mathbb{N};\\N\geq R}}j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(
V_{N}\right)  \right)  $ (by Proposition \ref{prop.plu.inf.cover}
\textbf{(b)}, applied to $Q=R$), there exists some $T\in\mathbb{N}$ such that
$T\geq R$ and $v\in j_{T}^{\left(  m\right)  }\left(  \wedge^{T+m+1}\left(
V_{T}\right)  \right)  $. Consider this $T$.

Since $w\in\mathcal{F}^{\left(  m\right)  }=\bigcup\limits_{\substack{N\in
\mathbb{N};\\N\geq T}}j_{N}^{\left(  m\right)  }\left(  \wedge^{N+m+1}\left(
V_{N}\right)  \right)  $ (by Proposition \ref{prop.plu.inf.cover}
\textbf{(b)}, applied to $Q=T$), there exists some $P\in\mathbb{N}$ such that
$P\geq T$ and $w\in j_{P}^{\left(  m\right)  }\left(  \wedge^{P+m+1}\left(
V_{P}\right)  \right)  $. Consider this $P$. There exists a $w^{\prime}%
\in\wedge^{P+m+1}\left(  V_{P}\right)  $ such that $w=j_{P}^{\left(  m\right)
}\left(  w^{\prime}\right)  $ (because $w\in j_{P}^{\left(  m\right)  }\left(
\wedge^{P+m+1}\left(  V_{P}\right)  \right)  $). Consider this $w^{\prime}$.

Applying Proposition \ref{prop.plu.inf.cover} \textbf{(a)}, we get
$j_{0}^{\left(  m\right)  }\left(  \wedge^{0+m+1}\left(  V_{0}\right)
\right)  \subseteq j_{1}^{\left(  m\right)  }\left(  \wedge^{1+m+1}\left(
V_{1}\right)  \right)  \subseteq j_{2}^{\left(  m\right)  }\left(
\wedge^{2+m+1}\left(  V_{2}\right)  \right)  \subseteq...$. Thus,
$j_{T}^{\left(  m\right)  }\left(  \wedge^{T+m+1}\left(  V_{T}\right)
\right)  \subseteq j_{P}^{\left(  m\right)  }\left(  \wedge^{P+m+1}\left(
V_{P}\right)  \right)  $ (since $T\leq P$), so that $v\in j_{T}^{\left(
m\right)  }\left(  \wedge^{T+m+1}\left(  V_{T}\right)  \right)  \subseteq
j_{P}^{\left(  m\right)  }\left(  \wedge^{P+m+1}\left(  V_{P}\right)  \right)
$. Hence, there exists a $v^{\prime}\in\wedge^{P+m+1}\left(  V_{P}\right)  $
such that $v=j_{P}^{\left(  m\right)  }\left(  v^{\prime}\right)  $. Consider
this $v^{\prime}$. Since $v=j_{P}^{\left(  m\right)  }\left(  v^{\prime
}\right)  $ and $w=j_{P}^{\left(  m\right)  }\left(  w^{\prime}\right)  $, we
have%
\begin{equation}
v\otimes w=j_{P}^{\left(  m\right)  }\left(  v^{\prime}\right)  \otimes
j_{P}^{\left(  m\right)  }\left(  w^{\prime}\right)  =\left(  j_{P}^{\left(
m\right)  }\otimes j_{P}^{\left(  m\right)  }\right)  \left(  v^{\prime
}\otimes w^{\prime}\right)  . \label{pf.plu.inf.-20}%
\end{equation}


Since $R\leq T\leq P$, we have $i_{R}\left(  \operatorname*{M}\left(
V_{R}\right)  \right)  \subseteq i_{P}\left(  \operatorname*{M}\left(
V_{P}\right)  \right)  $ (since Remark \ref{rmk.plu.inf.iN} \textbf{(b)}
yields $i_{0}\left(  \operatorname*{M}\left(  V_{0}\right)  \right)  \subseteq
i_{1}\left(  \operatorname*{M}\left(  V_{1}\right)  \right)  \subseteq
i_{2}\left(  \operatorname*{M}\left(  V_{2}\right)  \right)  \subseteq...$).
Thus, $M\in i_{R}\left(  \operatorname*{M}\left(  V_{R}\right)  \right)
\subseteq i_{P}\left(  \operatorname*{M}\left(  V_{P}\right)  \right)  $. In
other words, there exists an $A\in\operatorname*{M}\left(  V_{P}\right)  $
such that $M=i_{P}\left(  A\right)  $. Consider this $A$.

In the following, we will write the action of $\operatorname*{M}\left(
\infty\right)  $ on $\mathcal{F}$ as a left action. In other words, we will
abbreviate $\left(  \varrho\left(  N\right)  \right)  u$ by $Nu$, wherever
$N\in\operatorname*{M}\left(  \infty\right)  $ and $u\in\mathcal{F}$.
Similarly, we will write the action of $\operatorname*{M}\left(
\infty\right)  $ on $\mathcal{F}\otimes\mathcal{F}$ (this action is obtained
by tensoring the $\operatorname*{M}\left(  \infty\right)  $-module
$\mathcal{F}$ with itself); this action satisfies $\varrho^{\prime}\left(
A\right)  =\varrho\left(  A\right)  \otimes\varrho\left(  A\right)  $.

Let us also denote by $\varrho$ the action of the monoid $\operatorname*{M}%
\left(  V_{N}\right)  $ on $\wedge\left(  V_{N}\right)  $. Moreover, let us
denote by $\varrho^{\prime}$ the action of the monoid $\operatorname*{M}%
\left(  V_{N}\right)  $ on $\wedge\left(  V_{N}\right)  \otimes\wedge\left(
V_{N}\right)  $ (this action is obtained by tensoring the $\operatorname*{M}%
\left(  V_{N}\right)  $-module $\wedge\left(  V_{N}\right)  $ with itself).

We notice that every $\ell\in\mathbb{Z}$ satisfies%
\begin{equation}
\left(  \varrho\left(  M\right)  \right)  \circ j_{P}^{\left(  \ell\right)
}=j_{P}^{\left(  \ell\right)  }\circ\left(  \varrho\left(  A\right)  \right)
. \label{pf.plu.inf.-10}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.plu.inf.-10}):} Let $\ell\in\mathbb{Z}$.
Every $u\in\mathcal{F}^{\left(  \ell\right)  }$ satisfies%
\begin{align*}
\left(  \left(  \varrho\left(  M\right)  \right)  \circ j_{P}^{\left(
\ell\right)  }\right)  \left(  u\right)   &  =\left(  \varrho\left(  M\right)
\right)  \left(  j_{P}^{\left(  \ell\right)  }\left(  u\right)  \right)
=\underbrace{M}_{=i_{P}\left(  A\right)  }\cdot j_{P}^{\left(  \ell\right)
}\left(  u\right)  =i_{P}\left(  A\right)  \cdot j_{P}^{\left(  \ell\right)
}u=j_{P}^{\left(  \ell\right)  }\underbrace{\left(  Au\right)  }_{=\left(
\varrho\left(  A\right)  \right)  u}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition \ref{prop.plu.inf.iNjmN},
applied to }P\text{ and }\ell\text{ instead of }N\text{ and }m\right) \\
&  =j_{P}^{\left(  \ell\right)  }\left(  \left(  \varrho\left(  A\right)
\right)  u\right)  =\left(  j_{P}^{\left(  \ell\right)  }\circ\left(
\varrho\left(  A\right)  \right)  \right)  \left(  u\right)  .
\end{align*}
Thus, $\left(  \varrho\left(  M\right)  \right)  \circ j_{P}^{\left(
\ell\right)  }=j_{P}^{\left(  \ell\right)  }\circ\left(  \varrho\left(
A\right)  \right)  $, so that (\ref{pf.plu.inf.-10}) is proven.}

Applying Lemma \ref{lem.plu.inf.S.comp} to $N=P$, we obtain
\begin{equation}
\left(  j_{P}^{\left(  m+1\right)  }\otimes j_{P}^{\left(  m-1\right)
}\right)  \circ S_{P}^{\left(  P+m+1\right)  }=S\circ\left(  j_{P}^{\left(
m\right)  }\otimes j_{P}^{\left(  m\right)  }\right)  . \label{pf.plu.inf.1}%
\end{equation}


On the other hand, the map $S_{P}^{\left(  P+m+1\right)  }$ is
$\operatorname*{M}\left(  \infty\right)  $-invariant (by Lemma
\ref{lem.plu.inf.plu} \textbf{(c)}, applied to $N=P$ and $k=P+m+1$), so that%
\[
S_{P}^{\left(  P+m+1\right)  }\circ\left(  \varrho^{\prime}\left(  A\right)
\right)  =\left(  \varrho^{\prime}\left(  A\right)  \right)  \circ
S_{P}^{\left(  P+m+1\right)  }.
\]
Since $\varrho^{\prime}\left(  A\right)  =\varrho\left(  A\right)
\otimes\varrho\left(  A\right)  $, this rewrites as
\begin{equation}
S_{P}^{\left(  P+m+1\right)  }\circ\left(  \varrho\left(  A\right)
\otimes\varrho\left(  A\right)  \right)  =\left(  \varrho\left(  A\right)
\otimes\varrho\left(  A\right)  \right)  \circ S_{P}^{\left(  P+m+1\right)  }.
\label{pf.plu.inf.2}%
\end{equation}


Comparing%
\begin{align*}
&  S\circ\underbrace{\left(  \varrho^{\prime}\left(  M\right)  \right)
}_{=\varrho\left(  M\right)  \otimes\varrho\left(  M\right)  }\circ\left(
j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(  m\right)  }\right) \\
&  =S\circ\underbrace{\left(  \varrho\left(  M\right)  \otimes\varrho\left(
M\right)  \right)  \circ\left(  j_{P}^{\left(  m\right)  }\otimes
j_{P}^{\left(  m\right)  }\right)  }_{=\left(  \left(  \varrho\left(
M\right)  \right)  \circ j_{P}^{\left(  m\right)  }\right)  \otimes\left(
\left(  \varrho\left(  M\right)  \right)  \circ j_{P}^{\left(  m\right)
}\right)  }\\
&  =S\circ\left(  \underbrace{\left(  \left(  \varrho\left(  M\right)
\right)  \circ j_{P}^{\left(  m\right)  }\right)  }_{\substack{=j_{P}^{\left(
m\right)  }\circ\left(  \varrho\left(  A\right)  \right)  \\\text{(by
(\ref{pf.plu.inf.-10}), applied to }\ell=m\text{)}}}\otimes\underbrace{\left(
\left(  \varrho\left(  M\right)  \right)  \circ j_{P}^{\left(  m\right)
}\right)  }_{\substack{=j_{P}^{\left(  m\right)  }\circ\left(  \varrho\left(
A\right)  \right)  \\\text{(by (\ref{pf.plu.inf.-10}), applied to }%
\ell=m\text{)}}}\right) \\
&  =S\circ\underbrace{\left(  \left(  j_{P}^{\left(  m\right)  }\circ\left(
\varrho\left(  A\right)  \right)  \right)  \otimes\left(  j_{P}^{\left(
m\right)  }\circ\left(  \varrho\left(  A\right)  \right)  \right)  \right)
}_{=\left(  j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(  m\right)
}\right)  \circ\left(  \varrho\left(  A\right)  \otimes\varrho\left(
A\right)  \right)  }=\underbrace{S\circ\left(  j_{P}^{\left(  m\right)
}\otimes j_{P}^{\left(  m\right)  }\right)  }_{\substack{=\left(
j_{P}^{\left(  m+1\right)  }\otimes j_{P}^{\left(  m-1\right)  }\right)  \circ
S_{P}^{\left(  P+m+1\right)  }\\\text{(by (\ref{pf.plu.inf.1}))}}}\circ\left(
\varrho\left(  A\right)  \otimes\varrho\left(  A\right)  \right) \\
&  =\left(  j_{P}^{\left(  m+1\right)  }\otimes j_{P}^{\left(  m-1\right)
}\right)  \circ\underbrace{S_{P}^{\left(  P+m+1\right)  }\circ\left(
\varrho\left(  A\right)  \otimes\varrho\left(  A\right)  \right)
}_{\substack{=\left(  \varrho\left(  A\right)  \otimes\varrho\left(  A\right)
\right)  \circ S_{P}^{\left(  P+m+1\right)  }\\\text{(by (\ref{pf.plu.inf.2}%
))}}}=\left(  j_{P}^{\left(  m+1\right)  }\otimes j_{P}^{\left(  m-1\right)
}\right)  \circ\left(  \varrho\left(  A\right)  \otimes\varrho\left(
A\right)  \right)  \circ S_{P}^{\left(  P+m+1\right)  }%
\end{align*}
with%
\begin{align*}
&  \underbrace{\left(  \varrho^{\prime}\left(  M\right)  \right)  }%
_{=\varrho\left(  M\right)  \otimes\varrho\left(  M\right)  }\circ
\underbrace{S\circ\left(  j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(
m\right)  }\right)  }_{\substack{=\left(  j_{P}^{\left(  m+1\right)  }\otimes
j_{P}^{\left(  m-1\right)  }\right)  \circ S_{P}^{\left(  P+m+1\right)
}\\\text{(by (\ref{pf.plu.inf.1}))}}}\\
&  =\underbrace{\left(  \varrho\left(  M\right)  \otimes\varrho\left(
M\right)  \right)  \circ\left(  j_{P}^{\left(  m+1\right)  }\otimes
j_{P}^{\left(  m-1\right)  }\right)  }_{=\left(  \left(  \varrho\left(
M\right)  \right)  \circ j_{P}^{\left(  m+1\right)  }\right)  \otimes\left(
\left(  \varrho\left(  M\right)  \right)  \circ j_{P}^{\left(  m-1\right)
}\right)  }\circ S_{P}^{\left(  P+m+1\right)  }\\
&  =\left(  \underbrace{\left(  \left(  \varrho\left(  M\right)  \right)
\circ j_{P}^{\left(  m+1\right)  }\right)  }_{\substack{=j_{P}^{\left(
m+1\right)  }\circ\left(  \varrho\left(  A\right)  \right)  \\\text{(by
(\ref{pf.plu.inf.-10}), applied to }\ell=m+1\text{)}}}\otimes
\underbrace{\left(  \left(  \varrho\left(  M\right)  \right)  \circ
j_{P}^{\left(  m-1\right)  }\right)  }_{\substack{=j_{P}^{\left(  m-1\right)
}\circ\left(  \varrho\left(  A\right)  \right)  \\\text{(by
(\ref{pf.plu.inf.-10}), applied to }\ell=m-1\text{)}}}\right)  \circ
S_{P}^{\left(  P+m+1\right)  }\\
&  =\underbrace{\left(  \left(  j_{P}^{\left(  m+1\right)  }\circ\left(
\varrho\left(  A\right)  \right)  \right)  \otimes\left(  j_{P}^{\left(
m-1\right)  }\circ\left(  \varrho\left(  A\right)  \right)  \right)  \right)
}_{=\left(  j_{P}^{\left(  m+1\right)  }\otimes j_{P}^{\left(  m-1\right)
}\right)  \circ\left(  \varrho\left(  A\right)  \otimes\varrho\left(
A\right)  \right)  }\circ S_{P}^{\left(  P+m+1\right)  }\\
&  =\left(  j_{P}^{\left(  m+1\right)  }\otimes j_{P}^{\left(  m-1\right)
}\right)  \circ\left(  \varrho\left(  A\right)  \otimes\varrho\left(
A\right)  \right)  \circ S_{P}^{\left(  P+m+1\right)  },
\end{align*}
we obtain%
\begin{equation}
S\circ\left(  \varrho^{\prime}\left(  M\right)  \right)  \circ\left(
j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(  m\right)  }\right)  =\left(
\varrho^{\prime}\left(  M\right)  \right)  \circ S\circ\left(  j_{P}^{\left(
m\right)  }\otimes j_{P}^{\left(  m\right)  }\right)  . \label{pf.plu.inf.14}%
\end{equation}


Now,
\begin{align*}
&  \left(  S\circ\left(  \varrho^{\prime}\left(  M\right)  \right)  \right)
\underbrace{\left(  v\otimes w\right)  }_{\substack{=\left(  j_{P}^{\left(
m\right)  }\otimes j_{P}^{\left(  m\right)  }\right)  \left(  v^{\prime
}\otimes w^{\prime}\right)  \\\text{(by (\ref{pf.plu.inf.-20}))}}}\\
&  =\left(  S\circ\left(  \varrho^{\prime}\left(  M\right)  \right)  \right)
\left(  \left(  j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(  m\right)
}\right)  \left(  v^{\prime}\otimes w^{\prime}\right)  \right)
=\underbrace{\left(  S\circ\left(  \varrho^{\prime}\left(  M\right)  \right)
\circ\left(  j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(  m\right)
}\right)  \right)  }_{\substack{=\left(  \varrho^{\prime}\left(  M\right)
\right)  \circ S\circ\left(  j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(
m\right)  }\right)  \\\text{(by (\ref{pf.plu.inf.14}))}}}\left(  v^{\prime
}\otimes w^{\prime}\right) \\
&  =\left(  \left(  \varrho^{\prime}\left(  M\right)  \right)  \circ
S\circ\left(  j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(  m\right)
}\right)  \right)  \left(  v^{\prime}\otimes w^{\prime}\right)  =\left(
\left(  \varrho^{\prime}\left(  M\right)  \right)  \circ S\right)
\underbrace{\left(  \left(  j_{P}^{\left(  m\right)  }\otimes j_{P}^{\left(
m\right)  }\right)  \left(  v^{\prime}\otimes w^{\prime}\right)  \right)
}_{\substack{=v\otimes w\\\text{(by (\ref{pf.plu.inf.-20}))}}}\\
&  =\left(  \left(  \varrho^{\prime}\left(  M\right)  \right)  \circ S\right)
\left(  v\otimes w\right)  .
\end{align*}


Now forget that we fixed $v$ and $w$. We thus have proven that $\left(
S\circ\varrho^{\prime}\left(  M\right)  \right)  \left(  v\otimes w\right)
=\left(  \varrho^{\prime}\left(  M\right)  \circ S\right)  \left(  v\otimes
w\right)  $ for every $v\in\mathcal{F}^{\left(  m\right)  }$ and
$w\in\mathcal{F}^{\left(  m\right)  }$. In other words, the two maps
$S\circ\varrho^{\prime}\left(  M\right)  $ and $\varrho^{\prime}\left(
M\right)  \circ S$ are equal to each other on every pure tensor in
$\mathcal{F}^{\left(  m\right)  }\otimes\mathcal{F}^{\left(  m\right)  }$.
Thus, these two maps must be identical (on $\mathcal{F}^{\left(  m\right)
}\otimes\mathcal{F}^{\left(  m\right)  }$). In other words, $S\circ
\varrho^{\prime}\left(  M\right)  =\varrho^{\prime}\left(  M\right)  \circ S$.

Now forget that we fixed $M$. We have proven that $S\circ\varrho^{\prime
}\left(  M\right)  =\varrho^{\prime}\left(  M\right)  \circ S$ for every
$M\in\operatorname*{M}\left(  \infty\right)  $. In other words, $S$ is
$\operatorname*{M}\left(  \infty\right)  $-invariant. This proves Theorem
\ref{thm.plu.inf} \textbf{(c)}.

\textbf{(a)} Theorem \ref{thm.plu.inf} \textbf{(a)} follows from Theorem
\ref{thm.plu.inf} \textbf{(c)} since $\operatorname*{GL}\left(  \infty\right)
\subseteq\operatorname*{M}\left(  \infty\right)  $.

\textbf{(b)} $\Longrightarrow:$ Assume that $\tau\in\Omega$. We want to prove
that $S\left(  \tau\otimes\tau\right)  =0$.

Since $\Omega=\operatorname*{GL}\left(  \infty\right)  \cdot\psi_{0}$, we have
$\tau\in\Omega=\operatorname*{GL}\left(  \infty\right)  \cdot\psi_{0}$. In
other words, there exists $A\in\operatorname*{GL}\left(  \infty\right)  $ such
that $\tau=A\psi_{0}$. Consider this $A$.

It is easy to see that%
\begin{equation}
\overset{\vee}{v_{i}}\left(  \psi_{0}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for
every integer }i>0. \label{pf.plu.inf.b1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.plu.inf.b1}):} Let $i>0$ be an integer.
Then,%
\begin{align*}
\overset{\vee}{v_{i}}\left(  \psi_{0}\right)   &  =\overset{\vee}{v_{i}%
}\left(  v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\psi_{0}=v_{0}\wedge v_{-1}\wedge
v_{-2}\wedge...\right) \\
&  =\left\{
\begin{array}
[c]{l}%
0,\ \ \ \ \ \ \ \ \ \ \text{if }i\notin\left\{  0,-1,-2,...\right\}  ;\\
\left(  -1\right)  ^{j}v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...\wedge
v_{-\left(  j-1\right)  }\wedge v_{-\left(  j+1\right)  }\wedge v_{-\left(
j+2\right)  }\wedge...,\ \ \ \ \ \ \ \ \ \ \text{if }i\in\left\{
0,-1,-2,...\right\}
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\overset{\vee
}{v_{i}}\right)  ,
\end{align*}
where, in the case $i\in\left\{  0,-1,-2,...\right\}  $, we denote by $j$ the
integer $k$ satisfying $-k=i$. Since $i\notin\left\{  0,-1,-2,...\right\}  $
(because $i>0$), this simplifies to $\overset{\vee}{v_{i}}\left(  \psi
_{0}\right)  =0$. This proves (\ref{pf.plu.inf.b1}).} Also,%
\begin{equation}
\widehat{v_{i}}\left(  \psi_{0}\right)  =0\ \ \ \ \ \ \ \ \ \ \text{for every
integer }i\leq0. \label{pf.plu.inf.b2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.plu.inf.b2}):} Let $i\leq0$ be an integer.
Since $\psi_{0}=v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...$, we have%
\[
\widehat{v_{i}}\left(  \psi_{0}\right)  =\widehat{v_{i}}\left(  v_{0}\wedge
v_{-1}\wedge v_{-2}\wedge...\right)  =v_{i}\wedge v_{0}\wedge v_{-1}\wedge
v_{-2}\wedge...
\]
(by the definition of $\widehat{v_{i}}$). But the semiinfinite wedge
$v_{i}\wedge v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...$ contains the vector
$v_{i}$ twice (in fact, it contains the vector $v_{i}$ once in its very
beginning, and once again in its $v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...$
part (since $i\leq0$)), and thus must equal $0$ (since any semiinfinite wedge
which contains a vector more than once must equal $0$). We thus have
\[
\widehat{v_{i}}\left(  \psi_{0}\right)  =v_{i}\wedge v_{0}\wedge v_{-1}\wedge
v_{-2}\wedge...=0.
\]
This proves (\ref{pf.plu.inf.b2}).}

Since $S=\sum\limits_{i\in\mathbb{Z}}\widehat{v_{i}}\otimes\overset{\vee
}{v_{i}}$, we have%
\begin{align*}
S\left(  \psi_{0}\otimes\psi_{0}\right)   &  =\sum\limits_{i\in\mathbb{Z}%
}\underbrace{\left(  \widehat{v_{i}}\otimes\overset{\vee}{v_{i}}\right)
\left(  \psi_{0}\otimes\psi_{0}\right)  }_{=\widehat{v_{i}}\left(  \psi
_{0}\right)  \otimes\overset{\vee}{v_{i}}\left(  \psi_{0}\right)  }%
=\sum\limits_{i\in\mathbb{Z}}\widehat{v_{i}}\left(  \psi_{0}\right)
\otimes\overset{\vee}{v_{i}}\left(  \psi_{0}\right) \\
&  =\sum\limits_{\substack{i\in\mathbb{Z};\\i\leq0}}\underbrace{\widehat{v_{i}%
}\left(  \psi_{0}\right)  }_{\substack{=0\\\text{(by (\ref{pf.plu.inf.b2}))}%
}}\otimes\overset{\vee}{v_{i}}\left(  \psi_{0}\right)  +\sum
\limits_{\substack{i\in\mathbb{Z};\\i>0}}\widehat{v_{i}}\left(  \psi
_{0}\right)  \otimes\underbrace{\overset{\vee}{v_{i}}\left(  \psi_{0}\right)
}_{\substack{=0\\\text{(by (\ref{pf.plu.inf.b1}))}}}\\
&  =\underbrace{\sum\limits_{\substack{i\in\mathbb{Z};\\i\leq0}}0\otimes
\overset{\vee}{v_{i}}\left(  \psi_{0}\right)  }_{=0}+\underbrace{\sum
\limits_{\substack{i\in\mathbb{Z};\\i>0}}\widehat{v_{i}}\left(  \psi
_{0}\right)  \otimes0}_{=0}=0.
\end{align*}


Now, since $\tau=A\psi_{0}$, we have $\tau\otimes\tau=A\psi_{0}\otimes
A\psi_{0}=A\left(  \psi_{0}\otimes\psi_{0}\right)  $, so that%
\begin{align*}
S\left(  \tau\otimes\tau\right)   &  =S\left(  A\left(  \psi_{0}\otimes
\psi_{0}\right)  \right) \\
&  =A\cdot\underbrace{S\left(  \psi_{0}\otimes\psi_{0}\right)  }%
_{=0}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }S\text{ is }\operatorname*{M}%
\left(  \infty\right)  \text{-linear (by Theorem \ref{thm.plu.inf}
\textbf{(c)})}\right) \\
&  =A\cdot0=0.
\end{align*}
This proves the $\Longrightarrow$ direction of Theorem \ref{thm.plu.inf}
\textbf{(b)}.

$\Longleftarrow:$ Let $\tau\in\mathcal{F}^{\left(  0\right)  }$ be such that
$S\left(  \tau\otimes\tau\right)  =0$. We want to prove that $\tau\in\Omega$.

Since $\tau\in\mathcal{F}^{\left(  0\right)  }=\bigcup\limits_{\substack{N\in
\mathbb{N};\\N\geq0}}j_{N}^{\left(  0\right)  }\left(  \wedge^{N+0+1}\left(
V_{N}\right)  \right)  $ (by Proposition \ref{prop.plu.inf.cover}
\textbf{(b)}, applied to $m=0$ and $Q=0$), there exists some $N\in\mathbb{N}$
such that $N\geq0$ and $\tau\in j_{N}^{\left(  0\right)  }\left(
\wedge^{N+0+1}\left(  V_{N}\right)  \right)  $. Consider this $N$.

Lemma \ref{lem.plu.inf.S.comp} (applied to $m=0$) yields
\begin{equation}
\left(  j_{N}^{\left(  1\right)  }\otimes j_{N}^{\left(  -1\right)  }\right)
\circ S_{N}^{\left(  N+1\right)  }=S\circ\left(  j_{N}^{\left(  0\right)
}\otimes j_{N}^{\left(  0\right)  }\right)  . \label{pf.plu.inf.b5}%
\end{equation}


Recall that the map $j_{N}^{\left(  m\right)  }$ is injective for every
$m\in\mathbb{Z}$. In particular, the maps $j_{N}^{\left(  1\right)  }$ and
$j_{N}^{\left(  -1\right)  }$ are injective, so that the map $j_{N}^{\left(
1\right)  }\otimes j_{N}^{\left(  -1\right)  }$ is also injective.

But $\tau\in j_{N}^{\left(  0\right)  }\left(  \wedge^{N+0+1}\left(
V_{N}\right)  \right)  =j_{N}^{\left(  0\right)  }\left(  \wedge^{N+1}\left(
V_{N}\right)  \right)  $. In other words, there exists some $\tau^{\prime}%
\in\wedge^{N+1}\left(  V_{N}\right)  $ such that $\tau=j_{N}^{\left(
0\right)  }\left(  \tau^{\prime}\right)  $. Consider this $\tau^{\prime}$.

Since $\tau=j_{N}^{\left(  0\right)  }\left(  \tau^{\prime}\right)  $, we have
$\tau\otimes\tau=j_{N}^{\left(  0\right)  }\left(  \tau^{\prime}\right)
\otimes j_{N}^{\left(  0\right)  }\left(  \tau^{\prime}\right)  =\left(
j_{N}^{\left(  0\right)  }\otimes j_{N}^{\left(  0\right)  }\right)  \left(
\tau^{\prime}\otimes\tau^{\prime}\right)  $, so that%
\begin{align*}
S\left(  \tau\otimes\tau\right)   &  =S\left(  \left(  j_{N}^{\left(
0\right)  }\otimes j_{N}^{\left(  0\right)  }\right)  \left(  \tau^{\prime
}\otimes\tau^{\prime}\right)  \right)  =\underbrace{\left(  S\circ\left(
j_{N}^{\left(  0\right)  }\otimes j_{N}^{\left(  0\right)  }\right)  \right)
}_{\substack{=\left(  j_{N}^{\left(  1\right)  }\otimes j_{N}^{\left(
-1\right)  }\right)  \circ S_{N}^{\left(  N+1\right)  }\\\text{(by
(\ref{pf.plu.inf.b5}))}}}\left(  \tau^{\prime}\otimes\tau^{\prime}\right) \\
&  =\left(  \left(  j_{N}^{\left(  1\right)  }\otimes j_{N}^{\left(
-1\right)  }\right)  \circ S_{N}^{\left(  N+1\right)  }\right)  \left(
\tau^{\prime}\otimes\tau^{\prime}\right)  =\left(  j_{N}^{\left(  1\right)
}\otimes j_{N}^{\left(  -1\right)  }\right)  \left(  S_{N}^{\left(
N+1\right)  }\left(  \tau^{\prime}\otimes\tau^{\prime}\right)  \right)  .
\end{align*}
Compared with $S\left(  \tau\otimes\tau\right)  =0$, this yields $\left(
j_{N}^{\left(  1\right)  }\otimes j_{N}^{\left(  -1\right)  }\right)  \left(
S_{N}^{\left(  N+1\right)  }\left(  \tau^{\prime}\otimes\tau^{\prime}\right)
\right)  =0$. Since $j_{N}^{\left(  1\right)  }\otimes j_{N}^{\left(
-1\right)  }$ is injective, this yields $S_{N}^{\left(  N+1\right)  }\left(
\tau^{\prime}\otimes\tau^{\prime}\right)  =0$. But Lemma \ref{lem.plu.inf.plu}
\textbf{(b)} (applied to $N+1$ and $\tau^{\prime}$ instead of $k$ and $\tau$)
yields that $\tau^{\prime}$ belongs to $\Omega_{N}^{\left(  N+1\right)  }$ if
and only if $S_{N}^{\left(  N+1\right)  }\left(  \tau^{\prime}\otimes
\tau^{\prime}\right)  =0$. Since we know that $S_{N}^{\left(  N+1\right)
}\left(  \tau^{\prime}\otimes\tau^{\prime}\right)  =0$, we can thus conclude
that $\tau^{\prime}$ belongs to $\Omega_{N}^{\left(  N+1\right)  }$. Since
$\Omega_{N}^{\left(  N+1\right)  }$ is the orbit of $v_{N}\wedge v_{N-1}%
\wedge...\wedge v_{N-\left(  N+1\right)  +1}$ under the action of
$\operatorname*{GL}\left(  V_{N}\right)  $ (this is how $\Omega_{N}^{\left(
N+1\right)  }$ was defined), this yields that $\tau^{\prime}$ belongs to the
orbit of $v_{N}\wedge v_{N-1}\wedge...\wedge v_{N-\left(  N+1\right)  +1}$
under the action of $\operatorname*{GL}\left(  V_{N}\right)  $. In other
words, there exists some $A\in\operatorname*{GL}\left(  V_{N}\right)  $ such
that $\tau^{\prime}=A\cdot\left(  v_{N}\wedge v_{N-1}\wedge...\wedge
v_{N-\left(  N+1\right)  +1}\right)  $. Consider this $A$.

We have $\tau^{\prime}=A\cdot\left(  v_{N}\wedge v_{N-1}\wedge...\wedge
\underbrace{v_{N-\left(  N+1\right)  +1}}_{=v_{0}}\right)  =A\cdot\left(
v_{N}\wedge v_{N-1}\wedge...\wedge v_{0}\right)  $.

There clearly exists an invertible linear map $B\in\operatorname*{GL}\left(
V_{N}\right)  $ which sends $v_{N}$, $v_{N-1}$, $...$, $v_{0}$ to $v_{0}$,
$v_{-1}$, $...$, $v_{-N}$, respectively\footnote{\textit{Proof.} Since
$\left(  v_{N},v_{N-1},...,v_{-N}\right)  $ is a basis of $V_{N}$, there
exists a linear map $B\in\operatorname*{End}\left(  V_{N}\right)  $ which
sends $v_{i}$ to $\left\{
\begin{array}
[c]{c}%
v_{i-N},\ \ \ \ \ \ \ \ \ \ \text{if }i\geq0;\\
v_{-i},\ \ \ \ \ \ \ \ \ \ \text{if }i<0
\end{array}
\right.  $ for every $i\in\left\{  N,N-1,...,-N\right\}  $. This linear map
$B$ is invertible (since it permutes the elements of the basis $\left(
v_{N},v_{N-1},...,v_{-N}\right)  $ of $V_{N}$), and thus lies in
$\operatorname*{GL}\left(  V_{N}\right)  $, and it clearly sends $v_{N}$,
$v_{N-1}$, $...$, $v_{0}$ to $v_{0}$, $v_{-1}$, $...$, $v_{-N}$, respectively.
Qed.}. Pick such a $B$. Then, $B\cdot\left(  v_{N}\wedge v_{N-1}%
\wedge...\wedge v_{0}\right)  =v_{0}\wedge v_{-1}\wedge...\wedge v_{-N}$
(since $B$ sends $v_{N}$, $v_{N-1}$, $...$, $v_{0}$ to $v_{0}$, $v_{-1}$,
$...$, $v_{-N}$, respectively), so that $B^{-1}\cdot\left(  v_{0}\wedge
v_{-1}\wedge...\wedge v_{-N}\right)  =v_{N}\wedge v_{N-1}\wedge...\wedge
v_{0}$ and thus%
\[
A\underbrace{B^{-1}\cdot\left(  v_{0}\wedge v_{-1}\wedge...\wedge
v_{-N}\right)  }_{=v_{N}\wedge v_{N-1}\wedge...\wedge v_{0}}=A\cdot\left(
v_{N}\wedge v_{N-1}\wedge...\wedge v_{0}\right)  =\tau^{\prime}.
\]


Let $M=i_{N}\left(  AB^{-1}\right)  $. Then, $M=i_{N}\underbrace{\left(
AB^{-1}\right)  }_{\in\operatorname*{GL}\left(  V_{N}\right)  }\in
i_{N}\left(  \operatorname*{GL}\left(  V_{N}\right)  \right)  \subseteq
\operatorname*{GL}\left(  \infty\right)  $. Also,%
\begin{align*}
j_{N}^{\left(  0\right)  }\left(  v_{0}\wedge v_{-1}\wedge...\wedge
v_{-N}\right)   &  =v_{0}\wedge v_{-1}\wedge...\wedge v_{-N}\wedge
v_{-N-1}\wedge v_{-N-2}\wedge v_{-N-3}\wedge...\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }j_{N}^{\left(
0\right)  }\right) \\
&  =v_{0}\wedge v_{-1}\wedge v_{-2}\wedge...=\psi_{0}.
\end{align*}
Now,%
\begin{align*}
&  \underbrace{M}_{=i_{N}\left(  AB^{-1}\right)  }\cdot\underbrace{\psi_{0}%
}_{=j_{N}^{\left(  0\right)  }\left(  v_{0}\wedge v_{-1}\wedge...\wedge
v_{-N}\right)  }\\
&  =i_{N}\left(  AB^{-1}\right)  \cdot j_{N}^{\left(  0\right)  }\left(
v_{0}\wedge v_{-1}\wedge...\wedge v_{-N}\right)  =j_{N}^{\left(  0\right)
}\left(  \underbrace{AB^{-1}\cdot\left(  v_{0}\wedge v_{-1}\wedge...\wedge
v_{-N}\right)  }_{=\tau^{\prime}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition \ref{prop.plu.inf.iNjmN},
applied to }0\text{, }AB^{-1}\text{ and }v_{0}\wedge v_{-1}\wedge...\wedge
v_{-N}\text{ instead of }m\text{, }A\text{ and }u\right) \\
&  =j_{N}^{\left(  0\right)  }\left(  \tau^{\prime}\right)  =\tau.
\end{align*}
Thus, $\tau=\underbrace{M}_{\in\operatorname*{GL}\left(  \infty\right)  }%
\cdot\psi_{0}\in\operatorname*{GL}\left(  \infty\right)  \cdot\psi_{0}=\Omega
$. This proves the $\Longleftarrow$ direction of Theorem \ref{thm.plu.inf}
\textbf{(b)}.

\subsubsection{The semiinfinite Grassmannian}

Denote $\Omega\diagup\mathbb{C}^{\times}$ by $\operatorname*{Gr}$; this is
called the \textit{semiinfinite Grassmannian}.

Think of the space $V$ as $\mathbb{C}\left[  t,t^{-1}\right]  $ (by
identifying $v_{i}$ with $t^{-i}$). Then, $\left\langle v_{0},v_{-1}%
,v_{-2},...\right\rangle =\mathbb{C}\left[  t\right]  $.

\textbf{Exercise:} Then, $\operatorname*{Gr}$ is the set%
\[
\left\{  E\subseteq V\ \text{subspace\ }\mid\ \left(
\begin{array}
[c]{c}%
E\supseteq t^{N}\mathbb{C}\left[  t\right]  \text{ for sufficiently large
}N\text{, and}\\
\dim\left(  E\diagup t^{N}\mathbb{C}\left[  t\right]  \right)  =N\text{ for
sufficiently large }N
\end{array}
\right)  \right\}  .
\]
\footnote{Here, ``subspace'' means ``$\mathbb{C}$-vector subspace''.} (Note
that when the relations $E\supseteq t^{N}\mathbb{C}\left[  t\right]  $ and
$\dim\left(  E\diagup t^{N}\mathbb{C}\left[  t\right]  \right)  =N$ hold for
\textit{some} $N$, it is easy to see that they also hold for all greater $N$.)

We can also replace $\mathbb{C}\left[  t,t^{-1}\right]  $ with $\mathbb{C}%
\left(  \left(  t\right)  \right)  $ (the formal Laurent series), and then
\[
\operatorname*{Gr}=\left\{  E\subseteq V\text{ subspace}\ \mid\ \left(
\begin{array}
[c]{c}%
E\supseteq t^{N}\mathbb{C}\left[  \left[  t\right]  \right]  \text{ for
sufficiently large }N\text{, and}\\
\dim\left(  E\diagup t^{N}\mathbb{C}\left[  \left[  t\right]  \right]
\right)  =N\text{ for sufficiently large }N
\end{array}
\right)  \right\}  .
\]


For any $E\in\operatorname*{Gr}$, there exists some $N\in\mathbb{N}$ such that
$t^{N}\mathbb{C}\left[  t\right]  \subseteq E\subseteq t^{-N}\mathbb{C}\left[
t\right]  $, so that the quotient $E\diagup t^{N}\mathbb{C}\left[  t\right]
\subseteq t^{-N}\mathbb{C}\left[  t\right]  \diagup t^{N}\mathbb{C}\left[
t\right]  \cong\mathbb{C}^{2N}$.

Thus, $\operatorname*{Gr}=\bigcup\limits_{N\geq1}\operatorname*{Gr}\left(
N,2N\right)  $ (a nested union). (By a variation of this construction,
$\operatorname*{Gr}=\bigcup\limits_{N\geq1}\bigcup\limits_{M\geq
1}\operatorname*{Gr}\left(  N,N+M\right)  $.)

\subsubsection{The preimage of the Grassmannian under the Boson-Fermion
correspondence: the Hirota bilinear relations}

Now, how do we actually use these things to find solutions to the
Kadomtsev-Petviashvili equations and other integrable systems?

By Theorem \ref{thm.plu.inf} \textbf{(b)}, the elements of $\Omega$ are
exactly the nonzero elements $\tau$ of $\mathcal{F}^{\left(  0\right)  }$
satisfying $S\left(  \tau\otimes\tau\right)  =0$. We might wonder what happens
to these elements under the Boson-Fermion correspondence $\sigma$: how can
their preimages under $\sigma$ be described? In other words, can we find a
necessary and sufficient condition for a polynomial $\tau\in\mathcal{B}%
^{\left(  0\right)  }$ to satisfy $\sigma\left(  \tau\right)  \in\Omega$
(without using $\sigma$ in this very condition)?

Recall the power series $X\left(  u\right)  =\sum\limits_{i\in\mathbb{Z}}%
\xi_{i}u^{i}$ and $X^{\ast}\left(  u\right)  =\sum\limits_{i\in\mathbb{Z}}%
\xi_{i}^{\ast}u^{-i}$ defined in Definition \ref{def.euler.XGamma}. These
power series ``act'' on the fermionic space $\mathcal{F}$. The word ``act''
has been put in inverted commas here because it is not the power series but
their coefficients which really act on $\mathcal{F}$, whereas the power series
themselves only map elements of $\mathcal{F}$ to elements of $\mathcal{F}%
\left(  \left(  u\right)  \right)  $. This, actually, is an important
observation:%
\begin{equation}
\text{every }\omega\in\mathcal{F}\text{ satisfies }X\left(  u\right)
\omega\in\mathcal{F}\left(  \left(  u\right)  \right)  \text{ and }X^{\ast
}\left(  u\right)  \omega\in\mathcal{F}\left(  \left(  u\right)  \right)  .
\label{KdV.F((u))}%
\end{equation}
\footnote{\textit{Proof of (\ref{KdV.F((u))}):} Let $\omega\in\mathcal{F}$.
Since $X\left(  u\right)  =\sum\limits_{i\in\mathbb{Z}}\xi_{i}u^{i}$, we have
$X\left(  u\right)  \omega=\sum\limits_{i\in\mathbb{Z}}\xi_{i}\left(
\omega\right)  u^{i}\in\mathcal{F}\left(  \left(  u\right)  \right)  $,
because every sufficiently small $i\in\mathbb{Z}$ satisfies $\xi_{i}\left(
\omega\right)  =0$ (this is easy to see). On the other hand, since $X^{\ast
}\left(  u\right)  =\sum\limits_{i\in\mathbb{Z}}\xi_{i}^{\ast}u^{-i}$, we have
$X^{\ast}\left(  u\right)  =\sum\limits_{i\in\mathbb{Z}}\xi_{i}^{\ast}\left(
\omega\right)  u^{-i}\in\mathcal{F}\left(  \left(  u\right)  \right)  $, since
every sufficiently high $i\in\mathbb{Z}$ satisfies $\xi_{i}^{\ast}\left(
\omega\right)  =0$ (this, again, is easy to see). This proves
(\ref{KdV.F((u))}).}

Let $\tau\in\mathcal{B}^{\left(  0\right)  }$ be arbitrary. We want to find an
equivalent form for the equation $S\left(  \sigma\left(  \tau\right)
\otimes\sigma\left(  \tau\right)  \right)  =0$ which does not refer to
$\sigma$.

Let us give two definitions first:

\begin{definition}
\label{def.OMEGA}Let $A$ and $B$ be two $\mathbb{C}$-vector spaces, and let
$u$ be a symbol. Then, the map%
\begin{align*}
A\left(  \left(  u\right)  \right)  \times B\left(  \left(  u\right)  \right)
&  \rightarrow\left(  A\otimes B\right)  \left(  \left(  u\right)  \right)
,\\
\left(  \sum\limits_{i\in\mathbb{Z}}a_{i}u^{i},\sum\limits_{i\in\mathbb{Z}%
}b_{i}u^{i}\right)   &  \mapsto\sum\limits_{i\in\mathbb{Z}}\left(
\sum\limits_{j\in\mathbb{Z}}a_{j}\otimes b_{i-j}\right)  u^{i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{where all }a_{i}\text{ lie in }A\text{
and all }b_{i}\text{ lie in }B\right)
\end{align*}
is well-defined (in fact, it is easy to see that for any Laurent series
$\sum\limits_{i\in\mathbb{Z}}a_{i}u^{i}\in A\left(  \left(  u\right)  \right)
$ with all $a_{i}$ lying in $A$, any Laurent series $\sum\limits_{i\in
\mathbb{Z}}b_{i}u^{i}\in B\left(  \left(  u\right)  \right)  $ with all
$b_{i}$ lying in $B$, and any integer $i\in\mathbb{Z}$, the sum $\sum
\limits_{j\in\mathbb{Z}}a_{j}\otimes b_{i-j}$ has only finitely many addends
and vanishes if $i$ is small enough) and $\mathbb{C}$-bilinear. Hence, it
induces a $\mathbb{C}$-linear map%
\begin{align*}
A\left(  \left(  u\right)  \right)  \otimes B\left(  \left(  u\right)
\right)   &  \rightarrow\left(  A\otimes B\right)  \left(  \left(  u\right)
\right)  ,\\
\left(  \sum\limits_{i\in\mathbb{Z}}a_{i}u^{i}\right)  \otimes\left(
\sum\limits_{i\in\mathbb{Z}}b_{i}u^{i}\right)   &  \mapsto\sum\limits_{i\in
\mathbb{Z}}\left(  \sum\limits_{j\in\mathbb{Z}}a_{j}\otimes b_{i-j}\right)
u^{i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{where all }a_{i}\text{ lie in }A\text{
and all }b_{i}\text{ lie in }B\right)  .
\end{align*}
This map will be denoted by $\Omega_{A,B,u}$.
\end{definition}

More can be said about the map $\Omega_{A,B,u}$: It factors as a composition
of the canonical projection $A\left(  \left(  u\right)  \right)  \otimes
B\left(  \left(  u\right)  \right)  \rightarrow A\left(  \left(  u\right)
\right)  \otimes_{\mathbb{C}\left(  \left(  u\right)  \right)  }B\left(
\left(  u\right)  \right)  $ with a $\mathbb{C}\left(  \left(  u\right)
\right)  $-linear map $A\left(  \left(  u\right)  \right)  \otimes
_{\mathbb{C}\left(  \left(  u\right)  \right)  }B\left(  \left(  u\right)
\right)  \rightarrow\left(  A\otimes B\right)  \left(  \left(  u\right)
\right)  $. We won't need this in the following. What we will need is the
following observation:

\begin{remark}
\label{rmk.OMEGA.linear}Let $A$ and $B$ be two $\mathbb{C}$-algebras, and let
$u$ be a symbol. Then, the map $\Omega_{A,B,u}$ is $A\otimes B$-linear.
\end{remark}

\begin{definition}
Let $A$ be a $\mathbb{C}$-vector space, and let $u$ be a symbol. Then,
$\operatorname*{CT}\nolimits_{u}:A\left(  \left(  u\right)  \right)
\rightarrow A$ will denote the map which sends every Laurent series
$\sum\limits_{i\in\mathbb{Z}}a_{i}u^{i}\in A\left(  \left(  u\right)  \right)
$ (where all $a_{i}$ lie in $A$) to $a_{0}\in A$. The image of a Laurent
series $\alpha$ under $\operatorname*{CT}\nolimits_{u}$ will be called the
\textbf{constant term} of $\alpha$. The map $\operatorname*{CT}\nolimits_{u}$
is clearly $A$-linear.
\end{definition}

This notion of ``constant term'' we have thus defined for Laurent series is,
of course, completely analogous to the one used for polynomials and formal
power series. The label $\operatorname*{CT}\nolimits_{u}$ is an abbreviation
for ``constant term with respect to the variable $u$''.

Now, for every $\omega\in\mathcal{F}^{\left(  0\right)  }$ and $\rho
\in\mathcal{F}^{\left(  0\right)  }$, we have%
\begin{equation}
S\left(  \omega\otimes\rho\right)  =\operatorname*{CT}\nolimits_{u}\left(
\Omega_{\mathcal{F},\mathcal{F},u}\left(  X\left(  u\right)  \omega\otimes
X^{\ast}\left(  u\right)  \rho\right)  \right)  . \label{KdV.S=CT}%
\end{equation}
\footnote{\textit{Proof of (\ref{KdV.S=CT}):} Let $\omega\in\mathcal{F}%
^{\left(  0\right)  }$ and $\rho\in\mathcal{F}^{\left(  0\right)  }$. Since
$X\left(  u\right)  =\sum\limits_{i\in\mathbb{Z}}\xi_{i}u^{i}$ and $X^{\ast
}\left(  u\right)  =\sum\limits_{i\in\mathbb{Z}}\xi_{i}^{\ast}u^{-i}%
=\sum\limits_{i\in\mathbb{Z}}\xi_{-i}^{\ast}u^{i}$ (here, we substituted $-i$
for $i$ in the sum), we have%
\[
X\left(  u\right)  \omega\otimes X^{\ast}\left(  u\right)  \rho=\left(
\sum\limits_{i\in\mathbb{Z}}\xi_{i}u^{i}\right)  \omega\otimes\left(
\sum\limits_{i\in\mathbb{Z}}\xi_{-i}^{\ast}u^{i}\right)  \rho=\left(
\sum\limits_{i\in\mathbb{Z}}\xi_{i}\left(  \omega\right)  u^{i}\right)
\otimes\left(  \sum\limits_{i\in\mathbb{Z}}\xi_{-i}^{\ast}\left(  \rho\right)
u^{i}\right)  ,
\]
so that%
\begin{align*}
&  \Omega_{\mathcal{F},\mathcal{F},u}\left(  X\left(  u\right)  \omega\otimes
X^{\ast}\left(  u\right)  \rho\right) \\
&  =\Omega_{\mathcal{F},\mathcal{F},u}\left(  \left(  \sum\limits_{i\in
\mathbb{Z}}\xi_{i}\left(  \omega\right)  u^{i}\right)  \otimes\left(
\sum\limits_{i\in\mathbb{Z}}\xi_{-i}^{\ast}\left(  \rho\right)  u^{i}\right)
\right)  =\sum\limits_{i\in\mathbb{Z}}\left(  \sum\limits_{j\in\mathbb{Z}}%
\xi_{j}\left(  \omega\right)  \otimes\xi_{-\left(  i-j\right)  }^{\ast}\left(
\rho\right)  \right)  u^{i}%
\end{align*}
(by the definition of $\Omega_{\mathcal{F},\mathcal{F},u}$). Thus (by the
definition of $\operatorname*{CT}\nolimits_{u}$) we have%
\begin{align*}
&  \operatorname*{CT}\nolimits_{u}\left(  \Omega_{\mathcal{F},\mathcal{F}%
,u}\left(  X\left(  u\right)  \omega\otimes X^{\ast}\left(  u\right)
\rho\right)  \right) \\
&  =\sum\limits_{j\in\mathbb{Z}}\xi_{j}\left(  \omega\right)  \otimes
\xi_{-\left(  0-j\right)  }^{\ast}\left(  \rho\right)  =\sum\limits_{j\in
\mathbb{Z}}\xi_{j}\left(  \omega\right)  \otimes\xi_{j}^{\ast}\left(
\rho\right)  =\sum\limits_{i\in\mathbb{Z}}\underbrace{\xi_{i}}%
_{=\widehat{v_{i}}}\left(  \omega\right)  \otimes\underbrace{\xi_{i}^{\ast}%
}_{=\overset{\vee}{v_{i}}}\left(  \rho\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }i\text{ for
}j\text{ in the sum}\right) \\
&  =\sum\limits_{i\in\mathbb{Z}}\widehat{v_{i}}\left(  \omega\right)
\otimes\overset{\vee}{v_{i}}\left(  \rho\right)  =\underbrace{\left(
\sum\limits_{i\in\mathbb{Z}}\widehat{v_{i}}\otimes\overset{\vee}{v_{i}%
}\right)  }_{\substack{=S\\\text{(because this is how }S\text{ was defined)}%
}}\left(  \omega\otimes\rho\right)  =S\left(  \omega\otimes\rho\right)  ,
\end{align*}
so that (\ref{KdV.S=CT}) is proven.}

Now, let $\tau\in\mathcal{B}^{\left(  0\right)  }$. Due to (\ref{KdV.F((u))})
(applied to $\omega=\sigma\left(  \tau\right)  $), we have $X\left(  u\right)
\sigma\left(  \tau\right)  \in\mathcal{F}\left(  \left(  u\right)  \right)  $
and $X^{\ast}\left(  u\right)  \sigma\left(  \tau\right)  \in\mathcal{F}%
\left(  \left(  u\right)  \right)  $.

Now, let us abuse notation and denote by $\sigma$ the map from $\mathcal{B}%
\left(  \left(  u\right)  \right)  $ to $\mathcal{F}\left(  \left(  u\right)
\right)  $ which is canonically induced by the Boson-Fermion correspondence
$\sigma:\mathcal{B}\rightarrow\mathcal{F}$. Then, of course, this new map
$\sigma:\mathcal{B}\left(  \left(  u\right)  \right)  \rightarrow
\mathcal{F}\left(  \left(  u\right)  \right)  $ is also an isomorphism. Then,
the equalities $\Gamma\left(  u\right)  =\sigma^{-1}\circ X\left(  u\right)
\circ\sigma$ and $\Gamma^{\ast}\left(  u\right)  =\sigma^{-1}\circ X^{\ast
}\left(  u\right)  \circ\sigma$ (from Definition \ref{def.euler.XGamma}) are
not just abbreviations for termwise equalities (as we explained them back in
Definition \ref{def.euler.XGamma}), but also hold literally (if we interpret
$\sigma$ to mean our isomorphism $\sigma:\mathcal{B}\left(  \left(  u\right)
\right)  \rightarrow\mathcal{F}\left(  \left(  u\right)  \right)  $ rather
than the original Boson-Fermion correspondence $\sigma:\mathcal{B}%
\rightarrow\mathcal{F}$). As a consequence, $\sigma\circ\Gamma\left(
u\right)  =X\left(  u\right)  \circ\sigma$ and $\sigma\circ\Gamma^{\ast
}\left(  u\right)  =X^{\ast}\left(  u\right)  \circ\sigma$. Thus,%
\[
\sigma\left(  \Gamma\left(  u\right)  \tau\right)  =\underbrace{\left(
\sigma\circ\Gamma\left(  u\right)  \right)  }_{=X\left(  u\right)  \circ
\sigma}\tau=\left(  X\left(  u\right)  \circ\sigma\right)  \tau=X\left(
u\right)  \sigma\left(  \tau\right)
\]
and%
\[
\sigma\left(  \Gamma^{\ast}\left(  u\right)  \tau\right)  =\underbrace{\left(
\sigma\circ\Gamma^{\ast}\left(  u\right)  \right)  }_{=X^{\ast}\left(
u\right)  \circ\sigma}\tau=\left(  X^{\ast}\left(  u\right)  \circ
\sigma\right)  \tau=X^{\ast}\left(  u\right)  \sigma\left(  \tau\right)  ,
\]
so that%
\[
\underbrace{X\left(  u\right)  \sigma\left(  \tau\right)  }_{=\sigma\left(
\Gamma\left(  u\right)  \tau\right)  }\otimes\underbrace{X^{\ast}\left(
u\right)  \sigma\left(  \tau\right)  }_{=\sigma\left(  \Gamma^{\ast}\left(
u\right)  \tau\right)  }=\sigma\left(  \Gamma\left(  u\right)  \tau\right)
\otimes\sigma\left(  \Gamma^{\ast}\left(  u\right)  \tau\right)  =\left(
\sigma\otimes\sigma\right)  \left(  \Gamma\left(  u\right)  \tau\otimes
\Gamma^{\ast}\left(  u\right)  \tau\right)  .
\]
Now,
\begin{align*}
S\left(  \sigma\left(  \tau\right)  \otimes\sigma\left(  \tau\right)  \right)
&  =\operatorname*{CT}\nolimits_{u}\left(  \Omega_{\mathcal{F},\mathcal{F}%
,u}\underbrace{\left(  X\left(  u\right)  \sigma\left(  \tau\right)  \otimes
X^{\ast}\left(  u\right)  \sigma\left(  \tau\right)  \right)  }_{=\left(
\sigma\otimes\sigma\right)  \left(  \Gamma\left(  u\right)  \tau\otimes
\Gamma^{\ast}\left(  u\right)  \tau\right)  }\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{KdV.S=CT}), applied to }%
\omega=\sigma\left(  \tau\right)  \text{ and }\rho=\sigma\left(  \tau\right)
\right) \\
&  =\operatorname*{CT}\nolimits_{u}\left(  \Omega_{\mathcal{F},\mathcal{F}%
,u}\left(  \left(  \sigma\otimes\sigma\right)  \left(  \Gamma\left(  u\right)
\tau\otimes\Gamma^{\ast}\left(  u\right)  \tau\right)  \right)  \right) \\
&  =\underbrace{\left(  \operatorname*{CT}\nolimits_{u}\circ\Omega
_{\mathcal{F},\mathcal{F},u}\circ\left(  \sigma\otimes\sigma\right)  \right)
}_{\substack{=\left(  \sigma\otimes\sigma\right)  \circ\operatorname*{CT}%
\nolimits_{u}\circ\Omega_{\mathcal{B},\mathcal{B},u}\\\text{(since
}\operatorname*{CT}\nolimits_{u}\text{ and }\Omega_{A,B,u}\text{ are
functorial)}}}\left(  \Gamma\left(  u\right)  \tau\otimes\Gamma^{\ast}\left(
u\right)  \tau\right) \\
&  =\left(  \left(  \sigma\otimes\sigma\right)  \circ\operatorname*{CT}%
\nolimits_{u}\circ\Omega_{\mathcal{B},\mathcal{B},u}\right)  \left(
\Gamma\left(  u\right)  \tau\otimes\Gamma^{\ast}\left(  u\right)  \tau\right)
\\
&  =\left(  \sigma\otimes\sigma\right)  \left(  \operatorname*{CT}%
\nolimits_{u}\left(  \Omega_{\mathcal{B},\mathcal{B},u}\left(  \Gamma\left(
u\right)  \tau\otimes\Gamma^{\ast}\left(  u\right)  \tau\right)  \right)
\right)  .
\end{align*}
Therefore, the equation $S\left(  \sigma\left(  \tau\right)  \otimes
\sigma\left(  \tau\right)  \right)  =0$ is equivalent to \newline$\left(
\sigma\otimes\sigma\right)  \left(  \operatorname*{CT}\nolimits_{u}\left(
\Omega_{\mathcal{B},\mathcal{B},u}\left(  \Gamma\left(  u\right)  \tau
\otimes\Gamma^{\ast}\left(  u\right)  \tau\right)  \right)  \right)  =0$. This
latter equation, in turn, is equivalent to $\operatorname*{CT}\nolimits_{u}%
\left(  \Omega_{\mathcal{B},\mathcal{B},u}\left(  \Gamma\left(  u\right)
\tau\otimes\Gamma^{\ast}\left(  u\right)  \tau\right)  \right)  =0$ (since
$\sigma\otimes\sigma$ is an isomorphism\footnote{because $\sigma$ is an
isomorphism}). This, in turn, is equivalent to $\left(  z^{-1}\otimes
z\right)  \cdot\operatorname*{CT}\nolimits_{u}\left(  \Omega_{\mathcal{B}%
,\mathcal{B},u}\left(  \Gamma\left(  u\right)  \tau\otimes\Gamma^{\ast}\left(
u\right)  \tau\right)  \right)  =0$ (because $z^{-1}\otimes z$ is an
invertible element of $\mathcal{B}\otimes\mathcal{B}$). Since%
\begin{align*}
&  \left(  z^{-1}\otimes z\right)  \cdot\operatorname*{CT}\nolimits_{u}\left(
\Omega_{\mathcal{B},\mathcal{B},u}\left(  \Gamma\left(  u\right)  \tau
\otimes\Gamma^{\ast}\left(  u\right)  \tau\right)  \right) \\
&  =\operatorname*{CT}\nolimits_{u}\left(  \underbrace{\left(  z^{-1}\otimes
z\right)  \cdot\Omega_{\mathcal{B},\mathcal{B},u}\left(  \Gamma\left(
u\right)  \tau\otimes\Gamma^{\ast}\left(  u\right)  \tau\right)
}_{\substack{=\Omega_{\mathcal{B},\mathcal{B},u}\left(  \left(  z^{-1}\otimes
z\right)  \left(  \Gamma\left(  u\right)  \tau\otimes\Gamma^{\ast}\left(
u\right)  \tau\right)  \right)  \\\text{(since }\Omega_{\mathcal{B}%
,\mathcal{B},u}\text{ is }\mathcal{B}\otimes\mathcal{B}\text{-linear)}%
}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{CT}\nolimits_{u}%
\text{ is }\mathcal{B}\otimes\mathcal{B}\text{-linear (by Remark
\ref{rmk.OMEGA.linear})}\right) \\
&  =\operatorname*{CT}\nolimits_{u}\left(  \Omega_{\mathcal{B},\mathcal{B}%
,u}\underbrace{\left(  \left(  z^{-1}\otimes z\right)  \left(  \Gamma\left(
u\right)  \tau\otimes\Gamma^{\ast}\left(  u\right)  \tau\right)  \right)
}_{=z^{-1}\Gamma\left(  u\right)  \tau\otimes z\Gamma^{\ast}\left(  u\right)
\tau}\right) \\
&  =\operatorname*{CT}\nolimits_{u}\left(  \Omega_{\mathcal{B},\mathcal{B}%
,u}\left(  z^{-1}\Gamma\left(  u\right)  \tau\otimes z\Gamma^{\ast}\left(
u\right)  \tau\right)  \right)  ,
\end{align*}
this is equivalent to $\operatorname*{CT}\nolimits_{u}\left(  \Omega
_{\mathcal{B},\mathcal{B},u}\left(  z^{-1}\Gamma\left(  u\right)  \tau\otimes
z\Gamma^{\ast}\left(  u\right)  \tau\right)  \right)  =0$. Let us combine what
we have proven: We have proven the equivalence of assertions
\begin{equation}
\left(  S\left(  \sigma\left(  \tau\right)  \otimes\sigma\left(  \tau\right)
\right)  =0\right)  \ \Longleftrightarrow\ \left(  \operatorname*{CT}%
\nolimits_{u}\left(  \Omega_{\mathcal{B},\mathcal{B},u}\left(  z^{-1}%
\Gamma\left(  u\right)  \tau\otimes z\Gamma^{\ast}\left(  u\right)
\tau\right)  \right)  =0\right)  . \label{pf.hirota.firstrewriting}%
\end{equation}


Now, let us simplify $\operatorname*{CT}\nolimits_{u}\left(  \Omega
_{\mathcal{B},\mathcal{B},u}\left(  z^{-1}\Gamma\left(  u\right)  \tau\otimes
z\Gamma^{\ast}\left(  u\right)  \tau\right)  \right)  $.

For this, we recall that $\mathcal{B}^{\left(  0\right)  }=\widetilde{F}%
=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $. Thus, the elements of
$\mathcal{B}^{\left(  0\right)  }$ are polynomials in the countably many
indeterminates $x_{1}$, $x_{2}$, $x_{3}$, $...$. We are going to interpret the
elements of $\mathcal{B}^{\left(  0\right)  }\otimes\mathcal{B}^{\left(
0\right)  }$ as polynomials in ``twice as many'' indeterminates; by this we
mean the following:

\begin{Convention}
\label{conv.hirota.x'x''}Let $\left(  x_{1}^{\prime},x_{2}^{\prime}%
,x_{3}^{\prime},...\right)  $ and $\left(  x_{1}^{\prime\prime},x_{2}%
^{\prime\prime},x_{3}^{\prime\prime},...\right)  $ be two countable families
of new symbols. We denote the family $\left(  x_{1}^{\prime},x_{2}^{\prime
},x_{3}^{\prime},...\right)  $ by $x^{\prime}$, and we denote the family
$\left(  x_{1}^{\prime\prime},x_{2}^{\prime\prime},x_{3}^{\prime\prime
},...\right)  $ by $x^{\prime\prime}$. Thus, if $P\in\mathbb{C}\left[
x_{1},x_{2},x_{3},...\right]  $, we will denote by $P\left(  x^{\prime
}\right)  $ the polynomial $P\left(  x_{1}^{\prime},x_{2}^{\prime}%
,x_{3}^{\prime},...\right)  $, and we will denote by $P\left(  x^{\prime
\prime}\right)  $ the polynomial $P\left(  x_{1}^{\prime\prime},x_{2}%
^{\prime\prime},x_{3}^{\prime\prime},...\right)  $.

The $\mathbb{C}$-linear map%
\begin{align*}
\mathcal{B}^{\left(  0\right)  }\otimes\mathcal{B}^{\left(  0\right)  }  &
\rightarrow\mathbb{C}\left[  x_{1}^{\prime},x_{1}^{\prime\prime},x_{2}%
^{\prime},x_{2}^{\prime\prime},x_{3}^{\prime},x_{3}^{\prime\prime},...\right]
,\\
P\otimes Q  &  \mapsto P\left(  x^{\prime}\right)  Q\left(  x^{\prime\prime
}\right)
\end{align*}
is a $\mathbb{C}$-algebra isomorphism. By means of this isomorphism, we are
going to identify $\mathcal{B}^{\left(  0\right)  }\otimes\mathcal{B}^{\left(
0\right)  }$ with $\mathbb{C}\left[  x_{1}^{\prime},x_{1}^{\prime\prime}%
,x_{2}^{\prime},x_{2}^{\prime\prime},x_{3}^{\prime},x_{3}^{\prime\prime
},...\right]  $.
\end{Convention}

Another convention:

\begin{Convention}
\label{conv.hirota.P(y)}For any $P\in\mathcal{B}^{\left(  0\right)  }\left(
\left(  u\right)  \right)  $ and any family $\left(  y_{1},y_{2}%
,y_{3},...\right)  $ of pairwise commuting elements of a $\mathbb{C}$-algebra
$A$, we define an element $P\left(  y_{1},y_{2},y_{3},...\right)  $ of
$A\left(  \left(  u\right)  \right)  $ as follows: Write $P$ in the form
$P=\sum\limits_{i\in\mathbb{Z}}P_{i}\cdot u^{i}$ for some $P_{i}\in
\mathcal{B}^{\left(  0\right)  }$, and set $P\left(  y_{1},y_{2}%
,y_{3},...\right)  =\sum\limits_{i\in\mathbb{Z}}P_{i}\left(  y_{1},y_{2}%
,y_{3},...\right)  \cdot u^{i}$. (In words, $P\left(  y_{1},y_{2}%
,y_{3},...\right)  $ is defined by substituting $y_{1},y_{2},y_{3},...$ for
the variables $x_{1},x_{2},x_{3},...$ in $P$ while keeping the variable $u$ unchanged).
\end{Convention}

Now, let us notice that:

\begin{lemma}
\label{lem.hirota.PQ}For any $P\in\mathcal{B}^{\left(  0\right)  }\left(
\left(  u\right)  \right)  $ and $Q\in\mathcal{B}^{\left(  0\right)  }\left(
\left(  u\right)  \right)  $, we have%
\[
\Omega_{\mathcal{B},\mathcal{B},u}\left(  P\otimes Q\right)  =P\left(
x^{\prime}\right)  \cdot Q\left(  x^{\prime\prime}\right)
\]
(where $P\left(  x^{\prime}\right)  $ and $Q\left(  x^{\prime\prime}\right)  $
are to be understood according to Convention \ref{conv.hirota.P(y)} and
Convention \ref{conv.hirota.x'x''}, and where $\mathcal{B}^{\left(  0\right)
}\otimes\mathcal{B}^{\left(  0\right)  }$ is identified with $\mathbb{C}%
\left[  x_{1}^{\prime},x_{1}^{\prime\prime},x_{2}^{\prime},x_{2}^{\prime
\prime},x_{3}^{\prime},x_{3}^{\prime\prime},...\right]  $ according to
Convention \ref{conv.hirota.x'x''}).
\end{lemma}

\textit{Proof of Lemma \ref{lem.hirota.PQ}.} Let $P\in\mathcal{B}^{\left(
0\right)  }\left(  \left(  u\right)  \right)  $ and $Q\in\mathcal{B}^{\left(
0\right)  }\left(  \left(  u\right)  \right)  $. Write $P$ in the form
$P=\sum\limits_{i\in\mathbb{Z}}P_{i}\cdot u^{i}$ for some $P_{i}\in
\mathcal{B}^{\left(  0\right)  }$. Write $Q$ in the form $Q=\sum
\limits_{i\in\mathbb{Z}}Q_{i}\cdot u^{i}$ for some $Q_{i}\in\mathcal{B}%
^{\left(  0\right)  }$. Since $P=\sum\limits_{i\in\mathbb{Z}}P_{i}\cdot u^{i}$
and $Q=\sum\limits_{i\in\mathbb{Z}}Q_{i}\cdot u^{i}$, we have
\begin{align*}
\Omega_{\mathcal{B},\mathcal{B},u}\left(  P\otimes Q\right)   &
=\Omega_{\mathcal{B},\mathcal{B},u}\left(  \left(  \sum\limits_{i\in
\mathbb{Z}}P_{i}\cdot u^{i}\right)  \otimes\left(  \sum\limits_{i\in
\mathbb{Z}}Q_{i}\cdot u^{i}\right)  \right) \\
&  =\sum\limits_{i\in\mathbb{Z}}\left(  \sum\limits_{j\in\mathbb{Z}%
}\underbrace{P_{j}\otimes Q_{i-j}}_{\substack{=P_{j}\left(  x^{\prime}\right)
\cdot Q_{i-j}\left(  x^{\prime\prime}\right)  \\\text{(due to our
identification of}\\\mathcal{B}^{\left(  0\right)  }\otimes\mathcal{B}%
^{\left(  0\right)  }\text{ with}\\\mathbb{C}\left[  x_{1}^{\prime}%
,x_{1}^{\prime\prime},x_{2}^{\prime},x_{2}^{\prime\prime},x_{3}^{\prime}%
,x_{3}^{\prime\prime},...\right]  \text{)}}}\right)  u^{i}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\Omega_{\mathcal{B}%
,\mathcal{B},u}\right) \\
&  =\sum\limits_{i\in\mathbb{Z}}\left(  \sum\limits_{j\in\mathbb{Z}}%
P_{j}\left(  x^{\prime}\right)  \cdot Q_{i-j}\left(  x^{\prime\prime}\right)
\right)  u^{i}%
\end{align*}
and%
\begin{align*}
P\left(  x^{\prime}\right)  \cdot Q\left(  x^{\prime\prime}\right)   &
=\underbrace{\left(  \sum\limits_{i\in\mathbb{Z}}P_{i}\cdot u^{i}\right)
\left(  x^{\prime}\right)  }_{\substack{=\sum\limits_{i\in\mathbb{Z}}%
P_{i}\left(  x^{\prime}\right)  \cdot u^{i}=\sum\limits_{j\in\mathbb{Z}}%
P_{j}\left(  x^{\prime}\right)  \cdot u^{j}\\\text{(here, we renamed }i\text{
as }j\text{)}}}\cdot\underbrace{\left(  \sum\limits_{i\in\mathbb{Z}}Q_{i}\cdot
u^{i}\right)  \left(  x^{\prime\prime}\right)  }_{=\sum\limits_{i\in
\mathbb{Z}}Q_{i}\left(  x^{\prime\prime}\right)  \cdot u^{i}}\\
&  =\left(  \sum\limits_{j\in\mathbb{Z}}P_{j}\left(  x^{\prime}\right)  \cdot
u^{j}\right)  \cdot\left(  \sum\limits_{i\in\mathbb{Z}}Q_{i}\left(
x^{\prime\prime}\right)  \cdot u^{i}\right)  =\sum\limits_{j\in\mathbb{Z}}%
\sum\limits_{i\in\mathbb{Z}}P_{j}\left(  x^{\prime}\right)  \cdot u^{j}\cdot
Q_{i}\left(  x^{\prime\prime}\right)  \cdot u^{i}\\
&  =\sum\limits_{j\in\mathbb{Z}}\sum\limits_{i\in\mathbb{Z}}P_{j}\left(
x^{\prime}\right)  \cdot\underbrace{u^{j}\cdot Q_{i-j}\left(  x^{\prime\prime
}\right)  \cdot u^{i-j}}_{=Q_{i-j}\left(  x^{\prime\prime}\right)  \cdot
u^{i}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }i-j\text{ for
}i\text{ in the second sum}\right) \\
&  =\sum\limits_{j\in\mathbb{Z}}\sum\limits_{i\in\mathbb{Z}}P_{j}\left(
x^{\prime}\right)  \cdot Q_{i-j}\left(  x^{\prime\prime}\right)  \cdot
u^{i}=\sum\limits_{i\in\mathbb{Z}}\left(  \sum\limits_{j\in\mathbb{Z}}%
P_{j}\left(  x^{\prime}\right)  \cdot Q_{i-j}\left(  x^{\prime\prime}\right)
\right)  u^{i}=\Omega_{\mathcal{B},\mathcal{B},u}\left(  P\otimes Q\right)  .
\end{align*}
This proves Lemma \ref{lem.hirota.PQ}.

Now, Theorem \ref{thm.euler} (applied to $m=0$) yields%
\begin{align}
\Gamma\left(  u\right)   &  =uz\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}%
{j}u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}%
u^{-j}\right)  \ \ \ \ \ \ \ \ \ \ \text{and}\label{pf.hirota.2}\\
\Gamma^{\ast}\left(  u\right)   &  =z^{-1}\exp\left(  -\sum\limits_{j>0}%
\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(  \sum\limits_{j>0}\dfrac{a_{j}%
}{j}u^{-j}\right)  \label{pf.hirota.3}%
\end{align}
on $\mathcal{B}^{\left(  0\right)  }$. Thus,%
\begin{align*}
z^{-1}\Gamma\left(  u\right)  \tau &  =z^{-1}uz\exp\left(  \sum\limits_{j>0}%
\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac
{a_{j}}{j}u^{-j}\right)  \tau\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.hirota.2})}\right) \\
&  =u\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot
\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \tau\\
&  =u\exp\left(  \sum\limits_{j>0}\dfrac{jx_{j}}{j}u^{j}\right)  \cdot
\exp\left(  -\sum\limits_{j>0}\dfrac{\left(  \dfrac{\partial}{\partial x_{j}%
}\right)  }{j}u^{-j}\right)  \tau\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }a_{j}\text{ acts as }\dfrac{\partial}{\partial x_{j}}\text{ on
}\widetilde{F}\text{ for every }j>0\text{,}\\
\text{ and since }a_{-j}\text{ acts as }jx_{j}\text{ on }\widetilde{F}\text{
for every }j>0
\end{array}
\right) \\
&  =u\exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}}u^{-j}\right)
\tau,
\end{align*}
so that%
\begin{align}
\left(  z^{-1}\Gamma\left(  u\right)  \tau\right)  \left(  x^{\prime}\right)
&  =\left(  u\exp\left(  \sum\limits_{j>0}x_{j}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}}u^{-j}\right)
\tau\right)  \left(  x^{\prime}\right) \nonumber\\
&  =u\exp\left(  \sum\limits_{j>0}x_{j}^{\prime}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime}}%
u^{-j}\right)  \left(  \tau\left(  x^{\prime}\right)  \right)  .
\label{pf.hirota.5}%
\end{align}
Also,%
\begin{align*}
z\Gamma^{\ast}\left(  u\right)  \tau &  =zz^{-1}\exp\left(  -\sum
\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(  \sum
\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \tau\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.hirota.3})}\right) \\
&  =\exp\left(  -\sum\limits_{j>0}\dfrac{a_{-j}}{j}u^{j}\right)  \cdot
\exp\left(  \sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \tau\\
&  =\exp\left(  -\sum\limits_{j>0}\dfrac{jx_{j}}{j}u^{j}\right)  \cdot
\exp\left(  \sum\limits_{j>0}\dfrac{\left(  \dfrac{\partial}{\partial x_{j}%
}\right)  }{j}u^{-j}\right)  \tau\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }a_{j}\text{ acts as }\dfrac{\partial}{\partial x_{j}}\text{ on
}\widetilde{F}\text{ for every }j>0\text{, and }\\
\text{since }a_{-j}\text{ acts as }jx_{j}\text{ on }\widetilde{F}\text{ for
every }j>0
\end{array}
\right) \\
&  =\exp\left(  -\sum\limits_{j>0}x_{j}u^{j}\right)  \cdot\exp\left(
\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}}u^{-j}\right)
\tau,
\end{align*}
so that%
\begin{align}
\left(  z\Gamma^{\ast}\left(  u\right)  \tau\right)  \left(  x^{\prime\prime
}\right)   &  =\left(  \exp\left(  -\sum\limits_{j>0}x_{j}u^{j}\right)
\cdot\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}%
}u^{-j}\right)  \tau\right)  \left(  x^{\prime\prime}\right) \nonumber\\
&  =\exp\left(  -\sum\limits_{j>0}x_{j}^{\prime\prime}u^{j}\right)  \cdot
\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime\prime
}\right)  \right)  . \label{pf.hirota.6}%
\end{align}


Now,%
\begin{align}
&  \Omega_{\mathcal{B},\mathcal{B},u}\left(  z^{-1}\Gamma\left(  u\right)
\tau\otimes z\Gamma^{\ast}\left(  u\right)  \tau\right) \nonumber\\
&  =\left(  z^{-1}\Gamma\left(  u\right)  \tau\right)  \left(  x^{\prime
}\right)  \cdot\left(  z\Gamma^{\ast}\left(  u\right)  \tau\right)  \left(
x^{\prime\prime}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Lemma \ref{lem.hirota.PQ}, applied to
}P=z^{-1}\Gamma\left(  u\right)  \tau\text{ and }Q=z\Gamma^{\ast}\left(
u\right)  \tau\right) \nonumber\\
&  =u\exp\left(  \sum\limits_{j>0}x_{j}^{\prime}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime}}%
u^{-j}\right)  \left(  \tau\left(  x^{\prime}\right)  \right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\left(  -\sum\limits_{j>0}x_{j}^{\prime\prime
}u^{j}\right)  \cdot\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime\prime}}u^{-j}\right)  \left(  \tau\left(
x^{\prime\prime}\right)  \right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.hirota.5}) and
(\ref{pf.hirota.6})}\right) \nonumber\\
&  =u\exp\left(  \sum\limits_{j>0}x_{j}^{\prime}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}x_{j}^{\prime\prime}u^{j}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{1}{j}%
\dfrac{\partial}{\partial x_{j}^{\prime}}u^{-j}\right)  \left(  \tau\left(
x^{\prime}\right)  \right)  \cdot\exp\left(  \sum\limits_{j>0}\dfrac{1}%
{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}u^{-j}\right)  \left(
\tau\left(  x^{\prime\prime}\right)  \right)  . \label{pf.hirota.7}%
\end{align}


We are going to rewrite the right hand side of this equality. First of all,
notice that Theorem \ref{thm.exp(u+v)} (applied to $R=\left(  \mathcal{B}%
^{\left(  0\right)  }\otimes\mathcal{B}^{\left(  0\right)  }\right)  \left(
\left(  u\right)  \right)  $, \newline$I=\left(  \text{closure of the ideal of
}R\text{ generated by }x_{j}^{\prime}\text{ and }x_{j}^{\prime\prime}\text{
with }j\text{ ranging over all positive integers}\right)  $, $\alpha
=\sum\limits_{j>0}x_{j}^{\prime}u^{j}$ and $\beta=-\sum\limits_{j>0}%
x_{j}^{\prime\prime}u^{j}$) yields%
\[
\exp\left(  \sum\limits_{j>0}x_{j}^{\prime}u^{j}+\left(  -\sum\limits_{j>0}%
x_{j}^{\prime\prime}u^{j}\right)  \right)  =\exp\left(  \sum\limits_{j>0}%
x_{j}^{\prime}u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}x_{j}%
^{\prime\prime}u^{j}\right)  .
\]
Thus,%
\begin{align}
\exp\left(  \sum\limits_{j>0}x_{j}^{\prime}u^{j}\right)  \cdot\exp\left(
-\sum\limits_{j>0}x_{j}^{\prime\prime}u^{j}\right)   &  =\exp
\underbrace{\left(  \sum\limits_{j>0}x_{j}^{\prime}u^{j}+\left(
-\sum\limits_{j>0}x_{j}^{\prime\prime}u^{j}\right)  \right)  }_{=\sum
\limits_{j>0}u^{j}\left(  x_{j}^{\prime}-x_{j}^{\prime\prime}\right)
}\nonumber\\
&  =\exp\left(  \sum\limits_{j>0}u^{j}\left(  x_{j}^{\prime}-x_{j}%
^{\prime\prime}\right)  \right)  . \label{pf.hirota.8}%
\end{align}


Now, let us recall a very easy fact: If $\phi$ is an endomorphism of a vector
space $V$, and $v$ is a vector in $V$ such that $\phi v=0$, then $\left(
\exp\phi\right)  v$ is well-defined (in the sense that the power series
$\sum\limits_{n\geq0}\dfrac{1}{n!}\phi^{n}v$ converges) and satisfies $\left(
\exp\phi\right)  v=v$. Applying this fact to $V=\left(  \mathcal{B}^{\left(
0\right)  }\otimes\mathcal{B}^{\left(  0\right)  }\right)  \left[
u,u^{-1}\right]  $, $\phi=\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime\prime}}u^{-j}$ and $v=\tau\left(  x^{\prime}\right)
$, we see that $\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime
}\right)  \right)  $ is well-defined and satisfies
\begin{equation}
\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime}\right)
\right)  =\tau\left(  x^{\prime}\right)  \label{pf.hirota.9}%
\end{equation}
(since $\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime}\right)
\right)  =\sum\limits_{j>0}\dfrac{1}{j}\underbrace{\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}\left(  \tau\left(  x^{\prime}\right)  \right)  }%
_{=0}u^{-j}=0$). The same argument (with $x_{j}^{\prime}$ and $x_{j}%
^{\prime\prime}$ switching places) shows that $\exp\left(  \sum\limits_{j>0}%
\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime}}u^{-j}\right)  \left(
\tau\left(  x^{\prime\prime}\right)  \right)  $ is well-defined and satisfies%
\begin{equation}
\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime\prime}\right)
\right)  =\tau\left(  x^{\prime\prime}\right)  . \label{pf.hirota.10}%
\end{equation}


Now,%
\begin{align}
\exp\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(  \dfrac{\partial
}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial x_{j}^{\prime\prime}%
}\right)  \right)   &  =\exp\left(  \left(  -\sum\limits_{j>0}\dfrac{1}%
{j}\dfrac{\partial}{\partial x_{j}^{\prime}}u^{-j}\right)  +\sum
\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}%
u^{-j}\right) \nonumber\\
&  =\exp\left(  -\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime}}u^{-j}\right)  \circ\exp\left(  \sum\limits_{j>0}\dfrac{1}%
{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}u^{-j}\right)
\label{pf.hirota.11}%
\end{align}
\footnote{Here, the last equality sign follows from Theorem \ref{thm.exp(u+v)}%
, applied to
\begin{align*}
R  &  =\left(
\begin{array}
[c]{c}%
\text{closure of the }\mathbb{C}\left[  u,u^{-1}\right]  \text{-subalgebra of
}\operatorname*{End}\nolimits_{\mathbb{C}\left[  u,u^{-1}\right]  }\left(
\left(  \mathcal{B}^{\left(  0\right)  }\otimes\mathcal{B}^{\left(  0\right)
}\right)  \left[  u,u^{-1}\right]  \right) \\
\text{generated by }\dfrac{\partial}{\partial x_{j}^{\prime}}\text{ and
}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}\text{ with }j\text{ ranging
over all positive integers}%
\end{array}
\right)  ,\\
I  &  =\left(
\begin{array}
[c]{c}%
\text{closure of the ideal of }R\text{ generated by }\dfrac{\partial}{\partial
x_{j}^{\prime}}\text{ and }\dfrac{\partial}{\partial x_{j}^{\prime\prime}%
}\text{ with}\\
j\text{ ranging over all positive integers}%
\end{array}
\right)  ,\\
\alpha &  =-\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime}}u^{-j},\ \ \ \ \ \ \ \ \ \ \text{and}\ \ \ \ \ \ \ \ \ \ \beta
=\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}%
}u^{-j}.
\end{align*}
} and similarly%
\begin{equation}
\exp\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(  \dfrac{\partial
}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial x_{j}^{\prime\prime}%
}\right)  \right)  =\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime\prime}}u^{-j}\right)  \circ\exp\left(  -\sum
\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime}}%
u^{-j}\right)  . \label{pf.hirota.12}%
\end{equation}


But since $-\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(  \dfrac{\partial
}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial x_{j}^{\prime\prime}%
}\right)  $ is a derivation (from $\left(  \mathcal{B}^{\left(  0\right)
}\otimes\mathcal{B}^{\left(  0\right)  }\right)  \left[  u,u^{-1}\right]  $ to
\newline$\left(  \mathcal{B}^{\left(  0\right)  }\otimes\mathcal{B}^{\left(
0\right)  }\right)  \left[  u,u^{-1}\right]  $), its exponential $\exp\left(
-\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(  \dfrac{\partial}{\partial
x_{j}^{\prime}}-\dfrac{\partial}{\partial x_{j}^{\prime\prime}}\right)
\right)  $ is a $\mathbb{C}$-algebra homomorphism (since exponentials of
derivations are $\mathbb{C}$-algebra homomorphisms), so that%
\begin{align*}
&  \exp\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(  \dfrac{\partial
}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial x_{j}^{\prime\prime}%
}\right)  \right)  \left(  \tau\left(  x^{\prime}\right)  \tau\left(
x^{\prime\prime}\right)  \right) \\
&  =\underbrace{\exp\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(
\dfrac{\partial}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}\right)  \right)  }_{\substack{=\exp\left(
-\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime}}%
u^{-j}\right)  \circ\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime\prime}}u^{-j}\right)  \\\text{(by (\ref{pf.hirota.11}%
))}}}\left(  \tau\left(  x^{\prime}\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \cdot\underbrace{\exp\left(  -\sum\limits_{j>0}%
\dfrac{u^{-j}}{j}\left(  \dfrac{\partial}{\partial x_{j}^{\prime}}%
-\dfrac{\partial}{\partial x_{j}^{\prime\prime}}\right)  \right)
}_{\substack{=\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime\prime}}u^{-j}\right)  \circ\exp\left(  -\sum
\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime}}%
u^{-j}\right)  \\\text{(by (\ref{pf.hirota.12}))}}}\left(  \tau\left(
x^{\prime\prime}\right)  \right) \\
&  =\left(  \exp\left(  -\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime}}u^{-j}\right)  \circ\exp\left(  \sum\limits_{j>0}%
\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}u^{-j}\right)
\right)  \left(  \tau\left(  x^{\prime}\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \cdot\left(  \exp\left(  \sum\limits_{j>0}\dfrac{1}%
{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}u^{-j}\right)  \circ
\exp\left(  -\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime}}u^{-j}\right)  \right)  \left(  \tau\left(  x^{\prime\prime
}\right)  \right) \\
&  =\exp\left(  -\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime}}u^{-j}\right)  \underbrace{\left(  \exp\left(  \sum
\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}%
u^{-j}\right)  \left(  \tau\left(  x^{\prime}\right)  \right)  \right)
}_{\substack{=\tau\left(  x^{\prime}\right)  \\\text{(by (\ref{pf.hirota.9}%
))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}%
\dfrac{\partial}{\partial x_{j}^{\prime\prime}}u^{-j}\right)
\underbrace{\left(  \exp\left(  -\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial
}{\partial x_{j}^{\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime\prime
}\right)  \right)  \right)  }_{\substack{=\tau\left(  x^{\prime\prime}\right)
\\\text{(by (\ref{pf.hirota.10}))}}}\\
&  =\exp\left(  -\sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime}\right)  \right)
\cdot\exp\left(  \sum\limits_{j>0}\dfrac{1}{j}\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}u^{-j}\right)  \left(  \tau\left(  x^{\prime\prime
}\right)  \right)  .
\end{align*}
Hence, (\ref{pf.hirota.7}) becomes%
\begin{align}
&  \Omega_{\mathcal{B},\mathcal{B},u}\left(  z^{-1}\Gamma\left(  u\right)
\tau\otimes z\Gamma^{\ast}\left(  u\right)  \tau\right) \nonumber\\
&  =u\underbrace{\exp\left(  \sum\limits_{j>0}x_{j}^{\prime}u^{j}\right)
\cdot\exp\left(  -\sum\limits_{j>0}x_{j}^{\prime\prime}u^{j}\right)
}_{\substack{=\exp\left(  \sum\limits_{j>0}u^{j}\left(  x_{j}^{\prime}%
-x_{j}^{\prime\prime}\right)  \right)  \\\text{(by (\ref{pf.hirota.8}))}%
}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\underbrace{\exp\left(  -\sum\limits_{j>0}%
\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime}}u^{-j}\right)  \left(
\tau\left(  x^{\prime}\right)  \right)  \cdot\exp\left(  \sum\limits_{j>0}%
\dfrac{1}{j}\dfrac{\partial}{\partial x_{j}^{\prime\prime}}u^{-j}\right)
\left(  \tau\left(  x^{\prime\prime}\right)  \right)  }_{=\exp\left(
-\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(  \dfrac{\partial}{\partial
x_{j}^{\prime}}-\dfrac{\partial}{\partial x_{j}^{\prime\prime}}\right)
\right)  \left(  \tau\left(  x^{\prime}\right)  \tau\left(  x^{\prime\prime
}\right)  \right)  }\nonumber\\
&  =u\exp\left(  \sum\limits_{j>0}u^{j}\left(  x_{j}^{\prime}-x_{j}%
^{\prime\prime}\right)  \right)  \cdot\exp\left(  -\sum\limits_{j>0}%
\dfrac{u^{-j}}{j}\left(  \dfrac{\partial}{\partial x_{j}^{\prime}}%
-\dfrac{\partial}{\partial x_{j}^{\prime\prime}}\right)  \right)  \left(
\tau\left(  x^{\prime}\right)  \tau\left(  x^{\prime\prime}\right)  \right)  .
\label{pf.hirota.15}%
\end{align}
Thus, (\ref{pf.hirota.firstrewriting}) rewrites as%
\begin{align}
&  \left(  S\left(  \sigma\left(  \tau\right)  \otimes\sigma\left(
\tau\right)  \right)  =0\right) \nonumber\\
&  \Longleftrightarrow\ \left(  \operatorname*{CT}\nolimits_{u}\left(
u\exp\left(  \sum\limits_{j>0}u^{j}\left(  x_{j}^{\prime}-x_{j}^{\prime\prime
}\right)  \right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{u^{-j}}{j}\left(
\dfrac{\partial}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}\right)  \right)  \left(  \tau\left(  x^{\prime}\right)
\tau\left(  x^{\prime\prime}\right)  \right)  \right)  =0\right)  .
\label{pf.hirota.secondrewriting}%
\end{align}
This already gives a criterion for a $\tau\in\mathcal{B}^{\left(  0\right)  }$
to satisfy $\sigma\left(  \tau\right)  \in\Omega$, but it is yet a rather
messy one. We are going to simplify it in the following. First, we do a
substitution of variables:

\begin{Convention}
Let $\left(  y_{1},y_{2},y_{3},...\right)  $ be a sequence of new symbols. We
identify the $\mathbb{C}$-algebra $\mathbb{C}\left[  x_{1},y_{1},x_{2}%
,y_{2},x_{3},y_{3},...\right]  $ with the $\mathbb{C}$-algebra $\mathbb{C}%
\left[  x_{1}^{\prime},x_{1}^{\prime\prime},x_{2}^{\prime},x_{2}^{\prime
\prime},x_{3}^{\prime},x_{3}^{\prime\prime},...\right]  =\mathcal{B}^{\left(
0\right)  }\otimes\mathcal{B}^{\left(  0\right)  }$ by the following
substitution:%
\begin{align*}
x_{j}^{\prime}  &  =x_{j}-y_{j}\ \ \ \ \ \ \ \ \ \ \text{for every }j>0;\\
x_{j}^{\prime\prime}  &  =x_{j}+y_{j}\ \ \ \ \ \ \ \ \ \ \text{for every }j>0.
\end{align*}


If we define the sum and the difference of two sequences by componentwise
addition resp. subtraction, then this rewrites as follows:%
\begin{align*}
x^{\prime}  &  =x-y;\\
x^{\prime\prime}  &  =x+y.
\end{align*}

\end{Convention}

It is now easy to see that%
\[
x_{j}^{\prime}-x_{j}^{\prime\prime}=-2y_{j}\ \ \ \ \ \ \ \ \ \ \text{for every
}j>0,
\]
and%
\[
\dfrac{\partial}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial
x_{j}^{\prime\prime}}=-\dfrac{\partial}{\partial y_{j}}%
\ \ \ \ \ \ \ \ \ \ \text{for every }j>0
\]
(where $\dfrac{\partial}{\partial x_{j}^{\prime}}$ and $\dfrac{\partial
}{\partial x_{j}^{\prime\prime}}$ mean differentiation over the variables
$x_{j}^{\prime}$ and $x_{j}^{\prime\prime}$ in the polynomial ring
$\mathbb{C}\left[  x_{1}^{\prime},x_{1}^{\prime\prime},x_{2}^{\prime}%
,x_{2}^{\prime\prime},x_{3}^{\prime},x_{3}^{\prime\prime},...\right]  $,
whereas $\dfrac{\partial}{\partial y_{j}}$ means differentiation over the
variable $y_{j}$ in the polynomial ring $\mathbb{C}\left[  x_{1},y_{1}%
,x_{2},y_{2},x_{3},y_{3},...\right]  $). As a consequence,%
\begin{align*}
&  u\exp\left(  \sum\limits_{j>0}u^{j}\underbrace{\left(  x_{j}^{\prime}%
-x_{j}^{\prime\prime}\right)  }_{=-2y_{j}}\right)  \cdot\exp\left(
-\sum\limits_{j>0}\dfrac{u^{-j}}{j}\underbrace{\left(  \dfrac{\partial
}{\partial x_{j}^{\prime}}-\dfrac{\partial}{\partial x_{j}^{\prime\prime}%
}\right)  }_{=-\dfrac{\partial}{\partial y_{j}}}\right)  \left(  \tau\left(
\underbrace{x^{\prime}}_{=x-y}\right)  \tau\left(  \underbrace{x^{\prime
\prime}}_{=x+y}\right)  \right) \\
&  =u\exp\left(  -2\sum\limits_{j>0}u^{j}y_{j}\right)  \cdot\exp\left(
\sum\limits_{j>0}\dfrac{u^{-j}}{j}\dfrac{\partial}{\partial y_{j}}\right)
\left(  \tau\left(  x-y\right)  \tau\left(  x+y\right)  \right)  .
\end{align*}
Hence, (\ref{pf.hirota.secondrewriting}) rewrites as%
\begin{align}
&  \left(  S\left(  \sigma\left(  \tau\right)  \otimes\sigma\left(
\tau\right)  \right)  =0\right) \nonumber\\
&  \Longleftrightarrow\ \left(  \operatorname*{CT}\nolimits_{u}\left(
u\exp\left(  -2\sum\limits_{j>0}u^{j}y_{j}\right)  \cdot\exp\left(
\sum\limits_{j>0}\dfrac{u^{-j}}{j}\dfrac{\partial}{\partial y_{j}}\right)
\left(  \tau\left(  x-y\right)  \tau\left(  x+y\right)  \right)  \right)
=0\right)  . \label{pf.hirota.thirdrewriting}%
\end{align}


To simplify this even further, a new notation is needed:

\begin{definition}
\label{def.hirota.A(P,f,g)}Let $K$ be a commutative ring. Let $\left(
x_{1},x_{2},x_{3},...\right)  $, $\left(  z_{1},z_{2},z_{3},...\right)  $, and
$\left(  w_{1},w_{2},w_{3},...\right)  $ be three disjoint families of
indeterminates. Denote by $x$ the family $\left(  x_{1},x_{2},x_{3}%
,...\right)  $, and denote by $z$ the family $\left(  z_{1},z_{2}%
,z_{3},...\right)  $.

\textbf{(a)} For any polynomial $r\in K\left[  x_{1},x_{2},x_{3}%
,...,z_{1},z_{2},z_{3},...\right]  $, let $r\mid_{z=0}$ denote the polynomial
in $K\left[  x_{1},x_{2},x_{3},...\right]  $ obtained by substituting $\left(
0,0,0,...\right)  $ for $\left(  z_{1},z_{2},z_{3},...\right)  $ in $P$.

\textbf{(b)} Consider the differential operators $\dfrac{\partial}{\partial
z_{1}},\dfrac{\partial}{\partial z_{2}},\dfrac{\partial}{\partial z_{3}},...$
on $K\left[  x_{1},x_{2},x_{3},...,z_{1},z_{2},z_{3},...\right]  $. For any
power series $P\in K\left[  \left[  w_{1},w_{2},w_{3},...\right]  \right]  $,
let $P\left(  \partial_{z}\right)  $ mean the value of $P$ when applied to the
family $\left(  \dfrac{\partial}{\partial z_{1}},\dfrac{\partial}{\partial
z_{2}},\dfrac{\partial}{\partial z_{3}},...\right)  $ (that is, the result of
substituting $\dfrac{\partial}{\partial z_{j}}$ for each $w_{j}$ in $P$). This
value is a well-defined differential operator on $K\left[  x_{1},x_{2}%
,x_{3},...,z_{1},z_{2},z_{3},...\right]  $ (due to Remark
\ref{rmk.hirota.welldef} below).

\textbf{(c)} For any power series $P\in K\left[  \left[  w_{1},w_{2}%
,w_{3},...\right]  \right]  $ and any two polynomials $f\in K\left[
x_{1},x_{2},x_{3},...\right]  $ and $g\in K\left[  x_{1},x_{2},x_{3}%
,...\right]  $, define a polynomial $A\left(  P,f,g\right)  \in K\left[
x_{1},x_{2},x_{3},...\right]  $ by
\[
A\left(  P,f,g\right)  =\left(  P\left(  \partial_{z}\right)  \left(  f\left(
x-z\right)  g\left(  x+z\right)  \right)  \right)  \mid_{z=0}.
\]

\end{definition}

\begin{remark}
\label{rmk.hirota.welldef}Let $K$ be a commutative ring. Let $\left(
x_{1},x_{2},x_{3},...\right)  $, $\left(  z_{1},z_{2},z_{3},...\right)  $, and
$\left(  w_{1},w_{2},w_{3},...\right)  $ be three disjoint families of
indeterminates. Let $P\in K\left[  \left[  w_{1},w_{2},w_{3},...\right]
\right]  $ be a power series. Then, if we apply the power series $P$ to the
family $\left(  \dfrac{\partial}{\partial z_{1}},\dfrac{\partial}{\partial
z_{2}},\dfrac{\partial}{\partial z_{3}},...\right)  $, we obtain a
well-defined endomorphism of $K\left[  x_{1},x_{2},x_{3},...,z_{1},z_{2}%
,z_{3},...\right]  $.
\end{remark}

\textit{Proof of Remark \ref{rmk.hirota.welldef}.} Let $\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ be defined as in
Convention \ref{conv.fin}. Write the power series $P$ in the form
\[
P=\sum\limits_{\left(  i_{1},i_{2},i_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }}\lambda_{\left(
i_{1},i_{2},i_{3},...\right)  }w_{1}^{i_{1}}w_{2}^{i_{2}}w_{3}^{i_{3}}...
\]
for $\lambda_{\left(  i_{1},i_{2},i_{3},...\right)  }\in K$. Then, if we apply
the power series $P$ to the family $\left(  \dfrac{\partial}{\partial z_{1}%
},\dfrac{\partial}{\partial z_{2}},\dfrac{\partial}{\partial z_{3}%
},...\right)  $, we obtain%
\[
\sum\limits_{\left(  i_{1},i_{2},i_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }}\lambda_{\left(
i_{1},i_{2},i_{3},...\right)  }\left(  \dfrac{\partial}{\partial z_{1}%
}\right)  ^{i_{1}}\left(  \dfrac{\partial}{\partial z_{2}}\right)  ^{i_{2}%
}\left(  \dfrac{\partial}{\partial z_{3}}\right)  ^{i_{3}}....
\]
In order to prove that this is a well-defined endomorphism of $K\left[
x_{1},x_{2},x_{3},...,z_{1},z_{2},z_{3},...\right]  $, we must prove that for
every $r\in K\left[  x_{1},x_{2},x_{3},...,z_{1},z_{2},z_{3},...\right]  $,
the sum%
\[
\sum\limits_{\left(  i_{1},i_{2},i_{3},...\right)  \in\mathbb{N}%
_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }}\lambda_{\left(
i_{1},i_{2},i_{3},...\right)  }\left(  \left(  \dfrac{\partial}{\partial
z_{1}}\right)  ^{i_{1}}\left(  \dfrac{\partial}{\partial z_{2}}\right)
^{i_{2}}\left(  \dfrac{\partial}{\partial z_{3}}\right)  ^{i_{3}}...\right)
r
\]
is well-defined, i. e., has only finitely many nonzero addends. But this is
clear, because only finitely many $\left(  i_{1},i_{2},i_{3},...\right)
\in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}  }$ satisfy
$\left(  \left(  \dfrac{\partial}{\partial z_{1}}\right)  ^{i_{1}}\left(
\dfrac{\partial}{\partial z_{2}}\right)  ^{i_{2}}\left(  \dfrac{\partial
}{\partial z_{3}}\right)  ^{i_{3}}...\right)  r\neq0$ \ \ \ \ \footnote{This
is because $r$ is a polynomial, so that only finitely many variables occur in
$r$, and the degrees of the monomials of $r$ are bounded from above.}. Hence,
we have proven that the sum $\sum\limits_{\left(  i_{1},i_{2},i_{3}%
,...\right)  \in\mathbb{N}_{\operatorname*{fin}}^{\left\{  1,2,3,...\right\}
}}\lambda_{\left(  i_{1},i_{2},i_{3},...\right)  }\left(  \dfrac{\partial
}{\partial z_{1}}\right)  ^{i_{1}}\left(  \dfrac{\partial}{\partial z_{2}%
}\right)  ^{i_{2}}\left(  \dfrac{\partial}{\partial z_{3}}\right)  ^{i_{3}%
}...$ is a well-defined endomorphism of $K\left[  x_{1},x_{2},x_{3}%
,...,z_{1},z_{2},z_{3},...\right]  $. Since this sum is the result of applying
the power series $P$ to the family $\left(  \dfrac{\partial}{\partial z_{1}%
},\dfrac{\partial}{\partial z_{2}},\dfrac{\partial}{\partial z_{3}%
},...\right)  $, we thus conclude that applying the power series $P$ to the
family $\left(  \dfrac{\partial}{\partial z_{1}},\dfrac{\partial}{\partial
z_{2}},\dfrac{\partial}{\partial z_{3}},...\right)  $ yields a well-defined
endomorphism of $K\left[  x_{1},x_{2},x_{3},...,z_{1},z_{2},z_{3},...\right]
$. Remark \ref{rmk.hirota.welldef} is proven.

\textbf{Example:} If $P\left(  w\right)  =w_{1}$ (the first variable), then%
\[
A\left(  P,f,g\right)  =\left(  \dfrac{\partial}{\partial z_{1}}\left(
f\left(  x-z\right)  g\left(  x+z\right)  \right)  \right)  \mid_{z=0}%
=-\dfrac{\partial f}{\partial x_{1}}g+\dfrac{\partial g}{\partial x_{1}}f.
\]


\begin{lemma}
For any three polynomials $P,f,g$, we have $A\left(  P,f,g\right)  =A\left(
P_{-},g,f\right)  $, where $P_{-}\left(  w\right)  =P\left(  -w\right)  $.
\end{lemma}

\begin{corollary}
\label{cor.hirota.odd}For any two polynomials $P$ and $f$, we have $A\left(
P,f,f\right)  =0$ if $P$ is odd.
\end{corollary}

This is clear from the definition.

We now state the so-called \textit{Hirota bilinear relations}, which are a
simplified version of (\ref{pf.hirota.thirdrewriting}):

\begin{theorem}
[Hirota bilinear relations]\label{thm.hirota}Let $\tau\in\mathcal{B}^{\left(
0\right)  }$ be a nonzero vector. Let $\left(  y_{1},y_{2},y_{3},...\right)  $
and $\left(  w_{1},w_{2},w_{3},...\right)  $ be two families of new symbols.
Let $\widetilde{w}$ denote the sequence $\left(  \dfrac{w_{1}}{1},\dfrac
{w_{2}}{2},\dfrac{w_{3}}{3},...\right)  $. Define the elementary Schur
polynomials $S_{k}$ as in Definition \ref{def.schur.Sk}.

Then, $\sigma\left(  \tau\right)  \in\Omega$ if and only if%
\begin{equation}
A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}w_{s}\right)
,\tau,\tau\right)  =0, \label{thm.hirota.eqn}%
\end{equation}
where the term $A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  ,\tau,\tau\right)  $ is to be interpreted by applying Definition
\ref{def.hirota.A(P,f,g)} \textbf{(c)} to $K=\mathbb{C}\left[  \left[
y_{1},y_{2},y_{3},...\right]  \right]  $ (since $\sum\limits_{j=0}^{\infty
}S_{j}\left(  -2y\right)  S_{j+1}\left(  \widetilde{w}\right)  \exp\left(
\sum\limits_{s>0}y_{s}w_{s}\right)  \in\left(  \mathbb{C}\left[  \left[
y_{1},y_{2},y_{3},...\right]  \right]  \right)  \left[  \left[  w_{1}%
,w_{2},w_{3},...\right]  \right]  $ and $\tau\in\mathcal{B}^{\left(  0\right)
}=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  \subseteq\left(
\mathbb{C}\left[  \left[  y_{1},y_{2},y_{3},...\right]  \right]  \right)
\left[  x_{1},x_{2},x_{3},...\right]  $).
\end{theorem}

Before we prove this, we need a simple lemma about polynomials:

\begin{lemma}
\label{lem.hirota.y+z}Let $K$ be a commutative $\mathbb{Q}$-algebra. Let
$\left(  y_{1},y_{2},y_{3},...\right)  $ and $\left(  z_{1},z_{2}%
,z_{3},...\right)  $ be two sequences of new symbols. Denote the sequence
$\left(  y_{1},y_{2},y_{3},...\right)  $ by $y$. Denote the sequence $\left(
z_{1},z_{2},z_{3},...\right)  $ by $z$. Denote by $\widetilde{\partial_{y}}$
the sequence $\left(  \dfrac{1}{1}\dfrac{\partial}{\partial y_{1}},\dfrac
{1}{2}\dfrac{\partial}{\partial y_{2}},\dfrac{1}{3}\dfrac{\partial}{\partial
y_{3}},...\right)  $ of endomorphisms of $\left(  K\left[  \left[  y_{1}%
,y_{2},y_{3},...\right]  \right]  \right)  \left[  z_{1},z_{2},z_{3}%
,...\right]  $. Denote by $\widetilde{\partial_{z}}$ the sequence $\left(
\dfrac{1}{1}\dfrac{\partial}{\partial z_{1}},\dfrac{1}{2}\dfrac{\partial
}{\partial z_{2}},\dfrac{1}{3}\dfrac{\partial}{\partial z_{3}},...\right)  $
of endomorphisms of $\left(  K\left[  \left[  y_{1},y_{2},y_{3},...\right]
\right]  \right)  \left[  z_{1},z_{2},z_{3},...\right]  $. Let $P$ and $Q$ be
two elements of $K\left[  w_{1},w_{2},w_{3},...\right]  $ (where $\left(
w_{1},w_{2},w_{3},...\right)  $ is a further sequence of new symbols). Then,%
\[
Q\left(  \widetilde{\partial_{y}}\right)  \left(  P\left(  y+z\right)
\right)  =Q\left(  \widetilde{\partial_{z}}\right)  \left(  P\left(
y+z\right)  \right)  .
\]

\end{lemma}

\textit{Proof of Lemma \ref{lem.hirota.y+z}.} Let $D$ be the $K$-subalgebra of
$\operatorname*{End}\left(  \left(  K\left[  \left[  y_{1},y_{2}%
,y_{3},...\right]  \right]  \right)  \left[  z_{1},z_{2},z_{3},...\right]
\right)  $ generated by $\dfrac{\partial}{\partial y_{1}},\dfrac{\partial
}{\partial y_{2}},\dfrac{\partial}{\partial y_{3}},...,\dfrac{\partial
}{\partial z_{1}},\dfrac{\partial}{\partial z_{2}},\dfrac{\partial}{\partial
z_{3}},...$. Then, clearly, $D$ is a commutative $K$-algebra (since its
generators commute), and all elements of the sequences $\widetilde{\partial
_{y}}$ and $\widetilde{\partial_{z}}$ lie in $D$ (since $\widetilde{\partial
_{y}}=\left(  \dfrac{1}{1}\dfrac{\partial}{\partial y_{1}},\dfrac{1}{2}%
\dfrac{\partial}{\partial y_{2}},\dfrac{1}{3}\dfrac{\partial}{\partial y_{3}%
},...\right)  $ and $\widetilde{\partial_{z}}=\left(  \dfrac{1}{1}%
\dfrac{\partial}{\partial z_{1}},\dfrac{1}{2}\dfrac{\partial}{\partial z_{2}%
},\dfrac{1}{3}\dfrac{\partial}{\partial z_{3}},...\right)  $).

Let $I$ be the ideal of $D$ generated by $\dfrac{\partial}{\partial y_{i}%
}-\dfrac{\partial}{\partial z_{i}}$ with $i$ ranging over the positive
integers. Then, $\dfrac{\partial}{\partial y_{i}}-\dfrac{\partial}{\partial
z_{i}}\in I$ for every positive integer $i$. Hence, every positive integer $i$
satisfies $\dfrac{1}{i}\dfrac{\partial}{\partial y_{i}}\equiv\dfrac{1}%
{i}\dfrac{\partial}{\partial z_{i}}\operatorname{mod}I$ (since $\dfrac{1}%
{i}\dfrac{\partial}{\partial y_{i}}-\dfrac{1}{i}\dfrac{\partial}{\partial
z_{i}}=\dfrac{1}{i}\underbrace{\left(  \dfrac{\partial}{\partial y_{i}}%
-\dfrac{\partial}{\partial z_{i}}\right)  }_{\in I}\in I$). In other words,
for every positive integer $i$, the $i$-th element of the sequence
$\widetilde{\partial_{y}}$ is congruent to the $i$-th element of the sequence
$\widetilde{\partial_{z}}$ modulo $I$ (since the $i$-th element of the
sequence $\widetilde{\partial_{y}}$ is $\dfrac{1}{i}\dfrac{\partial}{\partial
y_{i}}$, while the $i$-th element of the sequence $\widetilde{\partial_{z}}$
is $\dfrac{1}{i}\dfrac{\partial}{\partial z_{i}}$). Thus, each element of the
sequence $\widetilde{\partial_{y}}$ is congruent to the corresponding element
of the sequence $\widetilde{\partial_{z}}$ modulo $I$. Hence, $Q\left(
\widetilde{\partial_{y}}\right)  \equiv Q\left(  \widetilde{\partial_{z}%
}\right)  \operatorname{mod}I$ (since $Q$ is a polynomial, and $I$ is an
ideal). Hence,
\begin{align*}
&  Q\left(  \widetilde{\partial_{y}}\right)  -Q\left(  \widetilde{\partial
_{z}}\right)  \in I\\
&  =\left(  \text{ideal of }D\text{ generated by }\dfrac{\partial}{\partial
y_{i}}-\dfrac{\partial}{\partial z_{i}}\text{ with }i\text{ ranging over the
positive integers}\right)  .
\end{align*}
In other words, $Q\left(  \widetilde{\partial_{y}}\right)  -Q\left(
\widetilde{\partial_{z}}\right)  $ is a $D$-linear combinations of terms of
the form $\dfrac{\partial}{\partial y_{i}}-\dfrac{\partial}{\partial z_{i}}$
with $i$ ranging over the positive integers. Thus, we can write $Q\left(
\widetilde{\partial_{y}}\right)  -Q\left(  \widetilde{\partial_{z}}\right)  $
in the form $Q\left(  \widetilde{\partial_{y}}\right)  -Q\left(
\widetilde{\partial_{z}}\right)  =\sum\limits_{i>0}d_{i}\circ\left(
\dfrac{\partial}{\partial y_{i}}-\dfrac{\partial}{\partial z_{i}}\right)  $,
where each $d_{i}$ is an element of $D$, and all but finitely many $i>0$
satisfy $d_{i}=0$. Consider these $d_{i}$.

But it is easy to see that%
\begin{equation}
\text{every positive integer }i\text{ satisfies }\left(  \dfrac{\partial
}{\partial y_{i}}-\dfrac{\partial}{\partial z_{i}}\right)  \left(  P\left(
y+z\right)  \right)  =0. \label{pf.hirota.y+z.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.hirota.y+z.1}):} Let $i$ be a positive
integer. Let us identify $\mathbb{C}\left[  w_{1},w_{2},w_{3},...\right]  $
with $\left(  \mathbb{C}\left[  w_{1},w_{2},...,w_{i-1},w_{i+1},w_{i+2}%
,...\right]  \right)  \left[  w_{i}\right]  $. Then, $P\in\mathbb{C}\left[
w_{1},w_{2},w_{3},...\right]  =\left(  \mathbb{C}\left[  w_{1},w_{2}%
,...,w_{i-1},w_{i+1},w_{i+2},...\right]  \right)  \left[  w_{i}\right]  $, so
that we can write $P$ as a polynomial in the variable $w_{i}$ over the ring
$\mathbb{C}\left[  w_{1},w_{2},...,w_{i-1},w_{i+1},w_{i+2},...\right]  $. In
other words, we can write $P$ in the form $P=\sum\limits_{n\in\mathbb{N}}%
p_{n}w_{i}^{n}$, where every $n\in\mathbb{N}$ satisfies $p_{n}\in
\mathbb{C}\left[  w_{1},w_{2},...,w_{i-1},w_{i+1},w_{i+2},...\right]  $ and
all but finitely many $n\in\mathbb{N}$ satisfy $p_{n}=0$. Consider these
$p_{n}$.
\par
Let $n\in\mathbb{N}$ be arbitrary. Consider $p_{n}\in\mathbb{C}\left[
w_{1},w_{2},...,w_{i-1},w_{i+1},w_{i+2},...\right]  $ as an element of
$\mathbb{C}\left[  w_{1},w_{2},w_{3},...\right]  $ (by means of the canonical
embedding $\mathbb{C}\left[  w_{1},w_{2},...,w_{i-1},w_{i+1},w_{i+2}%
,...\right]  \subseteq\mathbb{C}\left[  w_{1},w_{2},w_{3},...\right]  $).
Then, $p_{n}$ is a polynomial in which the variable $w_{i}$ does not occur.
Hence, $p_{n}\left(  y+z\right)  $ is a polynomial in which neither of the
variables $y_{i}$ and $z_{i}$ occur. Thus, $\dfrac{\partial}{\partial y_{i}%
}\left(  p_{n}\left(  y+z\right)  \right)  =0$ and $\dfrac{\partial}{\partial
z_{i}}\left(  p_{n}\left(  y+z\right)  \right)  =0$.
\par
On the other hand, it is very easy to check that $\dfrac{\partial}{\partial
y_{i}}\left(  y_{i}+z_{i}\right)  ^{n}=\dfrac{\partial}{\partial z_{i}}\left(
y_{i}+z_{i}\right)  ^{n}$ (in fact, this is obvious in the case when $n=0$,
and in every other case follows from $\dfrac{\partial}{\partial y_{i}}\left(
y_{i}+z_{i}\right)  ^{n}=n\left(  y_{i}+z_{i}\right)  ^{n-1}$ and
$\dfrac{\partial}{\partial z_{i}}\left(  y_{i}+z_{i}\right)  ^{n}=n\left(
y_{i}+z_{i}\right)  ^{n-1}$). Now, by the Leibniz rule,%
\begin{align*}
\dfrac{\partial}{\partial y_{i}}\left(  p_{n}\left(  y+z\right)  \cdot\left(
y_{i}+z_{i}\right)  ^{n}\right)   &  =\underbrace{\left(  \dfrac{\partial
}{\partial y_{i}}\left(  p_{n}\left(  y+z\right)  \right)  \right)
}_{=0=\dfrac{\partial}{\partial z_{i}}\left(  p_{n}\left(  y+z\right)
\right)  }\cdot\left(  y_{i}+z_{i}\right)  ^{n}+p_{n}\left(  y+z\right)
\cdot\underbrace{\dfrac{\partial}{\partial y_{i}}\left(  y_{i}+z_{i}\right)
^{n}}_{=\dfrac{\partial}{\partial z_{i}}\left(  y_{i}+z_{i}\right)  ^{n}}\\
&  =\left(  \dfrac{\partial}{\partial z_{i}}\left(  p_{n}\left(  y+z\right)
\right)  \right)  \cdot\left(  y_{i}+z_{i}\right)  ^{n}+p_{n}\left(
y+z\right)  \cdot\dfrac{\partial}{\partial z_{i}}\left(  y_{i}+z_{i}\right)
^{n}.
\end{align*}
Compared with%
\[
\dfrac{\partial}{\partial z_{i}}\left(  p_{n}\left(  y+z\right)  \cdot\left(
y_{i}+z_{i}\right)  ^{n}\right)  =\left(  \dfrac{\partial}{\partial z_{i}%
}\left(  p_{n}\left(  y+z\right)  \right)  \right)  \cdot\left(  y_{i}%
+z_{i}\right)  ^{n}+p_{n}\left(  y+z\right)  \cdot\dfrac{\partial}{\partial
z_{i}}\left(  y_{i}+z_{i}\right)  ^{n}%
\]
(this follows from the Leibniz rule), this yields%
\begin{equation}
\dfrac{\partial}{\partial y_{i}}\left(  p_{n}\left(  y+z\right)  \cdot\left(
y_{i}+z_{i}\right)  ^{n}\right)  =\dfrac{\partial}{\partial z_{i}}\left(
p_{n}\left(  y+z\right)  \cdot\left(  y_{i}+z_{i}\right)  ^{n}\right)  .
\label{pf.hirota.y+z.2}%
\end{equation}
\par
Now, forget that we fixed $n\in\mathbb{N}$. We have shown that every
$n\in\mathbb{N}$ satisfies (\ref{pf.hirota.y+z.2}). Now, since $P=\sum
\limits_{n\in\mathbb{N}}p_{n}w_{i}^{n}$, we have $P\left(  y+z\right)
=\sum\limits_{n\in\mathbb{N}}p_{n}\left(  y+z\right)  \cdot\left(  y_{i}%
+z_{i}\right)  ^{n}$, so that%
\begin{align*}
&  \left(  \dfrac{\partial}{\partial y_{i}}-\dfrac{\partial}{\partial z_{i}%
}\right)  \left(  P\left(  y+z\right)  \right) \\
&  =\left(  \dfrac{\partial}{\partial y_{i}}-\dfrac{\partial}{\partial z_{i}%
}\right)  \left(  \sum\limits_{n\in\mathbb{N}}p_{n}\left(  y+z\right)
\cdot\left(  y_{i}+z_{i}\right)  ^{n}\right) \\
&  =\sum\limits_{n\in\mathbb{N}}\underbrace{\dfrac{\partial}{\partial y_{i}%
}\left(  p_{n}\left(  y+z\right)  \cdot\left(  y_{i}+z_{i}\right)
^{n}\right)  }_{\substack{=\dfrac{\partial}{\partial z_{i}}\left(
p_{n}\left(  y+z\right)  \cdot\left(  y_{i}+z_{i}\right)  ^{n}\right)
\\\text{(by (\ref{pf.hirota.y+z.2}))}}}-\sum\limits_{n\in\mathbb{N}}%
\dfrac{\partial}{\partial z_{i}}\left(  p_{n}\left(  y+z\right)  \cdot\left(
y_{i}+z_{i}\right)  ^{n}\right) \\
&  =\sum\limits_{n\in\mathbb{N}}\dfrac{\partial}{\partial z_{i}}\left(
p_{n}\left(  y+z\right)  \cdot\left(  y_{i}+z_{i}\right)  ^{n}\right)
-\sum\limits_{n\in\mathbb{N}}\dfrac{\partial}{\partial z_{i}}\left(
p_{n}\left(  y+z\right)  \cdot\left(  y_{i}+z_{i}\right)  ^{n}\right)  =0.
\end{align*}
This proves (\ref{pf.hirota.y+z.1}).} Thus,%
\begin{align*}
&  Q\left(  \widetilde{\partial_{y}}\right)  \left(  P\left(  y+z\right)
\right)  -Q\left(  \widetilde{\partial_{z}}\right)  \left(  P\left(
y+z\right)  \right) \\
&  =\underbrace{\left(  Q\left(  \widetilde{\partial_{y}}\right)  -Q\left(
\widetilde{\partial_{z}}\right)  \right)  }_{=\sum\limits_{i>0}d_{i}%
\circ\left(  \dfrac{\partial}{\partial y_{i}}-\dfrac{\partial}{\partial z_{i}%
}\right)  }\left(  P\left(  y+z\right)  \right)  =\sum\limits_{i>0}\left(
d_{i}\circ\left(  \dfrac{\partial}{\partial y_{i}}-\dfrac{\partial}{\partial
z_{i}}\right)  \right)  \left(  P\left(  y+z\right)  \right) \\
&  =\sum\limits_{i>0}d_{i}\underbrace{\left(  \left(  \dfrac{\partial
}{\partial y_{i}}-\dfrac{\partial}{\partial z_{i}}\right)  \left(  P\left(
y+z\right)  \right)  \right)  }_{\substack{=0\\\text{(by
(\ref{pf.hirota.y+z.1}))}}}=\sum\limits_{i>0}\underbrace{d_{i}\left(
0\right)  }_{=0}=0.
\end{align*}
In other words, $Q\left(  \widetilde{\partial_{y}}\right)  \left(  P\left(
y+z\right)  \right)  =Q\left(  \widetilde{\partial_{z}}\right)  \left(
P\left(  y+z\right)  \right)  $. This proves Lemma \ref{lem.hirota.y+z}.

\textit{Proof of Theorem \ref{thm.hirota}.} We introduce a new family of
indeterminates $\left(  z_{1},z_{2},z_{3},...\right)  $. Denote this family by
$z$. (This $z$ has nothing to do with the element $z$ of $\mathcal{B}$. It is
best to forget about $\mathcal{B}$ here, and only think about $\mathcal{B}%
^{\left(  0\right)  }=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $.)
Denote by $\widetilde{\partial_{z}}$ the sequence $\left(  \dfrac{1}{1}%
\dfrac{\partial}{\partial z_{1}},\dfrac{1}{2}\dfrac{\partial}{\partial z_{2}%
},\dfrac{1}{3}\dfrac{\partial}{\partial z_{3}},...\right)  $.

Denote by $\widetilde{\partial_{y}}$ the sequence $\left(  \dfrac{1}{1}%
\dfrac{\partial}{\partial y_{1}},\dfrac{1}{2}\dfrac{\partial}{\partial y_{2}%
},\dfrac{1}{3}\dfrac{\partial}{\partial y_{3}},...\right)  $. Also, let $-2y$
be the sequence $\left(  -2y_{1},-2y_{2},-2y_{3},...\right)  $. Then,%
\begin{align}
\sum\limits_{k=0}^{\infty}S_{k}\left(  -2y\right)  u^{k}  &  =\sum
\limits_{k\geq0}S_{k}\left(  -2y\right)  u^{k}=\exp\left(  \sum\limits_{i\geq
1}-2y_{i}u^{i}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{def.schur.sk.genfun}), with
}-2y\text{ substituted for }x\text{ and }u\text{ substituted for }z\right)
\nonumber\\
&  =\exp\left(  \sum\limits_{j\geq1}-2y_{j}u^{j}\right)  =\exp\left(
-2\sum\limits_{j>0}u^{j}y_{j}\right)  \label{pf.hirota.29}%
\end{align}
and%
\begin{align}
\sum\limits_{k=0}^{\infty}S_{k}\left(  \widetilde{\partial_{y}}\right)
u^{-k}  &  =\sum\limits_{k\geq0}S_{k}\left(  \widetilde{\partial_{y}}\right)
u^{-k}=\exp\left(  \sum\limits_{i\geq1}\dfrac{1}{i}\dfrac{\partial}{\partial
y_{i}}u^{-i}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{def.schur.sk.genfun}), with
}\widetilde{\partial_{y}}\text{ substituted for }x\text{ and }u^{-1}\text{
substituted for }z\right) \nonumber\\
&  =\exp\left(  \sum\limits_{j\geq1}\dfrac{1}{j}\dfrac{\partial}{\partial
y_{j}}u^{-j}\right)  =\exp\left(  \sum\limits_{j>0}\dfrac{u^{-j}}{j}%
\dfrac{\partial}{\partial y_{j}}\right)  . \label{pf.hirota.30}%
\end{align}


Applying Lemma \ref{lem.hirota.newton} to $K=\left(  \mathbb{C}\left[  \left[
y_{1},y_{2},y_{3},...\right]  \right]  \right)  \left[  x_{1},x_{2}%
,x_{3},...\right]  $ and $P=\tau\left(  x+z\right)  \tau\left(  x-z\right)  $,
we obtain%
\begin{equation}
\exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)
\left(  \tau\left(  x+z\right)  \tau\left(  x-z\right)  \right)  =\tau\left(
x+y+z\right)  \tau\left(  x-y-z\right)  . \label{pf.hirota.31}%
\end{equation}


Now,%
\begin{align*}
&  \operatorname*{CT}\nolimits_{u}\left(  u\exp\left(  -2\sum\limits_{j>0}%
u^{j}y_{j}\right)  \exp\left(  \sum\limits_{j>0}\dfrac{u^{-j}}{j}%
\dfrac{\partial}{\partial y_{j}}\right)  \left(  \tau\left(  x-y\right)
\tau\left(  x+y\right)  \right)  \right) \\
&  =\operatorname*{CT}\nolimits_{u}\left(  u\underbrace{\exp\left(
-2\sum\limits_{j>0}u^{j}y_{j}\right)  }_{\substack{=\sum\limits_{k=0}^{\infty
}S_{k}\left(  -2y\right)  u^{k}\\\text{(by (\ref{pf.hirota.29}))}%
}}\underbrace{\exp\left(  \sum\limits_{j>0}\dfrac{u^{-j}}{j}\dfrac{\partial
}{\partial y_{j}}\right)  }_{\substack{=\sum\limits_{k=0}^{\infty}S_{k}\left(
\widetilde{\partial_{y}}\right)  u^{-k}\\\text{(by (\ref{pf.hirota.30}))}%
}}\left(  \tau\left(  x+y+z\right)  \tau\left(  x-y-z\right)  \right)
\right)  \mid_{z=0}\\
&  =\operatorname*{CT}\nolimits_{u}\left(  u\left(  \sum\limits_{k=0}^{\infty
}S_{k}\left(  -2y\right)  u^{k}\right)  \left(  \sum\limits_{k=0}^{\infty
}S_{k}\left(  \widetilde{\partial_{y}}\right)  u^{-k}\right)  \left(
\tau\left(  x+y+z\right)  \tau\left(  x-y-z\right)  \right)  \right)
\mid_{z=0}\\
&  =\sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  \underbrace{S_{j+1}%
\left(  \widetilde{\partial_{y}}\right)  \left(  \tau\left(  x+y+z\right)
\tau\left(  x-y-z\right)  \right)  }_{\substack{=S_{j+1}\left(
\widetilde{\partial_{z}}\right)  \left(  \tau\left(  x+y+z\right)  \tau\left(
x-y-z\right)  \right)  \\\text{(by Lemma \ref{lem.hirota.y+z}, applied
to}\\K=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  \text{, }P=\tau\left(
x+w\right)  \tau\left(  x-w\right)  \text{ and }Q=S_{j+1}\left(  w\right)
\text{)}}}\mid_{z=0}\\
&  =\sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{\partial_{z}}\right)  \underbrace{\left(  \tau\left(  x+y+z\right)
\tau\left(  x-y-z\right)  \right)  }_{\substack{=\exp\left(  \sum
\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}}\right)  \left(  \tau\left(
x+z\right)  \tau\left(  x-z\right)  \right)  \\\text{(by (\ref{pf.hirota.31}%
))}}}\mid_{z=0}\\
&  =\sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{\partial_{z}}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
\dfrac{\partial}{\partial z_{s}}\right)  \left(  \tau\left(  x+z\right)
\tau\left(  x-z\right)  \right)  \mid_{z=0}.
\end{align*}
Compared with the fact that (by the definition of $A\left(  \sum
\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(  \widetilde{w}%
\right)  \exp\left(  \sum\limits_{s>0}y_{s}w_{s}\right)  ,\tau,\tau\right)  $)
we have%
\begin{align*}
&  A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}w_{s}\right)
,\tau,\tau\right) \\
&  =\underbrace{\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  \right)  \left(  \partial_{z}\right)  }_{=\sum\limits_{j=0}%
^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(  \widetilde{\partial_{z}%
}\right)  \exp\left(  \sum\limits_{s>0}y_{s}\dfrac{\partial}{\partial z_{s}%
}\right)  }\left(  \tau\left(  x+z\right)  \tau\left(  x-z\right)  \right)
\mid_{z=0}\\
&  =\sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{\partial_{z}}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
\dfrac{\partial}{\partial z_{s}}\right)  \left(  \tau\left(  x+z\right)
\tau\left(  x-z\right)  \right)  \mid_{z=0},
\end{align*}
this yields%
\begin{align*}
&  \operatorname*{CT}\nolimits_{u}\left(  u\exp\left(  -2\sum\limits_{j>0}%
u^{j}y_{j}\right)  \exp\left(  \sum\limits_{j>0}\dfrac{u^{-j}}{j}%
\dfrac{\partial}{\partial y_{j}}\right)  \left(  \tau\left(  x-y\right)
\tau\left(  x+y\right)  \right)  \right) \\
&  =A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}w_{s}\right)
,\tau,\tau\right)  .
\end{align*}
Hence, (\ref{pf.hirota.thirdrewriting}) rewrites as follows:%
\[
\left(  S\left(  \sigma\left(  \tau\right)  \otimes\sigma\left(  \tau\right)
\right)  =0\right)  \ \Longleftrightarrow\ \left(  A\left(  \sum
\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(  \widetilde{w}%
\right)  \exp\left(  \sum\limits_{s>0}y_{s}w_{s}\right)  ,\tau,\tau\right)
=0\right)  .
\]
Since $S\left(  \sigma\left(  \tau\right)  \otimes\sigma\left(  \tau\right)
\right)  =0$ is equivalent to $\sigma\left(  \tau\right)  \in\Omega$ (by
Theorem \ref{thm.plu.inf} \textbf{(b)}, applied to $\sigma\left(  \tau\right)
$ instead of $\tau$), this rewrites as follows:%
\[
\left(  \sigma\left(  \tau\right)  \in\Omega\right)  \ \Longleftrightarrow
\ \left(  A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  ,\tau,\tau\right)  =0\right)  .
\]
This proves Theorem \ref{thm.hirota}.

Theorem \ref{thm.hirota} tells us that a nonzero $\tau\in\mathcal{B}^{\left(
0\right)  }$ satisfies $\sigma\left(  \tau\right)  \in\Omega$ if and only if
it satisfies the equation (\ref{thm.hirota.eqn}). The left hand side of this
equation is a power series with respect to the variables $y_{1},y_{2}%
,y_{3},...$. A power series is $0$ if and only if each of its coefficients is
$0$. Hence, the equation (\ref{thm.hirota.eqn}) holds if and only if for each
monomial in $y_{1},y_{2},y_{3},...$, the coefficient of the left hand side of
(\ref{thm.hirota.eqn}) in front of this monomial is $0$. Thus, the equation
(\ref{thm.hirota.eqn}) is equivalent to \textbf{a system of infinitely many
equations}, one for each monomial in $y_{1},y_{2},y_{3},...$. We don't know of
a good way to describe these equations (without using the variables
$y_{1},y_{2},y_{3},...$), but we can describe the equations corresponding to
the simplest among our monomials: the monomials of degree $0$ and those of
degree $1$.

In the following, we consider $\left(  \mathbb{C}\left[  \left[  y_{1}%
,y_{2},y_{3},...\right]  \right]  \right)  \left[  x_{1},x_{2},x_{3}%
,...\right]  $ as a subring of \newline$\left(  \mathbb{C}\left[  x_{1}%
,x_{2},x_{3},...\right]  \right)  \left[  \left[  y_{1},y_{2},y_{3}%
,...\right]  \right]  $. For every commutative ring $K$, every element $T$ of
$K\left[  \left[  y_{1},y_{2},y_{3},...\right]  \right]  $ and any
monomial\footnote{When we say ``monomial'', we mean a monomial without
coefficient.} $\mathfrak{m}$ in the variables $y_{1},y_{2},y_{3},...$, we
denote by $T\left[  \mathfrak{m}\right]  $ the coefficient of the monomial
$\mathfrak{m}$ in the power series $T$. (For example, $\left(  \exp\left(
x_{2}y_{2}\right)  \right)  \left[  y_{2}^{3}\right]  =\dfrac{x_{2}^{3}}{6}$;
note that $K=\mathbb{C}\left[  x_{1},x_{2},x_{3},...\right]  $ in this
example, so that $x_{2}$ counts as a constant!)

For every $P\in\left(  \mathbb{C}\left[  \left[  y_{1},y_{2},y_{3},...\right]
\right]  \right)  \left[  \left[  w_{1},w_{2},w_{3},...\right]  \right]  $ and
every monomial $\mathfrak{m}$ in the variables $y_{1},y_{2},y_{3},...$, we
have
\begin{equation}
\left(  A\left(  P,\tau,\tau\right)  \right)  \left[  \mathfrak{m}\right]
=A\left(  P\left[  \mathfrak{m}\right]  ,\tau,\tau\right)  .
\label{pf.hirota.66}%
\end{equation}
\footnote{\textit{Proof.} We have $P=\sum\limits_{\substack{\mathfrak{n}\text{
is a monomial}\\\text{in }y_{1},y_{2},y_{3},...}}P\left[  \mathfrak{n}\right]
\cdot\mathfrak{n}$. Since the map%
\begin{align*}
\left(  \mathbb{C}\left[  \left[  y_{1},y_{2},y_{3},...\right]  \right]
\right)  \left[  \left[  w_{1},w_{2},w_{3},...\right]  \right]   &
\rightarrow\left(  \mathbb{C}\left[  \left[  y_{1},y_{2},y_{3},...\right]
\right]  \right)  \left[  x_{1},x_{2},x_{3},...\right]  ,\\
Q  &  \mapsto A\left(  Q,\tau,\tau\right)
\end{align*}
is $\mathbb{C}\left[  \left[  y_{1},y_{2},y_{3},...\right]  \right]  $-linear,
we have
\[
A\left(  \sum\limits_{\substack{\mathfrak{n}\text{ is a monomial}\\\text{in
}y_{1},y_{2},y_{3},...}}P\left[  \mathfrak{n}\right]  \cdot\mathfrak{n}%
,\tau,\tau\right)  =\sum\limits_{\substack{\mathfrak{n}\text{ is a
monomial}\\\text{in }y_{1},y_{2},y_{3},...}}A\left(  P\left[  \mathfrak{n}%
\right]  ,\tau,\tau\right)  \cdot\mathfrak{n}.
\]
But $P=\sum\limits_{\substack{\mathfrak{n}\text{ is a monomial}\\\text{in
}y_{1},y_{2},y_{3},...}}P\left[  \mathfrak{n}\right]  \cdot\mathfrak{n}$ shows
that
\[
A\left(  P,\tau,\tau\right)  =A\left(  \sum\limits_{\substack{\mathfrak{n}%
\text{ is a monomial}\\\text{in }y_{1},y_{2},y_{3},...}}P\left[
\mathfrak{n}\right]  \cdot\mathfrak{n},\tau,\tau\right)  =\sum
\limits_{\substack{\mathfrak{n}\text{ is a monomial}\\\text{in }y_{1}%
,y_{2},y_{3},...}}A\left(  P\left[  \mathfrak{n}\right]  ,\tau,\tau\right)
\cdot\mathfrak{n},
\]
so that the coefficient of $A\left(  P,\tau,\tau\right)  $ before
$\mathfrak{m}$ equals $A\left(  P\left[  \mathfrak{m}\right]  ,\tau
,\tau\right)  $. Since we denoted the coefficient of $A\left(  P,\tau
,\tau\right)  $ before $\mathfrak{m}$ by $\left(  A\left(  P,\tau,\tau\right)
\right)  \left[  \mathfrak{m}\right]  $, this rewrites as $\left(  A\left(
P,\tau,\tau\right)  \right)  \left[  \mathfrak{m}\right]  =A\left(  P\left[
\mathfrak{m}\right]  ,\tau,\tau\right)  $, qed.}

Now, let us describe the equations that are obtained from
(\ref{thm.hirota.eqn}) by taking coefficients before monomials of degree $0$
and $1$:

\textbf{Monomials of degree }$0$\textbf{:} The only monomial of degree $0$ in
$y_{1},y_{2},y_{3},...$ is $1$. We have%
\begin{align*}
&  \left(  A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  ,\tau,\tau\right)  \right)  \left[  1\right] \\
&  =A\left(  \underbrace{\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(
-2y\right)  S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum
\limits_{s>0}y_{s}w_{s}\right)  \right)  \left[  1\right]  }_{=S_{1}\left(
\widetilde{w}\right)  =w_{1}},\tau,\tau\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.hirota.66}), applied to
}P=\sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}w_{s}\right)  \text{
and }\mathfrak{m}=1\right) \\
&  =A\left(  w_{1},\tau,\tau\right)  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{by
Corollary \ref{cor.hirota.odd}, since }w_{1}\text{ is odd}\right)  .
\end{align*}
Therefore, if we take coefficients with respect to the monomial $1$ in the
equation (\ref{pf.hirota.66}), we obtain a tautology.

\textbf{Monomials of degree }$1$\textbf{:} This will be more interesting. The
monomials of degree $1$ in $y_{1},y_{2},y_{3},...$ are $y_{1},y_{2},y_{3}%
,...$. Let $r$ be a positive integer. We have%
\begin{align}
&  \left(  A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  ,\tau,\tau\right)  \right)  \left[  y_{r}\right] \nonumber\\
&  =A\left(  \underbrace{\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(
-2y\right)  S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum
\limits_{s>0}y_{s}w_{s}\right)  \right)  \left[  y_{r}\right]  }%
_{\substack{=-2S_{r+1}\left(  \widetilde{w}\right)  +w_{1}w_{r}\\\text{(by
easy computations)}}},\tau,\tau\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.hirota.66}), applied to
}P=\sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)  S_{j+1}\left(
\widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}w_{s}\right)  \text{
and }\mathfrak{m}=y_{r}\right) \nonumber\\
&  =A\left(  -2S_{r+1}\left(  \widetilde{w}\right)  +w_{1}w_{r},\tau
,\tau\right)  . \label{pf.hirota.69}%
\end{align}
Denote the polynomial $-2S_{r+1}\left(  \widetilde{w}\right)  +w_{1}w_{r}$ by
$T_{r}\left(  w\right)  $. Then, (\ref{pf.hirota.69}) rewrites as%
\begin{equation}
\left(  A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  ,\tau,\tau\right)  \right)  \left[  y_{r}\right]  =A\left(
T_{r}\left(  w\right)  ,\tau,\tau\right)  . \label{pf.hirota.70}%
\end{equation}
We have $T_{1}\left(  w\right)  =w_{2}$, $T_{2}\left(  w\right)
=-\dfrac{w_{1}^{3}}{3}-\dfrac{2w_{3}}{3}$ and $T_{3}\left(  w\right)
=\dfrac{w_{1}w_{3}}{3}-\dfrac{w_{4}}{2}-\dfrac{w_{2}^{2}}{4}-\dfrac{w_{1}^{4}%
}{12}-\dfrac{w_{1}^{2}w_{2}}{2}$. Since $T_{1}\left(  w\right)  $ and
$T_{2}\left(  w\right)  $ are odd, we have $A\left(  T_{1}\left(  w\right)
,\tau,\tau\right)  =0$ and $A\left(  T_{2}\left(  w\right)  ,\tau,\tau\right)
=0$ (by Corollary \ref{cor.hirota.odd}). Therefore, taking coefficients with
respect to the monomials $y_{1}$ and $y_{2}$ in the equation
(\ref{pf.hirota.66}) yields tautologies. However, $T_{3}\left(  w\right)  $ is
\textbf{not odd}. Applying (\ref{pf.hirota.70}) to $r=3$, we obtain%
\begin{align*}
&  \left(  A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  ,\tau,\tau\right)  \right)  \left[  y_{3}\right] \\
&  =A\left(  T_{3}\left(  w\right)  ,\tau,\tau\right)  =A\left(  \dfrac
{w_{1}w_{3}}{3}-\dfrac{w_{4}}{2}-\dfrac{w_{2}^{2}}{4}-\dfrac{w_{1}^{4}}%
{12}-\dfrac{w_{1}^{2}w_{2}}{2},\tau,\tau\right) \\
&  =A\left(  \dfrac{w_{1}w_{3}}{3}-\dfrac{w_{2}^{2}}{4}-\dfrac{w_{1}^{4}}%
{12},\tau,\tau\right)  +\underbrace{A\left(  -\dfrac{w_{4}}{2}-\dfrac
{w_{1}^{2}w_{2}}{2},\tau,\tau\right)  }_{\substack{=0\\\text{(by Corollary
\ref{cor.hirota.odd}, since}\\-\dfrac{w_{4}}{2}-\dfrac{w_{1}^{2}w_{2}}%
{2}\text{ is odd)}}}\\
&  =A\left(  \dfrac{w_{1}w_{3}}{3}-\dfrac{w_{2}^{2}}{4}-\dfrac{w_{1}^{4}}%
{12},\tau,\tau\right)  =\left(  \left(  \dfrac{\dfrac{\partial}{\partial
z_{1}}\dfrac{\partial}{\partial z_{3}}}{3}-\dfrac{\left(  \dfrac{\partial
}{\partial z_{2}}\right)  ^{2}}{4}-\dfrac{\left(  \dfrac{\partial}{\partial
z_{1}}\right)  ^{4}}{12}\right)  \left(  \tau\left(  x-z\right)  \tau\left(
x+z\right)  \right)  \right)  \mid_{z=0}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }A\left(
\dfrac{w_{1}w_{3}}{3}-\dfrac{w_{2}^{2}}{4}-\dfrac{w_{1}^{4}}{12},\tau
,\tau\right)  \right) \\
&  =\dfrac{1}{12}\left(  \left(  4\dfrac{\partial}{\partial z_{1}}%
\dfrac{\partial}{\partial z_{3}}-3\left(  \dfrac{\partial}{\partial z_{2}%
}\right)  ^{2}-\left(  \dfrac{\partial}{\partial z_{1}}\right)  ^{4}\right)
\left(  \tau\left(  x-z\right)  \tau\left(  x+z\right)  \right)  \right)
\mid_{z=0}\\
&  =\dfrac{1}{12}\left(  \left(  4\dfrac{\partial}{\partial w_{1}}%
\dfrac{\partial}{\partial w_{3}}-3\left(  \dfrac{\partial}{\partial w_{2}%
}\right)  ^{2}-\left(  \dfrac{\partial}{\partial w_{1}}\right)  ^{4}\right)
\left(  \tau\left(  x-w\right)  \tau\left(  x+w\right)  \right)  \right)
\mid_{w=0}.
\end{align*}
Since $\dfrac{\partial}{\partial w_{j}}=\partial_{w_{j}}$ for every $j$, we
rewrite this as%
\begin{align*}
&  \left(  A\left(  \sum\limits_{j=0}^{\infty}S_{j}\left(  -2y\right)
S_{j+1}\left(  \widetilde{w}\right)  \exp\left(  \sum\limits_{s>0}y_{s}%
w_{s}\right)  ,\tau,\tau\right)  \right)  \left[  y_{3}\right] \\
&  =\dfrac{1}{12}\left(  \left(  4\partial_{w_{1}}\partial_{w_{3}}%
-3\partial_{w_{2}}^{2}-\partial_{w_{1}}^{4}\right)  \left(  \tau\left(
x-w\right)  \tau\left(  x+w\right)  \right)  \right)  \mid_{w=0}.
\end{align*}
Hence, taking coefficients with respect to the monomial $y_{3}$ in the
equation (\ref{thm.hirota.eqn}), we obtain%
\[
\dfrac{1}{12}\left(  \left(  4\partial_{w_{1}}\partial_{w_{3}}-3\partial
_{w_{2}}^{2}-\partial_{w_{1}}^{4}\right)  \left(  \tau\left(  x-w\right)
\tau\left(  x+w\right)  \right)  \right)  \mid_{w=0}=0.
\]
In other words,
\begin{equation}
\left(  \partial_{w_{1}}^{4}+3\partial_{w_{2}}^{2}-4\partial_{w_{1}}%
\partial_{w_{3}}\right)  \left(  \tau\left(  x-w\right)  \tau\left(
x+w\right)  \right)  \mid_{w=0}=0. \label{KdV.star}%
\end{equation}
This does not yet look like a PDE in any usual form. We will now transform it
into one.

We make the substitution $x_{1}=x$, $x_{2}=y$, $x_{3}=t$, $x_{m}=c_{m}$ for
$m\geq4$. Here, $x$, $y$, $t$ and $c_{m}$ (for $m\geq4$) are new symbols (in
particularly, $x$ and $y$ no longer denote the sequences $\left(  x_{1}%
,x_{2},x_{3},...\right)  $ and $\left(  y_{1},y_{2},y_{3},...\right)  $). Let
$u=2\partial_{x}^{2}\log\tau$.

\begin{proposition}
\label{prop.KdV.computation}The polynomial $\tau\left(  x,y,t,c_{4}%
,c_{5},...\right)  $ satisfies (\ref{KdV.star}) if and only if the function
$u$ satisfies the KP equation
\[
\dfrac{3}{4}\partial_{y}^{2}u=\partial_{x}\left(  \partial_{t}u-\dfrac{3}%
{2}u\partial_{x}u-\dfrac{1}{4}\partial_{x}^{3}u\right)
\]
(where $c_{4}$, $c_{5}$, $c_{6}$, $...$ are considered as constants).
\end{proposition}

\textit{Proof of Proposition \ref{prop.KdV.computation}.} Optional homework exercise.

Thus, we know that any element $\tau$ of $\Omega$ gives rise to a solution of
the KP equation (namely, the solution is $2\partial_{x}^{2}\log\left(
\sigma^{-1}\left(  \tau\right)  \right)  $). Two elements of $\Omega$
differing from each other by a scalar factor yield one and the same solution
of the KP equation. Hence, any element of $\operatorname*{Gr}$ gives rise to a
solution of the KP equation. Since we know how to produce elements of
$\operatorname*{Gr}$, we thus know how to produce solutions of the KP equation!

This does not give \textbf{all} solutions, and in fact we cannot even hope to
find all solutions explicitly (since they depend on boundary conditions, and
these can be arbitrarily nonexplicit), but we will use this to find a dense
subset of them (in an appropriate sense).

The KP equation is not the KdV (Korteweg-de Vries) equation; but if we have a
solution of the KP equation which does not depend on $y$, then this solution
satisfies $\partial_{t}u-\dfrac{3}{2}u\partial_{x}u-\dfrac{1}{4}\partial
_{x}^{3}u=\operatorname*{const}$, and with some work it gives rise to a
solution of the KdV equation (under appropriate decay-at-infinity conditions).

The equations corresponding to the coefficients of the monomials $y_{4}$,
$y_{5}$, $...$ in (\ref{pf.hirota.66}) correspond to the \textit{KP hierarchy}
of higher-order PDEs. There is no point in writing them up explicitly; they
become more and more complicated.

\begin{corollary}
\label{cor.KdV.schursols}Let $\lambda$ be a partition. Then, $2\partial
_{x}^{2}\log\left(  S_{\lambda}\left(  x,y,t,c_{4},c_{5},...\right)  \right)
$ is a solution of the KP equation (and of the whole KP hierarchy), where
$c_{4}$, $c_{5}$, $c_{6}$, $...$ are considered as constants.
\end{corollary}

\textit{Proof of Corollary \ref{cor.KdV.schursols}.} Write $\lambda$ in the
form $\lambda=\left(  \lambda_{0},\lambda_{1},\lambda_{2},...\right)  $. Let
$\left(  i_{0},i_{1},i_{2},...\right)  $ be the sequence defined by
$i_{k}=\lambda_{k}-k$ for every $k\in\mathbb{N}$. Then, $\left(  i_{0}%
,i_{1},i_{2},...\right)  $ is a $0$-degression, and we know that the
elementary semiinfinite wedge $v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}%
\wedge...$ is in $\Omega$. But Theorem \ref{thm.schur} yields $\sigma
^{-1}\left(  v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\right)
=S_{\lambda}\left(  x\right)  $ (since $\lambda=\left(  i_{0}+0,i_{1}%
+1,i_{2}+2,...\right)  $), so that $\sigma\left(  S_{\lambda}\left(  x\right)
\right)  =v_{i_{0}}\wedge v_{i_{1}}\wedge v_{i_{2}}\wedge...\in\Omega$. Hence,
the function $2\partial_{x}^{2}\log\left(  S_{\lambda}\left(  x,y,t,c_{4}%
,c_{5},...\right)  \right)  $ satisfies the KP equation (and the whole KP
hierarchy). This proves Corollary \ref{cor.KdV.schursols}.

\subsubsection{\textbf{[unfinished]} \texorpdfstring{$n$}{n}-soliton solutions
of KdV}

Now we will construct other solutions of the KdV equations (which are called
multisoliton solutions).

We will identify the $\mathcal{A}$-modules $\mathcal{B}^{\left(  0\right)  }$
and $\mathcal{F}^{\left(  0\right)  }$ along the Boson-Fermion correspondence
$\sigma$.

\begin{definition}
Define a quantum field $\Gamma\left(  u,v\right)  \in\left(
\operatorname*{End}\left(  \mathcal{B}^{\left(  0\right)  }\right)  \right)
\left[  \left[  u,u^{-1},v,v^{-1}\right]  \right]  $ by%
\begin{equation}
\Gamma\left(  u,v\right)  =\exp\left(  \sum\limits_{j\geq1}\dfrac{u^{j}-v^{j}%
}{j}a_{-j}\right)  \exp\left(  -\sum\limits_{j\geq1}\dfrac{u^{-j}-v^{-j}}%
{j}a_{j}\right)  . \label{n-soliton.Gamma(u,v).def1}%
\end{equation}

\end{definition}

It is possible to rewrite the equality (\ref{n-soliton.Gamma(u,v).def1}) in
the following form:%

\begin{equation}
\Gamma\left(  u,v\right)  =u\left.  :\Gamma\left(  u\right)  \Gamma^{\ast
}\left(  v\right)  :\right.  . \label{n-soliton.Gamma(u,v).def2}%
\end{equation}
However, before we can make sense of this equality
(\ref{n-soliton.Gamma(u,v).def2}), we need to explain what we mean by $\left.
:\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right)  :\right.  $. Theorem
\ref{thm.euler} (applied to $m=-1$ and to $m=0$) yields that%
\begin{equation}
\Gamma\left(  u\right)  =z\exp\left(  \sum\limits_{j>0}\dfrac{a_{-j}}{j}%
u^{j}\right)  \cdot\exp\left(  -\sum\limits_{j>0}\dfrac{a_{j}}{j}%
u^{-j}\right)  \ \ \ \ \ \ \ \ \ \ \text{on }\mathcal{B}^{\left(  -1\right)  }
\label{pf.n-soliton.0}%
\end{equation}
and%
\begin{equation}
\Gamma^{\ast}\left(  u\right)  =z^{-1}\exp\left(  -\sum\limits_{j>0}%
\dfrac{a_{-j}}{j}u^{j}\right)  \cdot\exp\left(  \sum\limits_{j>0}\dfrac{a_{j}%
}{j}u^{-j}\right)  \ \ \ \ \ \ \ \ \ \ \text{on }\mathcal{B}^{\left(
0\right)  }. \label{pf.n-soliton.1}%
\end{equation}
Renaming $u$ as $v$ in (\ref{pf.n-soliton.1}), we obtain%
\begin{equation}
\Gamma^{\ast}\left(  v\right)  =z^{-1}\exp\left(  -\sum\limits_{j>0}%
\dfrac{a_{-j}}{j}v^{j}\right)  \cdot\exp\left(  \sum\limits_{j>0}\dfrac{a_{j}%
}{j}v^{-j}\right)  \ \ \ \ \ \ \ \ \ \ \text{on }\mathcal{B}^{\left(
0\right)  }. \label{pf.n-soliton.2}%
\end{equation}
If we now extend the ``normal ordered product'' which we have defined on
$U\left(  \mathcal{A}\right)  $ to a ``normal ordered multiplication map''
$U\left(  \mathcal{A}\right)  \left[  z\right]  \left[  \left[  u,u^{-1}%
\right]  \right]  \times U\left(  \mathcal{A}\right)  \left[  z\right]
\left[  \left[  v,v^{-1}\right]  \right]  \rightarrow U\left(  \mathcal{A}%
\right)  \left[  z\right]  \left[  \left[  u,u^{-1},v,v^{-1}\right]  \right]
$

[...] [This isn't really that easy to formalize, and this formalization is wrong.]

[According to Etingof, one can put these power series on a firm footing by
defining a series $\gamma\in\left(  \operatorname*{Hom}\left(  A,B\right)
\right)  \left[  \left[  u,u^{-1}\right]  \right]  $ (where $A$ and $B$ are
two \textbf{graded} vector spaces) to be ``sampled-rational'' if every
homogeneous $w\in A$ and every homogeneous $f\in B^{\ast}$ satisfy
$\left\langle f,\gamma w\right\rangle \in\mathbb{C}\left(  u\right)  $.
Sampled-rational power series form a torsion-free $\mathbb{C}\left(  u\right)
$-module\footnote{But I don't think the composition of any two
sampled-rational power series is sampled-rational. Ideas?}. And limits are
defined sample-wise (see below). But it probably needs some explanations how
$\mathbb{C}\left(  u\right)  $ is embedded in $\mathbb{C}\left[  \left[
u,u^{-1}\right]  \right]  $ (or what it means for an element of $\mathbb{C}%
\left[  \left[  u,u^{-1}\right]  \right]  $ to be a rational function).]

We will use the following notation, generalizing Definition \ref{def.OMEGA}:

\begin{definition}
Let $A$ and $B$ be two $\mathbb{C}$-vector spaces, and let $\left(
u_{1},u_{2},...,u_{\ell}\right)  $ be a sequence of distinct symbols. For
every $\ell$-tuple $\mathbf{i}\in\mathbb{Z}^{\ell}$, define a monomial
$\mathbf{u}^{\mathbf{i}}\in\mathbb{C}\left(  \left(  u_{1},u_{2},...,u_{\ell
}\right)  \right)  $ by $\mathbf{u}^{\mathbf{i}}=u_{1}^{i_{1}}u_{2}^{i_{2}%
}...u_{\ell}^{i_{\ell}}$, where the $\ell$-tuple $\mathbf{i}$ is written in
the form $\mathbf{i}=\left(  i_{1},i_{2},...,i_{\ell}\right)  $. Then, the map%
\begin{align*}
A\left(  \left(  u_{1},u_{2},...,u_{\ell}\right)  \right)  \times B\left(
\left(  u_{1},u_{2},...,u_{\ell}\right)  \right)   &  \rightarrow\left(
A\otimes B\right)  \left(  \left(  u_{1},u_{2},...,u_{\ell}\right)  \right)
,\\
\left(  \sum\limits_{\mathbf{i}\in\mathbb{Z}^{\ell}}a_{\mathbf{i}}%
\mathbf{u}^{\mathbf{i}},\sum\limits_{\mathbf{i}\in\mathbb{Z}^{\ell}%
}b_{\mathbf{i}}\mathbf{u}^{\mathbf{i}}\right)   &  \mapsto\sum
\limits_{\mathbf{i}\in\mathbb{Z}^{\ell}}\left(  \sum\limits_{\mathbf{j}%
\in\mathbb{Z}^{\ell}}a_{\mathbf{j}}\otimes b_{\mathbf{i}-\mathbf{j}}\right)
\mathbf{u}^{\mathbf{i}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{where all }a_{\mathbf{i}}\text{ lie in
}A\text{ and all }b_{\mathbf{i}}\text{ lie in }B\right)
\end{align*}
is well-defined (in fact, it is easy to see that for any Laurent series
$\sum\limits_{\mathbf{i}\in\mathbb{Z}^{\ell}}a_{\mathbf{i}}\mathbf{u}%
^{\mathbf{i}}\in A\left(  \left(  u_{1},u_{2},...,u_{\ell}\right)  \right)  $
with all $a_{\mathbf{i}}$ lying in $A$, any Laurent series $\sum
\limits_{\mathbf{i}\in\mathbb{Z}^{\ell}}b_{\mathbf{i}}\mathbf{u}^{\mathbf{i}%
}\in B\left(  \left(  u_{1},u_{2},...,u_{\ell}\right)  \right)  $ with all
$b_{\mathbf{i}}$ lying in $B$, and any $\ell$-tuple $\mathbf{i}\in
\mathbb{Z}^{\ell}$, the sum $\sum\limits_{\mathbf{j}\in\mathbb{Z}^{\ell}%
}a_{\mathbf{j}}\otimes b_{\mathbf{i}-\mathbf{j}}$ has only finitely many
addends and vanishes if any coordinate of $\mathbf{i}$ is small enough) and
$\mathbb{C}$-bilinear. Hence, it induces a $\mathbb{C}$-linear map%
\begin{align*}
A\left(  \left(  u_{1},u_{2},...,u_{\ell}\right)  \right)  \otimes B\left(
\left(  u_{1},u_{2},...,u_{\ell}\right)  \right)   &  \rightarrow\left(
A\otimes B\right)  \left(  \left(  u_{1},u_{2},...,u_{\ell}\right)  \right)
,\\
\left(  \sum\limits_{\mathbf{i}\in\mathbb{Z}^{\ell}}a_{\mathbf{i}}%
\mathbf{u}^{\mathbf{i}}\right)  \otimes\left(  \sum\limits_{\mathbf{i}%
\in\mathbb{Z}^{\ell}}b_{\mathbf{i}}\mathbf{u}^{\mathbf{i}}\right)   &
\mapsto\sum\limits_{\mathbf{i}\in\mathbb{Z}^{\ell}}\left(  \sum
\limits_{\mathbf{j}\in\mathbb{Z}^{\ell}}a_{\mathbf{j}}\otimes b_{\mathbf{i}%
-\mathbf{j}}\right)  \mathbf{u}^{\mathbf{i}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{where all }a_{\mathbf{i}}\text{ lie in
}A\text{ and all }b_{\mathbf{i}}\text{ lie in }B\right)  .
\end{align*}
This map will be denoted by $\Omega_{A,B,\left(  u_{1},u_{2},...,u_{\ell
}\right)  }$. Clearly, when $\ell=1$, this map $\Omega_{A,B,\left(
u_{1}\right)  }$ is identical with the map $\Omega_{A,B,u_{1}}$ defined in
Definition \ref{def.OMEGA}.
\end{definition}

\begin{proposition}
\label{prop.KdV.grassm}If $\tau\in\Omega$ and $a\in\mathbb{C}$, then%
\[
\left(  1+a\Gamma\left(  u,v\right)  \right)  \tau\in\Omega_{u,v},
\]
where%
\[
\Omega_{u,v}=\left\{  \tau\in\mathcal{B}^{\left(  0\right)  }\left(  \left(
u,v\right)  \right)  \ \mid\ S\left(  \tau\otimes\tau\right)  =0\right\}  .
\]
(Here, the $S$ really means not the map $S:\mathcal{B}^{\left(  0\right)
}\otimes\mathcal{B}^{\left(  0\right)  }\rightarrow\mathcal{B}^{\left(
1\right)  }\otimes\mathcal{B}^{\left(  -1\right)  }$ itself, but rather the
map $\left(  \mathcal{B}^{\left(  0\right)  }\otimes\mathcal{B}^{\left(
0\right)  }\right)  \left(  \left(  u,v\right)  \right)  \rightarrow\left(
\mathcal{B}^{\left(  1\right)  }\otimes\mathcal{B}^{\left(  -1\right)
}\right)  \left(  \left(  u,v\right)  \right)  $ it induces. And $\tau
\otimes\tau$ means not $\tau\otimes\tau\in\mathcal{B}^{\left(  0\right)
}\otimes\mathcal{B}^{\left(  0\right)  }$ but rather $\Omega_{\mathcal{B}%
^{\left(  0\right)  },\mathcal{B}^{\left(  0\right)  },\left(  u,v\right)
}\left(  \tau\otimes\tau\right)  \in\left(  \mathcal{B}^{\left(  0\right)
}\otimes\mathcal{B}^{\left(  0\right)  }\right)  \left(  \left(  u,v\right)
\right)  $.)
\end{proposition}

\begin{corollary}
For any $a^{(1)},a^{(2)},...,a^{(n)}\in\mathbb{C}$, we have
\begin{align*}
&  \left(  1+a^{(1)}\Gamma\left(  u_{1},v_{1}\right)  \right)  \left(
1+a^{(2)}\Gamma\left(  u_{2},v_{2}\right)  \right)  ...\left(  1+a^{(n)}%
\Gamma\left(  u_{n},v_{n}\right)  \right)  \mathbf{1}\\
&  \in\Omega
\end{align*}
(in fact, in an appropriate $\Omega_{u_{1},v_{1},u_{2},v_{2},...}$ rather than
in $\Omega$ itself).
\end{corollary}

\textit{Idea of proof of Proposition.} We will prove $\Gamma\left(
u,v\right)  ^{2}=0$, but we will have to make sense of a term like
$\Gamma\left(  u,v\right)  ^{2}$ in order to define this. Thus, $1+a\Gamma
\left(  u,v\right)  $ will become $\exp\left(  a\Gamma\left(  u,v\right)
\right)  $.

We will formalize this proof later.

But first, here is the punchline of this:

\begin{proposition}
Let $a^{(1)},a^{(2)},...,a^{(n)}\in\mathbb{C}$. If $\tau=\left(
1+a^{(1)}\Gamma\left(  u_{1},v_{1}\right)  \right)  \left(  1+a^{(2)}%
\Gamma\left(  u_{2},v_{2}\right)  \right)  ...\left(  1+a^{(n)}\Gamma\left(
u_{n},v_{n}\right)  \right)  \mathbf{1}$, then $2\partial_{x}^{2}\log\tau$ is
given by a convergent series and defines a solution of KP depending on the
parameters $a^{(i)}$, $u_{i}$ and $v_{i}$.
\end{proposition}

This solution is called an $n$\textit{-soliton solution}.

For $n=1$, we have%
\[
\tau=\left(  1+a\Gamma\left(  u,v\right)  \right)  \mathbf{1}=1+a\exp\left(
\left(  u-v\right)  x+\left(  u^{2}-v^{2}\right)  y+\left(  u^{3}%
-v^{3}\right)  t+\left(  u^{4}-v^{4}\right)  c_{4}+...\right)  .
\]
Absorb the $c_{i}$ parameters into a single constant $c$, which can be
absorbed into $a$. So we get%
\[
\tau=1+a\exp\left(  \left(  u-v\right)  x+\left(  u^{2}-v^{2}\right)
y+\left(  u^{3}-v^{3}\right)  t\right)  .
\]
This $\tau$ satisfies
\[
2\partial_{x}^{2}\log\tau=\dfrac{\left(  u-v\right)  ^{2}}{2}\dfrac{1}%
{\cosh^{2}\left(  \dfrac{1}{2}\left(  \left(  u-v\right)  x+\left(
u^{2}-v^{2}\right)  y+\left(  u^{3}-v^{3}\right)  \tau\right)  \right)  }.
\]
Call this function $U$. To make it independent of $y$ (so we get a solution of
KdV equation), we set $v=-u$, and this becomes%
\[
U=\dfrac{2u^{2}}{\cosh^{2}\left(  ux+u^{3}t\right)  }.
\]
This is exactly the soliton solution of KdV.

But let us now give the promised proof of Proposition \ref{prop.KdV.grassm}.

\textit{Proof of Proposition \ref{prop.KdV.grassm}.} Recall that
$\Gamma\left(  u,v\right)  =u\left.  :\Gamma\left(  u\right)  \Gamma^{\ast
}\left(  v\right)  :\right.  $. We can show:

\begin{lemma}
\label{lem.KdV.GG}We have%
\[
\Gamma\left(  u\right)  \Gamma\left(  v\right)  =\left(  u-v\right)
\cdot\left.  :\Gamma\left(  u\right)  \Gamma\left(  v\right)  :\right.
\]
and%
\[
\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right)  =\dfrac{1}{u-v}\left.
:\Gamma\left(  u\right)  \Gamma^{\ast}\left(  v\right)  :\right.
\]
and%
\[
\Gamma^{\ast}\left(  u\right)  \Gamma\left(  v\right)  =\dfrac{1}{u-v}\left.
:\Gamma^{\ast}\left(  u\right)  \Gamma\left(  v\right)  :\right.
\]
and%
\[
\Gamma^{\ast}\left(  u\right)  \Gamma^{\ast}\left(  v\right)  =\left(
u-v\right)  \cdot\left.  :\Gamma^{\ast}\left(  u\right)  \Gamma^{\ast}\left(
v\right)  :\right.  .
\]

\end{lemma}

\textit{Proof of Lemma \ref{lem.KdV.GG}.} We have%
\[
...\exp\left(  \sum\limits_{j>0}\dfrac{a_{j}}{j}u^{-j}\right)  \exp\left(
\sum\limits_{k>0}\dfrac{a_{-k}}{k}v^{k}\right)  ...
\]
and we have to switch these two terms. We get something like%
\[
\exp\left(  -\log\left(  1-\dfrac{v}{u}\right)  \right)  =\dfrac{1}%
{1-\dfrac{v}{u}}=\dfrac{u}{u-v}.
\]
Etc.

We can generalize this: If $\varepsilon=1$ or $\varepsilon=-1$, we can define
$\Gamma_{\varepsilon}$ by $\Gamma_{+1}=\Gamma$ and $\Gamma_{-1}=\Gamma^{\ast}%
$. Then,

\begin{proposition}
We have%
\[
\Gamma_{\varepsilon_{1}}\left(  u_{1}\right)  \Gamma_{\varepsilon_{2}}\left(
u_{2}\right)  ...\Gamma_{\varepsilon_{n}}\left(  u_{n}\right)  =\prod
\limits_{i<j}\left(  u_{i}-u_{j}\right)  ^{\varepsilon_{i}\varepsilon_{j}%
}\left.  :\Gamma_{\varepsilon_{1}}\left(  u_{1}\right)  \Gamma_{\varepsilon
_{2}}\left(  u_{2}\right)  ...\Gamma_{\varepsilon_{n}}\left(  u_{n}\right)
:\right.  .
\]
Here, series are being expanded in the region where $\left\vert u_{1}%
\right\vert >\left\vert u_{2}\right\vert >...>\left\vert u_{n}\right\vert $.
\end{proposition}

\begin{corollary}
The matrix elements of $\Gamma_{\varepsilon_{1}}\left(  u_{1}\right)
\Gamma_{\varepsilon_{2}}\left(  u_{2}\right)  ...\Gamma_{\varepsilon_{n}%
}\left(  u_{n}\right)  $ (this means expressions of the form $\left(  w^{\ast
},\Gamma_{\varepsilon_{1}}\left(  u_{1}\right)  \Gamma_{\varepsilon_{2}%
}\left(  u_{2}\right)  ...\Gamma_{\varepsilon_{n}}\left(  u_{n}\right)
w\right)  $ with $w\in\mathcal{B}^{\left(  0\right)  }$ and $w^{\ast}%
\in\mathcal{B}^{\left(  0\right)  \ast}$ (where $^{\ast}$ means restricted
dual); a priori, these are only series) are series which converge to rational
functions of the form%
\[
P\left(  u\right)  \cdot\prod\limits_{i<j}\left(  u_{i}-u_{j}\right)
^{\varepsilon_{i}\varepsilon_{j}},\ \ \ \ \ \ \ \ \ \ \text{where }%
P\in\mathbb{C}\left[  u_{1}^{\pm1},u_{2}^{\pm1},...,u_{n}^{\pm1}\right]  .
\]

\end{corollary}

This follows from the Proposition since matrix elements of normal ordered
products are Laurent polynomials.

\begin{corollary}
We have $\Gamma\left(  u^{\prime},v^{\prime}\right)  \Gamma\left(  u,v\right)
=\dfrac{\left(  u^{\prime}-u\right)  \left(  v^{\prime}-v\right)  }{\left(
v^{\prime}-u\right)  \left(  u^{\prime}-v\right)  }\left.  :\Gamma\left(
u^{\prime},v^{\prime}\right)  \Gamma\left(  u,v\right)  :\right.  $.
\end{corollary}

Here, we cancelled $u-v$ and $u^{\prime}-v^{\prime}$ which is okay because our
rational functions lie in an integral domain.

As a corollary of this corollary, we have:

\begin{corollary}
If $u\neq v$, then $\lim\limits_{\substack{u^{\prime}\rightarrow
u;\\v^{\prime}\rightarrow v}}\Gamma\left(  u^{\prime},v^{\prime}\right)
\Gamma\left(  u,v\right)  =0$. By which we mean that for any $w\in
\mathcal{B}^{\left(  0\right)  }$ and $w^{\ast}\in\mathcal{B}^{\left(
0\right)  \ast}$, we have $\lim\limits_{\substack{u^{\prime}\rightarrow
u;\\v^{\prime}\rightarrow v}}\left(  w^{\ast},\Gamma\left(  u^{\prime
},v^{\prime}\right)  \Gamma\left(  u,v\right)  w\right)  =0$ as a rational function.
\end{corollary}

Informally, this can be written $\left(  \Gamma\left(  u,v\right)  \right)
^{2}=0$. But this does not really make sense in a formal sense since we are
not supposed to take squares of such power series.

\textit{Proof of Proposition \ref{prop.KdV.grassm}.} Recall that our idea was
to use $1+a\Gamma=\exp\left(  a\Gamma\right)  $ since $\Gamma^{2}=0$. But this
is not rigorous since we cannot speak of $\Gamma^{2}$. So here is the actual proof:

We have (abbreviating $\Gamma\left(  u,v\right)  $ by $\Gamma$ occasionally)%
\begin{align*}
&  S\left(  \left(  1+a\Gamma\left(  u,v\right)  \right)  \tau\otimes\left(
1+a\Gamma\left(  u,v\right)  \right)  \tau\right) \\
&  =\underbrace{S\left(  \tau\otimes\tau\right)  }_{\substack{=0\\\text{(since
}\tau\in\Omega\text{)}}}+a\underbrace{S\left(  \Gamma\otimes1+1\otimes
\Gamma\right)  \left(  \tau\otimes\tau\right)  }_{\substack{=0\\\text{(since
}S\text{ commutes with }\mathfrak{gl}_{\infty}\text{,}\\\text{and coefficients
of }\Gamma\text{ are in }\mathfrak{gl}_{\infty}\text{,}\\\text{and }S\left(
\tau\otimes\tau\right)  =0\text{)}}}+a^{2}S\left(  \Gamma\otimes\Gamma\right)
\left(  \tau\otimes\tau\right) \\
&  =a^{2}S\left(  \Gamma\otimes\Gamma\right)  \left(  \tau\otimes\tau\right)
.
\end{align*}
Remains to prove that $S\left(  \Gamma\otimes\Gamma\right)  \left(
\tau\otimes\tau\right)  =0$.

We have
\begin{align*}
&  S\left(  \Gamma\otimes\Gamma\right)  \left(  \tau\otimes\tau\right) \\
&  =\lim\limits_{\substack{u^{\prime}\rightarrow u;\\v^{\prime}\rightarrow
v}}\dfrac{1}{2}S\left(  \Gamma\left(  u,v\right)  \tau\otimes\Gamma\left(
u^{\prime},v^{\prime}\right)  \tau+\Gamma\left(  u^{\prime},v^{\prime}\right)
\tau\otimes\Gamma\left(  u,v\right)  \tau\right) \\
&  =\dfrac{1}{2}\lim\limits_{\substack{u^{\prime}\rightarrow u;\\v^{\prime
}\rightarrow v}}\underbrace{S\left(  \Gamma\left(  u^{\prime},v^{\prime
}\right)  \otimes1+1\otimes\Gamma\left(  u^{\prime},v^{\prime}\right)
\right)  \left(  \Gamma\left(  u,v\right)  \otimes1+1\otimes\Gamma\left(
u,v\right)  \right)  \left(  \tau\otimes\tau\right)  }%
_{\substack{=0\\\text{(since }S\text{ commutes with these things)}}}\\
&  \ \ \ \ \ \ \ \ \ \ -\dfrac{1}{2}\lim\limits_{\substack{u^{\prime
}\rightarrow u;\\v^{\prime}\rightarrow v}}S\left(  \underbrace{\Gamma\left(
u^{\prime},v^{\prime}\right)  \Gamma\left(  u,v\right)  }_{\rightarrow
0}\otimes1+1\otimes\underbrace{\Gamma\left(  u^{\prime},v^{\prime}\right)
\Gamma\left(  u,v\right)  }_{\rightarrow0}\right)  \left(  \tau\otimes
\tau\right) \\
&  =0.
\end{align*}
This proves Proposition \ref{prop.KdV.grassm}.

\subsection{\textbf{[unfinished]} Representations of
\texorpdfstring{$\operatorname*{Vir}$}{Vir} revisited}

We now come back to the representation theory of the Virasoro algebra
$\operatorname*{Vir}$.

Recall that to every pair $\lambda=\left(  c,h\right)  $, we can attach a
Verma module $M_{\lambda}^{+}=M_{c,h}^{+}$ over $\operatorname*{Vir}$. We will
denote this module by $M_{\lambda}=M_{c,h}$, and its $v_{\lambda}^{+}$ by
$v_{\lambda}$.

This module $M_{\lambda}$ has a symmetric bilinear form $\left(  \cdot
,\cdot\right)  :M_{\lambda}^{+}\times M_{\lambda}^{+}\rightarrow\mathbb{C}$
such that $\left(  v_{\lambda},v_{\lambda}\right)  =1$ and $\left(
L_{n}v,w\right)  =\left(  v,L_{-n}w\right)  $ for all $n\in\mathbb{Z}$, $v\in
M_{\lambda}$ and $w\in M_{\lambda}$. This form is called the
\textit{Shapovalov form}, and is obtained from the invariant bilinear form
$M_{\lambda}^{+}\times M_{-\lambda}^{-}\rightarrow\mathbb{C}$ by means of the
involution on $\operatorname*{Vir}$.

Also, if $\lambda\in\mathbb{R}^{2}$, the module $M_{\lambda}^{+}$ has a
Hermitian form $\left\langle \cdot,\cdot\right\rangle $ satisfying the same conditions.

We recall that $M_{\lambda}$ has a unique irreducible quotient $L_{\lambda}$.
We have asked questions about when it is unitary, etc.. We will try to answer
some of these questions today.

\begin{Convention}
Let us change the grading of the Virasoro algebra $\operatorname*{Vir}$ to
$\deg L_{i}=-i$. Correspondingly, $M_{\lambda}$ becomes $M_{\lambda}%
=\bigoplus\limits_{n\geq0}M_{\lambda}\left[  n\right]  $.
\end{Convention}

For any $n\geq0$, we have the polynomial $\det\nolimits_{n}\left(  c,h\right)
$ which is the determinant of the contravariant form $\left(  \cdot
,\cdot\right)  $ in degree $n$. This polynomial is defined up to a constant
scalar. Let us recall how it is defined:

Let $\left(  w_{j}\right)  $ be a basis of $U\left(  \operatorname*{Vir}%
\nolimits_{-}\right)  \left[  n\right]  $ (where $\operatorname*{Vir}%
\nolimits_{-}$ is $\left\langle L_{-1},L_{-2},L_{-3},...\right\rangle $; this
is now the \textbf{positive} part of $\operatorname*{Vir}$). Then,
$\det\nolimits_{n}\left(  c,h\right)  =\det\left(  \left(  w_{I}v_{\lambda
},w_{J}v_{\lambda}\right)  _{I,J}\right)  $. If we change the basis by a
matrix $S$, the determinant multiplies by $\left(  \det S\right)  ^{2}$.

For a Hermitian form, we can do the same when $\left(  c,h\right)  $ is real,
but then $\det\nolimits_{n}\left(  c,h\right)  $ is defined up to a
\textbf{positive} scalar, because now the determinant multiplies by
$\left\vert \det S\right\vert ^{2}$. Hence it makes sense to say that
$\det\nolimits_{n}\left(  c,h\right)  >0$.

\begin{proposition}
We have $\det\nolimits_{n}\left(  c,h\right)  =0$ if and only if there exists
a singular vector $w\neq0$ in $M_{c,h}$ of degree $\leq n$ and $>0$.

In particular, if $\det\nolimits_{n}\left(  c,h\right)  =0$, then
$\det\nolimits_{n+1}\left(  c,h\right)  =0$.
\end{proposition}

In fact, we will see that $\det\nolimits_{n+1}$ is divisible by $\det
\nolimits_{n}$.

\textit{Proof of Proposition.} Apparently this is supposed to follow from
something we did.

We recall examples:%
\begin{align*}
\det\nolimits_{1}  &  =2h,\\
\det\nolimits_{2}  &  =2h\left(  16h^{2}+2hc-10h+c\right)  .
\end{align*}
Also recall that $M_{c,h}$ is irreducible if and only if every positive $n$
satisfies $\det\nolimits_{n}\left(  c,h\right)  \neq0$.

\begin{proposition}
Let $\left(  c,h\right)  \in\mathbb{R}^{2}$. If $M_{c,h}$ is unitary, then
$\det\nolimits_{n}\left(  c,h\right)  >0$ for all positive $n$.

More generally, if $L_{c,h}\left[  n\right]  \cong M_{c,h}\left[  n\right]  $
for some positive $n$, and $L_{c,h}$ is unitary, then $\det\nolimits_{n}%
\left(  c,h\right)  >0$.
\end{proposition}

\textit{Proof of Proposition.} A positive-definite Hermitian matrix has
positive determinant.

\begin{theorem}
\label{thm.kac.leader}Fix $c$. Regard $\det\nolimits_{m}\left(  c,h\right)  $
as a polynomial in $h$. Then,%
\[
\det\nolimits_{m}\left(  c,h\right)  =K\cdot h^{\sum\limits_{\substack{r,s\geq
1;\\rs\leq m}}p\left(  m-rs\right)  }+\left(  \text{lower terms}\right)
\]
for some nonzero constant $K$ (which depends on the choice of the basis).
\end{theorem}

\textit{Proof.} We computed before the leading term of $\det\nolimits_{m}$ for
any graded Lie algebra.

$\left(  L_{-k}^{m_{k}}...L_{-1}^{m_{1}}v_{\lambda},L_{-k}^{n_{k}}%
...L_{-1}^{n_{1}}v_{\lambda}\right)  $: the main contribution to the leading
term comes from diagonal.

What degree in $h$ do we get?

If $\mu$ is a partition of $m$, we can write $m=1k_{1}\left(  \mu\right)
+2k_{2}\left(  \mu\right)  +...$, where $k_{i}\left(  \mu\right)  $ is the
number of times $i$ occurs in $\mu$.

$\left(  L_{-\ell}^{k_{\ell}}...L_{-1}^{k_{1}}v,L_{-\ell}^{k_{\ell}}%
...L_{-1}^{k_{1}}v\right)  =\left(  v,L_{1}^{k_{1}}...L_{\ell}^{k_{\ell}%
}L_{-\ell}^{k_{\ell}}...L_{-1}^{k_{1}}v\right)  $.

So $\mu$ contributes $k_{1}+...+k_{\ell}$ to the exponent of $h$.

So we conclude that the total exponent of $h$ is $\sum\limits_{\mu\vdash
m}\sum\limits_{i}k_{i}\left(  \mu\right)  $.

The rest is easy combinatorics:

Let $m\left(  r,s\right)  $ denote the number of partitions of $m$ in which
$r$ occurs exactly $s$ times. Then, $m\left(  r,s\right)  =p\left(
m-rs\right)  -p\left(  m-r\left(  s+1\right)  \right)  $. Thus, with $m$ and
$r$ fixed,%
\begin{align*}
\sum\limits_{s}sm\left(  r,s\right)   &  =\sum\limits_{s}s\left(  p\left(
m-rs\right)  -p\left(  m-r\left(  s+1\right)  \right)  \right) \\
&  =\sum\limits_{s}sp\left(  m-rs\right)  -\sum\limits_{s}sp\left(  m-r\left(
s+1\right)  \right) \\
&  =\sum\limits_{s}sp\left(  m-rs\right)  -\sum\limits_{s}\left(  s-1\right)
p\left(  m-rs\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }s-1\text{ for
}s\text{ in the second sum}\right) \\
&  =\sum\limits_{s}\underbrace{\left(  s-\left(  s-1\right)  \right)  }%
_{=1}p\left(  m-rs\right)  =\sum\limits_{s}p\left(  m-rs\right)  .
\end{align*}


So our job is to show that $\sum\limits_{\mu\vdash m}\sum\limits_{i}%
k_{i}\left(  \mu\right)  =\sum\limits_{\substack{r,s\geq1;\\rs\leq
m}}sm\left(  r,s\right)  $. But $\sum\limits_{\substack{s\geq1;\\s\leq
m}}sm\left(  r,s\right)  $ is the total number of occurrences of $r$ in all
partitions of $m$. Summed over $r$, it yields the total number of parts of all
partitions of $m$. But this is also $\sum\limits_{\mu\vdash m}\sum
\limits_{i}k_{i}\left(  \mu\right)  $, qed.

We now quote a theorem which was proved independently by Kac and Feigin-Fuchs:

\begin{theorem}
Suppose $rs\leq m$. Then, if%
\[
h=h_{r,s}\left(  c\right)  :=\dfrac{1}{48}\left(  \left(  13-c\right)  \left(
r^{2}+s^{2}\right)  +\sqrt{\left(  c-1\right)  \left(  c-25\right)  }\left(
r^{2}-s^{2}\right)  -24rs-2+2c\right)  ,
\]
then $\det\nolimits_{m}\left(  c,h\right)  =0$. (This is true for any of the
branches of the square root.)
\end{theorem}

\begin{theorem}
\label{thm.kac.thm1}If $h=h_{r,s}\left(  c\right)  $, then $M_{c,h}$ has a
nonzero singular vector in degree $1\leq d\leq rs$.
\end{theorem}

\begin{theorem}
[Kac, also proved by Feigin-Fuchs]\label{thm.kac.thm2}We have%
\[
\det\nolimits_{m}\left(  c,h\right)  =K_{m}\cdot\prod
\limits_{\substack{r,s\geq1;\\rs\leq m}}\left(  h-h_{r,s}\left(  c\right)
\right)  ^{p\left(  m-rs\right)  },
\]
where $K_{m}$ is some constant. Note that we should choose the same branch of
the square root in $\sqrt{\left(  c-1\right)  \left(  c-25\right)  }$ for
$h_{r,s}$ and $h_{s,r}$. The square roots ``cancel out'' and give way to a
polynomial in $h$ and $c$.
\end{theorem}

To prove these, we will use the following lemma:

\begin{lemma}
\label{lem.kac.linalg}Let $A\left(  t\right)  $ be a polynomial in one
variable $t$ with values in $\operatorname*{End}V$, where $V$ is a
finite-dimensional vector space. Suppose that $\dim\operatorname*{Ker}\left(
A\left(  0\right)  \right)  \geq n$. Then, $\det\left(  A\left(  t\right)
\right)  $ is divisible by $t^{n}$.
\end{lemma}

\textit{Proof of Lemma \ref{lem.kac.linalg}.} Pick a basis $e_{1}%
,e_{2},...,e_{m}$ of $V$ such that the first $n$ vectors $e_{1},e_{2}%
,...,e_{n}$ are in $\operatorname*{Ker}\left(  A\left(  0\right)  \right)  $.
Then, the matrix of $A\left(  t\right)  $ in this basis has first $n$ columns
divisible by $t$, so that its determinant $\det\left(  A\left(  t\right)
\right)  $ is divisible by $t^{n}$.

\textit{Proof of Theorem \ref{thm.kac.thm2}.} Let $A=A\left(  h\right)  $ be
the matrix of the contravariant form in degree $m$, considered as a polynomial
in $h$. If $h=h_{r,s}\left(  c\right)  $, we have a singular vector $w$ in
degree $1\leq d\leq rs$ (by Theorem \ref{thm.kac.thm1}), which generates a
Verma submodule $M_{c,h^{\prime}}\subseteq M_{c,h}$ (by Homework Set 3 problem
1) (the $c$ is the same since $c$ is central and thus acts by the same number
on all vectors).

So $M_{c,h}\left[  m\right]  \supseteq M_{c,h^{\prime}}\left[  m-d\right]  $.
We also have $\dim\left(  M_{c,h^{\prime}}\left[  m-d\right]  \right)
=p\left(  m-d\right)  \geq p\left(  m-rs\right)  $ (since $d\leq rs$) and
$M_{c,h^{\prime}}\left[  m-d\right]  \subseteq\operatorname*{Ker}\left(
\cdot,\cdot\right)  $ (when $h=h_{r,s}$). Hence, $\dim\left(
\operatorname*{Ker}\left(  \cdot,\cdot\right)  \right)  \geq p\left(
m-rs\right)  $. By Lemma \ref{lem.kac.linalg}, this yields that $\det
\nolimits_{m}\left(  c,h\right)  $ is divisible by $\left(  h-h_{r,s}\left(
c\right)  \right)  ^{p\left(  m-rs\right)  }$.

But it is easy to see that for Weil-generic $c$, the $h-h_{r,s}\left(
c\right)  $ are different, so that $\det\nolimits_{m}\left(  c,h\right)  $ is
divisible by $\prod\limits_{\substack{r,s\geq1;\\rs\leq m}}\left(
h-h_{r,s}\left(  c\right)  \right)  ^{p\left(  m-rs\right)  }$. But by Theorem
\ref{thm.kac.leader}, the leading term of $\det\nolimits_{m}\left(
c,h\right)  $ is $K\cdot h^{\sum\limits_{\substack{r,s\geq1;\\rs\leq
m}}p\left(  m-rs\right)  }$, which has exactly the same degree. So
$\det\nolimits_{m}\left(  c,h\right)  $ is a constant multiple of
$\prod\limits_{\substack{r,s\geq1;\\rs\leq m}}\left(  h-h_{r,s}\left(
c\right)  \right)  ^{p\left(  m-rs\right)  }$. Theorem \ref{thm.kac.thm2} is proven.

We will not prove Theorem \ref{thm.kac.thm1}, since we do not have the tools
for that.

\begin{corollary}
The module $M_{c,h}$ is irreducible if and only if $\left(  c,h\right)  $ does
not lie on the lines
\[
h-h_{r,r}\left(  c\right)  =0\ \Longleftrightarrow\ h+\left(  r^{2}-1\right)
\left(  c-1\right)  /24=0
\]
and quadrics (in fact, hyperbolas if we are over $\mathbb{R}$)%
\begin{align*}
&  \ \left(  h-h_{r,s}\left(  c\right)  \right)  \left(  h-h_{s,r}\left(
c\right)  \right)  =0\\
&  \Longleftrightarrow\ \left(  h-\dfrac{\left(  r-s\right)  ^{2}}{4}\right)
^{2}+\dfrac{h}{24}\left(  c-1\right)  \left(  r^{2}+s^{2}-2\right)  +\dfrac
{1}{576}\left(  r^{2}-1\right)  \left(  s^{2}-1\right)  \left(  c-1\right)
^{2}\\
&  \ \ \ \ \ \ \ \ \ \ +\dfrac{1}{48}\left(  c-1\right)  \left(  r-s\right)
^{2}\left(  rs+1\right)  =0.
\end{align*}

\end{corollary}

\begin{corollary}
\label{cor.kac.irred}\textbf{(1)} Let $h\geq0$ and $c\geq1$. Then, $L_{c,h}$
is unitary.

\textbf{(2)} Let $h>0$ and $c>1$. Then, $M_{c,h}\cong L_{c,h}$, so that
$M_{c,h}$ is irreducible.
\end{corollary}

\textit{Proof of Corollary \ref{cor.kac.irred}.} \textbf{(2)} Lines and
hyperbolas do not pass through the region.

For part \textbf{(1)} we need a lemma:

\begin{lemma}
Let $\mathfrak{g}$ be a graded Lie algebra (with $\dim\mathfrak{g}_{i}%
\neq\infty$) with a real structure $\dag$. Let $U\subseteq\mathfrak{g}%
_{0\mathbb{R}}^{\ast}$ be the set of all $\lambda$ such that $L_{\lambda}$ is
unitary. Then, $U$ is closed in the usual metric.

[\textbf{Note:} This lemma possibly needs additional assumptions, like the
assumption that the map $\dag$ reverses the degree (i. e., every
$j\in\mathbb{Z}$ satisfies $\dag\left(  \mathfrak{g}_{j}\right)
\subseteq\mathfrak{g}_{-j}$) and that $\mathfrak{g}_{0}$ is an abelian Lie algebra.]
\end{lemma}

\textit{Proof of Lemma.} It follows from the fact that if $\left(
A_{n}\right)  $ is a sequence of positive definite Hermitian matrices, and
$\lim\limits_{n\rightarrow\infty}A_{n}=A_{\infty}$, then $A_{\infty}$ is
nonnegative definite.

Okay, sorry, we are not going to use this lemma; we will derive the special
case we need.

Now I claim that if $h>0$ and $c>1$, then $L_{c,h}=M_{c,h}$ is unitary. We
know this is true for some points of this region (namely, the ones ``above the
zigzag line''). Then everything follows from the fact that if $A\left(
t\right)  $ is a continuous family of nondegenerate Hermitian matrices
parametrized by $t\in\left[  0,1\right]  $ such that $A\left(  0\right)  >0$,
then $A\left(  t\right)  >0$ for every $t$. (This fact is because the
signature of a nondegenerate Hermitian matrix is a continuous map to a
discrete set, and thus constant on connected components.)

e. g., consider $M_{1,h}$ as a limit of $M_{1+\dfrac{1}{n},h}$ (this is
irreducible for large $n$).

So the matrix of the form in $M_{1,h}\left[  m\right]  $ is a limit of the
matrices for $M_{1+\dfrac{1}{n},h}\left[  m\right]  $. So the matrix for
$M_{1,h}\left[  m\right]  $ is $\geq0$. But kernel lies in $J_{1,h}\left[
m\right]  $, so the form on $L_{1,h}\left[  m\right]  =\left(  M_{1,h}\diagup
J_{1,h}\right)  \left[  m\right]  $ is strictly positive.

By analyzing the Kac curves, we can show (although \textit{we} will
\textit{not} show) that in the region $0\leq c<1$, there are only countably
many points where we possibly can have unitarity:

$c\left(  m\right)  =1-\dfrac{6}{\left(  m+2\right)  \left(  m+3\right)  };$

$h_{r,s}\left(  m\right)  =\dfrac{\left(  \left(  m+3\right)  r-\left(
m+2\right)  s\right)  ^{2}-1}{4\left(  m+2\right)  \left(  m+3\right)  }$ with
$1\leq r\leq s\leq m+1$.

for $m\geq0$.

In fact we will show that at these points we indeed have unitary representations.

\begin{proposition}
\textbf{(1)} If $c\geq0$ and $L_{c,h}$ is unitary, then $h=0$.

\textbf{(2)} We have $L_{0,h}=M_{0,h}$ if and only if $h\neq\dfrac{m^{2}%
-1}{24}$ for all $m\geq0$.

\textbf{(3)} We have $L_{1,h}=M_{1,h}$ if and only if $h\neq\dfrac{m^{2}}{24}$
for all $m\geq0$.
\end{proposition}

\textit{Proof.} \textbf{(2)} and \textbf{(3)} follow immediately from the Kac
determinant formula. For \textbf{(1)}, just compute $\det\left(
\begin{array}
[c]{cc}%
\left(  L_{-N}^{2}v,L_{-N}^{2}v\right)  & \left(  L_{-N}^{2}v,L_{-2N}v\right)
\\
\left(  L_{-2N}v,L_{-N}^{2}v\right)  & \left(  L_{-2N}v,L_{-2N}v\right)
\end{array}
\right)  =4N^{3}h^{2}\left(  8h-5N\right)  $ (this is $<0$ for high enough $N$
as long as $h\neq0$), so that the only possibility for unitarity is $h=0$.
\end{document}
